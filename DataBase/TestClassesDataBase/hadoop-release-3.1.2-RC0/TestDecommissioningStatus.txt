/** 
 * This class tests the decommissioning of nodes.
 */
public class TestDecommissioningStatus {
  private static final long seed=0xDEADBEEFL;
  private static final int blockSize=8192;
  private static final int fileSize=16384;
  private static final int numDatanodes=2;
  private static MiniDFSCluster cluster;
  private static FileSystem fileSys;
  private static HostsFileWriter hostsFileWriter;
  private static Configuration conf;
  private Logger LOG;
  final ArrayList<String> decommissionedNodes=new ArrayList<String>(numDatanodes);
  @Before public void setUp() throws Exception {
    conf=new HdfsConfiguration();
    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_CONSIDERLOAD_KEY,false);
    hostsFileWriter=new HostsFileWriter();
    hostsFileWriter.initialize(conf,"work-dir/decommission");
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY,1000);
    conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_RECONSTRUCTION_PENDING_TIMEOUT_SEC_KEY,4);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,1);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_INTERVAL_KEY,1);
    conf.setLong(DFSConfigKeys.DFS_DATANODE_BALANCE_BANDWIDTHPERSEC_KEY,1);
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();
    cluster.waitActive();
    fileSys=cluster.getFileSystem();
    cluster.getNamesystem().getBlockManager().getDatanodeManager().setHeartbeatExpireInterval(3000);
    Logger.getLogger(DatanodeAdminManager.class).setLevel(Level.DEBUG);
    LOG=Logger.getLogger(TestDecommissioningStatus.class);
  }
  @After public void tearDown() throws Exception {
    if (hostsFileWriter != null) {
      hostsFileWriter.cleanup();
    }
    if (fileSys != null)     fileSys.close();
    if (cluster != null)     cluster.shutdown();
  }
  private String decommissionNode(  DFSClient client,  int nodeIndex) throws IOException {
    DatanodeInfo[] info=client.datanodeReport(DatanodeReportType.LIVE);
    String nodename=info[nodeIndex].getXferAddr();
    decommissionNode(nodename);
    return nodename;
  }
  private void decommissionNode(  String dnName) throws IOException {
    System.out.println("Decommissioning node: " + dnName);
    ArrayList<String> nodes=new ArrayList<String>(decommissionedNodes);
    nodes.add(dnName);
    hostsFileWriter.initExcludeHosts(nodes);
  }
  private void checkDecommissionStatus(  DatanodeDescriptor decommNode,  int expectedUnderRep,  int expectedDecommissionOnly,  int expectedUnderRepInOpenFiles){
    assertEquals("Unexpected num under-replicated blocks",expectedUnderRep,decommNode.getLeavingServiceStatus().getUnderReplicatedBlocks());
    assertEquals("Unexpected number of decom-only replicas",expectedDecommissionOnly,decommNode.getLeavingServiceStatus().getOutOfServiceOnlyReplicas());
    assertEquals("Unexpected number of replicas in under-replicated open files",expectedUnderRepInOpenFiles,decommNode.getLeavingServiceStatus().getUnderReplicatedInOpenFiles());
  }
  private void checkDFSAdminDecommissionStatus(  List<DatanodeDescriptor> expectedDecomm,  DistributedFileSystem dfs,  DFSAdmin admin) throws IOException {
    ByteArrayOutputStream baos=new ByteArrayOutputStream();
    PrintStream ps=new PrintStream(baos);
    PrintStream oldOut=System.out;
    System.setOut(ps);
    try {
      admin.report(new String[]{"-decommissioning"},0);
      String[] lines=baos.toString().split("\n");
      Integer num=null;
      int count=0;
      for (      String line : lines) {
        if (line.startsWith("Decommissioning datanodes")) {
          String temp=line.split(" ")[2];
          num=Integer.parseInt((String)temp.subSequence(1,temp.length() - 2));
        }
        if (line.contains("Decommission in progress")) {
          count++;
        }
      }
      assertTrue("No decommissioning output",num != null);
      assertEquals("Unexpected number of decomming DNs",expectedDecomm.size(),num.intValue());
      assertEquals("Unexpected number of decomming DNs",expectedDecomm.size(),count);
      List<DatanodeInfo> decomming=new ArrayList<DatanodeInfo>(Arrays.asList(dfs.getDataNodeStats(DatanodeReportType.DECOMMISSIONING)));
      assertEquals("Unexpected number of decomming DNs",expectedDecomm.size(),decomming.size());
      for (      DatanodeID id : expectedDecomm) {
        assertTrue("Did not find expected decomming DN " + id,decomming.contains(id));
      }
    }
  finally {
      System.setOut(oldOut);
    }
  }
  /** 
 * Tests Decommissioning Status in DFS.
 */
  @Test public void testDecommissionStatus() throws Exception {
    InetSocketAddress addr=new InetSocketAddress("localhost",cluster.getNameNodePort());
    DFSClient client=new DFSClient(addr,conf);
    DatanodeInfo[] info=client.datanodeReport(DatanodeReportType.LIVE);
    assertEquals("Number of Datanodes ",2,info.length);
    DistributedFileSystem fileSys=cluster.getFileSystem();
    DFSAdmin admin=new DFSAdmin(cluster.getConfiguration(0));
    short replicas=numDatanodes;
    Path file1=new Path("decommission.dat");
    DFSTestUtil.createFile(fileSys,file1,fileSize,fileSize,blockSize,replicas,seed);
    Path file2=new Path("decommission1.dat");
    FSDataOutputStream st1=AdminStatesBaseTest.writeIncompleteFile(fileSys,file2,replicas,(short)(fileSize / blockSize));
    for (    DataNode d : cluster.getDataNodes()) {
      DataNodeTestUtils.triggerBlockReport(d);
    }
    FSNamesystem fsn=cluster.getNamesystem();
    final DatanodeManager dm=fsn.getBlockManager().getDatanodeManager();
    for (int iteration=0; iteration < numDatanodes; iteration++) {
      String downnode=decommissionNode(client,iteration);
      dm.refreshNodes(conf);
      decommissionedNodes.add(downnode);
      BlockManagerTestUtil.recheckDecommissionState(dm);
      final List<DatanodeDescriptor> decommissioningNodes=dm.getDecommissioningNodes();
      if (iteration == 0) {
        assertEquals(decommissioningNodes.size(),1);
        DatanodeDescriptor decommNode=decommissioningNodes.get(0);
        checkDecommissionStatus(decommNode,3,0,1);
        checkDFSAdminDecommissionStatus(decommissioningNodes.subList(0,1),fileSys,admin);
      }
 else {
        assertEquals(decommissioningNodes.size(),2);
        DatanodeDescriptor decommNode1=decommissioningNodes.get(0);
        DatanodeDescriptor decommNode2=decommissioningNodes.get(1);
        checkDecommissionStatus(decommNode1,3,3,1);
        checkDecommissionStatus(decommNode2,4,4,2);
        checkDFSAdminDecommissionStatus(decommissioningNodes.subList(0,2),fileSys,admin);
      }
    }
    hostsFileWriter.initExcludeHost("");
    dm.refreshNodes(conf);
    st1.close();
    AdminStatesBaseTest.cleanupFile(fileSys,file1);
    AdminStatesBaseTest.cleanupFile(fileSys,file2);
  }
  /** 
 * Verify a DN remains in DECOMMISSION_INPROGRESS state if it is marked as dead before decommission has completed. That will allow DN to resume the replication process after it rejoins the cluster.
 */
  @Test(timeout=120000) public void testDecommissionStatusAfterDNRestart() throws Exception {
    DistributedFileSystem fileSys=(DistributedFileSystem)cluster.getFileSystem();
    Path f=new Path("decommission.dat");
    DFSTestUtil.createFile(fileSys,f,fileSize,fileSize,fileSize,(short)1,seed);
    RemoteIterator<LocatedFileStatus> fileList=fileSys.listLocatedStatus(f);
    BlockLocation[] blockLocations=fileList.next().getBlockLocations();
    String dnName=blockLocations[0].getNames()[0];
    FSNamesystem fsn=cluster.getNamesystem();
    final DatanodeManager dm=fsn.getBlockManager().getDatanodeManager();
    decommissionNode(dnName);
    dm.refreshNodes(conf);
    DataNodeProperties dataNodeProperties=cluster.stopDataNode(dnName);
    final List<DatanodeDescriptor> dead=new ArrayList<DatanodeDescriptor>();
    while (true) {
      dm.fetchDatanodes(null,dead,false);
      if (dead.size() == 1) {
        break;
      }
      Thread.sleep(1000);
    }
    BlockManagerTestUtil.checkHeartbeat(fsn.getBlockManager());
    BlockManagerTestUtil.recheckDecommissionState(dm);
    assertTrue("the node should be DECOMMISSION_IN_PROGRESSS",dead.get(0).isDecommissionInProgress());
    List<DatanodeDescriptor> decomlist=dm.getDecommissioningNodes();
    assertTrue("The node should be be decommissioning",decomlist.size() == 1);
    AdminStatesBaseTest.cleanupFile(fileSys,f);
    BlockManagerTestUtil.recheckDecommissionState(dm);
    assertTrue("the node should be decommissioned",dead.get(0).isDecommissioned());
    cluster.restartDataNode(dataNodeProperties,true);
    cluster.waitActive();
    hostsFileWriter.initExcludeHost("");
    dm.refreshNodes(conf);
  }
  /** 
 * Verify the support for decommissioning a datanode that is already dead. Under this scenario the datanode should immediately be marked as DECOMMISSIONED
 */
  @Test(timeout=120000) public void testDecommissionDeadDN() throws Exception {
    Logger log=Logger.getLogger(DatanodeAdminManager.class);
    log.setLevel(Level.DEBUG);
    DatanodeID dnID=cluster.getDataNodes().get(0).getDatanodeId();
    String dnName=dnID.getXferAddr();
    DataNodeProperties stoppedDN=cluster.stopDataNode(0);
    DFSTestUtil.waitForDatanodeState(cluster,dnID.getDatanodeUuid(),false,30000);
    FSNamesystem fsn=cluster.getNamesystem();
    final DatanodeManager dm=fsn.getBlockManager().getDatanodeManager();
    DatanodeDescriptor dnDescriptor=dm.getDatanode(dnID);
    decommissionNode(dnName);
    dm.refreshNodes(conf);
    BlockManagerTestUtil.recheckDecommissionState(dm);
    assertTrue(dnDescriptor.isDecommissioned());
    cluster.restartDataNode(stoppedDN,true);
    cluster.waitActive();
    hostsFileWriter.initExcludeHost("");
    dm.refreshNodes(conf);
  }
  @Test(timeout=120000) public void testDecommissionLosingData() throws Exception {
    ArrayList<String> nodes=new ArrayList<String>(2);
    FSNamesystem fsn=cluster.getNamesystem();
    BlockManager bm=fsn.getBlockManager();
    DatanodeManager dm=bm.getDatanodeManager();
    Path file1=new Path("decommissionLosingData.dat");
    DFSTestUtil.createFile(fileSys,file1,fileSize,fileSize,blockSize,(short)2,seed);
    Thread.sleep(1000);
    LOG.info("Shutdown dn1");
    DatanodeID dnID=cluster.getDataNodes().get(1).getDatanodeId();
    String dnName=dnID.getXferAddr();
    DatanodeDescriptor dnDescriptor1=dm.getDatanode(dnID);
    nodes.add(dnName);
    DataNodeProperties stoppedDN1=cluster.stopDataNode(1);
    DFSTestUtil.waitForDatanodeState(cluster,dnID.getDatanodeUuid(),false,30000);
    LOG.info("Shutdown dn0");
    dnID=cluster.getDataNodes().get(0).getDatanodeId();
    dnName=dnID.getXferAddr();
    DatanodeDescriptor dnDescriptor0=dm.getDatanode(dnID);
    nodes.add(dnName);
    DataNodeProperties stoppedDN0=cluster.stopDataNode(0);
    DFSTestUtil.waitForDatanodeState(cluster,dnID.getDatanodeUuid(),false,30000);
    LOG.info("Decommissioning nodes");
    hostsFileWriter.initExcludeHosts(nodes);
    dm.refreshNodes(conf);
    BlockManagerTestUtil.recheckDecommissionState(dm);
    assertTrue(dnDescriptor0.isDecommissioned());
    assertTrue(dnDescriptor1.isDecommissioned());
    long missingBlocks=bm.getMissingBlocksCount();
    long underreplicated=bm.getLowRedundancyBlocksCount();
    assertTrue(missingBlocks > 0);
    assertTrue(underreplicated > 0);
    LOG.info("Bring back dn0");
    cluster.restartDataNode(stoppedDN0,true);
    do {
      dnID=cluster.getDataNodes().get(0).getDatanodeId();
    }
 while (dnID == null);
    dnDescriptor0=dm.getDatanode(dnID);
    while (dnDescriptor0.numBlocks() == 0) {
      Thread.sleep(100);
    }
    LOG.info("Bring back dn1");
    cluster.restartDataNode(stoppedDN1,true);
    do {
      dnID=cluster.getDataNodes().get(1).getDatanodeId();
    }
 while (dnID == null);
    dnDescriptor1=dm.getDatanode(dnID);
    while (dnDescriptor1.numBlocks() == 0) {
      Thread.sleep(100);
    }
    Thread.sleep(2000);
    assertEquals(underreplicated,bm.getLowRedundancyBlocksCount());
    LOG.info("Starting two more nodes");
    cluster.startDataNodes(conf,2,true,null,null);
    cluster.waitActive();
    int count=0;
    while ((bm.getLowRedundancyBlocksCount() > 0 || bm.getPendingReconstructionBlocksCount() > 0) && count++ < 10) {
      Thread.sleep(1000);
    }
    assertEquals(0,bm.getLowRedundancyBlocksCount());
    assertEquals(0,bm.getPendingReconstructionBlocksCount());
    assertEquals(0,bm.getMissingBlocksCount());
    dnID=cluster.getDataNodes().get(3).getDatanodeId();
    cluster.stopDataNode(3);
    DFSTestUtil.waitForDatanodeState(cluster,dnID.getDatanodeUuid(),false,30000);
    dnID=cluster.getDataNodes().get(2).getDatanodeId();
    cluster.stopDataNode(2);
    DFSTestUtil.waitForDatanodeState(cluster,dnID.getDatanodeUuid(),false,30000);
    hostsFileWriter.initExcludeHost("");
    dm.refreshNodes(conf);
    fileSys.delete(file1,false);
  }
}
