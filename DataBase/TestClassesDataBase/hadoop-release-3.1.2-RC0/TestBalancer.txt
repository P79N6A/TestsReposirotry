/** 
 * This class tests if a balancer schedules tasks correctly.
 */
public class TestBalancer {
  private static final Log LOG=LogFactory.getLog(TestBalancer.class);
static {
    GenericTestUtils.setLogLevel(Balancer.LOG,Level.ALL);
    GenericTestUtils.setLogLevel(Dispatcher.LOG,Level.DEBUG);
  }
  final static long CAPACITY=5000L;
  final static String RACK0="/rack0";
  final static String RACK1="/rack1";
  final static String RACK2="/rack2";
  final private static String fileName="/tmp.txt";
  final static Path filePath=new Path(fileName);
  final static private String username="balancer";
  private static String principal;
  private static File baseDir;
  private static String keystoresDir;
  private static String sslConfDir;
  private static MiniKdc kdc;
  private static File keytabFile;
  private MiniDFSCluster cluster;
  @After public void shutdown() throws Exception {
    if (cluster != null) {
      cluster.shutdown();
      cluster=null;
    }
  }
  ClientProtocol client;
  static final long TIMEOUT=40000L;
  static final double CAPACITY_ALLOWED_VARIANCE=0.005;
  static final double BALANCE_ALLOWED_VARIANCE=0.11;
  static final int DEFAULT_BLOCK_SIZE=100;
  static final int DEFAULT_RAM_DISK_BLOCK_SIZE=5 * 1024 * 1024;
  private static final Random r=new Random();
static {
    initTestSetup();
  }
  public static void initTestSetup(){
    NameNodeConnector.setWrite2IdFile(false);
  }
  static void initConf(  Configuration conf){
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,DEFAULT_BLOCK_SIZE);
    conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,DEFAULT_BLOCK_SIZE);
    conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1L);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY,500);
    conf.setLong(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,1L);
    SimulatedFSDataset.setFactory(conf);
    conf.setLong(DFSConfigKeys.DFS_BALANCER_MOVEDWINWIDTH_KEY,2000L);
    conf.setLong(DFSConfigKeys.DFS_BALANCER_GETBLOCKS_MIN_BLOCK_SIZE_KEY,1L);
    conf.setInt(DFSConfigKeys.DFS_BALANCER_MAX_NO_MOVE_INTERVAL_KEY,5 * 1000);
  }
  static void initConfWithRamDisk(  Configuration conf,  long ramDiskCapacity){
    conf.setLong(DFS_BLOCK_SIZE_KEY,DEFAULT_RAM_DISK_BLOCK_SIZE);
    conf.setLong(DFS_DATANODE_MAX_LOCKED_MEMORY_KEY,ramDiskCapacity);
    conf.setInt(DFS_NAMENODE_LAZY_PERSIST_FILE_SCRUB_INTERVAL_SEC,3);
    conf.setLong(DFS_HEARTBEAT_INTERVAL_KEY,1);
    conf.setInt(DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY,500);
    conf.setInt(DFS_DATANODE_LAZY_WRITER_INTERVAL_SEC,1);
    conf.setInt(DFSConfigKeys.DFS_BALANCER_MAX_NO_MOVE_INTERVAL_KEY,5 * 1000);
    LazyPersistTestCase.initCacheManipulator();
    conf.setLong(DFSConfigKeys.DFS_BALANCER_GETBLOCKS_MIN_BLOCK_SIZE_KEY,1L);
  }
  private final ErasureCodingPolicy ecPolicy=StripedFileTestUtil.getDefaultECPolicy();
  private final int dataBlocks=ecPolicy.getNumDataUnits();
  private final int parityBlocks=ecPolicy.getNumParityUnits();
  private final int groupSize=dataBlocks + parityBlocks;
  private final int cellSize=ecPolicy.getCellSize();
  private final int stripesPerBlock=4;
  private final int defaultBlockSize=cellSize * stripesPerBlock;
  void initConfWithStripe(  Configuration conf){
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,defaultBlockSize);
    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_CONSIDERLOAD_KEY,false);
    conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1L);
    SimulatedFSDataset.setFactory(conf);
    conf.setLong(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,1L);
    conf.setLong(DFSConfigKeys.DFS_BALANCER_MOVEDWINWIDTH_KEY,2000L);
    conf.setLong(DFSConfigKeys.DFS_BALANCER_GETBLOCKS_MIN_BLOCK_SIZE_KEY,1L);
  }
  static void initSecureConf(  Configuration conf) throws Exception {
    baseDir=GenericTestUtils.getTestDir(TestBalancer.class.getSimpleName());
    FileUtil.fullyDelete(baseDir);
    assertTrue(baseDir.mkdirs());
    Properties kdcConf=MiniKdc.createConf();
    kdc=new MiniKdc(kdcConf,baseDir);
    kdc.start();
    SecurityUtil.setAuthenticationMethod(UserGroupInformation.AuthenticationMethod.KERBEROS,conf);
    UserGroupInformation.setConfiguration(conf);
    KerberosName.resetDefaultRealm();
    assertTrue("Expected configuration to enable security",UserGroupInformation.isSecurityEnabled());
    keytabFile=new File(baseDir,username + ".keytab");
    String keytab=keytabFile.getAbsolutePath();
    String krbInstance=Path.WINDOWS ? "127.0.0.1" : "localhost";
    principal=username + "/" + krbInstance+ "@"+ kdc.getRealm();
    String spnegoPrincipal="HTTP/" + krbInstance + "@"+ kdc.getRealm();
    kdc.createPrincipal(keytabFile,username,username + "/" + krbInstance,"HTTP/" + krbInstance);
    conf.set(DFS_NAMENODE_KERBEROS_PRINCIPAL_KEY,principal);
    conf.set(DFS_NAMENODE_KEYTAB_FILE_KEY,keytab);
    conf.set(DFS_DATANODE_KERBEROS_PRINCIPAL_KEY,principal);
    conf.set(DFS_DATANODE_KEYTAB_FILE_KEY,keytab);
    conf.set(DFS_WEB_AUTHENTICATION_KERBEROS_PRINCIPAL_KEY,spnegoPrincipal);
    conf.setBoolean(DFS_BLOCK_ACCESS_TOKEN_ENABLE_KEY,true);
    conf.set(DFS_DATA_TRANSFER_PROTECTION_KEY,"authentication");
    conf.set(DFS_HTTP_POLICY_KEY,HttpConfig.Policy.HTTPS_ONLY.name());
    conf.set(DFS_NAMENODE_HTTPS_ADDRESS_KEY,"localhost:0");
    conf.set(DFS_DATANODE_HTTPS_ADDRESS_KEY,"localhost:0");
    conf.setInt(IPC_CLIENT_CONNECT_MAX_RETRIES_ON_SASL_KEY,10);
    conf.setBoolean(DFS_BALANCER_KEYTAB_ENABLED_KEY,true);
    conf.set(DFS_BALANCER_ADDRESS_KEY,"localhost:0");
    conf.set(DFS_BALANCER_KEYTAB_FILE_KEY,keytab);
    conf.set(DFS_BALANCER_KERBEROS_PRINCIPAL_KEY,principal);
    keystoresDir=baseDir.getAbsolutePath();
    sslConfDir=KeyStoreTestUtil.getClasspathDir(TestBalancer.class);
    KeyStoreTestUtil.setupSSLConfig(keystoresDir,sslConfDir,conf,false);
    conf.set(DFS_CLIENT_HTTPS_KEYSTORE_RESOURCE_KEY,KeyStoreTestUtil.getClientSSLConfigFileName());
    conf.set(DFS_SERVER_HTTPS_KEYSTORE_RESOURCE_KEY,KeyStoreTestUtil.getServerSSLConfigFileName());
    initConf(conf);
  }
  @AfterClass public static void destroy() throws Exception {
    if (kdc != null) {
      kdc.stop();
      FileUtil.fullyDelete(baseDir);
      KeyStoreTestUtil.cleanupSSLConfig(keystoresDir,sslConfDir);
    }
  }
  public static void createFile(  MiniDFSCluster cluster,  Path filePath,  long fileLen,  short replicationFactor,  int nnIndex) throws IOException, InterruptedException, TimeoutException {
    FileSystem fs=cluster.getFileSystem(nnIndex);
    DFSTestUtil.createFile(fs,filePath,fileLen,replicationFactor,r.nextLong());
    DFSTestUtil.waitReplication(fs,filePath,replicationFactor);
  }
  private ExtendedBlock[] generateBlocks(  Configuration conf,  long size,  short numNodes) throws IOException, InterruptedException, TimeoutException {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numNodes).build();
    try {
      cluster.waitActive();
      client=NameNodeProxies.createProxy(conf,cluster.getFileSystem(0).getUri(),ClientProtocol.class).getProxy();
      short replicationFactor=(short)(numNodes - 1);
      long fileLen=size / replicationFactor;
      createFile(cluster,filePath,fileLen,replicationFactor,0);
      List<LocatedBlock> locatedBlocks=client.getBlockLocations(fileName,0,fileLen).getLocatedBlocks();
      int numOfBlocks=locatedBlocks.size();
      ExtendedBlock[] blocks=new ExtendedBlock[numOfBlocks];
      for (int i=0; i < numOfBlocks; i++) {
        ExtendedBlock b=locatedBlocks.get(i).getBlock();
        blocks[i]=new ExtendedBlock(b.getBlockPoolId(),b.getBlockId(),b.getNumBytes(),b.getGenerationStamp());
      }
      return blocks;
    }
  finally {
      cluster.shutdown();
    }
  }
  static Block[][] distributeBlocks(  ExtendedBlock[] blocks,  short replicationFactor,  final long[] distribution){
    long[] usedSpace=new long[distribution.length];
    System.arraycopy(distribution,0,usedSpace,0,distribution.length);
    List<List<Block>> blockReports=new ArrayList<List<Block>>(usedSpace.length);
    Block[][] results=new Block[usedSpace.length][];
    for (int i=0; i < usedSpace.length; i++) {
      blockReports.add(new ArrayList<Block>());
    }
    for (int i=0; i < blocks.length; i++) {
      for (int j=0; j < replicationFactor; j++) {
        boolean notChosen=true;
        while (notChosen) {
          int chosenIndex=r.nextInt(usedSpace.length);
          if (usedSpace[chosenIndex] > 0) {
            notChosen=false;
            blockReports.get(chosenIndex).add(blocks[i].getLocalBlock());
            usedSpace[chosenIndex]-=blocks[i].getNumBytes();
          }
        }
      }
    }
    for (int i=0; i < usedSpace.length; i++) {
      List<Block> nodeBlockList=blockReports.get(i);
      results[i]=nodeBlockList.toArray(new Block[nodeBlockList.size()]);
    }
    return results;
  }
  static long sum(  long[] x){
    long s=0L;
    for (    long a : x) {
      s+=a;
    }
    return s;
  }
  private void testUnevenDistribution(  Configuration conf,  long distribution[],  long capacities[],  String[] racks) throws Exception {
    int numDatanodes=distribution.length;
    if (capacities.length != numDatanodes || racks.length != numDatanodes) {
      throw new IllegalArgumentException("Array length is not the same");
    }
    final long totalUsedSpace=sum(distribution);
    ExtendedBlock[] blocks=generateBlocks(conf,totalUsedSpace,(short)numDatanodes);
    Block[][] blocksDN=distributeBlocks(blocks,(short)(numDatanodes - 1),distribution);
    conf.set(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY,"0.0f");
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(false).racks(racks).simulatedCapacities(capacities).build();
    cluster.waitActive();
    client=NameNodeProxies.createProxy(conf,cluster.getFileSystem(0).getUri(),ClientProtocol.class).getProxy();
    for (int i=0; i < blocksDN.length; i++)     cluster.injectBlocks(i,Arrays.asList(blocksDN[i]),null);
    final long totalCapacity=sum(capacities);
    runBalancer(conf,totalUsedSpace,totalCapacity);
    cluster.shutdown();
  }
  /** 
 * Wait until heartbeat gives expected results, within CAPACITY_ALLOWED_VARIANCE, summed over all nodes.  Times out after TIMEOUT msec.
 * @param expectedUsedSpace
 * @param expectedTotalSpace
 * @throws IOException - if getStats() fails
 * @throws TimeoutException
 */
  static void waitForHeartBeat(  long expectedUsedSpace,  long expectedTotalSpace,  ClientProtocol client,  MiniDFSCluster cluster) throws IOException, TimeoutException {
    long timeout=TIMEOUT;
    long failtime=(timeout <= 0L) ? Long.MAX_VALUE : Time.monotonicNow() + timeout;
    while (true) {
      long[] status=client.getStats();
      double totalSpaceVariance=Math.abs((double)status[0] - expectedTotalSpace) / expectedTotalSpace;
      double usedSpaceVariance=Math.abs((double)status[1] - expectedUsedSpace) / expectedUsedSpace;
      if (totalSpaceVariance < CAPACITY_ALLOWED_VARIANCE && usedSpaceVariance < CAPACITY_ALLOWED_VARIANCE)       break;
      if (Time.monotonicNow() > failtime) {
        throw new TimeoutException("Cluster failed to reached expected values of " + "totalSpace (current: " + status[0] + ", expected: "+ expectedTotalSpace+ "), or usedSpace (current: "+ status[1]+ ", expected: "+ expectedUsedSpace+ "), in more than "+ timeout+ " msec.");
      }
      try {
        Thread.sleep(100L);
      }
 catch (      InterruptedException ignored) {
      }
    }
  }
  /** 
 * Wait until balanced: each datanode gives utilization within BALANCE_ALLOWED_VARIANCE of average
 * @throws IOException
 * @throws TimeoutException
 */
  static void waitForBalancer(  long totalUsedSpace,  long totalCapacity,  ClientProtocol client,  MiniDFSCluster cluster,  BalancerParameters p) throws IOException, TimeoutException {
    waitForBalancer(totalUsedSpace,totalCapacity,client,cluster,p,0);
  }
  /** 
 * Make sure that balancer can't move pinned blocks. If specified favoredNodes when create file, blocks will be pinned use sticky bit.
 * @throws Exception
 */
  @Test(timeout=100000) public void testBalancerWithPinnedBlocks() throws Exception {
    assumeNotWindows();
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    conf.setBoolean(DFS_DATANODE_BLOCK_PINNING_ENABLED,true);
    long[] capacities=new long[]{CAPACITY,CAPACITY};
    String[] hosts={"host0","host1"};
    String[] racks={RACK0,RACK1};
    int numOfDatanodes=capacities.length;
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(capacities.length).hosts(hosts).racks(racks).simulatedCapacities(capacities).build();
    cluster.waitActive();
    client=NameNodeProxies.createProxy(conf,cluster.getFileSystem(0).getUri(),ClientProtocol.class).getProxy();
    long totalCapacity=sum(capacities);
    long totalUsedSpace=totalCapacity * 8 / 10;
    InetSocketAddress[] favoredNodes=new InetSocketAddress[numOfDatanodes];
    for (int i=0; i < favoredNodes.length; i++) {
      int port=cluster.getDataNodes().get(i).getXferAddress().getPort();
      favoredNodes[i]=new InetSocketAddress(hosts[i],port);
    }
    DFSTestUtil.createFile(cluster.getFileSystem(0),filePath,false,1024,totalUsedSpace / numOfDatanodes,DEFAULT_BLOCK_SIZE,(short)numOfDatanodes,0,false,favoredNodes);
    cluster.startDataNodes(conf,1,true,null,new String[]{RACK2},new long[]{CAPACITY});
    totalCapacity+=CAPACITY;
    waitForHeartBeat(totalUsedSpace,totalCapacity,client,cluster);
    Collection<URI> namenodes=DFSUtil.getInternalNsRpcUris(conf);
    int r=Balancer.run(namenodes,BalancerParameters.DEFAULT,conf);
    assertEquals(ExitStatus.NO_MOVE_PROGRESS.getExitCode(),r);
  }
  /** 
 * Verify balancer won't violate the default block placement policy.
 * @throws Exception
 */
  @Test(timeout=100000) public void testRackPolicyAfterBalance() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    long[] capacities=new long[]{CAPACITY,CAPACITY};
    String[] hosts={"host0","host1"};
    String[] racks={RACK0,RACK1};
    runBalancerAndVerifyBlockPlacmentPolicy(conf,capacities,hosts,racks,null,CAPACITY,"host2",RACK1,null);
  }
  /** 
 * Verify balancer won't violate upgrade domain block placement policy.
 * @throws Exception
 */
  @Test(timeout=100000) public void testUpgradeDomainPolicyAfterBalance() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    conf.setClass(DFSConfigKeys.DFS_BLOCK_REPLICATOR_CLASSNAME_KEY,BlockPlacementPolicyWithUpgradeDomain.class,BlockPlacementPolicy.class);
    long[] capacities=new long[]{CAPACITY,CAPACITY,CAPACITY};
    String[] hosts={"host0","host1","host2"};
    String[] racks={RACK0,RACK1,RACK1};
    String[] UDs={"ud0","ud1","ud2"};
    runBalancerAndVerifyBlockPlacmentPolicy(conf,capacities,hosts,racks,UDs,CAPACITY,"host3",RACK2,"ud2");
  }
  private void runBalancerAndVerifyBlockPlacmentPolicy(  Configuration conf,  long[] capacities,  String[] hosts,  String[] racks,  String[] UDs,  long newCapacity,  String newHost,  String newRack,  String newUD) throws Exception {
    int numOfDatanodes=capacities.length;
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(capacities.length).hosts(hosts).racks(racks).simulatedCapacities(capacities).build();
    DatanodeManager dm=cluster.getNamesystem().getBlockManager().getDatanodeManager();
    if (UDs != null) {
      for (int i=0; i < UDs.length; i++) {
        DatanodeID datanodeId=cluster.getDataNodes().get(i).getDatanodeId();
        dm.getDatanode(datanodeId).setUpgradeDomain(UDs[i]);
      }
    }
    try {
      cluster.waitActive();
      client=NameNodeProxies.createProxy(conf,cluster.getFileSystem(0).getUri(),ClientProtocol.class).getProxy();
      long totalCapacity=sum(capacities);
      long totalUsedSpace=totalCapacity * 8 / 10;
      final long fileSize=totalUsedSpace / numOfDatanodes;
      DFSTestUtil.createFile(cluster.getFileSystem(0),filePath,false,1024,fileSize,DEFAULT_BLOCK_SIZE,(short)numOfDatanodes,0,false);
      cluster.startDataNodes(conf,1,true,null,new String[]{newRack},new String[]{newHost},new long[]{newCapacity});
      if (newUD != null) {
        DatanodeID newId=cluster.getDataNodes().get(numOfDatanodes).getDatanodeId();
        dm.getDatanode(newId).setUpgradeDomain(newUD);
      }
      totalCapacity+=newCapacity;
      waitForHeartBeat(totalUsedSpace,totalCapacity,client,cluster);
      Collection<URI> namenodes=DFSUtil.getInternalNsRpcUris(conf);
      Balancer.run(namenodes,BalancerParameters.DEFAULT,conf);
      BlockPlacementPolicy placementPolicy=cluster.getNamesystem().getBlockManager().getBlockPlacementPolicy();
      List<LocatedBlock> locatedBlocks=client.getBlockLocations(fileName,0,fileSize).getLocatedBlocks();
      for (      LocatedBlock locatedBlock : locatedBlocks) {
        BlockPlacementStatus status=placementPolicy.verifyBlockPlacement(locatedBlock.getLocations(),numOfDatanodes);
        assertTrue(status.isPlacementPolicySatisfied());
      }
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Wait until balanced: each datanode gives utilization within BALANCE_ALLOWED_VARIANCE of average
 * @throws IOException
 * @throws TimeoutException
 */
  static void waitForBalancer(  long totalUsedSpace,  long totalCapacity,  ClientProtocol client,  MiniDFSCluster cluster,  BalancerParameters p,  int expectedExcludedNodes) throws IOException, TimeoutException {
    long timeout=TIMEOUT;
    long failtime=(timeout <= 0L) ? Long.MAX_VALUE : Time.monotonicNow() + timeout;
    if (!p.getIncludedNodes().isEmpty()) {
      totalCapacity=p.getIncludedNodes().size() * CAPACITY;
    }
    if (!p.getExcludedNodes().isEmpty()) {
      totalCapacity-=p.getExcludedNodes().size() * CAPACITY;
    }
    final double avgUtilization=((double)totalUsedSpace) / totalCapacity;
    boolean balanced;
    do {
      DatanodeInfo[] datanodeReport=client.getDatanodeReport(DatanodeReportType.ALL);
      assertEquals(datanodeReport.length,cluster.getDataNodes().size());
      balanced=true;
      int actualExcludedNodeCount=0;
      for (      DatanodeInfo datanode : datanodeReport) {
        double nodeUtilization=((double)datanode.getDfsUsed()) / datanode.getCapacity();
        if (Dispatcher.Util.isExcluded(p.getExcludedNodes(),datanode)) {
          assertTrue(nodeUtilization == 0);
          actualExcludedNodeCount++;
          continue;
        }
        if (!Dispatcher.Util.isIncluded(p.getIncludedNodes(),datanode)) {
          assertTrue(nodeUtilization == 0);
          actualExcludedNodeCount++;
          continue;
        }
        if (Math.abs(avgUtilization - nodeUtilization) > BALANCE_ALLOWED_VARIANCE) {
          balanced=false;
          if (Time.monotonicNow() > failtime) {
            throw new TimeoutException("Rebalancing expected avg utilization to become " + avgUtilization + ", but on datanode "+ datanode+ " it remains at "+ nodeUtilization+ " after more than "+ TIMEOUT+ " msec.");
          }
          try {
            Thread.sleep(100);
          }
 catch (          InterruptedException ignored) {
          }
          break;
        }
      }
      assertEquals(expectedExcludedNodes,actualExcludedNodeCount);
    }
 while (!balanced);
  }
  String long2String(  long[] array){
    if (array.length == 0) {
      return "<empty>";
    }
    StringBuilder b=new StringBuilder("[").append(array[0]);
    for (int i=1; i < array.length; i++) {
      b.append(", ").append(array[i]);
    }
    return b.append("]").toString();
  }
  /** 
 * Class which contains information about the new nodes to be added to the cluster for balancing.
 */
static abstract class NewNodeInfo {
    Set<String> nodesToBeExcluded=new HashSet<String>();
    Set<String> nodesToBeIncluded=new HashSet<String>();
    abstract String[] getNames();
    abstract int getNumberofNewNodes();
    abstract int getNumberofIncludeNodes();
    abstract int getNumberofExcludeNodes();
    public Set<String> getNodesToBeIncluded(){
      return nodesToBeIncluded;
    }
    public Set<String> getNodesToBeExcluded(){
      return nodesToBeExcluded;
    }
  }
  /** 
 * The host names of new nodes are specified
 */
static class HostNameBasedNodes extends NewNodeInfo {
    String[] hostnames;
    public HostNameBasedNodes(    String[] hostnames,    Set<String> nodesToBeExcluded,    Set<String> nodesToBeIncluded){
      this.hostnames=hostnames;
      this.nodesToBeExcluded=nodesToBeExcluded;
      this.nodesToBeIncluded=nodesToBeIncluded;
    }
    @Override String[] getNames(){
      return hostnames;
    }
    @Override int getNumberofNewNodes(){
      return hostnames.length;
    }
    @Override int getNumberofIncludeNodes(){
      return nodesToBeIncluded.size();
    }
    @Override int getNumberofExcludeNodes(){
      return nodesToBeExcluded.size();
    }
  }
  /** 
 * The number of data nodes to be started are specified. The data nodes will have same host name, but different port numbers.
 */
static class PortNumberBasedNodes extends NewNodeInfo {
    int newNodes;
    int excludeNodes;
    int includeNodes;
    public PortNumberBasedNodes(    int newNodes,    int excludeNodes,    int includeNodes){
      this.newNodes=newNodes;
      this.excludeNodes=excludeNodes;
      this.includeNodes=includeNodes;
    }
    @Override String[] getNames(){
      return null;
    }
    @Override int getNumberofNewNodes(){
      return newNodes;
    }
    @Override int getNumberofIncludeNodes(){
      return includeNodes;
    }
    @Override int getNumberofExcludeNodes(){
      return excludeNodes;
    }
  }
  private void doTest(  Configuration conf,  long[] capacities,  String[] racks,  long newCapacity,  String newRack,  boolean useTool) throws Exception {
    doTest(conf,capacities,racks,newCapacity,newRack,null,useTool,false);
  }
  private void doTest(  Configuration conf,  long[] capacities,  String[] racks,  long newCapacity,  String newRack,  NewNodeInfo nodes,  boolean useTool,  boolean useFile) throws Exception {
    doTest(conf,capacities,racks,newCapacity,newRack,nodes,useTool,useFile,false);
  }
  /** 
 * This test start a cluster with specified number of nodes, and fills it to be 30% full (with a single file replicated identically to all datanodes); It then adds one new empty node and starts balancing.
 * @param conf - configuration
 * @param capacities - array of capacities of original nodes in cluster
 * @param racks - array of racks for original nodes in cluster
 * @param newCapacity - new node's capacity
 * @param newRack - new node's rack
 * @param nodes - information about new nodes to be started.
 * @param useTool - if true run test via Cli with command-line argumentparsing, etc.   Otherwise invoke balancer API directly.
 * @param useFile - if true, the hosts to included or excluded will be stored in afile and then later read from the file.
 * @param useNamesystemSpy - spy on FSNamesystem if true
 * @throws Exception
 */
  private void doTest(  Configuration conf,  long[] capacities,  String[] racks,  long newCapacity,  String newRack,  NewNodeInfo nodes,  boolean useTool,  boolean useFile,  boolean useNamesystemSpy) throws Exception {
    LOG.info("capacities = " + long2String(capacities));
    LOG.info("racks      = " + Arrays.asList(racks));
    LOG.info("newCapacity= " + newCapacity);
    LOG.info("newRack    = " + newRack);
    LOG.info("useTool    = " + useTool);
    assertEquals(capacities.length,racks.length);
    int numOfDatanodes=capacities.length;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
      cluster.getConfiguration(0).setInt(DFSConfigKeys.DFS_REPLICATION_KEY,DFSConfigKeys.DFS_REPLICATION_DEFAULT);
      conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY,DFSConfigKeys.DFS_REPLICATION_DEFAULT);
      if (useNamesystemSpy) {
        LOG.info("Using Spy Namesystem");
        spyFSNamesystem(cluster.getNameNode());
      }
      cluster.startDataNodes(conf,numOfDatanodes,true,StartupOption.REGULAR,racks,null,capacities,false);
      cluster.waitClusterUp();
      cluster.waitActive();
      client=NameNodeProxies.createProxy(conf,cluster.getFileSystem(0).getUri(),ClientProtocol.class).getProxy();
      long totalCapacity=sum(capacities);
      long totalUsedSpace=totalCapacity * 3 / 10;
      createFile(cluster,filePath,totalUsedSpace / numOfDatanodes,(short)numOfDatanodes,0);
      if (nodes == null) {
        cluster.startDataNodes(conf,1,true,null,new String[]{newRack},null,new long[]{newCapacity});
        totalCapacity+=newCapacity;
      }
 else {
        if (nodes.getNumberofIncludeNodes() > 0) {
          for (          DataNode dn : cluster.getDataNodes())           nodes.getNodesToBeIncluded().add(dn.getDatanodeId().getHostName());
        }
        String[] newRacks=new String[nodes.getNumberofNewNodes()];
        long[] newCapacities=new long[nodes.getNumberofNewNodes()];
        for (int i=0; i < nodes.getNumberofNewNodes(); i++) {
          newRacks[i]=newRack;
          newCapacities[i]=newCapacity;
        }
        if (nodes.getNames() != null) {
          cluster.startDataNodes(conf,nodes.getNumberofNewNodes(),true,null,newRacks,nodes.getNames(),newCapacities);
          totalCapacity+=newCapacity * nodes.getNumberofNewNodes();
        }
 else {
          cluster.startDataNodes(conf,nodes.getNumberofNewNodes(),true,null,newRacks,null,newCapacities);
          totalCapacity+=newCapacity * nodes.getNumberofNewNodes();
          if (nodes.getNumberofIncludeNodes() > 0) {
            int totalNodes=cluster.getDataNodes().size();
            for (int i=0; i < nodes.getNumberofIncludeNodes(); i++) {
              nodes.getNodesToBeIncluded().add(cluster.getDataNodes().get(totalNodes - 1 - i).getDatanodeId().getXferAddr());
            }
          }
          if (nodes.getNumberofExcludeNodes() > 0) {
            int totalNodes=cluster.getDataNodes().size();
            for (int i=0; i < nodes.getNumberofExcludeNodes(); i++) {
              nodes.getNodesToBeExcluded().add(cluster.getDataNodes().get(totalNodes - 1 - i).getDatanodeId().getXferAddr());
            }
          }
        }
      }
      BalancerParameters.Builder pBuilder=new BalancerParameters.Builder();
      if (nodes != null) {
        pBuilder.setExcludedNodes(nodes.getNodesToBeExcluded());
        pBuilder.setIncludedNodes(nodes.getNodesToBeIncluded());
        pBuilder.setRunDuringUpgrade(false);
      }
      BalancerParameters p=pBuilder.build();
      int expectedExcludedNodes=0;
      if (nodes != null) {
        if (!nodes.getNodesToBeExcluded().isEmpty()) {
          expectedExcludedNodes=nodes.getNodesToBeExcluded().size();
        }
 else         if (!nodes.getNodesToBeIncluded().isEmpty()) {
          expectedExcludedNodes=cluster.getDataNodes().size() - nodes.getNodesToBeIncluded().size();
        }
      }
      if (useTool) {
        runBalancerCli(conf,totalUsedSpace,totalCapacity,p,useFile,expectedExcludedNodes);
      }
 else {
        runBalancer(conf,totalUsedSpace,totalCapacity,p,expectedExcludedNodes);
      }
    }
  finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
  private void runBalancer(  Configuration conf,  long totalUsedSpace,  long totalCapacity) throws Exception {
    runBalancer(conf,totalUsedSpace,totalCapacity,BalancerParameters.DEFAULT,0);
  }
  private void runBalancer(  Configuration conf,  long totalUsedSpace,  long totalCapacity,  BalancerParameters p,  int excludedNodes) throws Exception {
    waitForHeartBeat(totalUsedSpace,totalCapacity,client,cluster);
    int retry=5;
    while (retry > 0) {
      Collection<URI> namenodes=DFSUtil.getInternalNsRpcUris(conf);
      final int run=runBalancer(namenodes,p,conf);
      if (conf.getInt(DFSConfigKeys.DFS_DATANODE_BALANCE_MAX_NUM_CONCURRENT_MOVES_KEY,DFSConfigKeys.DFS_DATANODE_BALANCE_MAX_NUM_CONCURRENT_MOVES_DEFAULT) == 0) {
        assertEquals(ExitStatus.NO_MOVE_PROGRESS.getExitCode(),run);
        return;
      }
 else {
        assertEquals(ExitStatus.SUCCESS.getExitCode(),run);
      }
      waitForHeartBeat(totalUsedSpace,totalCapacity,client,cluster);
      LOG.info("  .");
      try {
        waitForBalancer(totalUsedSpace,totalCapacity,client,cluster,p,excludedNodes);
      }
 catch (      TimeoutException e) {
        retry--;
        if (retry == 0) {
          throw e;
        }
        LOG.warn("The cluster has not balanced yet, retry...");
        continue;
      }
      break;
    }
  }
  private static int runBalancer(  Collection<URI> namenodes,  final BalancerParameters p,  Configuration conf) throws IOException, InterruptedException {
    final long sleeptime=conf.getLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT) * 2000 + conf.getLong(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_DEFAULT) * 1000;
    LOG.info("namenodes  = " + namenodes);
    LOG.info("parameters = " + p);
    LOG.info("Print stack trace",new Throwable());
    System.out.println("Time Stamp               Iteration#  Bytes Already Moved  Bytes Left To Move  Bytes Being Moved");
    List<NameNodeConnector> connectors=Collections.emptyList();
    try {
      connectors=NameNodeConnector.newNameNodeConnectors(namenodes,Balancer.class.getSimpleName(),Balancer.BALANCER_ID_PATH,conf,BalancerParameters.DEFAULT.getMaxIdleIteration());
      boolean done=false;
      for (int iteration=0; !done; iteration++) {
        done=true;
        Collections.shuffle(connectors);
        for (        NameNodeConnector nnc : connectors) {
          final Balancer b=new Balancer(nnc,p,conf);
          final Result r=b.runOneIteration();
          r.print(iteration,System.out);
          b.resetData(conf);
          if (r.exitStatus == ExitStatus.IN_PROGRESS) {
            done=false;
          }
 else           if (r.exitStatus != ExitStatus.SUCCESS) {
            return r.exitStatus.getExitCode();
          }
 else {
            if (iteration > 0) {
              assertTrue(r.bytesAlreadyMoved > 0);
            }
          }
        }
        if (!done) {
          Thread.sleep(sleeptime);
        }
      }
    }
  finally {
      for (      NameNodeConnector nnc : connectors) {
        IOUtils.cleanup(LOG,nnc);
      }
    }
    return ExitStatus.SUCCESS.getExitCode();
  }
  private void runBalancerCli(  Configuration conf,  long totalUsedSpace,  long totalCapacity,  BalancerParameters p,  boolean useFile,  int expectedExcludedNodes) throws Exception {
    waitForHeartBeat(totalUsedSpace,totalCapacity,client,cluster);
    List<String> args=new ArrayList<String>();
    args.add("-policy");
    args.add("datanode");
    File excludeHostsFile=null;
    if (!p.getExcludedNodes().isEmpty()) {
      args.add("-exclude");
      if (useFile) {
        excludeHostsFile=GenericTestUtils.getTestDir("exclude-hosts-file");
        PrintWriter pw=new PrintWriter(excludeHostsFile);
        for (        String host : p.getExcludedNodes()) {
          pw.write(host + "\n");
        }
        pw.close();
        args.add("-f");
        args.add(excludeHostsFile.getAbsolutePath());
      }
 else {
        args.add(StringUtils.join(p.getExcludedNodes(),','));
      }
    }
    File includeHostsFile=null;
    if (!p.getIncludedNodes().isEmpty()) {
      args.add("-include");
      if (useFile) {
        includeHostsFile=GenericTestUtils.getTestDir("include-hosts-file");
        PrintWriter pw=new PrintWriter(includeHostsFile);
        for (        String host : p.getIncludedNodes()) {
          pw.write(host + "\n");
        }
        pw.close();
        args.add("-f");
        args.add(includeHostsFile.getAbsolutePath());
      }
 else {
        args.add(StringUtils.join(p.getIncludedNodes(),','));
      }
    }
    final Tool tool=new Cli();
    tool.setConf(conf);
    final int r=tool.run(args.toArray(new String[0]));
    assertEquals("Tools should exit 0 on success",0,r);
    waitForHeartBeat(totalUsedSpace,totalCapacity,client,cluster);
    LOG.info("Rebalancing with default ctor.");
    waitForBalancer(totalUsedSpace,totalCapacity,client,cluster,p,expectedExcludedNodes);
    if (excludeHostsFile != null && excludeHostsFile.exists()) {
      excludeHostsFile.delete();
    }
    if (includeHostsFile != null && includeHostsFile.exists()) {
      includeHostsFile.delete();
    }
  }
  /** 
 * one-node cluster test
 */
  private void oneNodeTest(  Configuration conf,  boolean useTool) throws Exception {
    doTest(conf,new long[]{CAPACITY},new String[]{RACK0},CAPACITY / 2,RACK0,useTool);
  }
  /** 
 * two-node cluster test 
 */
  private void twoNodeTest(  Configuration conf) throws Exception {
    doTest(conf,new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1},CAPACITY,RACK2,false);
  }
  /** 
 * test using a user-supplied conf 
 */
  public void integrationTest(  Configuration conf) throws Exception {
    initConf(conf);
    oneNodeTest(conf,false);
  }
  @Test(timeout=100000) public void testUnknownDatanodeSimple() throws Exception {
    Configuration conf=new HdfsConfiguration();
    initConf(conf);
    testUnknownDatanode(conf);
  }
  private void testUnknownDatanode(  Configuration conf) throws IOException, InterruptedException, TimeoutException {
    long distribution[]=new long[]{50 * CAPACITY / 100,70 * CAPACITY / 100,0 * CAPACITY / 100};
    long capacities[]=new long[]{CAPACITY,CAPACITY,CAPACITY};
    String racks[]=new String[]{RACK0,RACK1,RACK1};
    int numDatanodes=distribution.length;
    if (capacities.length != numDatanodes || racks.length != numDatanodes) {
      throw new IllegalArgumentException("Array length is not the same");
    }
    final long totalUsedSpace=sum(distribution);
    ExtendedBlock[] blocks=generateBlocks(conf,totalUsedSpace,(short)numDatanodes);
    Block[][] blocksDN=distributeBlocks(blocks,(short)(numDatanodes - 1),distribution);
    conf.set(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY,"0.0f");
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(false).racks(racks).simulatedCapacities(capacities).build();
    cluster.waitActive();
    client=NameNodeProxies.createProxy(conf,cluster.getFileSystem(0).getUri(),ClientProtocol.class).getProxy();
    for (int i=0; i < 3; i++) {
      cluster.injectBlocks(i,Arrays.asList(blocksDN[i]),null);
    }
    cluster.startDataNodes(conf,1,true,null,new String[]{RACK0},null,new long[]{CAPACITY});
    cluster.triggerHeartbeats();
    Collection<URI> namenodes=DFSUtil.getInternalNsRpcUris(conf);
    Set<String> datanodes=new HashSet<String>();
    datanodes.add(cluster.getDataNodes().get(0).getDatanodeId().getHostName());
    BalancerParameters.Builder pBuilder=new BalancerParameters.Builder();
    pBuilder.setExcludedNodes(datanodes);
    pBuilder.setRunDuringUpgrade(false);
    final int r=Balancer.run(namenodes,pBuilder.build(),conf);
    assertEquals(ExitStatus.SUCCESS.getExitCode(),r);
  }
  /** 
 * Test parse method in Balancer#Cli class with threshold value out of boundaries.
 */
  @Test(timeout=100000) public void testBalancerCliParseWithThresholdOutOfBoundaries(){
    String parameters[]=new String[]{"-threshold","0"};
    String reason="IllegalArgumentException is expected when threshold value" + " is out of boundary.";
    try {
      Balancer.Cli.parse(parameters);
      fail(reason);
    }
 catch (    IllegalArgumentException e) {
      assertEquals("Number out of range: threshold = 0.0",e.getMessage());
    }
    parameters=new String[]{"-threshold","101"};
    try {
      Balancer.Cli.parse(parameters);
      fail(reason);
    }
 catch (    IllegalArgumentException e) {
      assertEquals("Number out of range: threshold = 101.0",e.getMessage());
    }
  }
  /** 
 * Test a cluster with even distribution, then a new empty node is added to the cluster
 */
  @Test(timeout=100000) public void testBalancer0() throws Exception {
    testBalancer0Internal(new HdfsConfiguration());
  }
  void testBalancer0Internal(  Configuration conf) throws Exception {
    initConf(conf);
    oneNodeTest(conf,false);
    twoNodeTest(conf);
  }
  /** 
 * Test unevenly distributed cluster 
 */
  @Test(timeout=100000) public void testBalancer1() throws Exception {
    testBalancer1Internal(new HdfsConfiguration());
  }
  void testBalancer1Internal(  Configuration conf) throws Exception {
    initConf(conf);
    testUnevenDistribution(conf,new long[]{50 * CAPACITY / 100,10 * CAPACITY / 100},new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1});
  }
  @Test(expected=HadoopIllegalArgumentException.class) public void testBalancerWithZeroThreadsForMove() throws Exception {
    Configuration conf=new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_DATANODE_BALANCE_MAX_NUM_CONCURRENT_MOVES_KEY,0);
    testBalancer1Internal(conf);
  }
  @Test(timeout=100000) public void testBalancerWithNonZeroThreadsForMove() throws Exception {
    Configuration conf=new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_DATANODE_BALANCE_MAX_NUM_CONCURRENT_MOVES_KEY,8);
    testBalancer1Internal(conf);
  }
  @Test(timeout=100000) public void testBalancer2() throws Exception {
    testBalancer2Internal(new HdfsConfiguration());
  }
  void testBalancer2Internal(  Configuration conf) throws Exception {
    initConf(conf);
    testBalancerDefaultConstructor(conf,new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1},CAPACITY,RACK2);
  }
  private void testBalancerDefaultConstructor(  Configuration conf,  long[] capacities,  String[] racks,  long newCapacity,  String newRack) throws Exception {
    int numOfDatanodes=capacities.length;
    assertEquals(numOfDatanodes,racks.length);
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(capacities.length).racks(racks).simulatedCapacities(capacities).build();
    try {
      cluster.waitActive();
      client=NameNodeProxies.createProxy(conf,cluster.getFileSystem(0).getUri(),ClientProtocol.class).getProxy();
      long totalCapacity=sum(capacities);
      long totalUsedSpace=totalCapacity * 3 / 10;
      createFile(cluster,filePath,totalUsedSpace / numOfDatanodes,(short)numOfDatanodes,0);
      cluster.startDataNodes(conf,1,true,null,new String[]{newRack},new long[]{newCapacity});
      totalCapacity+=newCapacity;
      runBalancer(conf,totalUsedSpace,totalCapacity);
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Test parse method in Balancer#Cli class with wrong number of params
 */
  @Test(timeout=100000) public void testBalancerCliParseWithWrongParams(){
    String parameters[]=new String[]{"-threshold"};
    String reason="IllegalArgumentException is expected when value is not specified";
    try {
      Balancer.Cli.parse(parameters);
      fail(reason);
    }
 catch (    IllegalArgumentException e) {
    }
    parameters=new String[]{"-policy"};
    try {
      Balancer.Cli.parse(parameters);
      fail(reason);
    }
 catch (    IllegalArgumentException e) {
    }
    parameters=new String[]{"-threshold","1","-policy"};
    try {
      Balancer.Cli.parse(parameters);
      fail(reason);
    }
 catch (    IllegalArgumentException e) {
    }
    parameters=new String[]{"-threshold","1","-include"};
    try {
      Balancer.Cli.parse(parameters);
      fail(reason);
    }
 catch (    IllegalArgumentException e) {
    }
    parameters=new String[]{"-threshold","1","-exclude"};
    try {
      Balancer.Cli.parse(parameters);
      fail(reason);
    }
 catch (    IllegalArgumentException e) {
    }
    parameters=new String[]{"-include","-f"};
    try {
      Balancer.Cli.parse(parameters);
      fail(reason);
    }
 catch (    IllegalArgumentException e) {
    }
    parameters=new String[]{"-exclude","-f"};
    try {
      Balancer.Cli.parse(parameters);
      fail(reason);
    }
 catch (    IllegalArgumentException e) {
    }
    parameters=new String[]{"-include","testnode1","-exclude","testnode2"};
    try {
      Balancer.Cli.parse(parameters);
      fail("IllegalArgumentException is expected when both -exclude and -include are specified");
    }
 catch (    IllegalArgumentException e) {
    }
    parameters=new String[]{"-blockpools"};
    try {
      Balancer.Cli.parse(parameters);
      fail("IllegalArgumentException is expected when a value " + "is not specified for the blockpool flag");
    }
 catch (    IllegalArgumentException e) {
    }
    parameters=new String[]{"-source"};
    try {
      Balancer.Cli.parse(parameters);
      fail(reason + " for -source parameter");
    }
 catch (    IllegalArgumentException ignored) {
    }
  }
  @Test public void testBalancerCliParseBlockpools(){
    String[] parameters=new String[]{"-blockpools","bp-1,bp-2,bp-3"};
    BalancerParameters p=Balancer.Cli.parse(parameters);
    assertEquals(3,p.getBlockPools().size());
    parameters=new String[]{"-blockpools","bp-1"};
    p=Balancer.Cli.parse(parameters);
    assertEquals(1,p.getBlockPools().size());
    parameters=new String[]{"-blockpools","bp-1,,bp-2"};
    p=Balancer.Cli.parse(parameters);
    assertEquals(3,p.getBlockPools().size());
    parameters=new String[]{"-blockpools","bp-1,"};
    p=Balancer.Cli.parse(parameters);
    assertEquals(1,p.getBlockPools().size());
  }
  /** 
 * Verify balancer exits 0 on success.
 */
  @Test(timeout=100000) public void testExitZeroOnSuccess() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    oneNodeTest(conf,true);
  }
  /** 
 * Test a cluster with even distribution, then three nodes are added to the cluster, runs balancer with two of the nodes in the exclude list
 */
  @Test(timeout=100000) public void testBalancerWithExcludeList() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    Set<String> excludeHosts=new HashSet<String>();
    excludeHosts.add("datanodeY");
    excludeHosts.add("datanodeZ");
    doTest(conf,new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1},CAPACITY,RACK2,new HostNameBasedNodes(new String[]{"datanodeX","datanodeY","datanodeZ"},excludeHosts,BalancerParameters.DEFAULT.getIncludedNodes()),false,false);
  }
  /** 
 * Test a cluster with even distribution, then three nodes are added to the cluster, runs balancer with two of the nodes in the exclude list
 */
  @Test(timeout=100000) public void testBalancerWithExcludeListWithPorts() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    doTest(conf,new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1},CAPACITY,RACK2,new PortNumberBasedNodes(3,2,0),false,false);
  }
  /** 
 * Test a cluster with even distribution, then three nodes are added to the cluster, runs balancer with two of the nodes in the exclude list
 */
  @Test(timeout=100000) public void testBalancerCliWithExcludeList() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    Set<String> excludeHosts=new HashSet<String>();
    excludeHosts.add("datanodeY");
    excludeHosts.add("datanodeZ");
    doTest(conf,new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1},CAPACITY,RACK2,new HostNameBasedNodes(new String[]{"datanodeX","datanodeY","datanodeZ"},excludeHosts,BalancerParameters.DEFAULT.getIncludedNodes()),true,false);
  }
  /** 
 * Test a cluster with even distribution, then three nodes are added to the cluster, runs balancer with two of the nodes in the exclude list
 */
  @Test(timeout=100000) public void testBalancerCliWithExcludeListWithPorts() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    doTest(conf,new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1},CAPACITY,RACK2,new PortNumberBasedNodes(3,2,0),true,false);
  }
  /** 
 * Test a cluster with even distribution, then three nodes are added to the cluster, runs balancer with two of the nodes in the exclude list in a file
 */
  @Test(timeout=100000) public void testBalancerCliWithExcludeListInAFile() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    Set<String> excludeHosts=new HashSet<String>();
    excludeHosts.add("datanodeY");
    excludeHosts.add("datanodeZ");
    doTest(conf,new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1},CAPACITY,RACK2,new HostNameBasedNodes(new String[]{"datanodeX","datanodeY","datanodeZ"},excludeHosts,BalancerParameters.DEFAULT.getIncludedNodes()),true,true);
  }
  /** 
 * Test a cluster with even distribution,G then three nodes are added to the cluster, runs balancer with two of the nodes in the exclude list
 */
  @Test(timeout=100000) public void testBalancerCliWithExcludeListWithPortsInAFile() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    doTest(conf,new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1},CAPACITY,RACK2,new PortNumberBasedNodes(3,2,0),true,true);
  }
  /** 
 * Test a cluster with even distribution, then three nodes are added to the cluster, runs balancer with two of the nodes in the include list
 */
  @Test(timeout=100000) public void testBalancerWithIncludeList() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    Set<String> includeHosts=new HashSet<String>();
    includeHosts.add("datanodeY");
    doTest(conf,new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1},CAPACITY,RACK2,new HostNameBasedNodes(new String[]{"datanodeX","datanodeY","datanodeZ"},BalancerParameters.DEFAULT.getExcludedNodes(),includeHosts),false,false);
  }
  /** 
 * Test a cluster with even distribution, then three nodes are added to the cluster, runs balancer with two of the nodes in the include list
 */
  @Test(timeout=100000) public void testBalancerWithIncludeListWithPorts() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    doTest(conf,new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1},CAPACITY,RACK2,new PortNumberBasedNodes(3,0,1),false,false);
  }
  /** 
 * Test a cluster with even distribution, then three nodes are added to the cluster, runs balancer with two of the nodes in the include list
 */
  @Test(timeout=100000) public void testBalancerCliWithIncludeList() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    Set<String> includeHosts=new HashSet<String>();
    includeHosts.add("datanodeY");
    doTest(conf,new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1},CAPACITY,RACK2,new HostNameBasedNodes(new String[]{"datanodeX","datanodeY","datanodeZ"},BalancerParameters.DEFAULT.getExcludedNodes(),includeHosts),true,false);
  }
  /** 
 * Test a cluster with even distribution, then three nodes are added to the cluster, runs balancer with two of the nodes in the include list
 */
  @Test(timeout=100000) public void testBalancerCliWithIncludeListWithPorts() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    doTest(conf,new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1},CAPACITY,RACK2,new PortNumberBasedNodes(3,0,1),true,false);
  }
  /** 
 * Test a cluster with even distribution, then three nodes are added to the cluster, runs balancer with two of the nodes in the include list
 */
  @Test(timeout=100000) public void testBalancerCliWithIncludeListInAFile() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    Set<String> includeHosts=new HashSet<String>();
    includeHosts.add("datanodeY");
    doTest(conf,new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1},CAPACITY,RACK2,new HostNameBasedNodes(new String[]{"datanodeX","datanodeY","datanodeZ"},BalancerParameters.DEFAULT.getExcludedNodes(),includeHosts),true,true);
  }
  /** 
 * Test a cluster with even distribution, then three nodes are added to the cluster, runs balancer with two of the nodes in the include list
 */
  @Test(timeout=100000) public void testBalancerCliWithIncludeListWithPortsInAFile() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    doTest(conf,new long[]{CAPACITY,CAPACITY},new String[]{RACK0,RACK1},CAPACITY,RACK2,new PortNumberBasedNodes(3,0,1),true,true);
  }
  @Test(timeout=100000) public void testMaxIterationTime() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    int blockSize=10 * 1024 * 1024;
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,blockSize);
    conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,blockSize);
    conf.setInt(DFSConfigKeys.DFS_BALANCER_MOVERTHREADS_KEY,1);
    conf.setLong(DFSConfigKeys.DFS_DATANODE_BALANCE_BANDWIDTHPERSEC_KEY,64 * 1024);
    conf.setLong(DFSConfigKeys.DFS_CLIENT_SOCKET_TIMEOUT_KEY,2000L);
    conf.setLong(DFSConfigKeys.DFS_BALANCER_MAX_ITERATION_TIME_KEY,2000L);
    final long capacity=10L * blockSize;
    final long[] dnCapacities=new long[]{capacity,capacity};
    final short rep=1;
    final long seed=0xFAFAFA;
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
    try {
      cluster.getConfiguration(0).setInt(DFSConfigKeys.DFS_REPLICATION_KEY,1);
      conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY,1);
      cluster.startDataNodes(conf,1,true,null,null,dnCapacities);
      cluster.waitClusterUp();
      cluster.waitActive();
      final Path path=new Path("/testMaxIterationTime.dat");
      DistributedFileSystem fs=cluster.getFileSystem();
      DFSTestUtil.createFile(fs,path,4L * blockSize,rep,seed);
      cluster.startDataNodes(conf,1,true,null,null,dnCapacities);
      cluster.triggerHeartbeats();
      List<NameNodeConnector> connectors=Collections.emptyList();
      try {
        BalancerParameters bParams=BalancerParameters.DEFAULT;
        connectors=NameNodeConnector.newNameNodeConnectors(DFSUtil.getInternalNsRpcUris(conf),Balancer.class.getSimpleName(),Balancer.BALANCER_ID_PATH,conf,bParams.getMaxIdleIteration());
        for (        NameNodeConnector nnc : connectors) {
          LOG.info("NNC to work on: " + nnc);
          Balancer b=new Balancer(nnc,bParams,conf);
          long startTime=Time.monotonicNow();
          Result r=b.runOneIteration();
          long runtime=Time.monotonicNow() - startTime;
          assertEquals("We expect ExitStatus.IN_PROGRESS to be reported.",ExitStatus.IN_PROGRESS,r.exitStatus);
          assertTrue("Unexpected iteration runtime: " + runtime + "ms > 3.5s",runtime < 3500);
        }
      }
  finally {
        for (        NameNodeConnector nnc : connectors) {
          IOUtils.cleanupWithLogger(null,nnc);
        }
      }
    }
  finally {
      cluster.shutdown(true,true);
    }
  }
  @Test(timeout=300000) public void testBalancerWithRamDisk() throws Exception {
    final int SEED=0xFADED;
    final short REPL_FACT=1;
    Configuration conf=new Configuration();
    final int defaultRamDiskCapacity=10;
    final long ramDiskStorageLimit=((long)defaultRamDiskCapacity * DEFAULT_RAM_DISK_BLOCK_SIZE) + (DEFAULT_RAM_DISK_BLOCK_SIZE - 1);
    final long diskStorageLimit=((long)defaultRamDiskCapacity * DEFAULT_RAM_DISK_BLOCK_SIZE) + (DEFAULT_RAM_DISK_BLOCK_SIZE - 1);
    initConfWithRamDisk(conf,ramDiskStorageLimit);
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).storageCapacities(new long[]{ramDiskStorageLimit,diskStorageLimit}).storageTypes(new StorageType[]{RAM_DISK,DEFAULT}).build();
    cluster.waitActive();
    final String METHOD_NAME=GenericTestUtils.getMethodName();
    final Path path1=new Path("/" + METHOD_NAME + ".01.dat");
    final Path path2=new Path("/" + METHOD_NAME + ".02.dat");
    DistributedFileSystem fs=cluster.getFileSystem();
    DFSClient client=fs.getClient();
    DFSTestUtil.createFile(fs,path1,true,DEFAULT_RAM_DISK_BLOCK_SIZE,4 * DEFAULT_RAM_DISK_BLOCK_SIZE,DEFAULT_RAM_DISK_BLOCK_SIZE,REPL_FACT,SEED,true);
    DFSTestUtil.createFile(fs,path2,true,DEFAULT_RAM_DISK_BLOCK_SIZE,1 * DEFAULT_RAM_DISK_BLOCK_SIZE,DEFAULT_RAM_DISK_BLOCK_SIZE,REPL_FACT,SEED,true);
    Thread.sleep(6 * 1000);
    StorageType[][] storageTypes=new StorageType[][]{{RAM_DISK,DEFAULT}};
    long[][] storageCapacities=new long[][]{{ramDiskStorageLimit,diskStorageLimit}};
    cluster.startDataNodes(conf,REPL_FACT,storageTypes,true,null,null,null,storageCapacities,null,false,false,false,null);
    cluster.triggerHeartbeats();
    Collection<URI> namenodes=DFSUtil.getInternalNsRpcUris(conf);
    final BalancerParameters p=BalancerParameters.DEFAULT;
    final int r=Balancer.run(namenodes,p,conf);
    assertEquals(ExitStatus.NO_MOVE_PROGRESS.getExitCode(),r);
    DFSTestUtil.verifyFileReplicasOnStorageType(fs,client,path1,RAM_DISK);
    DFSTestUtil.verifyFileReplicasOnStorageType(fs,client,path2,RAM_DISK);
  }
  /** 
 * Check that the balancer exits when there is an unfinalized upgrade.
 */
  @Test(timeout=300000) public void testBalancerDuringUpgrade() throws Exception {
    final int SEED=0xFADED;
    Configuration conf=new HdfsConfiguration();
    conf.setLong(DFS_HEARTBEAT_INTERVAL_KEY,1);
    conf.setInt(DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY,500);
    conf.setLong(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,1);
    conf.setLong(DFSConfigKeys.DFS_BALANCER_GETBLOCKS_MIN_BLOCK_SIZE_KEY,1L);
    final int BLOCK_SIZE=1024 * 1024;
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).storageCapacities(new long[]{BLOCK_SIZE * 10}).storageTypes(new StorageType[]{DEFAULT}).storagesPerDatanode(1).build();
    cluster.waitActive();
    final String METHOD_NAME=GenericTestUtils.getMethodName();
    final Path path1=new Path("/" + METHOD_NAME + ".01.dat");
    DistributedFileSystem fs=cluster.getFileSystem();
    DFSTestUtil.createFile(fs,path1,BLOCK_SIZE,BLOCK_SIZE * 2,BLOCK_SIZE,(short)1,SEED);
    cluster.startDataNodes(conf,1,true,null,null);
    cluster.triggerHeartbeats();
    Collection<URI> namenodes=DFSUtil.getInternalNsRpcUris(conf);
    final BalancerParameters p=BalancerParameters.DEFAULT;
    fs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER);
    fs.rollingUpgrade(HdfsConstants.RollingUpgradeAction.PREPARE);
    fs.setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE);
    assertEquals(ExitStatus.UNFINALIZED_UPGRADE.getExitCode(),Balancer.run(namenodes,p,conf));
    BalancerParameters.Builder b=new BalancerParameters.Builder();
    b.setRunDuringUpgrade(true);
    final BalancerParameters runDuringUpgrade=b.build();
    assertEquals(ExitStatus.SUCCESS.getExitCode(),Balancer.run(namenodes,runDuringUpgrade,conf));
    fs.rollingUpgrade(HdfsConstants.RollingUpgradeAction.FINALIZE);
    assertEquals(ExitStatus.SUCCESS.getExitCode(),Balancer.run(namenodes,p,conf));
  }
  /** 
 * Test special case. Two replicas belong to same block should not in same node. We have 2 nodes. We have a block in (DN0,SSD) and (DN1,DISK). Replica in (DN0,SSD) should not be moved to (DN1,SSD). Otherwise DN1 has 2 replicas.
 */
  @Test(timeout=100000) public void testTwoReplicaShouldNotInSameDN() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    int blockSize=5 * 1024 * 1024;
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,blockSize);
    conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1L);
    conf.setLong(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,1L);
    conf.setLong(DFSConfigKeys.DFS_BALANCER_GETBLOCKS_MIN_BLOCK_SIZE_KEY,1L);
    int numOfDatanodes=2;
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(2).racks(new String[]{"/default/rack0","/default/rack0"}).storagesPerDatanode(2).storageTypes(new StorageType[][]{{StorageType.SSD,StorageType.DISK},{StorageType.SSD,StorageType.DISK}}).storageCapacities(new long[][]{{100 * blockSize,20 * blockSize},{20 * blockSize,100 * blockSize}}).build();
    cluster.waitActive();
    DistributedFileSystem fs=cluster.getFileSystem();
    Path barDir=new Path("/bar");
    fs.mkdir(barDir,new FsPermission((short)777));
    fs.setStoragePolicy(barDir,HdfsConstants.ONESSD_STORAGE_POLICY_NAME);
    long fileLen=30 * blockSize;
    Path fooFile=new Path(barDir,"foo");
    createFile(cluster,fooFile,fileLen,(short)numOfDatanodes,0);
    cluster.triggerHeartbeats();
    BalancerParameters p=BalancerParameters.DEFAULT;
    Collection<URI> namenodes=DFSUtil.getInternalNsRpcUris(conf);
    final int r=Balancer.run(namenodes,p,conf);
    assertEquals(ExitStatus.NO_MOVE_PROGRESS.getExitCode(),r);
  }
  /** 
 * Test running many balancer simultaneously. Case-1: First balancer is running. Now, running second one should get "Another balancer is running. Exiting.." IOException and fail immediately Case-2: When running second balancer 'balancer.id' file exists but the lease doesn't exists. Now, the second balancer should run successfully.
 */
  @Test(timeout=100000) public void testManyBalancerSimultaneously() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    long[] capacities=new long[]{4 * CAPACITY};
    String[] racks=new String[]{RACK0};
    long newCapacity=2 * CAPACITY;
    String newRack=RACK0;
    LOG.info("capacities = " + long2String(capacities));
    LOG.info("racks      = " + Arrays.asList(racks));
    LOG.info("newCapacity= " + newCapacity);
    LOG.info("newRack    = " + newRack);
    LOG.info("useTool    = " + false);
    assertEquals(capacities.length,racks.length);
    int numOfDatanodes=capacities.length;
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(capacities.length).racks(racks).simulatedCapacities(capacities).build();
    cluster.waitActive();
    client=NameNodeProxies.createProxy(conf,cluster.getFileSystem(0).getUri(),ClientProtocol.class).getProxy();
    long totalCapacity=sum(capacities);
    final long totalUsedSpace=totalCapacity * 3 / 10;
    createFile(cluster,filePath,totalUsedSpace / numOfDatanodes,(short)numOfDatanodes,0);
    cluster.startDataNodes(conf,1,true,null,new String[]{newRack},new long[]{newCapacity});
    FileSystem fs=cluster.getFileSystem(0);
    final FSDataOutputStream out=fs.create(Balancer.BALANCER_ID_PATH,false);
    out.writeBytes(InetAddress.getLocalHost().getHostName());
    out.hflush();
    assertTrue("'balancer.id' file doesn't exist!",fs.exists(Balancer.BALANCER_ID_PATH));
    final String[] args={"-policy","datanode"};
    final Tool tool=new Cli();
    tool.setConf(conf);
    int exitCode=tool.run(args);
    assertEquals("Exit status code mismatches",ExitStatus.IO_EXCEPTION.getExitCode(),exitCode);
    out.close();
    assertTrue("'balancer.id' file doesn't exist!",fs.exists(Balancer.BALANCER_ID_PATH));
    exitCode=tool.run(args);
    assertEquals("Exit status code mismatches",ExitStatus.SUCCESS.getExitCode(),exitCode);
  }
  /** 
 * Balancer should not move blocks with size < minBlockSize. 
 */
  @Test(timeout=60000) public void testMinBlockSizeAndSourceNodes() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    final short replication=3;
    final long[] lengths={10,10,10,10};
    final long[] capacities=new long[replication];
    final long totalUsed=capacities.length * sum(lengths);
    Arrays.fill(capacities,1000);
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(capacities.length).simulatedCapacities(capacities).build();
    final DistributedFileSystem dfs=cluster.getFileSystem();
    cluster.waitActive();
    client=NameNodeProxies.createProxy(conf,dfs.getUri(),ClientProtocol.class).getProxy();
    for (int i=0; i < lengths.length; i++) {
      final long size=lengths[i];
      final Path p=new Path("/file" + i + "_size"+ size);
      try (OutputStream out=dfs.create(p)){
        for (int j=0; j < size; j++) {
          out.write(j);
        }
      }
     }
    cluster.startDataNodes(conf,capacities.length,true,null,null,capacities);
    LOG.info("capacities    = " + Arrays.toString(capacities));
    LOG.info("totalUsedSpace= " + totalUsed);
    LOG.info("lengths       = " + Arrays.toString(lengths) + ", #="+ lengths.length);
    waitForHeartBeat(totalUsed,2 * capacities[0] * capacities.length,client,cluster);
    final Collection<URI> namenodes=DFSUtil.getInternalNsRpcUris(conf);
{
      final BalancerParameters p=Balancer.Cli.parse(new String[]{"-policy",BalancingPolicy.Node.INSTANCE.getName(),"-threshold","1"});
      assertEquals(p.getBalancingPolicy(),BalancingPolicy.Node.INSTANCE);
      assertEquals(p.getThreshold(),1.0,0.001);
      conf.setLong(DFSConfigKeys.DFS_BALANCER_GETBLOCKS_MIN_BLOCK_SIZE_KEY,50);
      final int r=Balancer.run(namenodes,p,conf);
      assertEquals(ExitStatus.NO_MOVE_PROGRESS.getExitCode(),r);
    }
    conf.setLong(DFSConfigKeys.DFS_BALANCER_GETBLOCKS_MIN_BLOCK_SIZE_KEY,1);
{
      final Set<String> sourceNodes=new HashSet<>();
      final List<DataNode> datanodes=cluster.getDataNodes();
      for (int i=capacities.length; i < datanodes.size(); i++) {
        sourceNodes.add(datanodes.get(i).getDisplayName());
      }
      final BalancerParameters p=Balancer.Cli.parse(new String[]{"-policy",BalancingPolicy.Node.INSTANCE.getName(),"-threshold","1","-source",StringUtils.join(sourceNodes,',')});
      assertEquals(p.getBalancingPolicy(),BalancingPolicy.Node.INSTANCE);
      assertEquals(p.getThreshold(),1.0,0.001);
      assertEquals(p.getSourceNodes(),sourceNodes);
      conf.setLong(DFSConfigKeys.DFS_BALANCER_GETBLOCKS_MIN_BLOCK_SIZE_KEY,50);
      final int r=Balancer.run(namenodes,p,conf);
      assertEquals(ExitStatus.NO_MOVE_BLOCK.getExitCode(),r);
    }
{
      final Set<String> sourceNodes=new HashSet<>();
      final List<DataNode> datanodes=cluster.getDataNodes();
      sourceNodes.add(datanodes.get(0).getDisplayName());
      final BalancerParameters p=Balancer.Cli.parse(new String[]{"-policy",BalancingPolicy.Node.INSTANCE.getName(),"-threshold","1","-source",StringUtils.join(sourceNodes,',')});
      assertEquals(p.getBalancingPolicy(),BalancingPolicy.Node.INSTANCE);
      assertEquals(p.getThreshold(),1.0,0.001);
      assertEquals(p.getSourceNodes(),sourceNodes);
      conf.setLong(DFSConfigKeys.DFS_BALANCER_GETBLOCKS_MIN_BLOCK_SIZE_KEY,1);
      final int r=Balancer.run(namenodes,p,conf);
      assertEquals(ExitStatus.NO_MOVE_BLOCK.getExitCode(),r);
    }
{
      final Set<String> sourceNodes=new HashSet<>();
      final List<DataNode> datanodes=cluster.getDataNodes();
      for (int i=0; i < capacities.length; i++) {
        sourceNodes.add(datanodes.get(i).getDisplayName());
      }
      final BalancerParameters p=Balancer.Cli.parse(new String[]{"-policy",BalancingPolicy.Node.INSTANCE.getName(),"-threshold","1","-source",StringUtils.join(sourceNodes,',')});
      assertEquals(p.getBalancingPolicy(),BalancingPolicy.Node.INSTANCE);
      assertEquals(p.getThreshold(),1.0,0.001);
      assertEquals(p.getSourceNodes(),sourceNodes);
      conf.setLong(DFSConfigKeys.DFS_BALANCER_GETBLOCKS_MIN_BLOCK_SIZE_KEY,1);
      final int r=Balancer.run(namenodes,p,conf);
      assertEquals(ExitStatus.SUCCESS.getExitCode(),r);
    }
  }
  public void integrationTestWithStripedFile(  Configuration conf) throws Exception {
    initConfWithStripe(conf);
    doTestBalancerWithStripedFile(conf);
  }
  @Test(timeout=200000) public void testBalancerWithStripedFile() throws Exception {
    Configuration conf=new Configuration();
    initConfWithStripe(conf);
    NameNodeConnector.setWrite2IdFile(true);
    doTestBalancerWithStripedFile(conf);
    NameNodeConnector.setWrite2IdFile(false);
  }
  private void doTestBalancerWithStripedFile(  Configuration conf) throws Exception {
    int numOfDatanodes=dataBlocks + parityBlocks + 3;
    int numOfRacks=dataBlocks;
    long capacity=20 * defaultBlockSize;
    long[] capacities=new long[numOfDatanodes];
    for (int i=0; i < capacities.length; i++) {
      capacities[i]=capacity;
    }
    String[] racks=new String[numOfDatanodes];
    for (int i=0; i < numOfDatanodes; i++) {
      racks[i]="/rack" + (i % numOfRacks);
    }
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numOfDatanodes).racks(racks).simulatedCapacities(capacities).build();
    try {
      cluster.waitActive();
      client=NameNodeProxies.createProxy(conf,cluster.getFileSystem(0).getUri(),ClientProtocol.class).getProxy();
      client.enableErasureCodingPolicy(StripedFileTestUtil.getDefaultECPolicy().getName());
      client.setErasureCodingPolicy("/",StripedFileTestUtil.getDefaultECPolicy().getName());
      long totalCapacity=sum(capacities);
      long fileLen=totalCapacity * 3 / 10;
      long totalUsedSpace=fileLen * (dataBlocks + parityBlocks) / dataBlocks;
      FileSystem fs=cluster.getFileSystem(0);
      DFSTestUtil.createFile(fs,filePath,fileLen,(short)3,r.nextLong());
      LocatedBlocks locatedBlocks=client.getBlockLocations(fileName,0,fileLen);
      StripedFileTestUtil.verifyLocatedStripedBlocks(locatedBlocks,groupSize);
      String newRack="/rack" + (++numOfRacks);
      cluster.startDataNodes(conf,2,true,null,new String[]{newRack,newRack},null,new long[]{capacity,capacity});
      totalCapacity+=capacity * 2;
      cluster.triggerHeartbeats();
      BalancerParameters p=BalancerParameters.DEFAULT;
      runBalancer(conf,totalUsedSpace,totalCapacity,p,0);
      locatedBlocks=client.getBlockLocations(fileName,0,fileLen);
      StripedFileTestUtil.verifyLocatedStripedBlocks(locatedBlocks,groupSize);
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Test Balancer runs fine when logging in with a keytab in kerberized env. Reusing testUnknownDatanode here for basic functionality testing.
 */
  @Test(timeout=300000) public void testBalancerWithKeytabs() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    try {
      initSecureConf(conf);
      final UserGroupInformation ugi=UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal,keytabFile.getAbsolutePath());
      ugi.doAs(new PrivilegedExceptionAction<Void>(){
        @Override public Void run() throws Exception {
          testUnknownDatanode(conf);
          assertTrue(UserGroupInformation.isLoginKeytabBased());
          return null;
        }
      }
);
    }
  finally {
      UserGroupInformation.reset();
      UserGroupInformation.setConfiguration(new Configuration());
    }
  }
  private static int numGetBlocksCalls;
  private static long startGetBlocksTime, endGetBlocksTime;
  private void spyFSNamesystem(  NameNode nn) throws IOException {
    FSNamesystem fsnSpy=NameNodeAdapter.spyOnNamesystem(nn);
    numGetBlocksCalls=0;
    endGetBlocksTime=startGetBlocksTime=Time.monotonicNow();
    doAnswer(new Answer<BlocksWithLocations>(){
      @Override public BlocksWithLocations answer(      InvocationOnMock invocation) throws Throwable {
        BlocksWithLocations blk=(BlocksWithLocations)invocation.callRealMethod();
        endGetBlocksTime=Time.monotonicNow();
        numGetBlocksCalls++;
        return blk;
      }
    }
).when(fsnSpy).getBlocks(any(DatanodeID.class),anyLong(),anyLong());
  }
  /** 
 * Test that makes the Balancer to disperse RPCs to the NameNode in order to avoid NN's RPC queue saturation.
 */
  void testBalancerRPCDelay() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    initConf(conf);
    conf.setInt(DFSConfigKeys.DFS_BALANCER_DISPATCHERTHREADS_KEY,30);
    int numDNs=20;
    long[] capacities=new long[numDNs];
    String[] racks=new String[numDNs];
    for (int i=0; i < numDNs; i++) {
      capacities[i]=CAPACITY;
      racks[i]=(i < numDNs / 2 ? RACK0 : RACK1);
    }
    doTest(conf,capacities,racks,CAPACITY,RACK2,new PortNumberBasedNodes(3,0,0),false,false,true);
    assertTrue("Number of getBlocks should be not less than " + Dispatcher.BALANCER_NUM_RPC_PER_SEC,numGetBlocksCalls > Dispatcher.BALANCER_NUM_RPC_PER_SEC);
    long d=1 + endGetBlocksTime - startGetBlocksTime;
    LOG.info("Balancer executed " + numGetBlocksCalls + " getBlocks in "+ d+ " msec.");
    assertTrue("Expected BALANCER_NUM_RPC_PER_SEC = " + Dispatcher.BALANCER_NUM_RPC_PER_SEC,(numGetBlocksCalls * 1000 / d) < Dispatcher.BALANCER_NUM_RPC_PER_SEC);
  }
  /** 
 * @param args
 */
  public static void main(  String[] args) throws Exception {
    TestBalancer balancerTest=new TestBalancer();
    balancerTest.testBalancer0();
    balancerTest.testBalancer1();
    balancerTest.testBalancer2();
  }
}
