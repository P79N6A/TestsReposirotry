/** 
 * This class tests the creation and validation of a checkpoint.
 */
public class TestCheckpoint {
static {
    GenericTestUtils.setLogLevel(FSImage.LOG,Level.ALL);
  }
  static final Log LOG=LogFactory.getLog(TestCheckpoint.class);
  static final String NN_METRICS="NameNodeActivity";
  static final long seed=0xDEADBEEFL;
  static final int blockSize=4096;
  static final int fileSize=8192;
  static final int numDatanodes=3;
  short replication=3;
  static final FilenameFilter tmpEditsFilter=new FilenameFilter(){
    @Override public boolean accept(    File dir,    String name){
      return name.startsWith(NameNodeFile.EDITS_TMP.getName());
    }
  }
;
  private CheckpointFaultInjector faultInjector;
  @Before public void setUp(){
    FileUtil.fullyDeleteContents(new File(MiniDFSCluster.getBaseDirectory()));
    faultInjector=Mockito.mock(CheckpointFaultInjector.class);
    CheckpointFaultInjector.instance=faultInjector;
  }
  @After public void checkForSNNThreads(){
    GenericTestUtils.assertNoThreadsMatching(".*SecondaryNameNode.*");
  }
  static void checkFile(  FileSystem fileSys,  Path name,  int repl) throws IOException {
    assertTrue(fileSys.exists(name));
    int replication=fileSys.getFileStatus(name).getReplication();
    assertEquals("replication for " + name,repl,replication);
  }
  static void cleanupFile(  FileSystem fileSys,  Path name) throws IOException {
    assertTrue(fileSys.exists(name));
    fileSys.delete(name,true);
    assertTrue(!fileSys.exists(name));
  }
  @Test public void testNameDirError() throws IOException {
    LOG.info("Starting testNameDirError");
    Configuration conf=new HdfsConfiguration();
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
    Collection<URI> nameDirs=cluster.getNameDirs(0);
    cluster.shutdown();
    cluster=null;
    for (    URI nameDirUri : nameDirs) {
      File dir=new File(nameDirUri.getPath());
      try {
        FileUtil.setWritable(dir,false);
        cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).format(false).build();
        fail("NN should have failed to start with " + dir + " set unreadable");
      }
 catch (      IOException ioe) {
        GenericTestUtils.assertExceptionContains("storage directory does not exist or is not accessible",ioe);
      }
 finally {
        cleanup(cluster);
        cluster=null;
        FileUtil.setWritable(dir,true);
      }
    }
  }
  /** 
 * Checks that an IOException in NNStorage.writeTransactionIdFile is handled correctly (by removing the storage directory) See https://issues.apache.org/jira/browse/HDFS-2011
 */
  @Test public void testWriteTransactionIdHandlesIOE() throws Exception {
    LOG.info("Check IOException handled correctly by writeTransactionIdFile");
    ArrayList<URI> fsImageDirs=new ArrayList<URI>();
    ArrayList<URI> editsDirs=new ArrayList<URI>();
    File filePath=new File(PathUtils.getTestDir(getClass()),"storageDirToCheck");
    assertTrue("Couldn't create directory storageDirToCheck",filePath.exists() || filePath.mkdirs());
    fsImageDirs.add(filePath.toURI());
    editsDirs.add(filePath.toURI());
    NNStorage nnStorage=new NNStorage(new HdfsConfiguration(),fsImageDirs,editsDirs);
    try {
      assertTrue("List of storage directories didn't have storageDirToCheck.",nnStorage.getEditsDirectories().iterator().next().toString().indexOf("storageDirToCheck") != -1);
      assertTrue("List of removed storage directories wasn't empty",nnStorage.getRemovedStorageDirs().isEmpty());
    }
  finally {
      assertTrue("Couldn't remove directory " + filePath.getAbsolutePath(),filePath.delete());
    }
    nnStorage.writeTransactionIdFileToStorage(1);
    List<StorageDirectory> listRsd=nnStorage.getRemovedStorageDirs();
    assertTrue("Removed directory wasn't what was expected",listRsd.size() > 0 && listRsd.get(listRsd.size() - 1).getRoot().toString().indexOf("storageDirToCheck") != -1);
    nnStorage.close();
  }
  @Test(timeout=30000) public void testReloadOnEditReplayFailure() throws IOException {
    Configuration conf=new HdfsConfiguration();
    FSDataOutputStream fos=null;
    SecondaryNameNode secondary=null;
    MiniDFSCluster cluster=null;
    FileSystem fs=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();
      cluster.waitActive();
      fs=cluster.getFileSystem();
      secondary=startSecondaryNameNode(conf);
      fos=fs.create(new Path("tmpfile0"));
      fos.write(new byte[]{0,1,2,3});
      secondary.doCheckpoint();
      fos.write(new byte[]{0,1,2,3});
      fos.hsync();
      Mockito.doThrow(new IOException("Injecting failure during merge")).when(faultInjector).duringMerge();
      try {
        secondary.doCheckpoint();
        fail("Fault injection failed.");
      }
 catch (      IOException ioe) {
      }
      Mockito.reset(faultInjector);
      fos.write(new byte[]{0,1,2,3});
      fos.hsync();
      assertTrue("Another checkpoint should have reloaded image",secondary.doCheckpoint());
    }
  finally {
      if (fs != null) {
        fs.close();
      }
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
      Mockito.reset(faultInjector);
    }
  }
  @Test(timeout=30000) public void testTooManyEditReplayFailures() throws IOException {
    Configuration conf=new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_MAX_RETRIES_KEY,1);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_KEY,1);
    FSDataOutputStream fos=null;
    SecondaryNameNode secondary=null;
    MiniDFSCluster cluster=null;
    FileSystem fs=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).checkExitOnShutdown(false).build();
      cluster.waitActive();
      fs=cluster.getFileSystem();
      fos=fs.create(new Path("tmpfile0"));
      fos.write(new byte[]{0,1,2,3});
      Mockito.doThrow(new IOException("Injecting failure during merge")).when(faultInjector).duringMerge();
      secondary=startSecondaryNameNode(conf);
      secondary.doWork();
      fail("2NN did not exit.");
    }
 catch (    ExitException ee) {
      ExitUtil.resetFirstExitException();
      assertEquals("Max retries",1,secondary.getMergeErrorCount() - 1);
    }
 finally {
      if (fs != null) {
        fs.close();
      }
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
      Mockito.reset(faultInjector);
    }
  }
  @Test public void testSecondaryNamenodeError1() throws IOException {
    LOG.info("Starting testSecondaryNamenodeError1");
    Configuration conf=new HdfsConfiguration();
    Path file1=new Path("checkpointxx.dat");
    MiniDFSCluster cluster=null;
    FileSystem fileSys=null;
    SecondaryNameNode secondary=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();
      cluster.waitActive();
      fileSys=cluster.getFileSystem();
      assertTrue(!fileSys.exists(file1));
      secondary=startSecondaryNameNode(conf);
      Mockito.doThrow(new IOException("Injecting failure after rolling edit logs")).when(faultInjector).afterSecondaryCallsRollEditLog();
      try {
        secondary.doCheckpoint();
        assertTrue(false);
      }
 catch (      IOException e) {
      }
      Mockito.reset(faultInjector);
      DFSTestUtil.createFile(fileSys,file1,fileSize,fileSize,blockSize,replication,seed);
      checkFile(fileSys,file1,replication);
    }
  finally {
      fileSys.close();
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(false).build();
      cluster.waitActive();
      fileSys=cluster.getFileSystem();
      checkFile(fileSys,file1,replication);
      cleanupFile(fileSys,file1);
      secondary=startSecondaryNameNode(conf);
      secondary.doCheckpoint();
      secondary.shutdown();
    }
  finally {
      fileSys.close();
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  @Test public void testSecondaryNamenodeError2() throws IOException {
    LOG.info("Starting testSecondaryNamenodeError2");
    Configuration conf=new HdfsConfiguration();
    Path file1=new Path("checkpointyy.dat");
    MiniDFSCluster cluster=null;
    FileSystem fileSys=null;
    SecondaryNameNode secondary=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();
      cluster.waitActive();
      fileSys=cluster.getFileSystem();
      assertTrue(!fileSys.exists(file1));
      secondary=startSecondaryNameNode(conf);
      Mockito.doThrow(new IOException("Injecting failure after uploading new image")).when(faultInjector).afterSecondaryUploadsNewImage();
      try {
        secondary.doCheckpoint();
        assertTrue(false);
      }
 catch (      IOException e) {
      }
      Mockito.reset(faultInjector);
      DFSTestUtil.createFile(fileSys,file1,fileSize,fileSize,blockSize,replication,seed);
      checkFile(fileSys,file1,replication);
    }
  finally {
      fileSys.close();
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(false).build();
      cluster.waitActive();
      fileSys=cluster.getFileSystem();
      checkFile(fileSys,file1,replication);
      cleanupFile(fileSys,file1);
      secondary=startSecondaryNameNode(conf);
      secondary.doCheckpoint();
      secondary.shutdown();
    }
  finally {
      fileSys.close();
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  @Test public void testSecondaryNamenodeError3() throws IOException {
    LOG.info("Starting testSecondaryNamenodeError3");
    Configuration conf=new HdfsConfiguration();
    Path file1=new Path("checkpointzz.dat");
    MiniDFSCluster cluster=null;
    FileSystem fileSys=null;
    SecondaryNameNode secondary=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();
      cluster.waitActive();
      fileSys=cluster.getFileSystem();
      assertTrue(!fileSys.exists(file1));
      secondary=startSecondaryNameNode(conf);
      Mockito.doThrow(new IOException("Injecting failure after rolling edit logs")).when(faultInjector).afterSecondaryCallsRollEditLog();
      try {
        secondary.doCheckpoint();
        assertTrue(false);
      }
 catch (      IOException e) {
      }
      Mockito.reset(faultInjector);
      secondary.shutdown();
      secondary=startSecondaryNameNode(conf);
      secondary.doCheckpoint();
      DFSTestUtil.createFile(fileSys,file1,fileSize,fileSize,blockSize,replication,seed);
      checkFile(fileSys,file1,replication);
    }
  finally {
      fileSys.close();
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(false).build();
      cluster.waitActive();
      fileSys=cluster.getFileSystem();
      checkFile(fileSys,file1,replication);
      cleanupFile(fileSys,file1);
      secondary=startSecondaryNameNode(conf);
      secondary.doCheckpoint();
      secondary.shutdown();
    }
  finally {
      fileSys.close();
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Simulate a secondary node failure to transfer image. Uses an unchecked error and fail transfer before even setting the length header. This used to cause image truncation. Regression test for HDFS-3330.
 */
  @Test public void testSecondaryFailsWithErrorBeforeSettingHeaders() throws IOException {
    Mockito.doThrow(new Error("If this exception is not caught by the " + "name-node, fs image will be truncated.")).when(faultInjector).beforeGetImageSetsHeaders();
    doSecondaryFailsToReturnImage();
  }
  private void doSecondaryFailsToReturnImage() throws IOException {
    LOG.info("Starting testSecondaryFailsToReturnImage");
    Configuration conf=new HdfsConfiguration();
    Path file1=new Path("checkpointRI.dat");
    MiniDFSCluster cluster=null;
    FileSystem fileSys=null;
    FSImage image=null;
    SecondaryNameNode secondary=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();
      cluster.waitActive();
      fileSys=cluster.getFileSystem();
      image=cluster.getNameNode().getFSImage();
      assertTrue(!fileSys.exists(file1));
      StorageDirectory sd=image.getStorage().getStorageDir(0);
      File latestImageBeforeCheckpoint=FSImageTestUtil.findLatestImageFile(sd);
      long fsimageLength=latestImageBeforeCheckpoint.length();
      secondary=startSecondaryNameNode(conf);
      try {
        secondary.doCheckpoint();
        fail("Checkpoint succeeded even though we injected an error!");
      }
 catch (      IOException e) {
        GenericTestUtils.assertExceptionContains("If this exception is not caught",e);
      }
      Mockito.reset(faultInjector);
      for (      StorageDirectory sd2 : image.getStorage().dirIterable(NameNodeDirType.IMAGE)) {
        File thisNewestImage=FSImageTestUtil.findLatestImageFile(sd2);
        long len=thisNewestImage.length();
        assertEquals(fsimageLength,len);
      }
    }
  finally {
      fileSys.close();
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  private File filePathContaining(  final String substring){
    return Mockito.argThat(new ArgumentMatcher<File>(){
      @Override public boolean matches(      Object argument){
        String path=((File)argument).getAbsolutePath();
        return path.contains(substring);
      }
    }
);
  }
  private void checkTempImages(  NNStorage storage) throws IOException {
    List<File> dirs=new ArrayList<File>();
    dirs.add(storage.getStorageDir(0).getCurrentDir());
    dirs.add(storage.getStorageDir(1).getCurrentDir());
    for (    File dir : dirs) {
      File[] list=dir.listFiles();
      for (      File f : list) {
        if (f.getName().contains(NNStorage.NameNodeFile.IMAGE_NEW.getName())) {
          throw new IOException("Found " + f);
        }
      }
    }
  }
  /** 
 * Simulate 2NN failing to send the whole file (error type 3) The length header in the HTTP transfer should prevent this from corrupting the NN.
 */
  @Test public void testNameNodeImageSendFailWrongSize() throws IOException {
    LOG.info("Starting testNameNodeImageSendFailWrongSize");
    Mockito.doReturn(true).when(faultInjector).shouldSendShortFile(filePathContaining("fsimage"));
    doSendFailTest("is not of the advertised size");
  }
  /** 
 * Simulate 2NN sending a corrupt image (error type 4) The digest header in the HTTP transfer should prevent this from corrupting the NN.
 */
  @Test public void testNameNodeImageSendFailWrongDigest() throws IOException {
    LOG.info("Starting testNameNodeImageSendFailWrongDigest");
    Mockito.doReturn(true).when(faultInjector).shouldCorruptAByte(Mockito.any(File.class));
    doSendFailTest("does not match advertised digest");
  }
  /** 
 * Run a test where the 2NN runs into some kind of error when sending the checkpoint back to the NN.
 * @param exceptionSubstring an expected substring of the triggered exception
 */
  private void doSendFailTest(  String exceptionSubstring) throws IOException {
    Configuration conf=new HdfsConfiguration();
    Path file1=new Path("checkpoint-doSendFailTest-doSendFailTest.dat");
    MiniDFSCluster cluster=null;
    FileSystem fileSys=null;
    SecondaryNameNode secondary=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();
      cluster.waitActive();
      fileSys=cluster.getFileSystem();
      assertTrue(!fileSys.exists(file1));
      secondary=startSecondaryNameNode(conf);
      try {
        secondary.doCheckpoint();
        fail("Did not get expected exception");
      }
 catch (      IOException e) {
        GenericTestUtils.assertExceptionContains(exceptionSubstring,e);
      }
      Mockito.reset(faultInjector);
      checkTempImages(cluster.getNameNode().getFSImage().getStorage());
      checkTempImages(secondary.getFSImage().getStorage());
      secondary.shutdown();
      secondary=null;
      secondary=startSecondaryNameNode(conf);
      secondary.doCheckpoint();
      DFSTestUtil.createFile(fileSys,file1,fileSize,fileSize,blockSize,replication,seed);
      checkFile(fileSys,file1,replication);
    }
  finally {
      fileSys.close();
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Test that the NN locks its storage and edits directories, and won't start up if the directories are already locked
 */
  @Test public void testNameDirLocking() throws IOException {
    Configuration conf=new HdfsConfiguration();
    MiniDFSCluster cluster=null;
    StorageDirectory savedSd=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
      NNStorage storage=cluster.getNameNode().getFSImage().getStorage();
      for (      StorageDirectory sd : storage.dirIterable(null)) {
        assertLockFails(sd);
        savedSd=sd;
      }
    }
  finally {
      cleanup(cluster);
      cluster=null;
    }
    assertNotNull(savedSd);
    assertClusterStartFailsWhenDirLocked(conf,savedSd);
  }
  /** 
 * Test that, if the edits dir is separate from the name dir, it is properly locked.
 */
  @Test public void testSeparateEditsDirLocking() throws IOException {
    Configuration conf=new HdfsConfiguration();
    File nameDir=new File(MiniDFSCluster.getBaseDirectory(),"name");
    File editsDir=new File(MiniDFSCluster.getBaseDirectory(),"testSeparateEditsDirLocking");
    conf.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY,nameDir.getAbsolutePath());
    conf.set(DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY,editsDir.getAbsolutePath());
    MiniDFSCluster cluster=null;
    StorageDirectory savedSd=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).manageNameDfsDirs(false).numDataNodes(0).build();
      NNStorage storage=cluster.getNameNode().getFSImage().getStorage();
      for (      StorageDirectory sd : storage.dirIterable(NameNodeDirType.EDITS)) {
        assertEquals(editsDir.getAbsoluteFile(),sd.getRoot());
        assertLockFails(sd);
        savedSd=sd;
      }
    }
  finally {
      cleanup(cluster);
      cluster=null;
    }
    assertNotNull(savedSd);
    assertClusterStartFailsWhenDirLocked(conf,savedSd);
  }
  /** 
 * Test that the SecondaryNameNode properly locks its storage directories.
 */
  @Test public void testSecondaryNameNodeLocking() throws Exception {
    Configuration conf=new HdfsConfiguration();
    MiniDFSCluster cluster=null;
    SecondaryNameNode secondary=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
      StorageDirectory savedSd=null;
      secondary=startSecondaryNameNode(conf);
      NNStorage storage=secondary.getFSImage().getStorage();
      for (      StorageDirectory sd : storage.dirIterable(null)) {
        assertLockFails(sd);
        savedSd=sd;
      }
      LOG.info("===> Shutting down first 2NN");
      secondary.shutdown();
      secondary=null;
      LOG.info("===> Locking a dir, starting second 2NN");
      LOG.info("Trying to lock" + savedSd);
      savedSd.lock();
      try {
        secondary=startSecondaryNameNode(conf);
        assertFalse("Should fail to start 2NN when " + savedSd + " is locked",savedSd.isLockSupported());
      }
 catch (      IOException ioe) {
        GenericTestUtils.assertExceptionContains("already locked",ioe);
      }
 finally {
        savedSd.unlock();
      }
    }
  finally {
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Test that, an attempt to lock a storage that is already locked by nodename, logs error message that includes JVM name of the namenode that locked it.
 */
  @Test public void testStorageAlreadyLockedErrorMessage() throws Exception {
    Configuration conf=new HdfsConfiguration();
    MiniDFSCluster cluster=null;
    StorageDirectory savedSd=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
      NNStorage storage=cluster.getNameNode().getFSImage().getStorage();
      for (      StorageDirectory sd : storage.dirIterable(null)) {
        assertLockFails(sd);
        savedSd=sd;
      }
      LogCapturer logs=GenericTestUtils.LogCapturer.captureLogs(LogFactory.getLog(Storage.class));
      try {
        savedSd.lock();
        fail("Namenode should not be able to lock a storage" + " that is already locked");
      }
 catch (      IOException ioe) {
        String lockingJvmName=Path.WINDOWS ? "" : " " + ManagementFactory.getRuntimeMXBean().getName();
        String expectedLogMessage="It appears that another node " + lockingJvmName + " has already locked the storage directory";
        assertTrue("Log output does not contain expected log message: " + expectedLogMessage,logs.getOutput().contains(expectedLogMessage));
      }
    }
  finally {
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Assert that the given storage directory can't be locked, because it's already locked.
 */
  private static void assertLockFails(  StorageDirectory sd){
    try {
      sd.lock();
      assertFalse(sd.isLockSupported());
    }
 catch (    IOException ioe) {
      GenericTestUtils.assertExceptionContains("already locked",ioe);
    }
  }
  /** 
 * Assert that, if sdToLock is locked, the cluster is not allowed to start up.
 * @param conf cluster conf to use
 * @param sdToLock the storage directory to lock
 */
  private static void assertClusterStartFailsWhenDirLocked(  Configuration conf,  StorageDirectory sdToLock) throws IOException {
    sdToLock.lock();
    MiniDFSCluster cluster=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).format(false).manageNameDfsDirs(false).numDataNodes(0).build();
      assertFalse("cluster should fail to start after locking " + sdToLock,sdToLock.isLockSupported());
    }
 catch (    IOException ioe) {
      GenericTestUtils.assertExceptionContains("already locked",ioe);
    }
 finally {
      cleanup(cluster);
      cluster=null;
      sdToLock.unlock();
    }
  }
  /** 
 * Test the importCheckpoint startup option. Verifies: 1. if the NN already contains an image, it will not be allowed to import a checkpoint. 2. if the NN does not contain an image, importing a checkpoint succeeds and re-saves the image
 */
  @Test public void testImportCheckpoint() throws Exception {
    Configuration conf=new HdfsConfiguration();
    Path testPath=new Path("/testfile");
    SecondaryNameNode snn=null;
    MiniDFSCluster cluster=null;
    Collection<URI> nameDirs=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
      nameDirs=cluster.getNameDirs(0);
      cluster.getFileSystem().mkdirs(testPath);
      snn=startSecondaryNameNode(conf);
      snn.doCheckpoint();
    }
  finally {
      cleanup(snn);
      cleanup(cluster);
      cluster=null;
    }
    LOG.info("Trying to import checkpoint when the NameNode already " + "contains an image. This should fail.");
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).format(false).startupOption(StartupOption.IMPORT).build();
      fail("NameNode did not fail to start when it already contained " + "an image");
    }
 catch (    IOException ioe) {
      GenericTestUtils.assertExceptionContains("NameNode already contains an image",ioe);
    }
 finally {
      cleanup(cluster);
      cluster=null;
    }
    LOG.info("Removing NN storage contents");
    for (    URI uri : nameDirs) {
      File dir=new File(uri.getPath());
      LOG.info("Cleaning " + dir);
      removeAndRecreateDir(dir);
    }
    LOG.info("Trying to import checkpoint");
    try {
      cluster=new MiniDFSCluster.Builder(conf).format(false).numDataNodes(0).startupOption(StartupOption.IMPORT).build();
      assertTrue("Path from checkpoint should exist after import",cluster.getFileSystem().exists(testPath));
      FSImageTestUtil.assertNNHasCheckpoints(cluster,Ints.asList(3));
    }
  finally {
      cleanup(cluster);
      cluster=null;
    }
  }
  private static void removeAndRecreateDir(  File dir) throws IOException {
    if (dir.exists())     if (!(FileUtil.fullyDelete(dir)))     throw new IOException("Cannot remove directory: " + dir);
    if (!dir.mkdirs())     throw new IOException("Cannot create directory " + dir);
  }
  SecondaryNameNode startSecondaryNameNode(  Configuration conf) throws IOException {
    conf.set(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,"0.0.0.0:0");
    return new SecondaryNameNode(conf);
  }
  SecondaryNameNode startSecondaryNameNode(  Configuration conf,  int index) throws IOException {
    Configuration snnConf=new Configuration(conf);
    snnConf.set(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,"0.0.0.0:0");
    snnConf.set(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_DIR_KEY,MiniDFSCluster.getBaseDirectory() + "/2nn-" + index);
    return new SecondaryNameNode(snnConf);
  }
  /** 
 * Tests checkpoint in HDFS.
 */
  @Test public void testCheckpoint() throws IOException {
    Path tmpDir=new Path("/tmp_tmp");
    Path file1=new Path("checkpoint.dat");
    Path file2=new Path("checkpoint2.dat");
    Configuration conf=new HdfsConfiguration();
    conf.set(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,"0.0.0.0:0");
    replication=(short)conf.getInt(DFSConfigKeys.DFS_REPLICATION_KEY,3);
    MiniDFSCluster cluster=null;
    FileSystem fileSys=null;
    SecondaryNameNode secondary=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();
      cluster.waitActive();
      fileSys=cluster.getFileSystem();
      assertTrue(!fileSys.exists(file1));
      assertTrue(!fileSys.exists(file2));
      DFSTestUtil.createFile(fileSys,file1,fileSize,fileSize,blockSize,replication,seed);
      checkFile(fileSys,file1,replication);
      for (int i=0; i < 1000; i++) {
        fileSys.mkdirs(tmpDir);
        fileSys.delete(tmpDir,true);
      }
      secondary=startSecondaryNameNode(conf);
      secondary.doCheckpoint();
      MetricsRecordBuilder rb=getMetrics(NN_METRICS);
      assertCounterGt("GetImageNumOps",0,rb);
      assertCounterGt("GetEditNumOps",0,rb);
      assertCounterGt("PutImageNumOps",0,rb);
      assertGaugeGt("GetImageAvgTime",0.0,rb);
      assertGaugeGt("GetEditAvgTime",0.0,rb);
      assertGaugeGt("PutImageAvgTime",0.0,rb);
    }
  finally {
      fileSys.close();
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(false).build();
      cluster.waitActive();
      fileSys=cluster.getFileSystem();
      checkFile(fileSys,file1,replication);
      cleanupFile(fileSys,file1);
      DFSTestUtil.createFile(fileSys,file2,fileSize,fileSize,blockSize,replication,seed);
      checkFile(fileSys,file2,replication);
      secondary=startSecondaryNameNode(conf);
      secondary.doCheckpoint();
      FSDirectory secondaryFsDir=secondary.getFSNamesystem().dir;
      INode rootInMap=secondaryFsDir.getInode(secondaryFsDir.rootDir.getId());
      assertSame(rootInMap,secondaryFsDir.rootDir);
      fileSys.delete(tmpDir,true);
      fileSys.mkdirs(tmpDir);
      secondary.doCheckpoint();
    }
  finally {
      fileSys.close();
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(false).build();
    cluster.waitActive();
    fileSys=cluster.getFileSystem();
    assertTrue(!fileSys.exists(file1));
    assertTrue(fileSys.exists(tmpDir));
    try {
      checkFile(fileSys,file2,replication);
    }
  finally {
      fileSys.close();
      cluster.shutdown();
      cluster=null;
    }
  }
  /** 
 * Tests save namespace.
 */
  @Test public void testSaveNamespace() throws IOException {
    MiniDFSCluster cluster=null;
    DistributedFileSystem fs=null;
    FileContext fc;
    try {
      Configuration conf=new HdfsConfiguration();
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(true).build();
      cluster.waitActive();
      fs=(cluster.getFileSystem());
      fc=FileContext.getFileContext(cluster.getURI(0));
      DFSAdmin admin=new DFSAdmin(conf);
      String[] args=new String[]{"-saveNamespace"};
      try {
        admin.run(args);
      }
 catch (      IOException eIO) {
        assertTrue(eIO.getLocalizedMessage().contains("Safe mode should be turned ON"));
      }
catch (      Exception e) {
        throw new IOException(e);
      }
      Path file=new Path("namespace.dat");
      DFSTestUtil.createFile(fs,file,fileSize,fileSize,blockSize,replication,seed);
      checkFile(fs,file,replication);
      Path symlink=new Path("file.link");
      fc.createSymlink(file,symlink,false);
      assertTrue(fc.getFileLinkStatus(symlink).isSymlink());
      Collection<URI> editsDirs=cluster.getNameEditsDirs(0);
      for (      URI uri : editsDirs) {
        File ed=new File(uri.getPath());
        assertTrue(new File(ed,"current/" + NNStorage.getInProgressEditsFileName(1)).length() > Integer.SIZE / Byte.SIZE);
      }
      fs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);
      try {
        admin.run(args);
      }
 catch (      Exception e) {
        throw new IOException(e);
      }
      final int EXPECTED_TXNS_FIRST_SEG=13;
      for (      URI uri : editsDirs) {
        File ed=new File(uri.getPath());
        File curDir=new File(ed,"current");
        LOG.info("Files in " + curDir + ":\n  "+ Joiner.on("\n  ").join(curDir.list()));
        File originalEdits=new File(curDir,NNStorage.getInProgressEditsFileName(1));
        assertFalse(originalEdits.exists());
        File finalizedEdits=new File(curDir,NNStorage.getFinalizedEditsFileName(1,EXPECTED_TXNS_FIRST_SEG));
        GenericTestUtils.assertExists(finalizedEdits);
        assertTrue(finalizedEdits.length() > Integer.SIZE / Byte.SIZE);
        GenericTestUtils.assertExists(new File(ed,"current/" + NNStorage.getInProgressEditsFileName(EXPECTED_TXNS_FIRST_SEG + 1)));
      }
      Collection<URI> imageDirs=cluster.getNameDirs(0);
      for (      URI uri : imageDirs) {
        File imageDir=new File(uri.getPath());
        File savedImage=new File(imageDir,"current/" + NNStorage.getImageFileName(EXPECTED_TXNS_FIRST_SEG));
        assertTrue("Should have saved image at " + savedImage,savedImage.exists());
      }
      cluster.shutdown();
      cluster=null;
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(false).build();
      cluster.waitActive();
      fs=(cluster.getFileSystem());
      checkFile(fs,file,replication);
      fc=FileContext.getFileContext(cluster.getURI(0));
      assertTrue(fc.getFileLinkStatus(symlink).isSymlink());
    }
  finally {
      if (fs != null)       fs.close();
      cleanup(cluster);
      cluster=null;
    }
  }
  @Test public void testCheckpointSignature() throws IOException {
    MiniDFSCluster cluster=null;
    Configuration conf=new HdfsConfiguration();
    SecondaryNameNode secondary=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(true).build();
      NameNode nn=cluster.getNameNode();
      NamenodeProtocols nnRpc=nn.getRpcServer();
      secondary=startSecondaryNameNode(conf);
      secondary.doCheckpoint();
      CheckpointSignature sig=nnRpc.rollEditLog();
      sig.setBlockpoolID("somerandomebpid");
      sig.clusterID="somerandomcid";
      try {
        sig.validateStorageInfo(nn.getFSImage());
        assertTrue("This test is expected to fail.",false);
      }
 catch (      Exception ignored) {
      }
    }
  finally {
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Tests the following sequence of events: - secondary successfully makes a checkpoint - it then fails while trying to upload it - it then fails again for the same reason - it then tries to checkpoint a third time
 */
  @Test public void testCheckpointAfterTwoFailedUploads() throws IOException {
    MiniDFSCluster cluster=null;
    SecondaryNameNode secondary=null;
    Configuration conf=new HdfsConfiguration();
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(true).build();
      secondary=startSecondaryNameNode(conf);
      Mockito.doThrow(new IOException("Injecting failure after rolling edit logs")).when(faultInjector).afterSecondaryCallsRollEditLog();
      try {
        secondary.doCheckpoint();
        fail("Should have failed upload");
      }
 catch (      IOException ioe) {
        LOG.info("Got expected failure",ioe);
        assertTrue(ioe.toString().contains("Injecting failure"));
      }
      try {
        secondary.doCheckpoint();
        fail("Should have failed upload");
      }
 catch (      IOException ioe) {
        LOG.info("Got expected failure",ioe);
        assertTrue(ioe.toString().contains("Injecting failure"));
      }
 finally {
        Mockito.reset(faultInjector);
      }
      secondary.doCheckpoint();
    }
  finally {
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Starts two namenodes and two secondary namenodes, verifies that secondary namenodes are configured correctly to talk to their respective namenodes and can do the checkpoint.
 * @throws IOException
 */
  @Test public void testMultipleSecondaryNamenodes() throws IOException {
    Configuration conf=new HdfsConfiguration();
    String nameserviceId1="ns1";
    String nameserviceId2="ns2";
    conf.set(DFSConfigKeys.DFS_NAMESERVICES,nameserviceId1 + "," + nameserviceId2);
    MiniDFSCluster cluster=null;
    SecondaryNameNode secondary1=null;
    SecondaryNameNode secondary2=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleFederatedTopology(conf.get(DFSConfigKeys.DFS_NAMESERVICES))).build();
      Configuration snConf1=new HdfsConfiguration(cluster.getConfiguration(0));
      Configuration snConf2=new HdfsConfiguration(cluster.getConfiguration(1));
      InetSocketAddress nn1RpcAddress=cluster.getNameNode(0).getNameNodeAddress();
      InetSocketAddress nn2RpcAddress=cluster.getNameNode(1).getNameNodeAddress();
      String nn1=nn1RpcAddress.getHostName() + ":" + nn1RpcAddress.getPort();
      String nn2=nn2RpcAddress.getHostName() + ":" + nn2RpcAddress.getPort();
      snConf1.set(DFSConfigKeys.DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY,"");
      snConf2.set(DFSConfigKeys.DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY,"");
      snConf1.set(DFSUtil.addKeySuffixes(DFSConfigKeys.DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY,nameserviceId1),nn1);
      snConf2.set(DFSUtil.addKeySuffixes(DFSConfigKeys.DFS_NAMENODE_SERVICE_RPC_ADDRESS_KEY,nameserviceId2),nn2);
      secondary1=startSecondaryNameNode(snConf1);
      secondary2=startSecondaryNameNode(snConf2);
      assertEquals(secondary1.getNameNodeAddress().getPort(),nn1RpcAddress.getPort());
      assertEquals(secondary2.getNameNodeAddress().getPort(),nn2RpcAddress.getPort());
      assertTrue(secondary1.getNameNodeAddress().getPort() != secondary2.getNameNodeAddress().getPort());
      secondary1.doCheckpoint();
      secondary2.doCheckpoint();
    }
  finally {
      cleanup(secondary1);
      secondary1=null;
      cleanup(secondary2);
      secondary2=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Test that the secondary doesn't have to re-download image if it hasn't changed.
 */
  @Test public void testSecondaryImageDownload() throws IOException {
    LOG.info("Starting testSecondaryImageDownload");
    Configuration conf=new HdfsConfiguration();
    conf.set(DFSConfigKeys.DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY,"0.0.0.0:0");
    Path dir=new Path("/checkpoint");
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(true).build();
    cluster.waitActive();
    FileSystem fileSys=cluster.getFileSystem();
    FSImage image=cluster.getNameNode().getFSImage();
    SecondaryNameNode secondary=null;
    try {
      assertTrue(!fileSys.exists(dir));
      secondary=startSecondaryNameNode(conf);
      File secondaryDir=MiniDFSCluster.getCheckpointDirectory(MiniDFSCluster.getBaseDirectory(),0,0)[0];
      File secondaryCurrent=new File(secondaryDir,"current");
      long expectedTxIdToDownload=cluster.getNameNode().getFSImage().getStorage().getMostRecentCheckpointTxId();
      File secondaryFsImageBefore=new File(secondaryCurrent,NNStorage.getImageFileName(expectedTxIdToDownload));
      File secondaryFsImageAfter=new File(secondaryCurrent,NNStorage.getImageFileName(expectedTxIdToDownload + 2));
      assertFalse("Secondary should start with empty current/ dir " + "but " + secondaryFsImageBefore + " exists",secondaryFsImageBefore.exists());
      assertTrue("Secondary should have loaded an image",secondary.doCheckpoint());
      assertTrue("Secondary should have downloaded original image",secondaryFsImageBefore.exists());
      assertTrue("Secondary should have created a new image",secondaryFsImageAfter.exists());
      long fsimageLength=secondaryFsImageBefore.length();
      assertEquals("Image size should not have changed",fsimageLength,secondaryFsImageAfter.length());
      fileSys.mkdirs(dir);
      assertFalse("Another checkpoint should not have to re-load image",secondary.doCheckpoint());
      for (      StorageDirectory sd : image.getStorage().dirIterable(NameNodeDirType.IMAGE)) {
        File imageFile=NNStorage.getImageFile(sd,NameNodeFile.IMAGE,expectedTxIdToDownload + 5);
        assertTrue("Image size increased",imageFile.length() > fsimageLength);
      }
    }
  finally {
      fileSys.close();
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Test NN restart if a failure happens in between creating the fsimage MD5 file and renaming the fsimage.
 */
  @Test(timeout=30000) public void testFailureBeforeRename() throws IOException {
    Configuration conf=new HdfsConfiguration();
    FSDataOutputStream fos=null;
    SecondaryNameNode secondary=null;
    MiniDFSCluster cluster=null;
    FileSystem fs=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();
      cluster.waitActive();
      fs=cluster.getFileSystem();
      secondary=startSecondaryNameNode(conf);
      fos=fs.create(new Path("tmpfile0"));
      fos.write(new byte[]{0,1,2,3});
      secondary.doCheckpoint();
      fos.write(new byte[]{0,1,2,3});
      fos.hsync();
      Mockito.doThrow(new IOException("Injecting failure after MD5Rename")).when(faultInjector).afterMD5Rename();
      try {
        secondary.doCheckpoint();
        fail("Fault injection failed.");
      }
 catch (      IOException ioe) {
      }
      Mockito.reset(faultInjector);
      cluster.restartNameNode();
    }
  finally {
      if (fs != null) {
        fs.close();
      }
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
      Mockito.reset(faultInjector);
    }
  }
  /** 
 * Test that a fault while downloading edits does not prevent future checkpointing
 */
  @Test(timeout=30000) public void testEditFailureBeforeRename() throws IOException {
    Configuration conf=new HdfsConfiguration();
    SecondaryNameNode secondary=null;
    MiniDFSCluster cluster=null;
    FileSystem fs=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();
      cluster.waitActive();
      fs=cluster.getFileSystem();
      secondary=startSecondaryNameNode(conf);
      DFSTestUtil.createFile(fs,new Path("tmpfile0"),1024,(short)1,0l);
      secondary.doCheckpoint();
      Mockito.doThrow(new IOException("Injecting failure before edit rename")).when(faultInjector).beforeEditsRename();
      DFSTestUtil.createFile(fs,new Path("tmpfile1"),1024,(short)1,0l);
      try {
        secondary.doCheckpoint();
        fail("Fault injection failed.");
      }
 catch (      IOException ioe) {
        GenericTestUtils.assertExceptionContains("Injecting failure before edit rename",ioe);
      }
      Mockito.reset(faultInjector);
      for (      StorageDirectory sd : secondary.getFSImage().getStorage().dirIterable(NameNodeDirType.EDITS)) {
        File[] tmpEdits=sd.getCurrentDir().listFiles(tmpEditsFilter);
        assertTrue("Expected a single tmp edits file in directory " + sd.toString(),tmpEdits.length == 1);
        RandomAccessFile randFile=new RandomAccessFile(tmpEdits[0],"rw");
        randFile.setLength(0);
        randFile.close();
      }
      secondary.doCheckpoint();
    }
  finally {
      if (secondary != null) {
        secondary.shutdown();
      }
      if (fs != null) {
        fs.close();
      }
      if (cluster != null) {
        cluster.shutdown();
      }
      Mockito.reset(faultInjector);
    }
  }
  /** 
 * Test that a fault while downloading edits the first time after the 2NN starts up does not prevent future checkpointing.
 */
  @Test(timeout=30000) public void testEditFailureOnFirstCheckpoint() throws IOException {
    Configuration conf=new HdfsConfiguration();
    SecondaryNameNode secondary=null;
    MiniDFSCluster cluster=null;
    FileSystem fs=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();
      cluster.waitActive();
      fs=cluster.getFileSystem();
      fs.mkdirs(new Path("test-file-1"));
      FSNamesystem fsns=cluster.getNamesystem();
      fsns.enterSafeMode(false);
      fsns.saveNamespace(0,0);
      fsns.leaveSafeMode(false);
      secondary=startSecondaryNameNode(conf);
      Mockito.doThrow(new IOException("Injecting failure before edit rename")).when(faultInjector).beforeEditsRename();
      try {
        secondary.doCheckpoint();
        fail("Fault injection failed.");
      }
 catch (      IOException ioe) {
        GenericTestUtils.assertExceptionContains("Injecting failure before edit rename",ioe);
      }
      Mockito.reset(faultInjector);
      secondary.doCheckpoint();
    }
  finally {
      if (secondary != null) {
        secondary.shutdown();
      }
      if (fs != null) {
        fs.close();
      }
      if (cluster != null) {
        cluster.shutdown();
      }
      Mockito.reset(faultInjector);
    }
  }
  /** 
 * Test that the secondary namenode correctly deletes temporary edits on startup.
 */
  @Test(timeout=60000) public void testDeleteTemporaryEditsOnStartup() throws IOException {
    Configuration conf=new HdfsConfiguration();
    SecondaryNameNode secondary=null;
    MiniDFSCluster cluster=null;
    FileSystem fs=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();
      cluster.waitActive();
      fs=cluster.getFileSystem();
      secondary=startSecondaryNameNode(conf);
      DFSTestUtil.createFile(fs,new Path("tmpfile0"),1024,(short)1,0l);
      secondary.doCheckpoint();
      Mockito.doThrow(new IOException("Injecting failure before edit rename")).when(faultInjector).beforeEditsRename();
      DFSTestUtil.createFile(fs,new Path("tmpfile1"),1024,(short)1,0l);
      try {
        secondary.doCheckpoint();
        fail("Fault injection failed.");
      }
 catch (      IOException ioe) {
        GenericTestUtils.assertExceptionContains("Injecting failure before edit rename",ioe);
      }
      Mockito.reset(faultInjector);
      for (      StorageDirectory sd : secondary.getFSImage().getStorage().dirIterable(NameNodeDirType.EDITS)) {
        File[] tmpEdits=sd.getCurrentDir().listFiles(tmpEditsFilter);
        assertTrue("Expected a single tmp edits file in directory " + sd.toString(),tmpEdits.length == 1);
      }
      secondary.shutdown();
      secondary=startSecondaryNameNode(conf);
      for (      StorageDirectory sd : secondary.getFSImage().getStorage().dirIterable(NameNodeDirType.EDITS)) {
        File[] tmpEdits=sd.getCurrentDir().listFiles(tmpEditsFilter);
        assertTrue("Did not expect a tmp edits file in directory " + sd.toString(),tmpEdits.length == 0);
      }
      secondary.doCheckpoint();
    }
  finally {
      if (secondary != null) {
        secondary.shutdown();
      }
      if (fs != null) {
        fs.close();
      }
      if (cluster != null) {
        cluster.shutdown();
      }
      Mockito.reset(faultInjector);
    }
  }
  /** 
 * Test case where two secondary namenodes are checkpointing the same NameNode. This differs from  {@link #testMultipleSecondaryNamenodes()}since that test runs against two distinct NNs. This case tests the following interleaving: - 2NN A downloads image (up to txid 2) - 2NN A about to save its own checkpoint - 2NN B downloads image (up to txid 4) - 2NN B uploads checkpoint (txid 4) - 2NN A uploads checkpoint (txid 2) It verifies that this works even though the earlier-txid checkpoint gets uploaded after the later-txid checkpoint.
 */
  @Test public void testMultipleSecondaryNNsAgainstSameNN() throws Exception {
    Configuration conf=new HdfsConfiguration();
    MiniDFSCluster cluster=null;
    SecondaryNameNode secondary1=null, secondary2=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).format(true).build();
      secondary1=startSecondaryNameNode(conf,1);
      secondary2=startSecondaryNameNode(conf,2);
      CheckpointStorage spyImage1=spyOnSecondaryImage(secondary1);
      DelayAnswer delayer=new DelayAnswer(LOG);
      Mockito.doAnswer(delayer).when(spyImage1).saveFSImageInAllDirs(Mockito.<FSNamesystem>any(),Mockito.anyLong());
      DoCheckpointThread checkpointThread=new DoCheckpointThread(secondary1);
      checkpointThread.start();
      delayer.waitForCall();
      secondary2.doCheckpoint();
      delayer.proceed();
      checkpointThread.join();
      checkpointThread.propagateExceptions();
      NNStorage storage=cluster.getNameNode().getFSImage().getStorage();
      assertEquals(4,storage.getMostRecentCheckpointTxId());
      assertNNHasCheckpoints(cluster,ImmutableList.of(2,4));
      secondary2.doCheckpoint();
      assertEquals(6,storage.getMostRecentCheckpointTxId());
      assertParallelFilesInvariant(cluster,ImmutableList.of(secondary1,secondary2));
      assertNNHasCheckpoints(cluster,ImmutableList.of(4,6));
    }
  finally {
      cleanup(secondary1);
      secondary1=null;
      cleanup(secondary2);
      secondary2=null;
      if (cluster != null) {
        cluster.shutdown();
        cluster=null;
      }
    }
  }
  /** 
 * Test case where two secondary namenodes are checkpointing the same NameNode. This differs from  {@link #testMultipleSecondaryNamenodes()}since that test runs against two distinct NNs. This case tests the following interleaving: - 2NN A) calls rollEdits() - 2NN B) calls rollEdits() - 2NN A) paused at getRemoteEditLogManifest() - 2NN B) calls getRemoteEditLogManifest() (returns up to txid 4) - 2NN B) uploads checkpoint fsimage_4 - 2NN A) allowed to proceed, also returns up to txid 4 - 2NN A) uploads checkpoint fsimage_4 as well, should fail gracefully It verifies that one of the two gets an error that it's uploading a duplicate checkpoint, and the other one succeeds.
 */
  @Test public void testMultipleSecondaryNNsAgainstSameNN2() throws Exception {
    Configuration conf=new HdfsConfiguration();
    MiniDFSCluster cluster=null;
    SecondaryNameNode secondary1=null, secondary2=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).format(true).build();
      secondary1=startSecondaryNameNode(conf,1);
      secondary2=startSecondaryNameNode(conf,2);
      final NamenodeProtocol origNN=secondary1.getNameNode();
      final Answer<Object> delegator=new GenericTestUtils.DelegateAnswer(origNN);
      NamenodeProtocol spyNN=Mockito.mock(NamenodeProtocol.class,delegator);
      DelayAnswer delayer=new DelayAnswer(LOG){
        @Override protected Object passThrough(        InvocationOnMock invocation) throws Throwable {
          return delegator.answer(invocation);
        }
      }
;
      secondary1.setNameNode(spyNN);
      Mockito.doAnswer(delayer).when(spyNN).getEditLogManifest(Mockito.anyLong());
      DoCheckpointThread checkpointThread=new DoCheckpointThread(secondary1);
      checkpointThread.start();
      delayer.waitForCall();
      secondary2.doCheckpoint();
      NNStorage storage=cluster.getNameNode().getFSImage().getStorage();
      assertEquals(4,storage.getMostRecentCheckpointTxId());
      delayer.proceed();
      checkpointThread.join();
      checkpointThread.propagateExceptions();
      assertEquals(4,storage.getMostRecentCheckpointTxId());
      secondary2.doCheckpoint();
      assertEquals(6,storage.getMostRecentCheckpointTxId());
      assertNNHasCheckpoints(cluster,ImmutableList.of(4,6));
      secondary1.setNameNode(origNN);
      secondary1.doCheckpoint();
      assertEquals(8,storage.getMostRecentCheckpointTxId());
      assertParallelFilesInvariant(cluster,ImmutableList.of(secondary1,secondary2));
      assertNNHasCheckpoints(cluster,ImmutableList.of(6,8));
    }
  finally {
      cleanup(secondary1);
      secondary1=null;
      cleanup(secondary2);
      secondary2=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Test case where the name node is reformatted while the secondary namenode is running. The secondary should shut itself down if if talks to a NN with the wrong namespace.
 */
  @Test public void testReformatNNBetweenCheckpoints() throws IOException {
    MiniDFSCluster cluster=null;
    SecondaryNameNode secondary=null;
    Configuration conf=new HdfsConfiguration();
    conf.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECTION_MAXIDLETIME_KEY,1);
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).format(true).build();
      int origPort=cluster.getNameNodePort();
      int origHttpPort=cluster.getNameNode().getHttpAddress().getPort();
      Configuration snnConf=new Configuration(conf);
      File checkpointDir=new File(MiniDFSCluster.getBaseDirectory(),"namesecondary");
      snnConf.set(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_DIR_KEY,checkpointDir.getAbsolutePath());
      secondary=startSecondaryNameNode(snnConf);
      secondary.doCheckpoint();
      cluster.shutdown();
      cluster=null;
      try {
        Thread.sleep(100);
      }
 catch (      InterruptedException ie) {
      }
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).nameNodePort(origPort).nameNodeHttpPort(origHttpPort).format(true).build();
      try {
        secondary.doCheckpoint();
        fail("Should have failed checkpoint against a different namespace");
      }
 catch (      IOException ioe) {
        LOG.info("Got expected failure",ioe);
        assertTrue(ioe.toString().contains("Inconsistent checkpoint"));
      }
    }
  finally {
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Test that the primary NN will not serve any files to a 2NN who doesn't share its namespace ID, and also will not accept any files from one.
 */
  @Test public void testNamespaceVerifiedOnFileTransfer() throws IOException {
    MiniDFSCluster cluster=null;
    Configuration conf=new HdfsConfiguration();
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).format(true).build();
      NamenodeProtocols nn=cluster.getNameNodeRpc();
      URL fsName=DFSUtil.getInfoServer(cluster.getNameNode().getServiceRpcAddress(),conf,DFSUtil.getHttpClientScheme(conf)).toURL();
      nn.rollEditLog();
      RemoteEditLogManifest manifest=nn.getEditLogManifest(1);
      RemoteEditLog log=manifest.getLogs().get(0);
      NNStorage dstImage=Mockito.mock(NNStorage.class);
      Mockito.doReturn(Lists.newArrayList(new File("/wont-be-written"))).when(dstImage).getFiles(Mockito.<NameNodeDirType>anyObject(),Mockito.anyString());
      File mockImageFile=File.createTempFile("image","");
      FileOutputStream imageFile=new FileOutputStream(mockImageFile);
      imageFile.write("data".getBytes());
      imageFile.close();
      Mockito.doReturn(mockImageFile).when(dstImage).findImageFile(Mockito.any(NameNodeFile.class),Mockito.anyLong());
      Mockito.doReturn(new StorageInfo(1,1,"X",1,NodeType.NAME_NODE).toColonSeparatedString()).when(dstImage).toColonSeparatedString();
      try {
        TransferFsImage.downloadImageToStorage(fsName,0,dstImage,false,false);
        fail("Storage info was not verified");
      }
 catch (      IOException ioe) {
        String msg=StringUtils.stringifyException(ioe);
        assertTrue(msg,msg.contains("but the secondary expected"));
      }
      try {
        TransferFsImage.downloadEditsToStorage(fsName,log,dstImage);
        fail("Storage info was not verified");
      }
 catch (      IOException ioe) {
        String msg=StringUtils.stringifyException(ioe);
        assertTrue(msg,msg.contains("but the secondary expected"));
      }
      try {
        TransferFsImage.uploadImageFromStorage(fsName,conf,dstImage,NameNodeFile.IMAGE,0);
        fail("Storage info was not verified");
      }
 catch (      IOException ioe) {
        String msg=StringUtils.stringifyException(ioe);
        assertTrue(msg,msg.contains("but the secondary expected"));
      }
    }
  finally {
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Test that, if a storage directory is failed when a checkpoint occurs, the non-failed storage directory receives the checkpoint.
 */
  @Test public void testCheckpointWithFailedStorageDir() throws Exception {
    MiniDFSCluster cluster=null;
    SecondaryNameNode secondary=null;
    File currentDir=null;
    Configuration conf=new HdfsConfiguration();
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).format(true).build();
      secondary=startSecondaryNameNode(conf);
      secondary.doCheckpoint();
      NamenodeProtocols nn=cluster.getNameNodeRpc();
      NNStorage storage=cluster.getNameNode().getFSImage().getStorage();
      StorageDirectory sd0=storage.getStorageDir(0);
      StorageDirectory sd1=storage.getStorageDir(1);
      currentDir=sd0.getCurrentDir();
      FileUtil.setExecutable(currentDir,false);
      secondary.doCheckpoint();
      GenericTestUtils.assertExists(new File(sd1.getCurrentDir(),NNStorage.getImageFileName(2)));
      FileUtil.setExecutable(currentDir,true);
      nn.restoreFailedStorage("true");
      nn.rollEditLog();
      secondary.doCheckpoint();
      assertNNHasCheckpoints(cluster,ImmutableList.of(8));
      assertParallelFilesInvariant(cluster,ImmutableList.of(secondary));
    }
  finally {
      if (currentDir != null) {
        FileUtil.setExecutable(currentDir,true);
      }
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Test case where the NN is configured with a name-only and an edits-only dir, with storage-restore turned on. In this case, if the name-only dir disappears and comes back, a new checkpoint after it has been restored should function correctly.
 * @throws Exception
 */
  @Test public void testCheckpointWithSeparateDirsAfterNameFails() throws Exception {
    MiniDFSCluster cluster=null;
    SecondaryNameNode secondary=null;
    File currentDir=null;
    Configuration conf=new HdfsConfiguration();
    File base_dir=new File(MiniDFSCluster.getBaseDirectory());
    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_RESTORE_KEY,true);
    conf.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY,MiniDFSCluster.getBaseDirectory() + "/name-only");
    conf.set(DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY,MiniDFSCluster.getBaseDirectory() + "/edits-only");
    conf.set(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_DIR_KEY,fileAsURI(new File(base_dir,"namesecondary1")).toString());
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).format(true).manageNameDfsDirs(false).build();
      secondary=startSecondaryNameNode(conf);
      secondary.doCheckpoint();
      NamenodeProtocols nn=cluster.getNameNodeRpc();
      NNStorage storage=cluster.getNameNode().getFSImage().getStorage();
      StorageDirectory sd0=storage.getStorageDir(0);
      assertEquals(NameNodeDirType.IMAGE,sd0.getStorageDirType());
      currentDir=sd0.getCurrentDir();
      assertEquals(0,FileUtil.chmod(currentDir.getAbsolutePath(),"000"));
      try {
        secondary.doCheckpoint();
        fail("Did not fail to checkpoint when there are no valid storage dirs");
      }
 catch (      IOException ioe) {
        GenericTestUtils.assertExceptionContains("No targets in destination storage",ioe);
      }
      assertEquals(0,FileUtil.chmod(currentDir.getAbsolutePath(),"755"));
      nn.restoreFailedStorage("true");
      nn.rollEditLog();
      secondary.doCheckpoint();
      assertNNHasCheckpoints(cluster,ImmutableList.of(8));
      assertParallelFilesInvariant(cluster,ImmutableList.of(secondary));
    }
  finally {
      if (currentDir != null) {
        FileUtil.chmod(currentDir.getAbsolutePath(),"755");
      }
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Test that the 2NN triggers a checkpoint after the configurable interval
 */
  @Test(timeout=30000) public void testCheckpointTriggerOnTxnCount() throws Exception {
    MiniDFSCluster cluster=null;
    SecondaryNameNode secondary=null;
    Configuration conf=new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY,10);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_CHECK_PERIOD_KEY,1);
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).format(true).build();
      FileSystem fs=cluster.getFileSystem();
      secondary=startSecondaryNameNode(conf);
      secondary.startCheckpointThread();
      final NNStorage storage=secondary.getFSImage().getStorage();
      GenericTestUtils.waitFor(new Supplier<Boolean>(){
        @Override public Boolean get(){
          LOG.info("Waiting for checkpoint txn id to go to 2");
          return storage.getMostRecentCheckpointTxId() == 2;
        }
      }
,200,15000);
      for (int i=0; i < 10; i++) {
        fs.mkdirs(new Path("/test" + i));
      }
      GenericTestUtils.waitFor(new Supplier<Boolean>(){
        @Override public Boolean get(){
          LOG.info("Waiting for checkpoint txn id to go > 2");
          return storage.getMostRecentCheckpointTxId() > 2;
        }
      }
,200,15000);
    }
  finally {
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Test case where the secondary does a checkpoint, then stops for a while. In the meantime, the NN saves its image several times, so that the logs that connect the 2NN's old checkpoint to the current txid get archived. Then, the 2NN tries to checkpoint again.
 */
  @Test public void testSecondaryHasVeryOutOfDateImage() throws IOException {
    MiniDFSCluster cluster=null;
    SecondaryNameNode secondary=null;
    Configuration conf=new HdfsConfiguration();
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(true).build();
      secondary=startSecondaryNameNode(conf);
      secondary.doCheckpoint();
      NamenodeProtocols nn=cluster.getNameNodeRpc();
      nn.setSafeMode(SafeModeAction.SAFEMODE_ENTER,false);
      for (int i=0; i < 3; i++) {
        nn.saveNamespace(0,0);
      }
      nn.setSafeMode(SafeModeAction.SAFEMODE_LEAVE,false);
      secondary.doCheckpoint();
    }
  finally {
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Regression test for HDFS-3678 "Edit log files are never being purged from 2NN"
 */
  @Test public void testSecondaryPurgesEditLogs() throws IOException {
    MiniDFSCluster cluster=null;
    SecondaryNameNode secondary=null;
    Configuration conf=new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_NUM_EXTRA_EDITS_RETAINED_KEY,0);
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).format(true).build();
      FileSystem fs=cluster.getFileSystem();
      fs.mkdirs(new Path("/foo"));
      secondary=startSecondaryNameNode(conf);
      for (int i=0; i < 5; i++) {
        secondary.doCheckpoint();
      }
      List<File> checkpointDirs=getCheckpointCurrentDirs(secondary);
      for (      File checkpointDir : checkpointDirs) {
        List<EditLogFile> editsFiles=FileJournalManager.matchEditLogs(checkpointDir);
        assertEquals("Edit log files were not purged from 2NN",1,editsFiles.size());
      }
    }
  finally {
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Regression test for HDFS-3835 - "Long-lived 2NN cannot perform a checkpoint if security is enabled and the NN restarts without outstanding delegation tokens"
 */
  @Test public void testSecondaryNameNodeWithDelegationTokens() throws IOException {
    MiniDFSCluster cluster=null;
    SecondaryNameNode secondary=null;
    Configuration conf=new HdfsConfiguration();
    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_DELEGATION_TOKEN_ALWAYS_USE_KEY,true);
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(true).build();
      assertNotNull(cluster.getNamesystem().getDelegationToken(new Text("atm")));
      secondary=startSecondaryNameNode(conf);
      secondary.doCheckpoint();
      cluster.getNameNodeRpc().setSafeMode(SafeModeAction.SAFEMODE_ENTER,false);
      cluster.getNameNodeRpc().saveNamespace(0,0);
      cluster.getNameNodeRpc().setSafeMode(SafeModeAction.SAFEMODE_LEAVE,false);
      secondary.doCheckpoint();
    }
  finally {
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  /** 
 * Regression test for HDFS-3849.  This makes sure that when we re-load the FSImage in the 2NN, we clear the existing leases.
 */
  @Test public void testSecondaryNameNodeWithSavedLeases() throws IOException {
    MiniDFSCluster cluster=null;
    SecondaryNameNode secondary=null;
    FSDataOutputStream fos=null;
    Configuration conf=new HdfsConfiguration();
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).format(true).build();
      FileSystem fs=cluster.getFileSystem();
      fos=fs.create(new Path("tmpfile"));
      fos.write(new byte[]{0,1,2,3});
      fos.hflush();
      assertEquals(1,cluster.getNamesystem().getLeaseManager().countLease());
      secondary=startSecondaryNameNode(conf);
      assertEquals(0,secondary.getFSNamesystem().getLeaseManager().countLease());
      secondary.doCheckpoint();
      assertEquals(1,secondary.getFSNamesystem().getLeaseManager().countLease());
      fos.close();
      fos=null;
      cluster.getNameNodeRpc().setSafeMode(SafeModeAction.SAFEMODE_ENTER,false);
      cluster.getNameNodeRpc().saveNamespace(0,0);
      cluster.getNameNodeRpc().setSafeMode(SafeModeAction.SAFEMODE_LEAVE,false);
      secondary.doCheckpoint();
      assertEquals(0,secondary.getFSNamesystem().getLeaseManager().countLease());
    }
  finally {
      if (fos != null) {
        fos.close();
      }
      cleanup(secondary);
      secondary=null;
      cleanup(cluster);
      cluster=null;
    }
  }
  @Test public void testCommandLineParsing() throws ParseException {
    SecondaryNameNode.CommandLineOpts opts=new SecondaryNameNode.CommandLineOpts();
    opts.parse();
    assertNull(opts.getCommand());
    opts.parse("-checkpoint");
    assertEquals(SecondaryNameNode.CommandLineOpts.Command.CHECKPOINT,opts.getCommand());
    assertFalse(opts.shouldForceCheckpoint());
    opts.parse("-checkpoint","force");
    assertEquals(SecondaryNameNode.CommandLineOpts.Command.CHECKPOINT,opts.getCommand());
    assertTrue(opts.shouldForceCheckpoint());
    opts.parse("-geteditsize");
    assertEquals(SecondaryNameNode.CommandLineOpts.Command.GETEDITSIZE,opts.getCommand());
    opts.parse("-format");
    assertTrue(opts.shouldFormat());
    try {
      opts.parse("-geteditsize","-checkpoint");
      fail("Should have failed bad parsing for two actions");
    }
 catch (    ParseException e) {
      LOG.warn("Encountered ",e);
    }
    try {
      opts.parse("-checkpoint","xx");
      fail("Should have failed for bad checkpoint arg");
    }
 catch (    ParseException e) {
      LOG.warn("Encountered ",e);
    }
  }
  @Test public void testLegacyOivImage() throws Exception {
    MiniDFSCluster cluster=null;
    SecondaryNameNode secondary=null;
    File tmpDir=GenericTestUtils.getTestDir("testLegacyOivImage");
    tmpDir.mkdirs();
    Configuration conf=new HdfsConfiguration();
    conf.set(DFSConfigKeys.DFS_NAMENODE_LEGACY_OIV_IMAGE_DIR_KEY,tmpDir.getAbsolutePath());
    conf.set(DFSConfigKeys.DFS_NAMENODE_NUM_CHECKPOINTS_RETAINED_KEY,"2");
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).format(true).build();
      secondary=startSecondaryNameNode(conf);
      secondary.doCheckpoint();
      String files1[]=tmpDir.list();
      assertEquals("Only one file is expected",1,files1.length);
      secondary.doCheckpoint();
      secondary.doCheckpoint();
      String files2[]=tmpDir.list();
      assertEquals("Two files are expected",2,files2.length);
      for (      String fName : files2) {
        assertFalse(fName.equals(files1[0]));
      }
    }
  finally {
      cleanup(secondary);
      cleanup(cluster);
      tmpDir.delete();
    }
  }
  private static void cleanup(  SecondaryNameNode snn){
    if (snn != null) {
      try {
        snn.shutdown();
      }
 catch (      Exception e) {
        LOG.warn("Could not shut down secondary namenode",e);
      }
    }
  }
  private static void cleanup(  MiniDFSCluster cluster){
    if (cluster != null) {
      try {
        cluster.shutdown();
      }
 catch (      Exception e) {
        LOG.warn("Could not shutdown MiniDFSCluster ",e);
      }
    }
  }
  /** 
 * Assert that if any two files have the same name across the 2NNs and NN, they should have the same content too.
 */
  private void assertParallelFilesInvariant(  MiniDFSCluster cluster,  ImmutableList<SecondaryNameNode> secondaries) throws Exception {
    List<File> allCurrentDirs=Lists.newArrayList();
    allCurrentDirs.addAll(getNameNodeCurrentDirs(cluster,0));
    for (    SecondaryNameNode snn : secondaries) {
      allCurrentDirs.addAll(getCheckpointCurrentDirs(snn));
    }
    FSImageTestUtil.assertParallelFilesAreIdentical(allCurrentDirs,ImmutableSet.of("VERSION"));
  }
  private static List<File> getCheckpointCurrentDirs(  SecondaryNameNode secondary){
    List<File> ret=Lists.newArrayList();
    for (    String u : secondary.getCheckpointDirectories()) {
      File checkpointDir=new File(URI.create(u).getPath());
      ret.add(new File(checkpointDir,"current"));
    }
    return ret;
  }
  private static CheckpointStorage spyOnSecondaryImage(  SecondaryNameNode secondary1){
    CheckpointStorage spy=Mockito.spy((CheckpointStorage)secondary1.getFSImage());
    ;
    secondary1.setFSImage(spy);
    return spy;
  }
  /** 
 * A utility class to perform a checkpoint in a different thread.
 */
private static class DoCheckpointThread extends Thread {
    private final SecondaryNameNode snn;
    private volatile Throwable thrown=null;
    DoCheckpointThread(    SecondaryNameNode snn){
      this.snn=snn;
    }
    @Override public void run(){
      try {
        snn.doCheckpoint();
      }
 catch (      Throwable t) {
        thrown=t;
      }
    }
    void propagateExceptions(){
      if (thrown != null) {
        throw new RuntimeException(thrown);
      }
    }
  }
}
