/** 
 * Test erasure coding block placement with skewed # nodes per rack.
 */
public class TestErasureCodingMultipleRacks {
  public static final Logger LOG=LoggerFactory.getLogger(TestErasureCodingMultipleRacks.class);
static {
    GenericTestUtils.setLogLevel(BlockPlacementPolicy.LOG,Level.TRACE);
    GenericTestUtils.setLogLevel(BlockPlacementPolicyDefault.LOG,Level.TRACE);
    GenericTestUtils.setLogLevel(BlockPlacementPolicyRackFaultTolerant.LOG,Level.TRACE);
    GenericTestUtils.setLogLevel(NetworkTopology.LOG,Level.DEBUG);
  }
  @Rule public Timeout globalTimeout=new Timeout(300000);
  public ErasureCodingPolicy getPolicy(){
    return StripedFileTestUtil.getDefaultECPolicy();
  }
  private MiniDFSCluster cluster;
  private ErasureCodingPolicy ecPolicy;
  private Configuration conf;
  private DistributedFileSystem dfs;
  @Before public void setup(){
    ecPolicy=getPolicy();
    conf=new HdfsConfiguration();
    conf.setBoolean(DFS_NAMENODE_REDUNDANCY_CONSIDERLOAD_KEY,false);
  }
  /** 
 * Setup cluster with desired number of DN, racks, and specified number of rack that only has 1 DN. Other racks will be evenly setup with the number of DNs. <p> This is not done as a  {@link Before}, so test cases can setup differently.
 * @param numDatanodes number of total Datanodes.
 * @param numRacks number of total racks
 * @param numSingleDnRacks number of racks that only has 1 DN
 * @throws Exception
 */
  public void setupCluster(  final int numDatanodes,  final int numRacks,  final int numSingleDnRacks) throws Exception {
    assert numDatanodes > numRacks;
    assert numRacks > numSingleDnRacks;
    assert numSingleDnRacks >= 0;
    final String[] racks=new String[numDatanodes];
    for (int i=0; i < numSingleDnRacks; i++) {
      racks[i]="/rack" + i;
    }
    for (int i=numSingleDnRacks; i < numDatanodes; i++) {
      racks[i]="/rack" + (numSingleDnRacks + (i % (numRacks - numSingleDnRacks)));
    }
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).racks(racks).build();
    dfs=cluster.getFileSystem();
    cluster.waitActive();
    dfs.setErasureCodingPolicy(new Path("/"),ecPolicy.getName());
  }
  @After public void teardown() throws Exception {
    if (cluster != null) {
      cluster.shutdown();
    }
  }
  @Test public void testSkewedRack1() throws Exception {
    final int dataUnits=ecPolicy.getNumDataUnits();
    final int parityUnits=ecPolicy.getNumParityUnits();
    setupCluster(dataUnits + parityUnits,2,1);
    final int filesize=ecPolicy.getNumDataUnits() * ecPolicy.getCellSize();
    byte[] contents=new byte[filesize];
    final Path path=new Path("/testfile");
    LOG.info("Writing file " + path);
    DFSTestUtil.writeFile(dfs,path,contents);
    BlockLocation[] blocks=dfs.getFileBlockLocations(path,0,Long.MAX_VALUE);
    assertEquals(ecPolicy.getNumDataUnits() + ecPolicy.getNumParityUnits(),blocks[0].getHosts().length);
  }
  @Test public void testSkewedRack2() throws Exception {
    final int dataUnits=ecPolicy.getNumDataUnits();
    final int parityUnits=ecPolicy.getNumParityUnits();
    setupCluster(dataUnits + parityUnits * 2,dataUnits,dataUnits - 1);
    final int filesize=ecPolicy.getNumDataUnits() * ecPolicy.getCellSize();
    byte[] contents=new byte[filesize];
    final Path path=new Path("/testfile");
    LOG.info("Writing file " + path);
    DFSTestUtil.writeFile(dfs,path,contents);
    BlockLocation[] blocks=dfs.getFileBlockLocations(path,0,Long.MAX_VALUE);
    assertEquals(ecPolicy.getNumDataUnits() + ecPolicy.getNumParityUnits(),blocks[0].getHosts().length);
  }
  @Test public void testSkewedRack3() throws Exception {
    final int dataUnits=ecPolicy.getNumDataUnits();
    final int parityUnits=ecPolicy.getNumParityUnits();
    int numRacks=dataUnits - parityUnits + 2;
    setupCluster(dataUnits + parityUnits * 4,numRacks,dataUnits - parityUnits);
    final int filesize=ecPolicy.getNumDataUnits() * ecPolicy.getCellSize();
    byte[] contents=new byte[filesize];
    for (int i=0; i < 10; ++i) {
      final Path path=new Path("/testfile" + i);
      LOG.info("Writing file " + path);
      DFSTestUtil.writeFile(dfs,path,contents);
      ExtendedBlock extendedBlock=DFSTestUtil.getFirstBlock(dfs,path);
      DFSTestUtil.waitForReplication(cluster,extendedBlock,numRacks,ecPolicy.getNumDataUnits() + ecPolicy.getNumParityUnits(),0);
      BlockLocation[] blocks=dfs.getFileBlockLocations(path,0,Long.MAX_VALUE);
      assertEquals(ecPolicy.getNumDataUnits() + ecPolicy.getNumParityUnits(),blocks[0].getHosts().length);
      assertRackFailureTolerated(blocks[0].getTopologyPaths());
    }
  }
  private void assertRackFailureTolerated(  final String[] topologies){
    final Map<String,Integer> racksCount=new HashMap<>();
    for (    String t : topologies) {
      final Integer count=racksCount.get(getRackName(t));
      if (count == null) {
        racksCount.put(getRackName(t),1);
      }
 else {
        racksCount.put(getRackName(t),count + 1);
      }
    }
    LOG.info("Rack count map is: {}",racksCount);
    for (    Integer count : racksCount.values()) {
      assertTrue(count <= ecPolicy.getNumParityUnits());
    }
  }
  private String getRackName(  final String topology){
    assert topology.indexOf('/',1) > 0;
    return topology.substring(0,topology.indexOf('/',1));
  }
}
