/** 
 * Test the ability of a DN to tolerate volume failures.
 */
public class TestDataNodeVolumeFailureToleration {
  private FileSystem fs;
  private MiniDFSCluster cluster;
  private Configuration conf;
  final int WAIT_FOR_HEARTBEATS=3000;
  final int WAIT_FOR_DEATH=15000;
  @Rule public Timeout timeout=new Timeout(120 * 1000);
  @Before public void setUp() throws Exception {
    conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,512L);
    conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1);
    conf.setInt(DFSConfigKeys.DFS_DF_INTERVAL_KEY,1000);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY,1000);
    conf.setInt(DFSConfigKeys.DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY,1);
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).build();
    cluster.waitActive();
    fs=cluster.getFileSystem();
  }
  @After public void tearDown() throws Exception {
    if (cluster != null) {
      cluster.shutdown();
      cluster=null;
    }
  }
  /** 
 * Test the DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY configuration option, ie the DN tolerates a failed-to-use scenario during its start-up.
 */
  @Test public void testValidVolumesAtStartup() throws Exception {
    assumeNotWindows();
    cluster.shutdownDataNodes();
    conf.setInt(DFSConfigKeys.DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY,1);
    File tld=new File(MiniDFSCluster.getBaseDirectory(),"badData");
    File dataDir1=new File(tld,"data1");
    File dataDir1Actual=new File(dataDir1,"1");
    dataDir1Actual.mkdirs();
    File dataDir2=new File(tld,"data2");
    prepareDirToFail(dataDir2);
    File dataDir2Actual=new File(dataDir2,"2");
    conf.set(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY,dataDir1Actual.getPath() + "," + dataDir2Actual.getPath());
    cluster.startDataNodes(conf,1,false,null,null);
    cluster.waitActive();
    try {
      assertTrue("The DN should have started up fine.",cluster.isDataNodeUp());
      DataNode dn=cluster.getDataNodes().get(0);
      String si=DataNodeTestUtils.getFSDataset(dn).getStorageInfo();
      assertTrue("The DN should have started with this directory",si.contains(dataDir1Actual.getPath()));
      assertFalse("The DN shouldn't have a bad directory.",si.contains(dataDir2Actual.getPath()));
    }
  finally {
      cluster.shutdownDataNodes();
      FileUtil.chmod(dataDir2.toString(),"755");
    }
  }
  /** 
 * Test the DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY configuration option, ie the DN shuts itself down when the number of failures experienced drops below the tolerated amount.
 */
  @Test public void testConfigureMinValidVolumes() throws Exception {
    assumeNotWindows();
    conf.setInt(DFSConfigKeys.DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY,0);
    conf.setTimeDuration(DFSConfigKeys.DFS_DATANODE_DISK_CHECK_MIN_GAP_KEY,0,TimeUnit.MILLISECONDS);
    cluster.startDataNodes(conf,2,true,null,null);
    cluster.waitActive();
    final DatanodeManager dm=cluster.getNamesystem().getBlockManager().getDatanodeManager();
    long origCapacity=DFSTestUtil.getLiveDatanodeCapacity(dm);
    long dnCapacity=DFSTestUtil.getDatanodeCapacity(dm,0);
    File dn2Vol1=cluster.getInstanceStorageDir(1,0);
    DataNodeTestUtils.injectDataDirFailure(dn2Vol1);
    Path file1=new Path("/test1");
    DFSTestUtil.createFile(fs,file1,1024,(short)3,1L);
    DFSTestUtil.waitReplication(fs,file1,(short)2);
    DFSTestUtil.waitForDatanodeStatus(dm,2,1,0,origCapacity - (1 * dnCapacity),WAIT_FOR_HEARTBEATS);
    DataNodeTestUtils.restoreDataDirFromFailure(dn2Vol1);
    Path file2=new Path("/test2");
    DFSTestUtil.createFile(fs,file2,1024,(short)3,1L);
    DFSTestUtil.waitReplication(fs,file2,(short)2);
  }
  /** 
 * Restart the datanodes with a new volume tolerated value.
 * @param volTolerated number of dfs data dir failures to tolerate
 * @param manageDfsDirs whether the mini cluster should manage data dirs
 * @throws IOException
 */
  private void restartDatanodes(  int volTolerated,  boolean manageDfsDirs) throws IOException {
    cluster.shutdownDataNodes();
    conf.setInt(DFSConfigKeys.DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY,volTolerated);
    cluster.startDataNodes(conf,1,manageDfsDirs,null,null);
    cluster.waitActive();
  }
  /** 
 * Test for different combination of volume configs and volumes tolerated  values.
 */
  @Test public void testVolumeAndTolerableConfiguration() throws Exception {
    testVolumeConfig(-2,0,false,true);
    testVolumeConfig(-1,0,true,true);
    testVolumeConfig(-1,1,true,true);
    testVolumeConfig(-1,2,false,true);
    testVolumeConfig(100,0,false,true);
    testVolumeConfig(0,1,false,false);
    testVolumeConfig(1,1,true,false);
    testVolumeConfig(0,0,true,false);
    testVolumeConfig(0,2,false,false);
  }
  /** 
 * Tests for a given volumes to be tolerated and volumes failed.
 */
  private void testVolumeConfig(  int volumesTolerated,  int volumesFailed,  boolean expectedBPServiceState,  boolean manageDfsDirs) throws IOException, InterruptedException {
    assumeNotWindows();
    final int dnIndex=0;
    File[] dirs={new File(cluster.getInstanceStorageDir(dnIndex,0),"current"),new File(cluster.getInstanceStorageDir(dnIndex,1),"current")};
    try {
      for (int i=0; i < volumesFailed; i++) {
        prepareDirToFail(dirs[i]);
      }
      restartDatanodes(volumesTolerated,manageDfsDirs);
    }
 catch (    DiskErrorException e) {
      GenericTestUtils.assertExceptionContains("Invalid value configured for " + "dfs.datanode.failed.volumes.tolerated",e);
    }
 finally {
      boolean bpServiceState;
      if (cluster.getDataNodes().size() == 0) {
        bpServiceState=false;
      }
 else {
        bpServiceState=cluster.getDataNodes().get(0).isBPServiceAlive(cluster.getNamesystem().getBlockPoolId());
      }
      assertEquals(expectedBPServiceState,bpServiceState);
      for (      File dir : dirs) {
        FileUtil.chmod(dir.toString(),"755");
      }
    }
  }
  /** 
 * Prepare directories for a failure, set dir permission to 000
 * @param dir
 * @throws IOException
 * @throws InterruptedException
 */
  private void prepareDirToFail(  File dir) throws IOException, InterruptedException {
    dir.mkdirs();
    assertEquals("Couldn't chmod local vol",0,FileUtil.chmod(dir.toString(),"000"));
  }
  /** 
 * Test that a volume that is considered failed on startup is seen as a failed volume by the NN.
 */
  @Test public void testFailedVolumeOnStartupIsCounted() throws Exception {
    assumeNotWindows();
    final DatanodeManager dm=cluster.getNamesystem().getBlockManager().getDatanodeManager();
    long origCapacity=DFSTestUtil.getLiveDatanodeCapacity(dm);
    File dir=new File(cluster.getInstanceStorageDir(0,0),"current");
    try {
      prepareDirToFail(dir);
      restartDatanodes(1,false);
      assertEquals(true,cluster.getDataNodes().get(0).isBPServiceAlive(cluster.getNamesystem().getBlockPoolId()));
      DFSTestUtil.waitForDatanodeStatus(dm,1,0,1,origCapacity / 2,WAIT_FOR_HEARTBEATS);
    }
  finally {
      FileUtil.chmod(dir.toString(),"755");
    }
  }
}
