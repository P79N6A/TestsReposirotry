public class TestOverReplicatedBlocks {
  /** 
 * Test processOverReplicatedBlock can handle corrupt replicas fine. It make sure that it won't treat corrupt replicas as valid ones  thus prevents NN deleting valid replicas but keeping corrupt ones.
 */
  @Test public void testProcesOverReplicateBlock() throws Exception {
    Configuration conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_DATANODE_SCAN_PERIOD_HOURS_KEY,100L);
    conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY,1000L);
    conf.set(DFSConfigKeys.DFS_NAMENODE_RECONSTRUCTION_PENDING_TIMEOUT_SEC_KEY,Integer.toString(2));
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
    FileSystem fs=cluster.getFileSystem();
    try {
      final Path fileName=new Path("/foo1");
      DFSTestUtil.createFile(fs,fileName,2,(short)3,0L);
      DFSTestUtil.waitReplication(fs,fileName,(short)3);
      ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,fileName);
      cluster.corruptReplica(0,block);
      DataNodeProperties dnProps=cluster.stopDataNode(0);
      File scanCursor=new File(new File(MiniDFSCluster.getFinalizedDir(cluster.getInstanceStorageDir(0,0),cluster.getNamesystem().getBlockPoolId()).getParent()).getParent(),"scanner.cursor");
      for (int i=0; !scanCursor.delete(); i++) {
        assertTrue("Could not delete " + scanCursor.getAbsolutePath() + " in one minute",i < 60);
        try {
          Thread.sleep(1000);
        }
 catch (        InterruptedException ignored) {
        }
      }
      cluster.restartDataNode(dnProps);
      DFSTestUtil.waitReplication(fs,fileName,(short)2);
      String blockPoolId=cluster.getNamesystem().getBlockPoolId();
      final DatanodeID corruptDataNode=InternalDataNodeTestUtils.getDNRegistrationForBP(cluster.getDataNodes().get(2),blockPoolId);
      final FSNamesystem namesystem=cluster.getNamesystem();
      final BlockManager bm=namesystem.getBlockManager();
      final HeartbeatManager hm=bm.getDatanodeManager().getHeartbeatManager();
      try {
        namesystem.writeLock();
synchronized (hm) {
          String corruptMachineName=corruptDataNode.getXferAddr();
          for (          DatanodeDescriptor datanode : hm.getDatanodes()) {
            if (!corruptMachineName.equals(datanode.getXferAddr())) {
              datanode.getStorageInfos()[0].setUtilizationForTesting(100L,100L,0,100L);
              datanode.updateHeartbeat(BlockManagerTestUtil.getStorageReportsForDatanode(datanode),0L,0L,0,0,null);
            }
          }
          NameNodeAdapter.setReplication(namesystem,fileName.toString(),(short)1);
          assertEquals(1,bm.countNodes(bm.getStoredBlock(block.getLocalBlock())).liveReplicas());
        }
      }
  finally {
        namesystem.writeUnlock();
      }
    }
  finally {
      cluster.shutdown();
    }
  }
  static final long SMALL_BLOCK_SIZE=DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_DEFAULT;
  static final long SMALL_FILE_LENGTH=SMALL_BLOCK_SIZE * 4;
  /** 
 * The test verifies that replica for deletion is chosen on a node, with the oldest heartbeat, when this heartbeat is larger than the tolerable heartbeat interval. It creates a file with several blocks and replication 4. The last DN is configured to send heartbeats rarely. Test waits until the tolerable heartbeat interval expires, and reduces replication of the file. All replica deletions should be scheduled for the last node. No replicas will actually be deleted, since last DN doesn't send heartbeats. 
 */
  @Test public void testChooseReplicaToDelete() throws Exception {
    MiniDFSCluster cluster=null;
    FileSystem fs=null;
    try {
      Configuration conf=new HdfsConfiguration();
      conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,SMALL_BLOCK_SIZE);
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
      fs=cluster.getFileSystem();
      final FSNamesystem namesystem=cluster.getNamesystem();
      final BlockManager bm=namesystem.getBlockManager();
      conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,300);
      cluster.startDataNodes(conf,1,true,null,null,null);
      DataNode lastDN=cluster.getDataNodes().get(3);
      DatanodeRegistration dnReg=InternalDataNodeTestUtils.getDNRegistrationForBP(lastDN,namesystem.getBlockPoolId());
      String lastDNid=dnReg.getDatanodeUuid();
      final Path fileName=new Path("/foo2");
      DFSTestUtil.createFile(fs,fileName,SMALL_FILE_LENGTH,(short)4,0L);
      DFSTestUtil.waitReplication(fs,fileName,(short)4);
      DatanodeDescriptor nodeInfo=null;
      long lastHeartbeat=0;
      long waitTime=DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_DEFAULT * 1000 * (DFSConfigKeys.DFS_NAMENODE_TOLERATE_HEARTBEAT_MULTIPLIER_DEFAULT + 1);
      do {
        nodeInfo=bm.getDatanodeManager().getDatanode(dnReg);
        lastHeartbeat=nodeInfo.getLastUpdateMonotonic();
      }
 while (monotonicNow() - lastHeartbeat < waitTime);
      fs.setReplication(fileName,(short)3);
      BlockLocation locs[]=fs.getFileBlockLocations(fs.getFileStatus(fileName),0,Long.MAX_VALUE);
      namesystem.readLock();
      final int dnBlocks=bm.getExcessSize4Testing(dnReg.getDatanodeUuid());
      assertEquals("Replicas on node " + lastDNid + " should have been deleted",SMALL_FILE_LENGTH / SMALL_BLOCK_SIZE,dnBlocks);
      namesystem.readUnlock();
      for (      BlockLocation location : locs)       assertEquals("Block should still have 4 replicas",4,location.getNames().length);
    }
  finally {
      if (fs != null)       fs.close();
      if (cluster != null)       cluster.shutdown();
    }
  }
  /** 
 * Test over replicated block should get invalidated when decreasing the replication for a partial block.
 */
  @Test public void testInvalidateOverReplicatedBlock() throws Exception {
    Configuration conf=new HdfsConfiguration();
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
    try {
      final FSNamesystem namesystem=cluster.getNamesystem();
      final BlockManager bm=namesystem.getBlockManager();
      FileSystem fs=cluster.getFileSystem();
      Path p=new Path(MiniDFSCluster.getBaseDirectory(),"/foo1");
      FSDataOutputStream out=fs.create(p,(short)2);
      out.writeBytes("HDFS-3119: " + p);
      out.hsync();
      fs.setReplication(p,(short)1);
      out.close();
      ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,p);
      assertEquals("Expected only one live replica for the block",1,bm.countNodes(bm.getStoredBlock(block.getLocalBlock())).liveReplicas());
    }
  finally {
      cluster.shutdown();
    }
  }
}
