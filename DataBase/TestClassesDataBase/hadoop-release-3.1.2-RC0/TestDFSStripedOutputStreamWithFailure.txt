/** 
 * Test striped file write operation with data node failures with fixed parameter test cases.
 */
public class TestDFSStripedOutputStreamWithFailure extends TestDFSStripedOutputStreamWithFailureBase {
  public static final Logger LOG=LoggerFactory.getLogger(TestDFSStripedOutputStreamWithFailure.class);
  @Test(timeout=300000) public void testMultipleDatanodeFailure56() throws Exception {
    runTestWithMultipleFailure(getLength(56));
  }
  /** 
 * Randomly pick a length and run tests with multiple data failures. TODO: enable this later
 */
  public void testMultipleDatanodeFailureRandomLength() throws Exception {
    int lenIndex=RANDOM.nextInt(lengths.size());
    LOG.info("run testMultipleDatanodeFailureRandomLength with length index: " + lenIndex);
    runTestWithMultipleFailure(getLength(lenIndex));
  }
  @Test(timeout=240000) public void testBlockTokenExpired() throws Exception {
    final int length=dataBlocks * cellSize * 3;
    final HdfsConfiguration conf=newHdfsConfiguration();
    conf.setBoolean(DFSConfigKeys.DFS_BLOCK_ACCESS_TOKEN_ENABLE_KEY,true);
    conf.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,0);
    conf.setInt(HdfsClientConfigKeys.Retry.WINDOW_BASE_KEY,10);
    for (int dn=0; dn < dataBlocks + parityBlocks; dn+=2) {
      try {
        setup(conf);
        runTest(length,new int[]{length / 2},new int[]{dn},true);
      }
 catch (      Exception e) {
        LOG.error("failed, dn=" + dn + ", length="+ length);
        throw e;
      }
 finally {
        tearDown();
      }
    }
  }
  @Test(timeout=90000) public void testAddBlockWhenNoSufficientDataBlockNumOfNodes() throws Exception {
    HdfsConfiguration conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,blockSize);
    try {
      setup(conf);
      ArrayList<DataNode> dataNodes=cluster.getDataNodes();
      int numDatanodes=dataNodes.size();
      while (numDatanodes >= dataBlocks) {
        cluster.stopDataNode(0);
        numDatanodes--;
      }
      cluster.restartNameNodes();
      cluster.triggerHeartbeats();
      DatanodeInfo[] info=dfs.getClient().datanodeReport(DatanodeReportType.LIVE);
      assertEquals("Mismatches number of live Dns",numDatanodes,info.length);
      final Path dirFile=new Path(dir,"ecfile");
      LambdaTestUtils.intercept(IOException.class,"File " + dirFile + " could only be written to "+ numDatanodes+ " of the "+ dataBlocks+ " required nodes for "+ ecPolicy.getName(),() -> {
        try (FSDataOutputStream out=dfs.create(dirFile,true)){
          out.write("something".getBytes());
          out.flush();
        }
         return 0;
      }
);
    }
  finally {
      tearDown();
    }
  }
  private void testCloseWithExceptionsInStreamer(  int numFailures,  boolean shouldFail) throws Exception {
    assertTrue(numFailures <= ecPolicy.getNumDataUnits() + ecPolicy.getNumParityUnits());
    final Path dirFile=new Path(dir,"ecfile-" + numFailures);
    try (FSDataOutputStream out=dfs.create(dirFile,true)){
      out.write("idempotent close".getBytes());
      LambdaTestUtils.intercept(IOException.class,out::close);
      assertTrue(out.getWrappedStream() instanceof DFSStripedOutputStream);
      DFSStripedOutputStream stripedOut=(DFSStripedOutputStream)out.getWrappedStream();
      for (int i=0; i < numFailures; i++) {
        stripedOut.getStripedDataStreamer(i).getLastException().set(new IOException("injected failure"));
      }
      if (shouldFail) {
        LambdaTestUtils.intercept(IOException.class,out::close);
      }
      out.close();
    }
   }
  @Test public void testIdempotentCloseWithFailedStreams() throws Exception {
    HdfsConfiguration conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,blockSize);
    try {
      setup(conf);
      while (cluster.getDataNodes().size() >= dataBlocks) {
        cluster.stopDataNode(0);
      }
      cluster.restartNameNodes();
      cluster.triggerHeartbeats();
      testCloseWithExceptionsInStreamer(1,false);
      testCloseWithExceptionsInStreamer(ecPolicy.getNumParityUnits(),false);
      testCloseWithExceptionsInStreamer(ecPolicy.getNumParityUnits() + 1,true);
      testCloseWithExceptionsInStreamer(ecPolicy.getNumDataUnits(),true);
    }
  finally {
      tearDown();
    }
  }
  @Test public void testCloseAfterAbort() throws Exception {
    HdfsConfiguration conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,blockSize);
    try {
      setup(conf);
      final Path dirFile=new Path(dir,"ecfile");
      FSDataOutputStream out=dfs.create(dirFile,true);
      assertTrue(out.getWrappedStream() instanceof DFSStripedOutputStream);
      DFSStripedOutputStream stripedOut=(DFSStripedOutputStream)out.getWrappedStream();
      stripedOut.abort();
      LambdaTestUtils.intercept(IOException.class,"Lease timeout",stripedOut::close);
    }
  finally {
      tearDown();
    }
  }
  @Test(timeout=90000) public void testAddBlockWhenNoSufficientParityNumOfNodes() throws IOException {
    HdfsConfiguration conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,blockSize);
    try {
      setup(conf);
      ArrayList<DataNode> dataNodes=cluster.getDataNodes();
      int killDns=(parityBlocks - 1);
      int numDatanodes=dataNodes.size() - killDns;
      for (int i=0; i < killDns; i++) {
        cluster.stopDataNode(i);
      }
      cluster.restartNameNodes();
      cluster.triggerHeartbeats();
      DatanodeInfo[] info=dfs.getClient().datanodeReport(DatanodeReportType.LIVE);
      assertEquals("Mismatches number of live Dns",numDatanodes,info.length);
      Path srcPath=new Path(dir,"testAddBlockWhenNoSufficientParityNodes");
      int fileLength=cellSize - 1000;
      final byte[] expected=StripedFileTestUtil.generateBytes(fileLength);
      DFSTestUtil.writeFile(dfs,srcPath,new String(expected));
      LOG.info("writing finished. Seek and read the file to verify.");
      StripedFileTestUtil.verifySeek(dfs,srcPath,fileLength,ecPolicy,blockGroupSize);
    }
  finally {
      tearDown();
    }
  }
  /** 
 * When the two DataNodes with partial data blocks fail.
 */
  @Test public void testCloseWithExceptionsInStreamer() throws Exception {
    final HdfsConfiguration conf=newHdfsConfiguration();
    final int[] fileLengths={cellSize * (dataBlocks * 2 - 2),(cellSize * dataBlocks) + 123};
    int[] dnIndex=null;
    if (parityBlocks > 1) {
      dnIndex=new int[]{dataBlocks - 2,dataBlocks - 1};
    }
 else {
      dnIndex=new int[]{dataBlocks - 1};
    }
    for (    int length : fileLengths) {
      final int[] killPos=getKillPositions(length,dnIndex.length);
      try {
        LOG.info("runTestWithMultipleFailure2: length==" + length + ", killPos="+ Arrays.toString(killPos)+ ", dnIndex="+ Arrays.toString(dnIndex));
        setup(conf);
        runTest(length,killPos,dnIndex,false);
      }
 catch (      Throwable e) {
        final String err="failed, killPos=" + Arrays.toString(killPos) + ", dnIndex="+ Arrays.toString(dnIndex)+ ", length="+ length;
        LOG.error(err);
        throw e;
      }
 finally {
        tearDown();
      }
    }
  }
  /** 
 * Test writing very short EC files with many failures.
 */
  @Test public void runTestWithShortStripe() throws Exception {
    final HdfsConfiguration conf=newHdfsConfiguration();
    final int length=cellSize - 123;
    final int[] dnIndex=new int[dataBlocks + parityBlocks - 1];
    for (int i=0; i < dnIndex.length; i++) {
      dnIndex[i]=i;
    }
    final int[] killPos=getKillPositions(length,dnIndex.length);
    try {
      LOG.info("runTestWithShortStripe: length==" + length + ", killPos="+ Arrays.toString(killPos)+ ", dnIndex="+ Arrays.toString(dnIndex));
      setup(conf);
      runTest(length,killPos,dnIndex,false);
    }
 catch (    Throwable e) {
      final String err="failed, killPos=" + Arrays.toString(killPos) + ", dnIndex="+ Arrays.toString(dnIndex)+ ", length="+ length;
      LOG.error(err);
      throw e;
    }
 finally {
      tearDown();
    }
  }
}
