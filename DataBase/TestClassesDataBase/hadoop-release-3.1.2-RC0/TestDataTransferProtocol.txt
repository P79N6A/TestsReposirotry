/** 
 * This tests data transfer protocol handling in the Datanode. It sends various forms of wrong data and verifies that Datanode handles it well.
 */
public class TestDataTransferProtocol {
  private static final Log LOG=LogFactory.getLog("org.apache.hadoop.hdfs.TestDataTransferProtocol");
  private static final DataChecksum DEFAULT_CHECKSUM=DataChecksum.newDataChecksum(DataChecksum.Type.CRC32C,512);
  DatanodeID datanode;
  InetSocketAddress dnAddr;
  final ByteArrayOutputStream sendBuf=new ByteArrayOutputStream(128);
  final DataOutputStream sendOut=new DataOutputStream(sendBuf);
  final Sender sender=new Sender(sendOut);
  final ByteArrayOutputStream recvBuf=new ByteArrayOutputStream(128);
  final DataOutputStream recvOut=new DataOutputStream(recvBuf);
  private void sendRecvData(  String testDescription,  boolean eofExpected) throws IOException {
    Socket sock=null;
    try {
      if (testDescription != null) {
        LOG.info("Testing : " + testDescription);
      }
      LOG.info("Going to write:" + StringUtils.byteToHexString(sendBuf.toByteArray()));
      sock=new Socket();
      sock.connect(dnAddr,HdfsConstants.READ_TIMEOUT);
      sock.setSoTimeout(HdfsConstants.READ_TIMEOUT);
      OutputStream out=sock.getOutputStream();
      byte[] retBuf=new byte[recvBuf.size()];
      DataInputStream in=new DataInputStream(sock.getInputStream());
      out.write(sendBuf.toByteArray());
      out.flush();
      try {
        in.readFully(retBuf);
      }
 catch (      EOFException eof) {
        if (eofExpected) {
          LOG.info("Got EOF as expected.");
          return;
        }
        throw eof;
      }
      String received=StringUtils.byteToHexString(retBuf);
      String expected=StringUtils.byteToHexString(recvBuf.toByteArray());
      LOG.info("Received: " + received);
      LOG.info("Expected: " + expected);
      if (eofExpected) {
        throw new IOException("Did not receive IOException when an exception " + "is expected while reading from " + datanode);
      }
      assertEquals(expected,received);
    }
  finally {
      IOUtils.closeSocket(sock);
    }
  }
  void readFile(  FileSystem fs,  Path path,  int fileLen) throws IOException {
    byte[] arr=new byte[fileLen];
    FSDataInputStream in=fs.open(path);
    in.readFully(arr);
  }
  private void writeZeroLengthPacket(  ExtendedBlock block,  String description) throws IOException {
    PacketHeader hdr=new PacketHeader(8,block.getNumBytes(),100,true,0,false);
    hdr.write(sendOut);
    sendOut.writeInt(0);
    sendResponse(Status.SUCCESS,"",null,recvOut);
    new PipelineAck(100,new int[]{PipelineAck.combineHeader(PipelineAck.ECN.DISABLED,Status.SUCCESS)}).write(recvOut);
    sendRecvData(description,false);
  }
  private void sendResponse(  Status status,  String firstBadLink,  String message,  DataOutputStream out) throws IOException {
    Builder builder=BlockOpResponseProto.newBuilder().setStatus(status);
    if (firstBadLink != null) {
      builder.setFirstBadLink(firstBadLink);
    }
    if (message != null) {
      builder.setMessage(message);
    }
    builder.build().writeDelimitedTo(out);
  }
  private void testWrite(  ExtendedBlock block,  BlockConstructionStage stage,  long newGS,  String description,  Boolean eofExcepted) throws IOException {
    sendBuf.reset();
    recvBuf.reset();
    writeBlock(block,stage,newGS,DEFAULT_CHECKSUM);
    if (eofExcepted) {
      sendResponse(Status.ERROR,null,null,recvOut);
      sendRecvData(description,true);
    }
 else     if (stage == BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {
      sendResponse(Status.SUCCESS,"",null,recvOut);
      sendRecvData(description,false);
    }
 else {
      writeZeroLengthPacket(block,description);
    }
  }
  @Test public void testOpWrite() throws IOException {
    int numDataNodes=1;
    final long BLOCK_ID_FUDGE=128;
    Configuration conf=new HdfsConfiguration();
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();
    try {
      cluster.waitActive();
      String poolId=cluster.getNamesystem().getBlockPoolId();
      datanode=InternalDataNodeTestUtils.getDNRegistrationForBP(cluster.getDataNodes().get(0),poolId);
      dnAddr=NetUtils.createSocketAddr(datanode.getXferAddr());
      FileSystem fileSys=cluster.getFileSystem();
      Path file=new Path("dataprotocol.dat");
      DFSTestUtil.createFile(fileSys,file,1L,(short)numDataNodes,0L);
      ExtendedBlock firstBlock=DFSTestUtil.getFirstBlock(fileSys,file);
      testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_CREATE,0L,"Cannot create an existing block",true);
      testWrite(firstBlock,BlockConstructionStage.DATA_STREAMING,0L,"Unexpected stage",true);
      long newGS=firstBlock.getGenerationStamp() + 1;
      testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_STREAMING_RECOVERY,newGS,"Cannot recover data streaming to a finalized replica",true);
      newGS=firstBlock.getGenerationStamp() + 1;
      testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_APPEND,newGS,"Append to a finalized replica",false);
      firstBlock.setGenerationStamp(newGS);
      file=new Path("dataprotocol1.dat");
      DFSTestUtil.createFile(fileSys,file,1L,(short)numDataNodes,0L);
      firstBlock=DFSTestUtil.getFirstBlock(fileSys,file);
      newGS=firstBlock.getGenerationStamp() + 1;
      testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_APPEND_RECOVERY,newGS,"Recover appending to a finalized replica",false);
      file=new Path("dataprotocol2.dat");
      DFSTestUtil.createFile(fileSys,file,1L,(short)numDataNodes,0L);
      firstBlock=DFSTestUtil.getFirstBlock(fileSys,file);
      newGS=firstBlock.getGenerationStamp() + 1;
      testWrite(firstBlock,BlockConstructionStage.PIPELINE_CLOSE_RECOVERY,newGS,"Recover failed close to a finalized replica",false);
      firstBlock.setGenerationStamp(newGS);
      long newBlockId=firstBlock.getBlockId() + BLOCK_ID_FUDGE;
      ExtendedBlock newBlock=new ExtendedBlock(firstBlock.getBlockPoolId(),newBlockId,0,firstBlock.getGenerationStamp());
      testWrite(newBlock,BlockConstructionStage.PIPELINE_SETUP_CREATE,0L,"Create a new block",false);
      newGS=newBlock.getGenerationStamp() + 1;
      newBlock.setBlockId(newBlock.getBlockId() + 1);
      testWrite(newBlock,BlockConstructionStage.PIPELINE_SETUP_STREAMING_RECOVERY,newGS,"Recover a new block",true);
      newGS=newBlock.getGenerationStamp() + 1;
      testWrite(newBlock,BlockConstructionStage.PIPELINE_SETUP_APPEND,newGS,"Cannot append to a new block",true);
      newBlock.setBlockId(newBlock.getBlockId() + 1);
      newGS=newBlock.getGenerationStamp() + 1;
      testWrite(newBlock,BlockConstructionStage.PIPELINE_SETUP_APPEND_RECOVERY,newGS,"Cannot append to a new block",true);
      Path file1=new Path("dataprotocol1.dat");
      DFSTestUtil.createFile(fileSys,file1,1L,(short)numDataNodes,0L);
      DFSOutputStream out=(DFSOutputStream)(fileSys.append(file1).getWrappedStream());
      out.write(1);
      out.hflush();
      FSDataInputStream in=fileSys.open(file1);
      firstBlock=DFSTestUtil.getAllBlocks(in).get(0).getBlock();
      firstBlock.setNumBytes(2L);
      try {
        testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_CREATE,0L,"Cannot create a RBW block",true);
        newGS=firstBlock.getGenerationStamp() + 1;
        testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_APPEND,newGS,"Cannot append to a RBW replica",true);
        testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_APPEND_RECOVERY,newGS,"Recover append to a RBW replica",false);
        firstBlock.setGenerationStamp(newGS);
        file=new Path("dataprotocol2.dat");
        DFSTestUtil.createFile(fileSys,file,1L,(short)numDataNodes,0L);
        out=(DFSOutputStream)(fileSys.append(file).getWrappedStream());
        out.write(1);
        out.hflush();
        in=fileSys.open(file);
        firstBlock=DFSTestUtil.getAllBlocks(in).get(0).getBlock();
        firstBlock.setNumBytes(2L);
        newGS=firstBlock.getGenerationStamp() + 1;
        testWrite(firstBlock,BlockConstructionStage.PIPELINE_SETUP_STREAMING_RECOVERY,newGS,"Recover a RBW replica",false);
      }
  finally {
        IOUtils.closeStream(in);
        IOUtils.closeStream(out);
      }
    }
  finally {
      cluster.shutdown();
    }
  }
  @Test public void testDataTransferProtocol() throws IOException {
    Random random=new Random();
    int oneMil=1024 * 1024;
    Path file=new Path("dataprotocol.dat");
    int numDataNodes=1;
    Configuration conf=new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY,numDataNodes);
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();
    try {
      cluster.waitActive();
      datanode=cluster.getFileSystem().getDataNodeStats(DatanodeReportType.LIVE)[0];
      dnAddr=NetUtils.createSocketAddr(datanode.getXferAddr());
      FileSystem fileSys=cluster.getFileSystem();
      int fileLen=Math.min(conf.getInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,4096),4096);
      DFSTestUtil.createFile(fileSys,file,fileLen,fileLen,fileSys.getDefaultBlockSize(file),fileSys.getDefaultReplication(file),0L);
      final ExtendedBlock firstBlock=DFSTestUtil.getFirstBlock(fileSys,file);
      final String poolId=firstBlock.getBlockPoolId();
      long newBlockId=firstBlock.getBlockId() + 1;
      recvBuf.reset();
      sendBuf.reset();
      recvOut.writeShort((short)(DataTransferProtocol.DATA_TRANSFER_VERSION - 1));
      sendOut.writeShort((short)(DataTransferProtocol.DATA_TRANSFER_VERSION - 1));
      sendRecvData("Wrong Version",true);
      sendBuf.reset();
      sendOut.writeShort((short)DataTransferProtocol.DATA_TRANSFER_VERSION);
      sendOut.writeByte(Op.WRITE_BLOCK.code - 1);
      sendRecvData("Wrong Op Code",true);
      sendBuf.reset();
      DataChecksum badChecksum=Mockito.spy(DEFAULT_CHECKSUM);
      Mockito.doReturn(-1).when(badChecksum).getBytesPerChecksum();
      writeBlock(poolId,newBlockId,badChecksum);
      recvBuf.reset();
      sendResponse(Status.ERROR,null,null,recvOut);
      sendRecvData("wrong bytesPerChecksum while writing",true);
      sendBuf.reset();
      recvBuf.reset();
      writeBlock(poolId,++newBlockId,DEFAULT_CHECKSUM);
      PacketHeader hdr=new PacketHeader(4,0,100,false,-1 - random.nextInt(oneMil),false);
      hdr.write(sendOut);
      sendResponse(Status.SUCCESS,"",null,recvOut);
      new PipelineAck(100,new int[]{PipelineAck.combineHeader(PipelineAck.ECN.DISABLED,Status.ERROR)}).write(recvOut);
      sendRecvData("negative DATA_CHUNK len while writing block " + newBlockId,true);
      sendBuf.reset();
      recvBuf.reset();
      writeBlock(poolId,++newBlockId,DEFAULT_CHECKSUM);
      hdr=new PacketHeader(8,0,100,true,0,false);
      hdr.write(sendOut);
      sendOut.writeInt(0);
      sendOut.flush();
      sendResponse(Status.SUCCESS,"",null,recvOut);
      new PipelineAck(100,new int[]{PipelineAck.combineHeader(PipelineAck.ECN.DISABLED,Status.SUCCESS)}).write(recvOut);
      sendRecvData("Writing a zero len block blockid " + newBlockId,false);
      String bpid=cluster.getNamesystem().getBlockPoolId();
      ExtendedBlock blk=new ExtendedBlock(bpid,firstBlock.getLocalBlock());
      long blkid=blk.getBlockId();
      sendBuf.reset();
      recvBuf.reset();
      blk.setBlockId(blkid - 1);
      sender.readBlock(blk,BlockTokenSecretManager.DUMMY_TOKEN,"cl",0L,fileLen,true,CachingStrategy.newDefaultStrategy());
      sendRecvData("Wrong block ID " + newBlockId + " for read",false);
      sendBuf.reset();
      blk.setBlockId(blkid);
      sender.readBlock(blk,BlockTokenSecretManager.DUMMY_TOKEN,"cl",-1L,fileLen,true,CachingStrategy.newDefaultStrategy());
      sendRecvData("Negative start-offset for read for block " + firstBlock.getBlockId(),false);
      sendBuf.reset();
      sender.readBlock(blk,BlockTokenSecretManager.DUMMY_TOKEN,"cl",fileLen,fileLen,true,CachingStrategy.newDefaultStrategy());
      sendRecvData("Wrong start-offset for reading block " + firstBlock.getBlockId(),false);
      recvBuf.reset();
      BlockOpResponseProto.newBuilder().setStatus(Status.SUCCESS).setReadOpChecksumInfo(ReadOpChecksumInfoProto.newBuilder().setChecksum(DataTransferProtoUtil.toProto(DEFAULT_CHECKSUM)).setChunkOffset(0L)).build().writeDelimitedTo(recvOut);
      sendBuf.reset();
      sender.readBlock(blk,BlockTokenSecretManager.DUMMY_TOKEN,"cl",0L,-1L - random.nextInt(oneMil),true,CachingStrategy.newDefaultStrategy());
      sendRecvData("Negative length for reading block " + firstBlock.getBlockId(),false);
      recvBuf.reset();
      sendResponse(Status.ERROR,null,"opReadBlock " + firstBlock + " received exception java.io.IOException:  "+ "Offset 0 and length 4097 don't match block "+ firstBlock+ " ( blockLen 4096 )",recvOut);
      sendBuf.reset();
      sender.readBlock(blk,BlockTokenSecretManager.DUMMY_TOKEN,"cl",0L,fileLen + 1,true,CachingStrategy.newDefaultStrategy());
      sendRecvData("Wrong length for reading block " + firstBlock.getBlockId(),false);
      sendBuf.reset();
      sender.readBlock(blk,BlockTokenSecretManager.DUMMY_TOKEN,"cl",0L,fileLen,true,CachingStrategy.newDefaultStrategy());
      readFile(fileSys,file,fileLen);
    }
  finally {
      cluster.shutdown();
    }
  }
  @Test public void testPacketHeader() throws IOException {
    PacketHeader hdr=new PacketHeader(4,1024,100,false,4096,false);
    ByteArrayOutputStream baos=new ByteArrayOutputStream();
    hdr.write(new DataOutputStream(baos));
    PacketHeader readBack=new PacketHeader();
    ByteArrayInputStream bais=new ByteArrayInputStream(baos.toByteArray());
    readBack.readFields(new DataInputStream(bais));
    assertEquals(hdr,readBack);
    readBack=new PacketHeader();
    readBack.readFields(ByteBuffer.wrap(baos.toByteArray()));
    assertEquals(hdr,readBack);
    assertTrue(hdr.sanityCheck(99));
    assertFalse(hdr.sanityCheck(100));
  }
  @Test public void TestPipeLineAckCompatibility() throws IOException {
    DataTransferProtos.PipelineAckProto proto=DataTransferProtos.PipelineAckProto.newBuilder().setSeqno(0).addReply(Status.CHECKSUM_OK).build();
    DataTransferProtos.PipelineAckProto newProto=DataTransferProtos.PipelineAckProto.newBuilder().mergeFrom(proto).addFlag(PipelineAck.combineHeader(PipelineAck.ECN.SUPPORTED,Status.CHECKSUM_OK)).build();
    ByteArrayOutputStream oldAckBytes=new ByteArrayOutputStream();
    proto.writeDelimitedTo(oldAckBytes);
    PipelineAck oldAck=new PipelineAck();
    oldAck.readFields(new ByteArrayInputStream(oldAckBytes.toByteArray()));
    assertEquals(PipelineAck.combineHeader(PipelineAck.ECN.DISABLED,Status.CHECKSUM_OK),oldAck.getHeaderFlag(0));
    PipelineAck newAck=new PipelineAck();
    ByteArrayOutputStream newAckBytes=new ByteArrayOutputStream();
    newProto.writeDelimitedTo(newAckBytes);
    newAck.readFields(new ByteArrayInputStream(newAckBytes.toByteArray()));
    assertEquals(PipelineAck.combineHeader(PipelineAck.ECN.SUPPORTED,Status.CHECKSUM_OK),newAck.getHeaderFlag(0));
  }
  void writeBlock(  String poolId,  long blockId,  DataChecksum checksum) throws IOException {
    writeBlock(new ExtendedBlock(poolId,blockId),BlockConstructionStage.PIPELINE_SETUP_CREATE,0L,checksum);
  }
  void writeBlock(  ExtendedBlock block,  BlockConstructionStage stage,  long newGS,  DataChecksum checksum) throws IOException {
    sender.writeBlock(block,StorageType.DEFAULT,BlockTokenSecretManager.DUMMY_TOKEN,"cl",new DatanodeInfo[1],new StorageType[1],null,stage,0,block.getNumBytes(),block.getNumBytes(),newGS,checksum,CachingStrategy.newDefaultStrategy(),false,false,null,null,new String[0]);
  }
}
