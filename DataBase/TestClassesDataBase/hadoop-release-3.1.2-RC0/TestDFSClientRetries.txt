/** 
 * These tests make sure that DFSClient retries fetching data from DFS properly in case of errors.
 */
public class TestDFSClientRetries {
  private static final String ADDRESS="0.0.0.0";
  final static private int PING_INTERVAL=1000;
  final static private int MIN_SLEEP_TIME=1000;
  public static final Log LOG=LogFactory.getLog(TestDFSClientRetries.class.getName());
  static private Configuration conf=null;
private static class TestServer extends Server {
    private boolean sleep;
    private Class<? extends Writable> responseClass;
    public TestServer(    int handlerCount,    boolean sleep) throws IOException {
      this(handlerCount,sleep,LongWritable.class,null);
    }
    public TestServer(    int handlerCount,    boolean sleep,    Class<? extends Writable> paramClass,    Class<? extends Writable> responseClass) throws IOException {
      super(ADDRESS,0,paramClass,handlerCount,conf);
      this.sleep=sleep;
      this.responseClass=responseClass;
    }
    @Override public Writable call(    RPC.RpcKind rpcKind,    String protocol,    Writable param,    long receiveTime) throws IOException {
      if (sleep) {
        try {
          Thread.sleep(PING_INTERVAL + MIN_SLEEP_TIME);
        }
 catch (        InterruptedException e) {
        }
      }
      if (responseClass != null) {
        try {
          return responseClass.newInstance();
        }
 catch (        Exception e) {
          throw new RuntimeException(e);
        }
      }
 else {
        return param;
      }
    }
  }
  private static void writeData(  OutputStream out,  int len) throws IOException {
    byte[] buf=new byte[4096 * 16];
    while (len > 0) {
      int toWrite=Math.min(len,buf.length);
      out.write(buf,0,toWrite);
      len-=toWrite;
    }
  }
  @Before public void setupConf(){
    conf=new HdfsConfiguration();
  }
  /** 
 * This makes sure that when DN closes clients socket after client had successfully connected earlier, the data can still be fetched.
 */
  @Test public void testWriteTimeoutAtDataNode() throws IOException, InterruptedException {
    final int writeTimeout=100;
    conf.setInt(HdfsClientConfigKeys.DFS_DATANODE_SOCKET_WRITE_TIMEOUT_KEY,writeTimeout);
    final int blockSize=10 * 1024 * 1024;
    conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,blockSize);
    conf.setInt(HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_KEY,1);
    final int bufferSize=4096;
    conf.setInt(CommonConfigurationKeys.IO_FILE_BUFFER_SIZE_KEY,bufferSize);
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
    try {
      cluster.waitActive();
      FileSystem fs=cluster.getFileSystem();
      Path filePath=new Path("/testWriteTimeoutAtDataNode");
      OutputStream out=fs.create(filePath,true,bufferSize);
      writeData(out,2 * blockSize);
      out.close();
      byte[] buf=new byte[1024 * 1024];
      InputStream in=fs.open(filePath,bufferSize);
      IOUtils.readFully(in,buf,0,bufferSize / 2);
      for (int i=0; i < 10; i++) {
        Thread.sleep(2 * writeTimeout);
        IOUtils.readFully(in,buf,0,buf.length);
      }
      in.close();
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Verify that client will correctly give up after the specified number of times trying to add a block
 */
  @SuppressWarnings({"serial","unchecked"}) @Test public void testNotYetReplicatedErrors() throws IOException {
    final String exceptionMsg="Nope, not replicated yet...";
    final int maxRetries=1;
    conf.setInt(HdfsClientConfigKeys.BlockWrite.LOCATEFOLLOWINGBLOCK_RETRIES_KEY,maxRetries);
    NamenodeProtocols mockNN=mock(NamenodeProtocols.class);
    Answer<Object> answer=new ThrowsException(new IOException()){
      int retryCount=0;
      @Override public Object answer(      InvocationOnMock invocation) throws Throwable {
        retryCount++;
        System.out.println("addBlock has been called " + retryCount + " times");
        if (retryCount > maxRetries + 1)         throw new IOException("Retried too many times: " + retryCount);
 else         throw new RemoteException(NotReplicatedYetException.class.getName(),exceptionMsg);
      }
    }
;
    when(mockNN.addBlock(anyString(),anyString(),any(ExtendedBlock.class),any(DatanodeInfo[].class),anyLong(),any(String[].class),Matchers.<EnumSet<AddBlockFlag>>any())).thenAnswer(answer);
    Mockito.doReturn(new HdfsFileStatus.Builder().replication(1).blocksize(1024).perm(new FsPermission((short)777)).owner("owner").group("group").symlink(new byte[0]).fileId(1010).build()).when(mockNN).getFileInfo(anyString());
    Mockito.doReturn(new HdfsFileStatus.Builder().replication(1).blocksize(1024).perm(new FsPermission((short)777)).owner("owner").group("group").symlink(new byte[0]).fileId(1010).build()).when(mockNN).create(anyString(),(FsPermission)anyObject(),anyString(),(EnumSetWritable<CreateFlag>)anyObject(),anyBoolean(),anyShort(),anyLong(),(CryptoProtocolVersion[])anyObject(),anyObject());
    final DFSClient client=new DFSClient(null,mockNN,conf,null);
    OutputStream os=client.create("testfile",true);
    os.write(20);
    try {
      os.close();
    }
 catch (    Exception e) {
      assertTrue("Retries are not being stopped correctly: " + e.getMessage(),e.getMessage().equals(exceptionMsg));
    }
  }
  /** 
 * This tests that DFSInputStream failures are counted for a given read operation, and not over the lifetime of the stream. It is a regression test for HDFS-127.
 */
  @Test public void testFailuresArePerOperation() throws Exception {
    long fileSize=4096;
    Path file=new Path("/testFile");
    conf.setInt(HdfsClientConfigKeys.Retry.WINDOW_BASE_KEY,10);
    conf.setInt(DFS_CLIENT_SOCKET_TIMEOUT_KEY,2 * 1000);
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();
    try {
      cluster.waitActive();
      FileSystem fs=cluster.getFileSystem();
      NamenodeProtocols preSpyNN=cluster.getNameNodeRpc();
      NamenodeProtocols spyNN=spy(preSpyNN);
      DFSClient client=new DFSClient(null,spyNN,conf,null);
      int maxBlockAcquires=client.getConf().getMaxBlockAcquireFailures();
      assertTrue(maxBlockAcquires > 0);
      DFSTestUtil.createFile(fs,file,fileSize,(short)1,12345L);
      doAnswer(new FailNTimesAnswer(preSpyNN,maxBlockAcquires + 1)).when(spyNN).getBlockLocations(anyString(),anyLong(),anyLong());
      try {
        IOUtils.copyBytes(client.open(file.toString()),new IOUtils.NullOutputStream(),conf,true);
        fail("Didn't get exception");
      }
 catch (      IOException ioe) {
        DFSClient.LOG.info("Got expected exception",ioe);
      }
      doAnswer(new FailNTimesAnswer(preSpyNN,maxBlockAcquires)).when(spyNN).getBlockLocations(anyString(),anyLong(),anyLong());
      IOUtils.copyBytes(client.open(file.toString()),new IOUtils.NullOutputStream(),conf,true);
      DFSClient.LOG.info("Starting test case for failure reset");
      doAnswer(new FailNTimesAnswer(preSpyNN,maxBlockAcquires)).when(spyNN).getBlockLocations(anyString(),anyLong(),anyLong());
      DFSInputStream is=client.open(file.toString());
      byte buf[]=new byte[10];
      IOUtils.readFully(is,buf,0,buf.length);
      DFSClient.LOG.info("First read successful after some failures.");
      doAnswer(new FailNTimesAnswer(preSpyNN,maxBlockAcquires)).when(spyNN).getBlockLocations(anyString(),anyLong(),anyLong());
      is.openInfo(true);
      is.seek(0);
      IOUtils.readFully(is,buf,0,buf.length);
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Test DFSClient can continue to function after renewLease RPC receives SocketTimeoutException.
 */
  @Test public void testLeaseRenewSocketTimeout() throws Exception {
    String file1="/testFile1";
    String file2="/testFile2";
    conf.setInt(HdfsClientConfigKeys.Retry.WINDOW_BASE_KEY,10);
    conf.setInt(DFS_CLIENT_SOCKET_TIMEOUT_KEY,2 * 1000);
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();
    try {
      cluster.waitActive();
      NamenodeProtocols spyNN=spy(cluster.getNameNodeRpc());
      Mockito.doThrow(new SocketTimeoutException()).when(spyNN).renewLease(Mockito.anyString());
      DFSClient client=new DFSClient(null,spyNN,conf,null);
      LeaseRenewer leaseRenewer=client.getLeaseRenewer();
      leaseRenewer.setRenewalTime(100);
      OutputStream out1=client.create(file1,false);
      Mockito.verify(spyNN,timeout(10000).times(1)).renewLease(Mockito.anyString());
      verifyEmptyLease(leaseRenewer);
      try {
        out1.write(new byte[256]);
        fail("existing output stream should be aborted");
      }
 catch (      IOException e) {
      }
      client.exists(file2);
      Mockito.doNothing().when(spyNN).renewLease(Mockito.anyString());
      leaseRenewer=client.getLeaseRenewer();
      leaseRenewer.setRenewalTime(100);
      OutputStream out2=client.create(file2,false);
      Mockito.verify(spyNN,timeout(10000).times(2)).renewLease(Mockito.anyString());
      out2.write(new byte[256]);
      out2.close();
      verifyEmptyLease(leaseRenewer);
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Test that getAdditionalBlock() and close() are idempotent. This allows a client to safely retry a call and still produce a correct file. See HDFS-3031.
 */
  @Test public void testIdempotentAllocateBlockAndClose() throws Exception {
    final String src="/testIdempotentAllocateBlock";
    Path file=new Path(src);
    conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,4096);
    final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();
    try {
      cluster.waitActive();
      FileSystem fs=cluster.getFileSystem();
      NamenodeProtocols preSpyNN=cluster.getNameNodeRpc();
      NamenodeProtocols spyNN=spy(preSpyNN);
      DFSClient client=new DFSClient(null,spyNN,conf,null);
      doAnswer(new Answer<LocatedBlock>(){
        private int getBlockCount(        LocatedBlock ret) throws IOException {
          LocatedBlocks lb=cluster.getNameNodeRpc().getBlockLocations(src,0,Long.MAX_VALUE);
          assertEquals(lb.getLastLocatedBlock().getBlock(),ret.getBlock());
          return lb.getLocatedBlocks().size();
        }
        @Override public LocatedBlock answer(        InvocationOnMock invocation) throws Throwable {
          LOG.info("Called addBlock: " + Arrays.toString(invocation.getArguments()));
          final LocatedBlock ret;
          try {
            ret=(LocatedBlock)invocation.callRealMethod();
          }
 catch (          NotReplicatedYetException e) {
            throw new RemoteException(e.getClass().getName(),e.getMessage());
          }
          final int blockCount=getBlockCount(ret);
          final LocatedBlock ret2;
          try {
            ret2=(LocatedBlock)invocation.callRealMethod();
          }
 catch (          NotReplicatedYetException e) {
            throw new AssertionError("Unexpected exception",e);
          }
          final int blockCount2=getBlockCount(ret2);
          assertEquals(blockCount,blockCount2);
          return ret2;
        }
      }
).when(spyNN).addBlock(Mockito.anyString(),Mockito.anyString(),Mockito.<ExtendedBlock>any(),Mockito.<DatanodeInfo[]>any(),Mockito.anyLong(),Mockito.<String[]>any(),Mockito.<EnumSet<AddBlockFlag>>any());
      doAnswer(new Answer<Boolean>(){
        @Override public Boolean answer(        InvocationOnMock invocation) throws Throwable {
          LOG.info("Called complete:");
          if (!(Boolean)invocation.callRealMethod()) {
            LOG.info("Complete call returned false, not faking a retry RPC");
            return false;
          }
          try {
            boolean ret=(Boolean)invocation.callRealMethod();
            LOG.info("Complete call returned true, faked second RPC. " + "Returned: " + ret);
            return ret;
          }
 catch (          Throwable t) {
            LOG.error("Idempotent retry threw exception",t);
            throw t;
          }
        }
      }
).when(spyNN).complete(Mockito.anyString(),Mockito.anyString(),Mockito.<ExtendedBlock>any(),anyLong());
      OutputStream stm=client.create(file.toString(),true);
      try {
        AppendTestUtil.write(stm,0,10000);
        stm.close();
        stm=null;
      }
  finally {
        IOUtils.cleanup(LOG,stm);
      }
      Mockito.verify(spyNN,Mockito.atLeastOnce()).addBlock(Mockito.anyString(),Mockito.anyString(),Mockito.<ExtendedBlock>any(),Mockito.<DatanodeInfo[]>any(),Mockito.anyLong(),Mockito.<String[]>any(),Mockito.<EnumSet<AddBlockFlag>>any());
      Mockito.verify(spyNN,Mockito.atLeastOnce()).complete(Mockito.anyString(),Mockito.anyString(),Mockito.<ExtendedBlock>any(),anyLong());
      AppendTestUtil.check(fs,file,10000);
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Mock Answer implementation of NN.getBlockLocations that will return a poisoned block list a certain number of times before returning a proper one.
 */
private static class FailNTimesAnswer implements Answer<LocatedBlocks> {
    private int failuresLeft;
    private final NamenodeProtocols realNN;
    public FailNTimesAnswer(    NamenodeProtocols preSpyNN,    int timesToFail){
      failuresLeft=timesToFail;
      this.realNN=preSpyNN;
    }
    @Override public LocatedBlocks answer(    InvocationOnMock invocation) throws IOException {
      Object args[]=invocation.getArguments();
      LocatedBlocks realAnswer=realNN.getBlockLocations((String)args[0],(Long)args[1],(Long)args[2]);
      if (failuresLeft-- > 0) {
        NameNode.LOG.info("FailNTimesAnswer injecting failure.");
        return makeBadBlockList(realAnswer);
      }
      NameNode.LOG.info("FailNTimesAnswer no longer failing.");
      return realAnswer;
    }
    private LocatedBlocks makeBadBlockList(    LocatedBlocks goodBlockList){
      LocatedBlock goodLocatedBlock=goodBlockList.get(0);
      LocatedBlock badLocatedBlock=new LocatedBlock(goodLocatedBlock.getBlock(),new DatanodeInfo[]{DFSTestUtil.getDatanodeInfo("1.2.3.4","bogus",1234)});
      badLocatedBlock.setStartOffset(goodLocatedBlock.getStartOffset());
      List<LocatedBlock> badBlocks=new ArrayList<LocatedBlock>();
      badBlocks.add(badLocatedBlock);
      return new LocatedBlocks(goodBlockList.getFileLength(),false,badBlocks,null,true,null,null);
    }
  }
  /** 
 * Test that a DFSClient waits for random time before retry on busy blocks.
 */
  @Test public void testDFSClientRetriesOnBusyBlocks() throws IOException {
    System.out.println("Testing DFSClient random waiting on busy blocks.");
    int xcievers=2;
    int fileLen=6 * 1024 * 1024;
    int threads=50;
    int retries=3;
    int timeWin=300;
    long timestamp=Time.now();
    boolean pass=busyTest(xcievers,threads,fileLen,timeWin,retries);
    long timestamp2=Time.now();
    if (pass) {
      LOG.info("Test 1 succeeded! Time spent: " + (timestamp2 - timestamp) / 1000.0 + " sec.");
    }
 else {
      LOG.warn("Test 1 failed, but relax. Time spent: " + (timestamp2 - timestamp) / 1000.0 + " sec.");
    }
    retries=50;
    timestamp=Time.now();
    pass=busyTest(xcievers,threads,fileLen,timeWin,retries);
    timestamp2=Time.now();
    assertTrue("Something wrong! Test 2 got Exception with maxmum retries!",pass);
    LOG.info("Test 2 succeeded! Time spent: " + (timestamp2 - timestamp) / 1000.0 + " sec.");
    retries=3;
    timeWin=1000;
    timestamp=Time.now();
    pass=busyTest(xcievers,threads,fileLen,timeWin,retries);
    timestamp2=Time.now();
    if (pass) {
      LOG.info("Test 3 succeeded! Time spent: " + (timestamp2 - timestamp) / 1000.0 + " sec.");
    }
 else {
      LOG.warn("Test 3 failed, but relax. Time spent: " + (timestamp2 - timestamp) / 1000.0 + " sec.");
    }
    retries=50;
    timeWin=1000;
    timestamp=Time.now();
    pass=busyTest(xcievers,threads,fileLen,timeWin,retries);
    timestamp2=Time.now();
    assertTrue("Something wrong! Test 4 got Exception with maxmum retries!",pass);
    LOG.info("Test 4 succeeded! Time spent: " + (timestamp2 - timestamp) / 1000.0 + " sec.");
  }
  private boolean busyTest(  int xcievers,  int threads,  int fileLen,  int timeWin,  int retries) throws IOException {
    boolean ret=true;
    short replicationFactor=1;
    long blockSize=128 * 1024 * 1024;
    int bufferSize=4096;
    int originalXcievers=conf.getInt(DFSConfigKeys.DFS_DATANODE_MAX_RECEIVER_THREADS_KEY,DFSConfigKeys.DFS_DATANODE_MAX_RECEIVER_THREADS_DEFAULT);
    conf.setInt(DFSConfigKeys.DFS_DATANODE_MAX_RECEIVER_THREADS_KEY,xcievers);
    conf.setInt(HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_KEY,retries);
    conf.setInt(HdfsClientConfigKeys.Retry.WINDOW_BASE_KEY,timeWin);
    conf.setInt(DFSConfigKeys.DFS_DATANODE_SOCKET_REUSE_KEEPALIVE_KEY,0);
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(replicationFactor).build();
    cluster.waitActive();
    FileSystem fs=cluster.getFileSystem();
    Path file1=new Path("test_data.dat");
    file1=file1.makeQualified(fs.getUri(),fs.getWorkingDirectory());
    try {
      FSDataOutputStream stm=fs.create(file1,true,bufferSize,replicationFactor,blockSize);
      assertTrue(file1 + " should be a file",fs.getFileStatus(file1).isFile());
      System.out.println("Path : \"" + file1 + "\"");
      LOG.info("Path : \"" + file1 + "\"");
      byte[] buffer=AppendTestUtil.randomBytes(Time.now(),fileLen);
      stm.write(buffer,0,fileLen);
      stm.close();
      long len=fs.getFileStatus(file1).getLen();
      assertTrue(file1 + " should be of size " + fileLen+ " but found to be of size "+ len,len == fileLen);
      byte[] read_buf=new byte[fileLen];
      InputStream in=fs.open(file1,fileLen);
      IOUtils.readFully(in,read_buf,0,fileLen);
      assert (Arrays.equals(buffer,read_buf));
      in.close();
      read_buf=null;
      MessageDigest m=MessageDigest.getInstance("SHA");
      m.update(buffer,0,fileLen);
      byte[] hash_sha=m.digest();
      Thread[] readers=new Thread[threads];
      Counter counter=new Counter(0);
      for (int i=0; i < threads; ++i) {
        DFSClientReader reader=new DFSClientReader(file1,cluster,hash_sha,fileLen,counter);
        readers[i]=new Thread(reader);
        readers[i].start();
      }
      for (int i=0; i < threads; ++i) {
        readers[i].join();
      }
      if (counter.get() == threads)       ret=true;
 else       ret=false;
    }
 catch (    InterruptedException e) {
      System.out.println("Thread got InterruptedException.");
      e.printStackTrace();
      ret=false;
    }
catch (    Exception e) {
      e.printStackTrace();
      ret=false;
    }
 finally {
      conf.setInt(DFSConfigKeys.DFS_DATANODE_MAX_RECEIVER_THREADS_KEY,originalXcievers);
      fs.delete(file1,false);
      cluster.shutdown();
    }
    return ret;
  }
  private void verifyEmptyLease(  LeaseRenewer leaseRenewer) throws Exception {
    int sleepCount=0;
    while (!leaseRenewer.isEmpty() && sleepCount++ < 20) {
      Thread.sleep(500);
    }
    assertTrue("Lease should be empty.",leaseRenewer.isEmpty());
  }
class DFSClientReader implements Runnable {
    DFSClient client;
    final Configuration conf;
    final byte[] expected_sha;
    FileSystem fs;
    final Path filePath;
    final MiniDFSCluster cluster;
    final int len;
    final Counter counter;
    DFSClientReader(    Path file,    MiniDFSCluster cluster,    byte[] hash_sha,    int fileLen,    Counter cnt){
      filePath=file;
      this.cluster=cluster;
      counter=cnt;
      len=fileLen;
      conf=new HdfsConfiguration();
      expected_sha=hash_sha;
      try {
        cluster.waitActive();
      }
 catch (      IOException e) {
        e.printStackTrace();
      }
    }
    @Override public void run(){
      try {
        fs=cluster.getNewFileSystemInstance(0);
        int bufferSize=len;
        byte[] buf=new byte[bufferSize];
        InputStream in=fs.open(filePath,bufferSize);
        IOUtils.readFully(in,buf,0,bufferSize);
        MessageDigest m=MessageDigest.getInstance("SHA");
        m.update(buf,0,bufferSize);
        byte[] hash_sha=m.digest();
        buf=null;
        in.close();
        fs.close();
        assertTrue("hashed keys are not the same size",hash_sha.length == expected_sha.length);
        assertTrue("hashed keys are not equal",Arrays.equals(hash_sha,expected_sha));
        counter.inc();
        LOG.info("Thread correctly read the block.");
      }
 catch (      BlockMissingException e) {
        LOG.info("Bad - BlockMissingException is caught.");
        e.printStackTrace();
      }
catch (      Exception e) {
        e.printStackTrace();
      }
    }
  }
class Counter {
    int counter;
    Counter(    int n){
      counter=n;
    }
    public synchronized void inc(){
      ++counter;
    }
    public int get(){
      return counter;
    }
  }
  @Test public void testGetFileChecksum() throws Exception {
    final String f="/testGetFileChecksum";
    final Path p=new Path(f);
    final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
    try {
      cluster.waitActive();
      final FileSystem fs=cluster.getFileSystem();
      DFSTestUtil.createFile(fs,p,1L << 20,(short)3,20100402L);
      final FileChecksum cs1=fs.getFileChecksum(p);
      assertTrue(cs1 != null);
      final List<LocatedBlock> locatedblocks=DFSClient.callGetBlockLocations(cluster.getNameNodeRpc(),f,0,Long.MAX_VALUE).getLocatedBlocks();
      final DatanodeInfo first=locatedblocks.get(0).getLocations()[0];
      cluster.stopDataNode(first.getXferAddr());
      final FileChecksum cs2=fs.getFileChecksum(p);
      assertEquals(cs1,cs2);
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Test that timeout occurs when DN does not respond to RPC. Start up a server and ask it to sleep for n seconds. Make an RPC to the server and set rpcTimeout to less than n and ensure that socketTimeoutException is obtained
 */
  @Test public void testClientDNProtocolTimeout() throws IOException {
    final Server server=new TestServer(1,true);
    server.start();
    final InetSocketAddress addr=NetUtils.getConnectAddress(server);
    DatanodeID fakeDnId=DFSTestUtil.getLocalDatanodeID(addr.getPort());
    ExtendedBlock b=new ExtendedBlock("fake-pool",new Block(12345L));
    LocatedBlock fakeBlock=new LocatedBlock(b,new DatanodeInfo[0]);
    ClientDatanodeProtocol proxy=null;
    try {
      proxy=DFSUtilClient.createClientDatanodeProtocolProxy(fakeDnId,conf,500,false,fakeBlock);
      proxy.getReplicaVisibleLength(new ExtendedBlock("bpid",1));
      fail("Did not get expected exception: SocketTimeoutException");
    }
 catch (    SocketTimeoutException e) {
      LOG.info("Got the expected Exception: SocketTimeoutException");
    }
 finally {
      if (proxy != null) {
        RPC.stopProxy(proxy);
      }
      server.stop();
    }
  }
  /** 
 * Test that checksum failures are recovered from by the next read on the same DFSInputStream. Corruption information is not persisted from read call to read call, so the client should expect consecutive calls to behave the same way. See HDFS-3067.
 */
  @Test public void testRetryOnChecksumFailure() throws Exception {
    HdfsConfiguration conf=new HdfsConfiguration();
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).build();
    try {
      final short REPL_FACTOR=1;
      final long FILE_LENGTH=512L;
      cluster.waitActive();
      FileSystem fs=cluster.getFileSystem();
      Path path=new Path("/corrupted");
      DFSTestUtil.createFile(fs,path,FILE_LENGTH,REPL_FACTOR,12345L);
      DFSTestUtil.waitReplication(fs,path,REPL_FACTOR);
      ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,path);
      int blockFilesCorrupted=cluster.corruptBlockOnDataNodes(block);
      assertEquals("All replicas not corrupted",REPL_FACTOR,blockFilesCorrupted);
      InetSocketAddress nnAddr=new InetSocketAddress("localhost",cluster.getNameNodePort());
      DFSClient client=new DFSClient(nnAddr,conf);
      DFSInputStream dis=client.open(path.toString());
      byte[] arr=new byte[(int)FILE_LENGTH];
      for (int i=0; i < 2; ++i) {
        try {
          dis.read(arr,0,(int)FILE_LENGTH);
          fail("Expected ChecksumException not thrown");
        }
 catch (        ChecksumException ex) {
          GenericTestUtils.assertExceptionContains("Checksum",ex);
        }
      }
      client.close();
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Test client retry with namenode restarting. 
 */
  @Test(timeout=300000) public void testNamenodeRestart() throws Exception {
    namenodeRestartTest(new Configuration(),false);
  }
  public static void namenodeRestartTest(  final Configuration conf,  final boolean isWebHDFS) throws Exception {
    GenericTestUtils.setLogLevel(DFSClient.LOG,Level.ALL);
    final List<Exception> exceptions=new ArrayList<Exception>();
    final Path dir=new Path("/testNamenodeRestart");
    if (isWebHDFS) {
      conf.setBoolean(HdfsClientConfigKeys.HttpClient.RETRY_POLICY_ENABLED_KEY,true);
    }
 else {
      conf.setBoolean(HdfsClientConfigKeys.Retry.POLICY_ENABLED_KEY,true);
    }
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_MIN_DATANODES_KEY,1);
    conf.setInt(MiniDFSCluster.DFS_NAMENODE_SAFEMODE_EXTENSION_TESTING_KEY,5000);
    final short numDatanodes=3;
    final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDatanodes).build();
    try {
      cluster.waitActive();
      final DistributedFileSystem dfs=cluster.getFileSystem();
      final FileSystem fs=isWebHDFS ? WebHdfsTestUtil.getWebHdfsFileSystem(conf,WebHdfsConstants.WEBHDFS_SCHEME) : dfs;
      final URI uri=dfs.getUri();
      assertTrue(HdfsUtils.isHealthy(uri));
      final long length=1L << 20;
      final Path file1=new Path(dir,"foo");
      DFSTestUtil.createFile(fs,file1,length,numDatanodes,20120406L);
      final FileStatus s1=fs.getFileStatus(file1);
      assertEquals(length,s1.getLen());
      final Path file4=new Path(dir,"file4");
      final FSDataOutputStream out4=fs.create(file4,false,4096,fs.getDefaultReplication(file4),1024L,null);
      final byte[] bytes=new byte[1000];
      new Random().nextBytes(bytes);
      out4.write(bytes);
      out4.write(bytes);
      if (isWebHDFS) {
        out4.close();
      }
 else {
        out4.hflush();
      }
      assertTrue(HdfsUtils.isHealthy(uri));
      cluster.shutdownNameNode(0);
      assertFalse(HdfsUtils.isHealthy(uri));
      final Thread file4thread=new Thread(new Runnable(){
        @Override public void run(){
          try {
            if (!isWebHDFS) {
              out4.write(bytes);
              out4.write(bytes);
              out4.write(bytes);
              out4.close();
            }
          }
 catch (          Exception e) {
            exceptions.add(e);
          }
        }
      }
);
      file4thread.start();
      final Thread reader=new Thread(new Runnable(){
        @Override public void run(){
          try {
            final FileSystem fs=createFsWithDifferentUsername(conf,isWebHDFS);
            final FSDataInputStream in=fs.open(file1);
            int count=0;
            for (; in.read() != -1; count++)             ;
            in.close();
            assertEquals(s1.getLen(),count);
          }
 catch (          Exception e) {
            exceptions.add(e);
          }
        }
      }
);
      reader.start();
      final Path file3=new Path(dir,"file");
      final Thread thread=new Thread(new Runnable(){
        @Override public void run(){
          try {
            final FileSystem fs=createFsWithDifferentUsername(conf,isWebHDFS);
            DFSTestUtil.createFile(fs,file3,length,numDatanodes,20120406L);
          }
 catch (          Exception e) {
            exceptions.add(e);
          }
        }
      }
);
      thread.start();
      new Thread(new Runnable(){
        @Override public void run(){
          try {
            TimeUnit.SECONDS.sleep(30);
            assertFalse(HdfsUtils.isHealthy(uri));
            cluster.restartNameNode(0,false);
            cluster.waitActive();
            assertTrue(HdfsUtils.isHealthy(uri));
          }
 catch (          Exception e) {
            exceptions.add(e);
          }
        }
      }
).start();
      final FileStatus s2=fs.getFileStatus(file1);
      assertEquals(s1,s2);
      thread.join();
      assertEmpty(exceptions);
      assertEquals(s1.getLen(),fs.getFileStatus(file3).getLen());
      assertEquals(fs.getFileChecksum(file1),fs.getFileChecksum(file3));
      reader.join();
      assertEmpty(exceptions);
      file4thread.join();
      assertEmpty(exceptions);
{
        final FSDataInputStream in=fs.open(file4);
        int count=0;
        for (int r; (r=in.read()) != -1; count++) {
          Assert.assertEquals(String.format("count=%d",count),bytes[count % bytes.length],(byte)r);
        }
        if (!isWebHDFS) {
          Assert.assertEquals(5 * bytes.length,count);
        }
 else {
          Assert.assertEquals(2 * bytes.length,count);
        }
        in.close();
      }
      assertTrue(HdfsUtils.isHealthy(uri));
      dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);
      assertFalse(HdfsUtils.isHealthy(uri));
      new Thread(new Runnable(){
        @Override public void run(){
          try {
            TimeUnit.SECONDS.sleep(30);
            assertFalse(HdfsUtils.isHealthy(uri));
            dfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);
            assertTrue(HdfsUtils.isHealthy(uri));
          }
 catch (          Exception e) {
            exceptions.add(e);
          }
        }
      }
).start();
      final Path file2=new Path(dir,"bar");
      DFSTestUtil.createFile(fs,file2,length,numDatanodes,20120406L);
      assertEquals(fs.getFileChecksum(file1),fs.getFileChecksum(file2));
      assertTrue(HdfsUtils.isHealthy(uri));
      final Path nonExisting=new Path(dir,"nonExisting");
      LOG.info("setPermission: " + nonExisting);
      try {
        fs.setPermission(nonExisting,new FsPermission((short)0));
        fail();
      }
 catch (      FileNotFoundException fnfe) {
        LOG.info("GOOD!",fnfe);
      }
      assertEmpty(exceptions);
    }
  finally {
      cluster.shutdown();
    }
  }
  static void assertEmpty(  final List<Exception> exceptions){
    if (!exceptions.isEmpty()) {
      final StringBuilder b=new StringBuilder("There are ").append(exceptions.size()).append(" exception(s):");
      for (int i=0; i < exceptions.size(); i++) {
        b.append("\n  Exception ").append(i).append(": ").append(StringUtils.stringifyException(exceptions.get(i)));
      }
      fail(b.toString());
    }
  }
  private static FileSystem createFsWithDifferentUsername(  final Configuration conf,  final boolean isWebHDFS) throws IOException, InterruptedException {
    final String username=UserGroupInformation.getCurrentUser().getShortUserName() + "_XXX";
    final UserGroupInformation ugi=UserGroupInformation.createUserForTesting(username,new String[]{"supergroup"});
    return isWebHDFS ? WebHdfsTestUtil.getWebHdfsFileSystemAs(ugi,conf,WebHdfsConstants.WEBHDFS_SCHEME) : DFSTestUtil.getFileSystemAs(ugi,conf);
  }
  @Test public void testMultipleLinearRandomRetry(){
    parseMultipleLinearRandomRetry(null,"");
    parseMultipleLinearRandomRetry(null,"11");
    parseMultipleLinearRandomRetry(null,"11,22,33");
    parseMultipleLinearRandomRetry(null,"11,22,33,44,55");
    parseMultipleLinearRandomRetry(null,"AA");
    parseMultipleLinearRandomRetry(null,"11,AA");
    parseMultipleLinearRandomRetry(null,"11,22,33,FF");
    parseMultipleLinearRandomRetry(null,"11,-22");
    parseMultipleLinearRandomRetry(null,"-11,22");
    parseMultipleLinearRandomRetry("[22x11ms]","11,22");
    parseMultipleLinearRandomRetry("[22x11ms, 44x33ms]","11,22,33,44");
    parseMultipleLinearRandomRetry("[22x11ms, 44x33ms, 66x55ms]","11,22,33,44,55,66");
    parseMultipleLinearRandomRetry("[22x11ms, 44x33ms, 66x55ms]","   11,   22, 33,  44, 55,  66   ");
  }
  static void parseMultipleLinearRandomRetry(  String expected,  String s){
    final MultipleLinearRandomRetry r=MultipleLinearRandomRetry.parseCommaSeparatedString(s);
    LOG.info("input=" + s + ", parsed="+ r+ ", expected="+ expected);
    if (r == null) {
      assertEquals(expected,null);
    }
 else {
      assertEquals("MultipleLinearRandomRetry" + expected,r.toString());
    }
  }
  @Test public void testDFSClientConfigurationLocateFollowingBlockInitialDelay() throws Exception {
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();
    try {
      cluster.waitActive();
      NamenodeProtocols nn=cluster.getNameNodeRpc();
      DFSClient client=new DFSClient(null,nn,conf,null);
      assertEquals(client.getConf().getBlockWriteLocateFollowingInitialDelayMs(),400);
      conf.setInt(HdfsClientConfigKeys.BlockWrite.LOCATEFOLLOWINGBLOCK_INITIAL_DELAY_MS_KEY,1000);
      client=new DFSClient(null,nn,conf,null);
      assertEquals(client.getConf().getBlockWriteLocateFollowingInitialDelayMs(),1000);
    }
  finally {
      cluster.shutdown();
    }
  }
  @Test(timeout=120000) public void testLeaseRenewAndDFSOutputStreamDeadLock() throws Exception {
    CountDownLatch testLatch=new CountDownLatch(1);
    DFSClientFaultInjector.set(new DFSClientFaultInjector(){
      public void delayWhenRenewLeaseTimeout(){
        try {
          testLatch.await();
        }
 catch (        InterruptedException e) {
          e.printStackTrace();
        }
      }
    }
);
    String file1="/testFile1";
    conf.setInt(DFS_CLIENT_SOCKET_TIMEOUT_KEY,1000);
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();
    try {
      cluster.waitActive();
      final NamenodeProtocols spyNN=spy(cluster.getNameNodeRpc());
      doAnswer(new SleepFixedTimeAnswer(1500,testLatch)).when(spyNN).complete(anyString(),anyString(),any(ExtendedBlock.class),anyLong());
      DFSClient client=new DFSClient(null,spyNN,conf,null);
      LeaseRenewer leaseRenewer=client.getLeaseRenewer();
      leaseRenewer.setRenewalTime(100);
      final OutputStream out1=client.create(file1,false);
      out1.write(new byte[256]);
      Thread closeThread=new Thread(new Runnable(){
        @Override public void run(){
          try {
            Mockito.doThrow(new SocketTimeoutException()).when(spyNN).renewLease(Mockito.anyString());
          }
 catch (          IOException e) {
            e.printStackTrace();
          }
        }
      }
);
      closeThread.start();
      out1.close();
    }
  finally {
      cluster.shutdown();
    }
  }
private static class SleepFixedTimeAnswer implements Answer<Object> {
    private final int sleepTime;
    private final CountDownLatch testLatch;
    SleepFixedTimeAnswer(    int sleepTime,    CountDownLatch latch){
      this.sleepTime=sleepTime;
      this.testLatch=latch;
    }
    @Override public Object answer(    InvocationOnMock invocation) throws Throwable {
      boolean interrupted=false;
      try {
        Thread.sleep(sleepTime);
      }
 catch (      InterruptedException ie) {
        interrupted=true;
      }
      try {
        return invocation.callRealMethod();
      }
  finally {
        testLatch.countDown();
        if (interrupted) {
          Thread.currentThread().interrupt();
        }
      }
    }
  }
}
