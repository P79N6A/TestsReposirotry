public class TestNameNodePrunesMissingStorages {
  static final Log LOG=LogFactory.getLog(TestNameNodePrunesMissingStorages.class);
  private static void runTest(  final String testCaseName,  final boolean createFiles,  final int numInitialStorages,  final int expectedStoragesAfterTest) throws IOException {
    Configuration conf=new HdfsConfiguration();
    MiniDFSCluster cluster=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).storagesPerDatanode(numInitialStorages).build();
      cluster.waitActive();
      final DataNode dn0=cluster.getDataNodes().get(0);
      final DatanodeID dnId=dn0.getDatanodeId();
      final DatanodeDescriptor dnDescriptor=cluster.getNamesystem().getBlockManager().getDatanodeManager().getDatanode(dnId);
      assertThat(dnDescriptor.getStorageInfos().length,is(numInitialStorages));
      final String bpid=cluster.getNamesystem().getBlockPoolId();
      final DatanodeRegistration dnReg=dn0.getDNRegistrationForBP(bpid);
      DataNodeTestUtils.triggerBlockReport(dn0);
      if (createFiles) {
        final Path path=new Path("/",testCaseName);
        DFSTestUtil.createFile(cluster.getFileSystem(),path,1024,(short)1,0x1BAD5EED);
        DataNodeTestUtils.triggerBlockReport(dn0);
      }
      final StorageReport reports[]=dn0.getFSDataset().getStorageReports(bpid);
      final StorageReport prunedReports[]=new StorageReport[numInitialStorages - 1];
      System.arraycopy(reports,0,prunedReports,0,prunedReports.length);
      cluster.stopDataNode(0);
      cluster.getNameNodeRpc().sendHeartbeat(dnReg,prunedReports,0L,0L,0,0,0,null,true,SlowPeerReports.EMPTY_REPORT,SlowDiskReports.EMPTY_REPORT);
      assertThat(dnDescriptor.getStorageInfos().length,is(expectedStoragesAfterTest));
    }
  finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
  /** 
 * Test that the NameNode prunes empty storage volumes that are no longer reported by the DataNode.
 * @throws IOException
 */
  @Test(timeout=300000) public void testUnusedStorageIsPruned() throws IOException {
    runTest(GenericTestUtils.getMethodName(),false,1,0);
  }
  /** 
 * Verify that the NameNode does not prune storages with blocks simply as a result of a heartbeat being sent missing that storage.
 * @throws IOException
 */
  @Test(timeout=300000) public void testStorageWithBlocksIsNotPruned() throws IOException {
    runTest(GenericTestUtils.getMethodName(),true,1,1);
  }
  /** 
 * Regression test for HDFS-7960.<p/> Shutting down a datanode, removing a storage directory, and restarting the DataNode should not produce zombie storages.
 */
  @Test(timeout=300000) public void testRemovingStorageDoesNotProduceZombies() throws Exception {
    Configuration conf=new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY,1);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY,1000);
    final int NUM_STORAGES_PER_DN=2;
    final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).storagesPerDatanode(NUM_STORAGES_PER_DN).build();
    try {
      cluster.waitActive();
      for (      DataNode dn : cluster.getDataNodes()) {
        assertEquals(NUM_STORAGES_PER_DN,cluster.getNamesystem().getBlockManager().getDatanodeManager().getDatanode(dn.getDatanodeId()).getStorageInfos().length);
      }
      final Path TEST_PATH=new Path("/foo1");
      DistributedFileSystem fs=cluster.getFileSystem();
      DFSTestUtil.createFile(fs,TEST_PATH,1024,(short)3,0xcafecafe);
      for (      DataNode dn : cluster.getDataNodes()) {
        DataNodeTestUtils.triggerBlockReport(dn);
      }
      ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,new Path("/foo1"));
      cluster.getNamesystem().writeLock();
      final String storageIdToRemove;
      String datanodeUuid;
      try {
        BlockInfo storedBlock=cluster.getNamesystem().getBlockManager().getStoredBlock(block.getLocalBlock());
        Iterator<DatanodeStorageInfo> storageInfoIter=cluster.getNamesystem().getBlockManager().blocksMap.getStorages(storedBlock).iterator();
        assertTrue(storageInfoIter.hasNext());
        DatanodeStorageInfo info=storageInfoIter.next();
        storageIdToRemove=info.getStorageID();
        datanodeUuid=info.getDatanodeDescriptor().getDatanodeUuid();
      }
  finally {
        cluster.getNamesystem().writeUnlock();
      }
      final DataNode datanodeToRemoveStorageFrom;
      int datanodeToRemoveStorageFromIdx=0;
      while (true) {
        if (datanodeToRemoveStorageFromIdx >= cluster.getDataNodes().size()) {
          Assert.fail("failed to find datanode with uuid " + datanodeUuid);
          datanodeToRemoveStorageFrom=null;
          break;
        }
        DataNode dn=cluster.getDataNodes().get(datanodeToRemoveStorageFromIdx);
        if (dn.getDatanodeUuid().equals(datanodeUuid)) {
          datanodeToRemoveStorageFrom=dn;
          break;
        }
        datanodeToRemoveStorageFromIdx++;
      }
      StorageLocation volumeLocationToRemove=null;
      try (FsVolumeReferences volumes=datanodeToRemoveStorageFrom.getFSDataset().getFsVolumeReferences()){
        assertEquals(NUM_STORAGES_PER_DN,volumes.size());
        for (        FsVolumeSpi volume : volumes) {
          if (volume.getStorageID().equals(storageIdToRemove)) {
            volumeLocationToRemove=volume.getStorageLocation();
          }
        }
      }
       ;
      assertNotNull(volumeLocationToRemove);
      datanodeToRemoveStorageFrom.shutdown();
      FileUtil.fullyDelete(new File(volumeLocationToRemove.getUri()));
      FileOutputStream fos=new FileOutputStream(new File(volumeLocationToRemove.getUri()));
      try {
        fos.write(1);
      }
  finally {
        fos.close();
      }
      cluster.restartDataNode(datanodeToRemoveStorageFromIdx);
      LOG.info("waiting for the datanode to remove " + storageIdToRemove);
      GenericTestUtils.waitFor(new Supplier<Boolean>(){
        @Override public Boolean get(){
          final DatanodeDescriptor dnDescriptor=cluster.getNamesystem().getBlockManager().getDatanodeManager().getDatanode(datanodeToRemoveStorageFrom.getDatanodeUuid());
          assertNotNull(dnDescriptor);
          DatanodeStorageInfo[] infos=dnDescriptor.getStorageInfos();
          for (          DatanodeStorageInfo info : infos) {
            if (info.getStorageID().equals(storageIdToRemove)) {
              LOG.info("Still found storage " + storageIdToRemove + " on "+ info+ ".");
              return false;
            }
          }
          assertEquals(NUM_STORAGES_PER_DN - 1,infos.length);
          return true;
        }
      }
,1000,30000);
    }
  finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
  private static void rewriteVersionFile(  File versionFile,  String newStorageId) throws IOException {
    BufferedReader in=new BufferedReader(new FileReader(versionFile));
    File newVersionFile=new File(versionFile.getParent(),UUID.randomUUID().toString());
    Writer out=new BufferedWriter(new OutputStreamWriter(new FileOutputStream(newVersionFile),"UTF-8"));
    final String STORAGE_ID="storageID=";
    boolean success=false;
    try {
      String line;
      while ((line=in.readLine()) != null) {
        if (line.startsWith(STORAGE_ID)) {
          out.write(STORAGE_ID + newStorageId + "\n");
        }
 else {
          out.write(line + "\n");
        }
      }
      in.close();
      in=null;
      out.close();
      out=null;
      success=versionFile.delete();
      success&=newVersionFile.renameTo(versionFile);
    }
  finally {
      if (in != null) {
        in.close();
      }
      if (out != null) {
        out.close();
      }
      if (!success) {
        versionFile.delete();
      }
    }
  }
  @Test(timeout=300000) public void testRenamingStorageIds() throws Exception {
    Configuration conf=new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_DATANODE_FAILED_VOLUMES_TOLERATED_KEY,0);
    final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).storagesPerDatanode(1).build();
    GenericTestUtils.setLogLevel(BlockManager.LOG,Level.ALL);
    try {
      cluster.waitActive();
      final Path TEST_PATH=new Path("/foo1");
      DistributedFileSystem fs=cluster.getFileSystem();
      DFSTestUtil.createFile(fs,TEST_PATH,1,(short)1,0xdeadbeef);
      DataNode dn=cluster.getDataNodes().get(0);
      FsVolumeReferences volumeRefs=dn.getFSDataset().getFsVolumeReferences();
      final String newStorageId=DatanodeStorage.generateUuid();
      try {
        File currentDir=new File(new File(volumeRefs.get(0).getStorageLocation().getUri()),"current");
        File versionFile=new File(currentDir,"VERSION");
        rewriteVersionFile(versionFile,newStorageId);
      }
  finally {
        volumeRefs.close();
      }
      final ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,TEST_PATH);
      cluster.restartDataNodes();
      GenericTestUtils.waitFor(new Supplier<Boolean>(){
        @Override public Boolean get(){
          cluster.getNamesystem().writeLock();
          try {
            Iterator<DatanodeStorageInfo> storageInfoIter=cluster.getNamesystem().getBlockManager().getStorages(block.getLocalBlock()).iterator();
            if (!storageInfoIter.hasNext()) {
              LOG.info("Expected to find a storage for " + block.getBlockName() + ", but nothing was found.  "+ "Continuing to wait.");
              return false;
            }
            DatanodeStorageInfo info=storageInfoIter.next();
            if (!newStorageId.equals(info.getStorageID())) {
              LOG.info("Expected " + block.getBlockName() + " to "+ "be in storage id "+ newStorageId+ ", but it "+ "was in "+ info.getStorageID()+ ".  Continuing "+ "to wait.");
              return false;
            }
            LOG.info("Successfully found " + block.getBlockName() + " in "+ "be in storage id "+ newStorageId);
          }
  finally {
            cluster.getNamesystem().writeUnlock();
          }
          return true;
        }
      }
,20,100000);
    }
  finally {
      cluster.shutdown();
    }
  }
  @Test(timeout=300000) public void testNameNodePrunesUnreportedStorages() throws Exception {
    Configuration conf=new HdfsConfiguration();
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).storagesPerDatanode(2).build();
    try {
      cluster.waitActive();
      DFSTestUtil.createFile(cluster.getFileSystem(),new Path("file1"),102400,102400,102400,(short)1,0x1BAD5EE);
      DFSTestUtil.createFile(cluster.getFileSystem(),new Path("file2"),102400,102400,102400,(short)1,0x1BAD5EED);
      DataNode dn=cluster.getDataNodes().get(0);
      BlockManager bm=cluster.getNameNode().getNamesystem().getBlockManager();
      DatanodeDescriptor dnDescriptor=bm.getDatanodeManager().getDatanode(cluster.getDataNodes().get(0).getDatanodeUuid());
      DatanodeStorageInfo[] dnStoragesInfosBeforeRestart=dnDescriptor.getStorageInfos();
      Collection<String> oldDirs=new ArrayList<String>(dn.getConf().getTrimmedStringCollection(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY));
      String newDirs=oldDirs.iterator().next();
      conf.set(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY,newDirs);
      cluster.stopDataNode(0);
      cluster.startDataNodes(conf,1,false,null,null);
      dn=cluster.getDataNodes().get(0);
      cluster.waitActive();
      assertArrayEquals(dnStoragesInfosBeforeRestart,dnDescriptor.getStorageInfos());
      int numFailedStoragesWithBlocks=0;
      DatanodeStorageInfo failedStorageInfo=null;
      for (      DatanodeStorageInfo dnStorageInfo : dnDescriptor.getStorageInfos()) {
        if (dnStorageInfo.areBlocksOnFailedStorage()) {
          numFailedStoragesWithBlocks++;
          failedStorageInfo=dnStorageInfo;
        }
      }
      assertEquals(1,numFailedStoragesWithBlocks);
      bm.getDatanodeManager().getHeartbeatManager().heartbeatCheck();
      assertTrue(!failedStorageInfo.areBlocksOnFailedStorage());
      cluster.triggerHeartbeats();
      assertEquals(DataNode.getStorageLocations(dn.getConf()).size(),dnDescriptor.getStorageInfos().length);
    }
  finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
}
