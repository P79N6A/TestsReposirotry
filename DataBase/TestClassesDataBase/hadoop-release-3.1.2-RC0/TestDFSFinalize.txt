/** 
 * This test ensures the appropriate response from the system when  the system is finalized.
 */
public class TestDFSFinalize {
  private static final Log LOG=LogFactory.getLog("org.apache.hadoop.hdfs.TestDFSFinalize");
  private Configuration conf;
  private int testCounter=0;
  private MiniDFSCluster cluster=null;
  /** 
 * Writes an INFO log message containing the parameters.
 */
  void log(  String label,  int numDirs){
    LOG.info("============================================================");
    LOG.info("***TEST " + (testCounter++) + "*** "+ label+ ":"+ " numDirs="+ numDirs);
  }
  /** 
 * Verify that the current directory exists and that the previous directory does not exist.  Verify that current hasn't been modified by comparing  the checksum of all it's containing files with their original checksum.
 */
  static void checkResult(  String[] nameNodeDirs,  String[] dataNodeDirs,  String bpid) throws Exception {
    List<File> dirs=Lists.newArrayList();
    for (int i=0; i < nameNodeDirs.length; i++) {
      File curDir=new File(nameNodeDirs[i],"current");
      dirs.add(curDir);
      FSImageTestUtil.assertReasonableNameCurrentDir(curDir);
    }
    FSImageTestUtil.assertParallelFilesAreIdentical(dirs,Collections.<String>emptySet());
    File dnCurDirs[]=new File[dataNodeDirs.length];
    for (int i=0; i < dataNodeDirs.length; i++) {
      dnCurDirs[i]=new File(dataNodeDirs[i],"current");
      assertEquals(UpgradeUtilities.checksumContents(DATA_NODE,dnCurDirs[i],false),UpgradeUtilities.checksumMasterDataNodeContents());
    }
    for (int i=0; i < nameNodeDirs.length; i++) {
      assertFalse(new File(nameNodeDirs[i],"previous").isDirectory());
    }
    if (bpid == null) {
      for (int i=0; i < dataNodeDirs.length; i++) {
        assertFalse(new File(dataNodeDirs[i],"previous").isDirectory());
      }
    }
 else {
      for (int i=0; i < dataNodeDirs.length; i++) {
        File bpRoot=BlockPoolSliceStorage.getBpRoot(bpid,dnCurDirs[i]);
        assertFalse(new File(bpRoot,"previous").isDirectory());
        File bpCurFinalizeDir=new File(bpRoot,"current/" + DataStorage.STORAGE_DIR_FINALIZED);
        assertEquals(UpgradeUtilities.checksumContents(DATA_NODE,bpCurFinalizeDir,true),UpgradeUtilities.checksumMasterBlockPoolFinalizedContents());
      }
    }
  }
  /** 
 * This test attempts to finalize the NameNode and DataNode.
 */
  @Test public void testFinalize() throws Exception {
    UpgradeUtilities.initialize();
    for (int numDirs=1; numDirs <= 2; numDirs++) {
      conf=new HdfsConfiguration();
      conf.setInt(DFSConfigKeys.DFS_DATANODE_SCAN_PERIOD_HOURS_KEY,-1);
      conf.setBoolean(DFSConfigKeys.DFS_DATANODE_DUPLICATE_REPLICA_DELETION,false);
      conf=UpgradeUtilities.initializeStorageStateConf(numDirs,conf);
      String[] nameNodeDirs=conf.getStrings(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY);
      String[] dataNodeDirs=conf.getStrings(DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY);
      log("Finalize NN & DN with existing previous dir",numDirs);
      UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"current");
      UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"previous");
      UpgradeUtilities.createDataNodeStorageDirs(dataNodeDirs,"current");
      UpgradeUtilities.createDataNodeStorageDirs(dataNodeDirs,"previous");
      cluster=new MiniDFSCluster.Builder(conf).format(false).manageDataDfsDirs(false).manageNameDfsDirs(false).startupOption(StartupOption.REGULAR).build();
      cluster.finalizeCluster(conf);
      cluster.triggerBlockReports();
      Thread.sleep(1000);
      checkResult(nameNodeDirs,dataNodeDirs,null);
      log("Finalize NN & DN without existing previous dir",numDirs);
      cluster.finalizeCluster(conf);
      cluster.triggerBlockReports();
      Thread.sleep(1000);
      checkResult(nameNodeDirs,dataNodeDirs,null);
      cluster.shutdown();
      UpgradeUtilities.createEmptyDirs(nameNodeDirs);
      UpgradeUtilities.createEmptyDirs(dataNodeDirs);
      log("Finalize NN & BP with existing previous dir",numDirs);
      String bpid=UpgradeUtilities.getCurrentBlockPoolID(cluster);
      UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"current");
      UpgradeUtilities.createNameNodeStorageDirs(nameNodeDirs,"previous");
      UpgradeUtilities.createDataNodeStorageDirs(dataNodeDirs,"current");
      UpgradeUtilities.createBlockPoolStorageDirs(dataNodeDirs,"current",bpid);
      UpgradeUtilities.createBlockPoolStorageDirs(dataNodeDirs,"previous",bpid);
      cluster=new MiniDFSCluster.Builder(conf).format(false).manageDataDfsDirs(false).manageNameDfsDirs(false).startupOption(StartupOption.REGULAR).build();
      cluster.finalizeCluster(conf);
      cluster.triggerBlockReports();
      Thread.sleep(1000);
      checkResult(nameNodeDirs,dataNodeDirs,bpid);
      log("Finalize NN & BP without existing previous dir",numDirs);
      cluster.finalizeCluster(conf);
      cluster.triggerBlockReports();
      Thread.sleep(1000);
      checkResult(nameNodeDirs,dataNodeDirs,bpid);
      cluster.shutdown();
      UpgradeUtilities.createEmptyDirs(nameNodeDirs);
      UpgradeUtilities.createEmptyDirs(dataNodeDirs);
    }
  }
  @After public void tearDown() throws Exception {
    LOG.info("Shutting down MiniDFSCluster");
    if (cluster != null) {
      cluster.shutdown();
      cluster=null;
    }
  }
  public static void main(  String[] args) throws Exception {
    new TestDFSFinalize().testFinalize();
  }
}
