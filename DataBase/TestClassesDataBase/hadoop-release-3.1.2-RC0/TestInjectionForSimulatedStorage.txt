/** 
 * This class tests the replication and injection of blocks of a DFS file for simulated storage.
 */
public class TestInjectionForSimulatedStorage {
  private final int checksumSize=16;
  private final int blockSize=checksumSize * 2;
  private final int numBlocks=4;
  private final int filesize=blockSize * numBlocks;
  private final int numDataNodes=4;
  private static final Log LOG=LogFactory.getLog("org.apache.hadoop.hdfs.TestInjectionForSimulatedStorage");
  private void waitForBlockReplication(  String filename,  ClientProtocol namenode,  int expected,  long maxWaitSec) throws IOException {
    long start=Time.monotonicNow();
    LOG.info("Checking for block replication for " + filename);
    LocatedBlocks blocks=namenode.getBlockLocations(filename,0,Long.MAX_VALUE);
    assertEquals(numBlocks,blocks.locatedBlockCount());
    for (int i=0; i < numBlocks; ++i) {
      LOG.info("Checking for block:" + (i + 1));
      while (true) {
        blocks=namenode.getBlockLocations(filename,0,Long.MAX_VALUE);
        assertEquals(numBlocks,blocks.locatedBlockCount());
        LocatedBlock block=blocks.get(i);
        int actual=block.getLocations().length;
        if (actual == expected) {
          LOG.info("Got enough replicas for " + (i + 1) + "th block "+ block.getBlock()+ ", got "+ actual+ ".");
          break;
        }
        LOG.info("Not enough replicas for " + (i + 1) + "th block "+ block.getBlock()+ " yet. Expecting "+ expected+ ", got "+ actual+ ".");
        if (maxWaitSec > 0 && (Time.monotonicNow() - start) > (maxWaitSec * 1000)) {
          throw new IOException("Timedout while waiting for all blocks to " + " be replicated for " + filename);
        }
        try {
          Thread.sleep(500);
        }
 catch (        InterruptedException ignored) {
        }
      }
    }
  }
  @Test public void testInjection() throws IOException {
    MiniDFSCluster cluster=null;
    String testFile="/replication-test-file";
    Path testPath=new Path(testFile);
    byte buffer[]=new byte[1024];
    for (int i=0; i < buffer.length; i++) {
      buffer[i]='1';
    }
    try {
      Configuration conf=new HdfsConfiguration();
      conf.set(DFSConfigKeys.DFS_REPLICATION_KEY,Integer.toString(numDataNodes));
      conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,checksumSize);
      SimulatedFSDataset.setFactory(conf);
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes).build();
      cluster.waitActive();
      String bpid=cluster.getNamesystem().getBlockPoolId();
      DFSClient dfsClient=new DFSClient(new InetSocketAddress("localhost",cluster.getNameNodePort()),conf);
      DFSTestUtil.createFile(cluster.getFileSystem(),testPath,filesize,filesize,blockSize,(short)numDataNodes,0L);
      waitForBlockReplication(testFile,dfsClient.getNamenode(),numDataNodes,20);
      List<Map<DatanodeStorage,BlockListAsLongs>> blocksList=cluster.getAllBlockReports(bpid);
      cluster.shutdown();
      cluster=null;
      LOG.info("Restarting minicluster");
      conf=new HdfsConfiguration();
      SimulatedFSDataset.setFactory(conf);
      conf.set(DFSConfigKeys.DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_KEY,"0.0f");
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDataNodes * 2).format(false).build();
      cluster.waitActive();
      Set<Block> uniqueBlocks=new HashSet<Block>();
      for (      Map<DatanodeStorage,BlockListAsLongs> map : blocksList) {
        for (        BlockListAsLongs blockList : map.values()) {
          for (          Block b : blockList) {
            uniqueBlocks.add(new Block(b));
          }
        }
      }
      LOG.info("Inserting " + uniqueBlocks.size() + " blocks");
      cluster.injectBlocks(0,uniqueBlocks,null);
      dfsClient=new DFSClient(new InetSocketAddress("localhost",cluster.getNameNodePort()),conf);
      waitForBlockReplication(testFile,dfsClient.getNamenode(),numDataNodes,-1);
    }
  finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
}
