@RunWith(Parameterized.class) public class TestEditLogJournalFailures {
  private int editsPerformed=0;
  private MiniDFSCluster cluster;
  private FileSystem fs;
  private boolean useAsyncEdits;
  @Parameters public static Collection<Object[]> data(){
    Collection<Object[]> params=new ArrayList<Object[]>();
    params.add(new Object[]{Boolean.FALSE});
    params.add(new Object[]{Boolean.TRUE});
    return params;
  }
  public TestEditLogJournalFailures(  boolean useAsyncEdits){
    this.useAsyncEdits=useAsyncEdits;
  }
  private Configuration getConf(){
    Configuration conf=new HdfsConfiguration();
    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_EDITS_ASYNC_LOGGING,useAsyncEdits);
    return conf;
  }
  /** 
 * Create the mini cluster for testing and sub in a custom runtime so that edit log journal failures don't actually cause the JVM to exit.
 */
  @Before public void setUpMiniCluster() throws IOException {
    setUpMiniCluster(getConf(),true);
  }
  public void setUpMiniCluster(  Configuration conf,  boolean manageNameDfsDirs) throws IOException {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).manageNameDfsDirs(manageNameDfsDirs).checkExitOnShutdown(false).build();
    cluster.waitActive();
    fs=cluster.getFileSystem();
  }
  @After public void shutDownMiniCluster() throws IOException {
    if (fs != null) {
      fs.close();
      fs=null;
    }
    if (cluster != null) {
      try {
        cluster.shutdown();
        cluster=null;
      }
 catch (      ExitException ee) {
      }
    }
  }
  @Test public void testSingleFailedEditsDirOnFlush() throws IOException {
    assertTrue(doAnEdit());
    invalidateEditsDirAtIndex(0,true,false);
    assertTrue(doAnEdit());
    assertFalse(cluster.getNameNode().isInSafeMode());
  }
  @Test public void testAllEditsDirsFailOnFlush() throws IOException {
    assertTrue(doAnEdit());
    invalidateEditsDirAtIndex(0,true,false);
    invalidateEditsDirAtIndex(1,true,false);
    try {
      doAnEdit();
      fail("The previous edit could not be synced to any persistent storage, " + "should have halted the NN");
    }
 catch (    RemoteException re) {
      assertTrue(re.getClassName().contains("ExitException"));
      GenericTestUtils.assertExceptionContains("Could not sync enough journals to persistent storage. " + "Unsynced transactions: 1",re);
    }
  }
  @Test public void testAllEditsDirFailOnWrite() throws IOException {
    assertTrue(doAnEdit());
    invalidateEditsDirAtIndex(0,true,true);
    invalidateEditsDirAtIndex(1,true,true);
    try {
      doAnEdit();
      fail("The previous edit could not be synced to any persistent storage, " + " should have halted the NN");
    }
 catch (    RemoteException re) {
      assertTrue(re.getClassName().contains("ExitException"));
      GenericTestUtils.assertExceptionContains("Could not sync enough journals to persistent storage due to " + "No journals available to flush. " + "Unsynced transactions: 1",re);
    }
  }
  @Test public void testSingleFailedEditsDirOnSetReadyToFlush() throws IOException {
    assertTrue(doAnEdit());
    invalidateEditsDirAtIndex(0,false,false);
    assertTrue(doAnEdit());
    assertFalse(cluster.getNameNode().isInSafeMode());
  }
  @Test public void testSingleRequiredFailedEditsDirOnSetReadyToFlush() throws IOException {
    String[] editsDirs=cluster.getConfiguration(0).getTrimmedStrings(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY);
    shutDownMiniCluster();
    Configuration conf=getConf();
    conf.set(DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_REQUIRED_KEY,editsDirs[0]);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_MINIMUM_KEY,0);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_CHECKED_VOLUMES_MINIMUM_KEY,0);
    setUpMiniCluster(conf,true);
    assertTrue(doAnEdit());
    invalidateEditsDirAtIndex(0,false,false);
    JournalAndStream nonRequiredJas=getJournalAndStream(1);
    EditLogFileOutputStream nonRequiredSpy=spyOnStream(nonRequiredJas);
    assertTrue(nonRequiredJas.isActive());
    try {
      doAnEdit();
      fail("A single failure of a required journal should have halted the NN");
    }
 catch (    RemoteException re) {
      assertTrue(re.getClassName().contains("ExitException"));
      GenericTestUtils.assertExceptionContains("setReadyToFlush failed for required journal",re);
    }
    Mockito.verify(nonRequiredSpy,Mockito.never()).setReadyToFlush();
    assertFalse(nonRequiredJas.isActive());
  }
  @Test public void testMultipleRedundantFailedEditsDirOnSetReadyToFlush() throws IOException {
    shutDownMiniCluster();
    Configuration conf=getConf();
    String[] nameDirs=new String[4];
    for (int i=0; i < nameDirs.length; i++) {
      File nameDir=new File(PathUtils.getTestDir(getClass()),"name-dir" + i);
      nameDir.mkdirs();
      nameDirs[i]=nameDir.getAbsolutePath();
    }
    conf.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY,StringUtils.join(nameDirs,","));
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_MINIMUM_KEY,2);
    setUpMiniCluster(conf,false);
    assertTrue(doAnEdit());
    invalidateEditsDirAtIndex(0,false,false);
    assertTrue(doAnEdit());
    invalidateEditsDirAtIndex(1,false,false);
    assertTrue(doAnEdit());
    invalidateEditsDirAtIndex(2,false,false);
    try {
      doAnEdit();
      fail("A failure of more than the minimum number of redundant journals " + "should have halted ");
    }
 catch (    RemoteException re) {
      assertTrue(re.getClassName().contains("ExitException"));
      GenericTestUtils.assertExceptionContains("Could not sync enough journals to persistent storage due to " + "setReadyToFlush failed for too many journals. " + "Unsynced transactions: 1",re);
    }
  }
  /** 
 * Replace the journal at index <code>index</code> with one that throws an exception on flush.
 * @param index the index of the journal to take offline.
 * @return the original <code>EditLogOutputStream</code> of the journal.
 */
  private void invalidateEditsDirAtIndex(  int index,  boolean failOnFlush,  boolean failOnWrite) throws IOException {
    JournalAndStream jas=getJournalAndStream(index);
    EditLogFileOutputStream spyElos=spyOnStream(jas);
    if (failOnWrite) {
      doThrow(new IOException("fail on write()")).when(spyElos).write((FSEditLogOp)any());
    }
    if (failOnFlush) {
      doThrow(new IOException("fail on flush()")).when(spyElos).flush();
    }
 else {
      doThrow(new IOException("fail on setReadyToFlush()")).when(spyElos).setReadyToFlush();
    }
  }
  private EditLogFileOutputStream spyOnStream(  JournalAndStream jas){
    EditLogFileOutputStream elos=(EditLogFileOutputStream)jas.getCurrentStream();
    EditLogFileOutputStream spyElos=spy(elos);
    jas.setCurrentStreamForTests(spyElos);
    return spyElos;
  }
  /** 
 * Pull out one of the JournalAndStream objects from the edit log.
 */
  private JournalAndStream getJournalAndStream(  int index){
    FSImage fsimage=cluster.getNamesystem().getFSImage();
    FSEditLog editLog=fsimage.getEditLog();
    return editLog.getJournals().get(index);
  }
  /** 
 * Do a mutative metadata operation on the file system.
 * @return true if the operation was successful, false otherwise.
 */
  private boolean doAnEdit() throws IOException {
    return fs.mkdirs(new Path("/tmp",Integer.toString(editsPerformed++)));
  }
}
