public class TestFileAppend4 {
  static final Log LOG=LogFactory.getLog(TestFileAppend4.class);
  static final long BLOCK_SIZE=1024;
  static final long BBW_SIZE=500;
  static final Object[] NO_ARGS=new Object[]{};
  Configuration conf;
  MiniDFSCluster cluster;
  Path file1;
  FSDataOutputStream stm;
{
    DFSTestUtil.setNameNodeLogLevel(Level.ALL);
    GenericTestUtils.setLogLevel(DataNode.LOG,Level.ALL);
    GenericTestUtils.setLogLevel(DFSClient.LOG,Level.ALL);
  }
  @Before public void setUp() throws Exception {
    this.conf=new Configuration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY,1000);
    conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1);
    conf.setInt(HdfsClientConfigKeys.DFS_CLIENT_SOCKET_TIMEOUT_KEY,5000);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_RECONSTRUCTION_PENDING_TIMEOUT_SEC_KEY,5);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,1);
    conf.setInt(IPC_CLIENT_CONNECT_MAX_RETRIES_KEY,1);
  }
  private void recoverFile(  final FileSystem fs) throws Exception {
    LOG.info("Recovering File Lease");
    cluster.setLeasePeriod(1000,HdfsConstants.LEASE_HARDLIMIT_PERIOD);
    int tries=60;
    boolean recovered=false;
    FSDataOutputStream out=null;
    while (!recovered && tries-- > 0) {
      try {
        out=fs.append(file1);
        LOG.info("Successfully opened for append");
        recovered=true;
      }
 catch (      IOException e) {
        LOG.info("Failed open for append, waiting on lease recovery");
        try {
          Thread.sleep(1000);
        }
 catch (        InterruptedException ex) {
        }
      }
    }
    if (out != null) {
      out.close();
    }
    if (!recovered) {
      fail("Recovery should take < 1 min");
    }
    LOG.info("Past out lease recovery");
  }
  /** 
 * Test case that stops a writer after finalizing a block but before calling completeFile, and then tries to recover the lease from another thread.
 */
  @Test(timeout=60000) public void testRecoverFinalizedBlock() throws Throwable {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(5).build();
    try {
      cluster.waitActive();
      NamenodeProtocols preSpyNN=cluster.getNameNodeRpc();
      NamenodeProtocols spyNN=spy(preSpyNN);
      GenericTestUtils.DelayAnswer delayer=new GenericTestUtils.DelayAnswer(LOG);
      doAnswer(delayer).when(spyNN).complete(anyString(),anyString(),(ExtendedBlock)anyObject(),anyLong());
      DFSClient client=new DFSClient(null,spyNN,conf,null);
      file1=new Path("/testRecoverFinalized");
      final OutputStream stm=client.create("/testRecoverFinalized",true);
      AppendTestUtil.write(stm,0,4096);
      final AtomicReference<Throwable> err=new AtomicReference<Throwable>();
      Thread t=new Thread(){
        @Override public void run(){
          try {
            stm.close();
          }
 catch (          Throwable t) {
            err.set(t);
          }
        }
      }
;
      t.start();
      LOG.info("Waiting for close to get to latch...");
      delayer.waitForCall();
      LOG.info("Killing lease checker");
      client.getLeaseRenewer().interruptAndJoin();
      FileSystem fs1=cluster.getFileSystem();
      FileSystem fs2=AppendTestUtil.createHdfsWithDifferentUsername(fs1.getConf());
      LOG.info("Recovering file");
      recoverFile(fs2);
      LOG.info("Telling close to proceed.");
      delayer.proceed();
      LOG.info("Waiting for close to finish.");
      t.join();
      LOG.info("Close finished.");
      Throwable thrownByClose=err.get();
      assertNotNull(thrownByClose);
      assertTrue(thrownByClose instanceof LeaseExpiredException);
      GenericTestUtils.assertExceptionContains("File is not open for writing",thrownByClose);
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Test case that stops a writer after finalizing a block but before calling completeFile, recovers a file from another writer, starts writing from that writer, and then has the old lease holder call completeFile
 */
  @Test(timeout=60000) public void testCompleteOtherLeaseHoldersFile() throws Throwable {
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(5).build();
    try {
      cluster.waitActive();
      NamenodeProtocols preSpyNN=cluster.getNameNodeRpc();
      NamenodeProtocols spyNN=spy(preSpyNN);
      GenericTestUtils.DelayAnswer delayer=new GenericTestUtils.DelayAnswer(LOG);
      doAnswer(delayer).when(spyNN).complete(anyString(),anyString(),(ExtendedBlock)anyObject(),anyLong());
      DFSClient client=new DFSClient(null,spyNN,conf,null);
      file1=new Path("/testCompleteOtherLease");
      final OutputStream stm=client.create("/testCompleteOtherLease",true);
      AppendTestUtil.write(stm,0,4096);
      final AtomicReference<Throwable> err=new AtomicReference<Throwable>();
      Thread t=new Thread(){
        @Override public void run(){
          try {
            stm.close();
          }
 catch (          Throwable t) {
            err.set(t);
          }
        }
      }
;
      t.start();
      LOG.info("Waiting for close to get to latch...");
      delayer.waitForCall();
      LOG.info("Killing lease checker");
      client.getLeaseRenewer().interruptAndJoin();
      FileSystem fs1=cluster.getFileSystem();
      FileSystem fs2=AppendTestUtil.createHdfsWithDifferentUsername(fs1.getConf());
      LOG.info("Recovering file");
      recoverFile(fs2);
      LOG.info("Opening file for append from new fs");
      FSDataOutputStream appenderStream=fs2.append(file1);
      LOG.info("Writing some data from new appender");
      AppendTestUtil.write(appenderStream,0,4096);
      LOG.info("Telling old close to proceed.");
      delayer.proceed();
      LOG.info("Waiting for close to finish.");
      t.join();
      LOG.info("Close finished.");
      Throwable thrownByClose=err.get();
      assertNotNull(thrownByClose);
      assertTrue(thrownByClose instanceof LeaseExpiredException);
      GenericTestUtils.assertExceptionContains("not the lease owner",thrownByClose);
      appenderStream.close();
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Test the updation of NeededReplications for the Appended Block
 */
  @Test(timeout=60000) public void testUpdateNeededReplicationsForAppendedFile() throws Exception {
    Configuration conf=new Configuration();
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).build();
    DistributedFileSystem fileSystem=null;
    try {
      fileSystem=cluster.getFileSystem();
      Path f=new Path("/testAppend");
      FSDataOutputStream create=fileSystem.create(f,(short)2);
      create.write("/testAppend".getBytes());
      create.close();
      FSDataOutputStream append=fileSystem.append(f);
      append.write("/testAppend".getBytes());
      append.close();
      cluster.startDataNodes(conf,1,true,null,null);
      DFSTestUtil.waitReplication(fileSystem,f,(short)2);
    }
  finally {
      if (null != fileSystem) {
        fileSystem.close();
      }
      cluster.shutdown();
    }
  }
  /** 
 * Test that an append with no locations fails with an exception showing insufficient locations.
 */
  @Test(timeout=60000) public void testAppendInsufficientLocations() throws Exception {
    Configuration conf=new Configuration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY,1000);
    conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1);
    conf.setInt(HdfsClientConfigKeys.DFS_CLIENT_SOCKET_TIMEOUT_KEY,3000);
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(4).build();
    DistributedFileSystem fileSystem=null;
    try {
      fileSystem=cluster.getFileSystem();
      Path f=new Path("/testAppend");
      FSDataOutputStream create=fileSystem.create(f,(short)2);
      create.write("/testAppend".getBytes());
      create.close();
      DFSTestUtil.waitReplication(fileSystem,f,(short)2);
      LocatedBlocks lbs=fileSystem.dfs.getNamenode().getBlockLocations("/testAppend",0,Long.MAX_VALUE);
      List<DataNode> dnsOfCluster=cluster.getDataNodes();
      DatanodeInfo[] dnsWithLocations=lbs.getLastLocatedBlock().getLocations();
      for (      DataNode dn : dnsOfCluster) {
        for (        DatanodeInfo loc : dnsWithLocations) {
          if (dn.getDatanodeId().equals(loc)) {
            dn.shutdown();
            DFSTestUtil.waitForDatanodeDeath(dn);
          }
        }
      }
      DFSTestUtil.waitReplication(fileSystem,f,(short)0);
      try {
        fileSystem.append(f);
        fail("Append should fail because insufficient locations");
      }
 catch (      IOException e) {
        LOG.info("Expected exception: ",e);
      }
      FSDirectory dir=cluster.getNamesystem().getFSDirectory();
      final INodeFile inode=INodeFile.valueOf(dir.getINode("/testAppend"),"/testAppend");
      assertTrue("File should remain closed",!inode.isUnderConstruction());
    }
  finally {
      if (null != fileSystem) {
        fileSystem.close();
      }
      cluster.shutdown();
    }
  }
}
