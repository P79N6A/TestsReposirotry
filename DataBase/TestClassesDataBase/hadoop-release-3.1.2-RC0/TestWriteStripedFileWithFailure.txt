public class TestWriteStripedFileWithFailure {
  public static final Log LOG=LogFactory.getLog(TestWriteStripedFileWithFailure.class);
  private MiniDFSCluster cluster;
  private FileSystem fs;
  private Configuration conf=new HdfsConfiguration();
static {
    GenericTestUtils.setLogLevel(DFSOutputStream.LOG,Level.ALL);
    GenericTestUtils.setLogLevel(DataStreamer.LOG,Level.ALL);
  }
  private final ErasureCodingPolicy ecPolicy=StripedFileTestUtil.getDefaultECPolicy();
  private final short dataBlocks=(short)ecPolicy.getNumDataUnits();
  private final short parityBlocks=(short)ecPolicy.getNumParityUnits();
  private final int numDNs=dataBlocks + parityBlocks;
  private final int blockSize=4 * ecPolicy.getCellSize();
  private final int smallFileLength=blockSize * dataBlocks - 123;
  private final int largeFileLength=blockSize * dataBlocks + 123;
  private final int[] fileLengths={smallFileLength,largeFileLength};
  public void setup() throws IOException {
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,blockSize);
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDNs).build();
    cluster.getFileSystem().getClient().setErasureCodingPolicy("/",StripedFileTestUtil.getDefaultECPolicy().getName());
    fs=cluster.getFileSystem();
  }
  public void tearDown() throws IOException {
    if (cluster != null) {
      cluster.shutdown();
    }
  }
  @Ignore @Test(timeout=300000) public void testWriteStripedFileWithDNFailure() throws IOException {
    for (    int fileLength : fileLengths) {
      for (int dataDelNum=1; dataDelNum <= parityBlocks; dataDelNum++) {
        for (int parityDelNum=0; (dataDelNum + parityDelNum) <= parityBlocks; parityDelNum++) {
          try {
            setup();
            writeFileWithDNFailure(fileLength,dataDelNum,parityDelNum);
          }
 catch (          IOException ioe) {
            String fileType=fileLength < (blockSize * dataBlocks) ? "smallFile" : "largeFile";
            LOG.error("Failed to write file with DN failure:" + " fileType = " + fileType + ", dataDelNum = "+ dataDelNum+ ", parityDelNum = "+ parityDelNum);
            throw ioe;
          }
 finally {
            tearDown();
          }
        }
      }
    }
  }
  /** 
 * Test writing a file with shutting down some DNs(data DNs or parity DNs or both).
 * @param fileLength file length
 * @param dataDNFailureNum the shutdown number of data DNs
 * @param parityDNFailureNum the shutdown number of parity DNs
 * @throws IOException
 */
  private void writeFileWithDNFailure(  int fileLength,  int dataDNFailureNum,  int parityDNFailureNum) throws IOException {
    String fileType=fileLength < (blockSize * dataBlocks) ? "smallFile" : "largeFile";
    String src="/dnFailure_" + dataDNFailureNum + "_"+ parityDNFailureNum+ "_"+ fileType;
    LOG.info("writeFileWithDNFailure: file = " + src + ", fileType = "+ fileType+ ", dataDNFailureNum = "+ dataDNFailureNum+ ", parityDNFailureNum = "+ parityDNFailureNum);
    Path srcPath=new Path(src);
    final AtomicInteger pos=new AtomicInteger();
    final FSDataOutputStream out=fs.create(srcPath);
    final DFSStripedOutputStream stripedOut=(DFSStripedOutputStream)out.getWrappedStream();
    int[] dataDNFailureIndices=StripedFileTestUtil.randomArray(0,dataBlocks,dataDNFailureNum);
    Assert.assertNotNull(dataDNFailureIndices);
    int[] parityDNFailureIndices=StripedFileTestUtil.randomArray(dataBlocks,dataBlocks + parityBlocks,parityDNFailureNum);
    Assert.assertNotNull(parityDNFailureIndices);
    int[] failedDataNodes=new int[dataDNFailureNum + parityDNFailureNum];
    System.arraycopy(dataDNFailureIndices,0,failedDataNodes,0,dataDNFailureIndices.length);
    System.arraycopy(parityDNFailureIndices,0,failedDataNodes,dataDNFailureIndices.length,parityDNFailureIndices.length);
    final int killPos=fileLength / 2;
    for (; pos.get() < fileLength; ) {
      final int i=pos.getAndIncrement();
      if (i == killPos) {
        for (        int failedDn : failedDataNodes) {
          StripedFileTestUtil.killDatanode(cluster,stripedOut,failedDn,pos);
        }
      }
      write(out,i);
    }
    out.close();
    int dnFailureNum=dataDNFailureNum + parityDNFailureNum;
    Assert.assertEquals(cluster.getDataNodes().size(),numDNs - dnFailureNum);
    byte[] smallBuf=new byte[1024];
    byte[] largeBuf=new byte[fileLength + 100];
    final byte[] expected=StripedFileTestUtil.generateBytes(fileLength);
    StripedFileTestUtil.verifyLength(fs,srcPath,fileLength);
    StripedFileTestUtil.verifySeek(fs,srcPath,fileLength,ecPolicy,blockSize * dataBlocks);
    StripedFileTestUtil.verifyStatefulRead(fs,srcPath,fileLength,expected,smallBuf);
    StripedFileTestUtil.verifyPread((DistributedFileSystem)fs,srcPath,fileLength,expected,largeBuf);
    fs.delete(srcPath,true);
  }
  void write(  FSDataOutputStream out,  int i) throws IOException {
    try {
      out.write(StripedFileTestUtil.getByte(i));
    }
 catch (    IOException e) {
      throw new IOException("Failed at i=" + i,e);
    }
  }
}
