/** 
 * This class tests if block replacement request to data nodes work correctly.
 */
public class TestBlockReplacement {
  private static final Log LOG=LogFactory.getLog("org.apache.hadoop.hdfs.TestBlockReplacement");
  MiniDFSCluster cluster;
  @Test public void testThrottler() throws IOException {
    Configuration conf=new HdfsConfiguration();
    FileSystem.setDefaultUri(conf,"hdfs://localhost:0");
    long bandwidthPerSec=1024 * 1024L;
    final long TOTAL_BYTES=6 * bandwidthPerSec;
    long bytesToSend=TOTAL_BYTES;
    long start=Time.monotonicNow();
    DataTransferThrottler throttler=new DataTransferThrottler(bandwidthPerSec);
    long bytesSent=1024 * 512L;
    throttler.throttle(bytesSent);
    bytesToSend-=bytesSent;
    bytesSent=1024 * 768L;
    throttler.throttle(bytesSent);
    bytesToSend-=bytesSent;
    try {
      Thread.sleep(1000);
    }
 catch (    InterruptedException ignored) {
    }
    throttler.throttle(bytesToSend);
    long end=Time.monotonicNow();
    assertTrue(TOTAL_BYTES * 1000 / (end - start) <= bandwidthPerSec);
  }
  @Test public void testBlockReplacement() throws Exception {
    final Configuration CONF=new HdfsConfiguration();
    final String[] INITIAL_RACKS={"/RACK0","/RACK1","/RACK2"};
    final String[] NEW_RACKS={"/RACK2"};
    final short REPLICATION_FACTOR=(short)3;
    final int DEFAULT_BLOCK_SIZE=1024;
    final Random r=new Random();
    CONF.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,DEFAULT_BLOCK_SIZE);
    CONF.setInt(HdfsClientConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,DEFAULT_BLOCK_SIZE / 2);
    CONF.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY,500);
    cluster=new MiniDFSCluster.Builder(CONF).numDataNodes(REPLICATION_FACTOR).racks(INITIAL_RACKS).build();
    try {
      cluster.waitActive();
      FileSystem fs=cluster.getFileSystem();
      Path fileName=new Path("/tmp.txt");
      DFSTestUtil.createFile(fs,fileName,DEFAULT_BLOCK_SIZE,REPLICATION_FACTOR,r.nextLong());
      DFSTestUtil.waitReplication(fs,fileName,REPLICATION_FACTOR);
      InetSocketAddress addr=new InetSocketAddress("localhost",cluster.getNameNodePort());
      DFSClient client=new DFSClient(addr,CONF);
      List<LocatedBlock> locatedBlocks=client.getNamenode().getBlockLocations("/tmp.txt",0,DEFAULT_BLOCK_SIZE).getLocatedBlocks();
      assertEquals(1,locatedBlocks.size());
      LocatedBlock block=locatedBlocks.get(0);
      DatanodeInfo[] oldNodes=block.getLocations();
      assertEquals(oldNodes.length,3);
      ExtendedBlock b=block.getBlock();
      cluster.startDataNodes(CONF,1,true,null,NEW_RACKS);
      cluster.waitActive();
      DatanodeInfo[] datanodes=client.datanodeReport(DatanodeReportType.ALL);
      DatanodeInfo newNode=null;
      for (      DatanodeInfo node : datanodes) {
        Boolean isNewNode=true;
        for (        DatanodeInfo oldNode : oldNodes) {
          if (node.equals(oldNode)) {
            isNewNode=false;
            break;
          }
        }
        if (isNewNode) {
          newNode=node;
          break;
        }
      }
      assertTrue(newNode != null);
      DatanodeInfo source=null;
      ArrayList<DatanodeInfo> proxies=new ArrayList<DatanodeInfo>(2);
      for (      DatanodeInfo node : datanodes) {
        if (node != newNode) {
          if (node.getNetworkLocation().equals(newNode.getNetworkLocation())) {
            source=node;
          }
 else {
            proxies.add(node);
          }
        }
      }
      assertTrue(source != null && proxies.size() == 2);
      LOG.info("Testcase 1: Proxy " + newNode + " does not contain the block "+ b);
      assertFalse(replaceBlock(b,source,newNode,proxies.get(0)));
      LOG.info("Testcase 2: Destination " + proxies.get(1) + " contains the block "+ b);
      assertFalse(replaceBlock(b,source,proxies.get(0),proxies.get(1)));
      LOG.info("Testcase 3: Source=" + source + " Proxy="+ proxies.get(0)+ " Destination="+ newNode);
      assertTrue(replaceBlock(b,source,proxies.get(0),newNode));
      checkBlocks(new DatanodeInfo[]{newNode,proxies.get(0),proxies.get(1)},fileName.toString(),DEFAULT_BLOCK_SIZE,REPLICATION_FACTOR,client);
      LOG.info("Testcase 4: invalid del hint " + proxies.get(0));
      assertTrue(replaceBlock(b,proxies.get(0),proxies.get(1),source));
      checkBlocks(new DatanodeInfo[]{},fileName.toString(),DEFAULT_BLOCK_SIZE,REPLICATION_FACTOR,client);
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Test to verify that the copying of pinned block to a different destination datanode will throw IOException with error code Status.ERROR_BLOCK_PINNED.
 */
  @Test(timeout=90000) public void testBlockReplacementWithPinnedBlocks() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).storageTypes(new StorageType[]{StorageType.DISK,StorageType.ARCHIVE}).build();
    try {
      cluster.waitActive();
      final DistributedFileSystem dfs=cluster.getFileSystem();
      String fileName="/testBlockReplacementWithPinnedBlocks/file";
      final Path file=new Path(fileName);
      DFSTestUtil.createFile(dfs,file,1024,(short)1,1024);
      LocatedBlock lb=dfs.getClient().getLocatedBlocks(fileName,0).get(0);
      DatanodeInfo[] oldNodes=lb.getLocations();
      assertEquals("Wrong block locations",oldNodes.length,1);
      DatanodeInfo source=oldNodes[0];
      ExtendedBlock b=lb.getBlock();
      DatanodeInfo[] datanodes=dfs.getDataNodeStats();
      DatanodeInfo destin=null;
      for (      DatanodeInfo datanodeInfo : datanodes) {
        if (!oldNodes[0].equals(datanodeInfo)) {
          destin=datanodeInfo;
          break;
        }
      }
      assertNotNull("Failed to choose destination datanode!",destin);
      assertFalse("Source and destin datanode should be different",source.equals(destin));
      for (int i=0; i < cluster.getDataNodes().size(); i++) {
        DataNode dn=cluster.getDataNodes().get(i);
        LOG.info("Simulate block pinning in datanode " + dn);
        InternalDataNodeTestUtils.mockDatanodeBlkPinning(dn,true);
      }
      assertTrue("Status code mismatches!",replaceBlock(b,source,source,destin,StorageType.ARCHIVE,Status.ERROR_BLOCK_PINNED));
    }
  finally {
      cluster.shutdown();
    }
  }
  @Test public void testBlockMoveAcrossStorageInSameNode() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).storageTypes(new StorageType[]{StorageType.DISK,StorageType.ARCHIVE}).build();
    try {
      cluster.waitActive();
      final DistributedFileSystem dfs=cluster.getFileSystem();
      final Path file=new Path("/testBlockMoveAcrossStorageInSameNode/file");
      DFSTestUtil.createFile(dfs,file,1024,(short)1,1024);
      LocatedBlocks locatedBlocks=dfs.getClient().getLocatedBlocks(file.toString(),0);
      LocatedBlock locatedBlock=locatedBlocks.get(0);
      ExtendedBlock block=locatedBlock.getBlock();
      DatanodeInfo[] locations=locatedBlock.getLocations();
      assertEquals(1,locations.length);
      StorageType[] storageTypes=locatedBlock.getStorageTypes();
      assertTrue(storageTypes[0] == StorageType.DISK);
      DatanodeInfo source=locations[0];
      assertTrue(replaceBlock(block,source,source,source,StorageType.ARCHIVE,Status.SUCCESS));
      Thread.sleep(3000);
      locatedBlocks=dfs.getClient().getLocatedBlocks(file.toString(),0);
      locatedBlock=locatedBlocks.get(0);
      assertEquals("Storage should be only one",1,locatedBlock.getLocations().length);
      assertTrue("Block should be moved to ARCHIVE",locatedBlock.getStorageTypes()[0] == StorageType.ARCHIVE);
    }
  finally {
      cluster.shutdown();
    }
  }
  private void checkBlocks(  DatanodeInfo[] includeNodes,  String fileName,  long fileLen,  short replFactor,  DFSClient client) throws IOException, TimeoutException {
    boolean notDone;
    final long TIMEOUT=20000L;
    long starttime=Time.monotonicNow();
    long failtime=starttime + TIMEOUT;
    do {
      try {
        Thread.sleep(100);
      }
 catch (      InterruptedException e) {
      }
      List<LocatedBlock> blocks=client.getNamenode().getBlockLocations(fileName,0,fileLen).getLocatedBlocks();
      assertEquals(1,blocks.size());
      DatanodeInfo[] nodes=blocks.get(0).getLocations();
      notDone=(nodes.length != replFactor);
      if (notDone) {
        LOG.info("Expected replication factor is " + replFactor + " but the real replication factor is "+ nodes.length);
      }
 else {
        List<DatanodeInfo> nodeLocations=Arrays.asList(nodes);
        for (        DatanodeInfo node : includeNodes) {
          if (!nodeLocations.contains(node)) {
            notDone=true;
            LOG.info("Block is not located at " + node);
            break;
          }
        }
      }
      if (Time.monotonicNow() > failtime) {
        String expectedNodesList="";
        String currentNodesList="";
        for (        DatanodeInfo dn : includeNodes)         expectedNodesList+=dn + ", ";
        for (        DatanodeInfo dn : nodes)         currentNodesList+=dn + ", ";
        LOG.info("Expected replica nodes are: " + expectedNodesList);
        LOG.info("Current actual replica nodes are: " + currentNodesList);
        throw new TimeoutException("Did not achieve expected replication to expected nodes " + "after more than " + TIMEOUT + " msec.  See logs for details.");
      }
    }
 while (notDone);
    LOG.info("Achieved expected replication values in " + (Time.now() - starttime) + " msec.");
  }
  private boolean replaceBlock(  ExtendedBlock block,  DatanodeInfo source,  DatanodeInfo sourceProxy,  DatanodeInfo destination) throws IOException {
    return DFSTestUtil.replaceBlock(block,source,sourceProxy,destination,StorageType.DEFAULT,Status.SUCCESS);
  }
  private boolean replaceBlock(  ExtendedBlock block,  DatanodeInfo source,  DatanodeInfo sourceProxy,  DatanodeInfo destination,  StorageType targetStorageType,  Status opStatus) throws IOException, SocketException {
    return DFSTestUtil.replaceBlock(block,source,sourceProxy,destination,targetStorageType,opStatus);
  }
  /** 
 * Standby namenode doesn't queue Delete block request when the add block request is in the edit log which are yet to be read.
 * @throws Exception
 */
  @Test public void testDeletedBlockWhenAddBlockIsInEdit() throws Exception {
    Configuration conf=new HdfsConfiguration();
    cluster=new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(1).build();
    DFSClient client=null;
    try {
      cluster.waitActive();
      assertEquals("Number of namenodes is not 2",2,cluster.getNumNameNodes());
      cluster.transitionToActive(0);
      assertTrue("Namenode 0 should be in active state",cluster.getNameNode(0).isActiveState());
      assertTrue("Namenode 1 should be in standby state",cluster.getNameNode(1).isStandbyState());
      DataNodeTestUtils.triggerHeartbeat(cluster.getDataNodes().get(0));
      FileSystem fs=cluster.getFileSystem(0);
      cluster.getDataNodes().get(0).triggerBlockReport(new BlockReportOptions.Factory().setIncremental(false).build());
      Path fileName=new Path("/tmp.txt");
      DFSTestUtil.createFile(fs,fileName,10L,(short)1,1234L);
      DFSTestUtil.waitReplication(fs,fileName,(short)1);
      client=new DFSClient(cluster.getFileSystem(0).getUri(),conf);
      List<LocatedBlock> locatedBlocks=client.getNamenode().getBlockLocations("/tmp.txt",0,10L).getLocatedBlocks();
      assertTrue(locatedBlocks.size() == 1);
      assertTrue(locatedBlocks.get(0).getLocations().length == 1);
      cluster.startDataNodes(conf,1,true,null,null,null,null);
      assertEquals("Number of datanodes should be 2",2,cluster.getDataNodes().size());
      DataNode dn0=cluster.getDataNodes().get(0);
      DataNode dn1=cluster.getDataNodes().get(1);
      String activeNNBPId=cluster.getNamesystem(0).getBlockPoolId();
      DatanodeDescriptor sourceDnDesc=NameNodeAdapter.getDatanode(cluster.getNamesystem(0),dn0.getDNRegistrationForBP(activeNNBPId));
      DatanodeDescriptor destDnDesc=NameNodeAdapter.getDatanode(cluster.getNamesystem(0),dn1.getDNRegistrationForBP(activeNNBPId));
      ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,fileName);
      LOG.info("replaceBlock:  " + replaceBlock(block,(DatanodeInfo)sourceDnDesc,(DatanodeInfo)sourceDnDesc,(DatanodeInfo)destDnDesc));
      for (int tries=0; tries < 20; tries++) {
        Thread.sleep(1000);
        DataNodeTestUtils.triggerDeletionReport(cluster.getDataNodes().get(0));
        locatedBlocks=client.getNamenode().getBlockLocations("/tmp.txt",0,10L).getLocatedBlocks();
        if (locatedBlocks.get(0).getLocations().length == 1) {
          break;
        }
      }
      cluster.transitionToStandby(0);
      cluster.transitionToActive(1);
      assertTrue("Namenode 1 should be in active state",cluster.getNameNode(1).isActiveState());
      assertTrue("Namenode 0 should be in standby state",cluster.getNameNode(0).isStandbyState());
      client.close();
      client=new DFSClient(cluster.getFileSystem(1).getUri(),conf);
      List<LocatedBlock> locatedBlocks1=client.getNamenode().getBlockLocations("/tmp.txt",0,10L).getLocatedBlocks();
      assertEquals(1,locatedBlocks1.size());
      assertEquals("The block should be only on 1 datanode ",1,locatedBlocks1.get(0).getLocations().length);
    }
  finally {
      IOUtils.cleanup(null,client);
      cluster.shutdown();
    }
  }
  /** 
 * @param args
 */
  public static void main(  String[] args) throws Exception {
    (new TestBlockReplacement()).testBlockReplacement();
  }
}
