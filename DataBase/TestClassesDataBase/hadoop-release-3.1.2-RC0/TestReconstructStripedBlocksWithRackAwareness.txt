public class TestReconstructStripedBlocksWithRackAwareness {
  public static final Logger LOG=LoggerFactory.getLogger(TestReconstructStripedBlocksWithRackAwareness.class);
static {
    GenericTestUtils.setLogLevel(BlockPlacementPolicy.LOG,Level.ALL);
    GenericTestUtils.setLogLevel(BlockManager.blockLog,Level.ALL);
    GenericTestUtils.setLogLevel(BlockManager.LOG,Level.ALL);
  }
  private final ErasureCodingPolicy ecPolicy=StripedFileTestUtil.getDefaultECPolicy();
  private final int cellSize=ecPolicy.getCellSize();
  private final short dataBlocks=(short)ecPolicy.getNumDataUnits();
  private final short parityBlocks=(short)ecPolicy.getNumParityUnits();
  private final String[] hosts=getHosts(dataBlocks + parityBlocks + 1);
  private final String[] racks=getRacks(dataBlocks + parityBlocks + 1,dataBlocks);
  private static String[] getHosts(  int numHosts){
    String[] hosts=new String[numHosts];
    for (int i=0; i < hosts.length; i++) {
      hosts[i]="host" + (i + 1);
    }
    return hosts;
  }
  private static String[] getRacks(  int numHosts,  int numRacks){
    String[] racks=new String[numHosts];
    int numHostEachRack=numHosts / numRacks;
    int residue=numHosts % numRacks;
    int j=0;
    for (int i=1; i <= numRacks; i++) {
      int limit=i <= residue ? numHostEachRack + 1 : numHostEachRack;
      for (int k=0; k < limit; k++) {
        racks[j++]="/r" + i;
      }
    }
    assert j == numHosts;
    return racks;
  }
  private MiniDFSCluster cluster;
  private static final HdfsConfiguration conf=new HdfsConfiguration();
  private DistributedFileSystem fs;
  @BeforeClass public static void setup() throws Exception {
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,1);
    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_CONSIDERLOAD_KEY,false);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_DECOMMISSION_INTERVAL_KEY,1);
  }
  @After public void tearDown(){
    if (cluster != null) {
      cluster.shutdown();
    }
  }
  private MiniDFSCluster.DataNodeProperties stopDataNode(  String hostname) throws IOException {
    MiniDFSCluster.DataNodeProperties dnProp=null;
    for (int i=0; i < cluster.getDataNodes().size(); i++) {
      DataNode dn=cluster.getDataNodes().get(i);
      if (dn.getDatanodeId().getHostName().equals(hostname)) {
        dnProp=cluster.stopDataNode(i);
        cluster.setDataNodeDead(dn.getDatanodeId());
        LOG.info("stop datanode " + dn.getDatanodeId().getHostName());
      }
    }
    return dnProp;
  }
  private DataNode getDataNode(  String host){
    for (    DataNode dn : cluster.getDataNodes()) {
      if (dn.getDatanodeId().getHostName().equals(host)) {
        return dn;
      }
    }
    return null;
  }
  /** 
 * When there are all the internal blocks available but they are not placed on enough racks, NameNode should avoid normal decoding reconstruction but copy an internal block to a new rack. In this test, we first need to create a scenario that a striped block has all the internal blocks but distributed in <6 racks. Then we check if the redundancy monitor can correctly schedule the reconstruction work for it.
 */
  @Test public void testReconstructForNotEnoughRacks() throws Exception {
    LOG.info("cluster hosts: {}, racks: {}",Arrays.asList(hosts),Arrays.asList(racks));
    cluster=new MiniDFSCluster.Builder(conf).racks(racks).hosts(hosts).numDataNodes(hosts.length).build();
    cluster.waitActive();
    fs=cluster.getFileSystem();
    fs.enableErasureCodingPolicy(StripedFileTestUtil.getDefaultECPolicy().getName());
    fs.setErasureCodingPolicy(new Path("/"),StripedFileTestUtil.getDefaultECPolicy().getName());
    FSNamesystem fsn=cluster.getNamesystem();
    BlockManager bm=fsn.getBlockManager();
    MiniDFSCluster.DataNodeProperties lastHost=stopDataNode(hosts[hosts.length - 1]);
    final Path file=new Path("/foo");
    DFSTestUtil.createFile(fs,file,cellSize * dataBlocks * 2,(short)1,0L);
    GenericTestUtils.waitFor(() -> bm.numOfUnderReplicatedBlocks() == 0,100,30000);
    LOG.info("Created file {}",file);
    final INodeFile fileNode=fsn.getFSDirectory().getINode4Write(file.toString()).asFile();
    BlockInfoStriped blockInfo=(BlockInfoStriped)fileNode.getLastBlock();
    Set<String> rackSet=new HashSet<>();
    for (    DatanodeStorageInfo storage : blockInfo.storages) {
      rackSet.add(storage.getDatanodeDescriptor().getNetworkLocation());
    }
    Assert.assertEquals("rackSet size is wrong: " + rackSet,dataBlocks - 1,rackSet.size());
    cluster.restartDataNode(lastHost);
    cluster.waitActive();
    NetworkTopology topology=bm.getDatanodeManager().getNetworkTopology();
    LOG.info("topology is: {}",topology);
    Assert.assertEquals(hosts.length,topology.getNumOfLeaves());
    Assert.assertEquals(dataBlocks,topology.getNumOfRacks());
    for (    DataNode dn : cluster.getDataNodes()) {
      DataNodeTestUtils.setHeartbeatsDisabledForTests(dn,true);
    }
    fsn.writeLock();
    try {
      bm.processMisReplicatedBlocks();
    }
  finally {
      fsn.writeUnlock();
    }
    boolean scheduled=false;
    for (int i=0; i < 5; i++) {
      for (      DatanodeStorageInfo storage : blockInfo.storages) {
        if (storage != null) {
          DatanodeDescriptor dn=storage.getDatanodeDescriptor();
          Assert.assertEquals("Block to be erasure coded is wrong for datanode:" + dn,0,dn.getNumberOfBlocksToBeErasureCoded());
          if (dn.getNumberOfBlocksToBeReplicated() == 1) {
            scheduled=true;
          }
        }
      }
      if (scheduled) {
        break;
      }
      Thread.sleep(1000);
    }
    Assert.assertTrue(scheduled);
  }
  @Test public void testChooseExcessReplicasToDelete() throws Exception {
    cluster=new MiniDFSCluster.Builder(conf).racks(racks).hosts(hosts).numDataNodes(hosts.length).build();
    cluster.waitActive();
    fs=cluster.getFileSystem();
    fs.enableErasureCodingPolicy(StripedFileTestUtil.getDefaultECPolicy().getName());
    fs.setErasureCodingPolicy(new Path("/"),StripedFileTestUtil.getDefaultECPolicy().getName());
    MiniDFSCluster.DataNodeProperties lastHost=stopDataNode(hosts[hosts.length - 1]);
    final Path file=new Path("/foo");
    DFSTestUtil.createFile(fs,file,cellSize * dataBlocks * 2,(short)1,0L);
    MiniDFSCluster.DataNodeProperties host1=stopDataNode("host1");
    cluster.restartDataNode(lastHost);
    cluster.waitActive();
    final short blockNum=(short)(dataBlocks + parityBlocks);
    DFSTestUtil.waitForReplication(fs,file,blockNum,15 * 1000);
    cluster.restartDataNode(host1);
    cluster.waitActive();
    for (    DataNode dn : cluster.getDataNodes()) {
      if (dn.getDatanodeId().getHostName().equals("host1")) {
        DataNodeTestUtils.triggerBlockReport(dn);
        break;
      }
    }
    DFSTestUtil.waitForReplication(fs,file,blockNum,15 * 1000);
    LocatedBlocks blks=fs.getClient().getLocatedBlocks(file.toString(),0);
    LocatedStripedBlock block=(LocatedStripedBlock)blks.getLastLocatedBlock();
    for (    DatanodeInfo dn : block.getLocations()) {
      Assert.assertFalse(dn.getHostName().equals("host1"));
    }
  }
  /** 
 * In case we have 10 internal blocks on 5 racks, where 9 of blocks are live and 1 decommissioning, make sure the reconstruction happens correctly.
 */
  @Test public void testReconstructionWithDecommission() throws Exception {
    final String[] rackNames=getRacks(dataBlocks + parityBlocks + 2,dataBlocks);
    final String[] hostNames=getHosts(dataBlocks + parityBlocks + 2);
    cluster=new MiniDFSCluster.Builder(conf).racks(rackNames).hosts(hostNames).numDataNodes(hostNames.length).build();
    cluster.waitActive();
    fs=cluster.getFileSystem();
    fs.enableErasureCodingPolicy(StripedFileTestUtil.getDefaultECPolicy().getName());
    fs.setErasureCodingPolicy(new Path("/"),StripedFileTestUtil.getDefaultECPolicy().getName());
    final BlockManager bm=cluster.getNamesystem().getBlockManager();
    final DatanodeManager dm=bm.getDatanodeManager();
    MiniDFSCluster.DataNodeProperties h9=stopDataNode(hostNames[hostNames.length - 3]);
    MiniDFSCluster.DataNodeProperties h10=stopDataNode(hostNames[hostNames.length - 2]);
    final Path file=new Path("/foo");
    DFSTestUtil.createFile(fs,file,cellSize * dataBlocks * 2,(short)1,0L);
    final BlockInfo blockInfo=cluster.getNamesystem().getFSDirectory().getINode(file.toString()).asFile().getLastBlock();
    cluster.restartDataNode(h9);
    cluster.waitActive();
    MiniDFSCluster.DataNodeProperties h11=stopDataNode(hostNames[hostNames.length - 1]);
    boolean recovered=bm.countNodes(blockInfo).liveReplicas() >= dataBlocks + parityBlocks;
    for (int i=0; i < 10 & !recovered; i++) {
      Thread.sleep(1000);
      recovered=bm.countNodes(blockInfo).liveReplicas() >= dataBlocks + parityBlocks;
    }
    Assert.assertTrue(recovered);
    DataNode datanode9=getDataNode(hostNames[hostNames.length - 3]);
    Assert.assertNotNull(datanode9);
    final DatanodeDescriptor dn9=dm.getDatanode(datanode9.getDatanodeId());
    dn9.startDecommission();
    cluster.restartDataNode(h10);
    cluster.restartDataNode(h11);
    cluster.waitActive();
    DataNodeTestUtils.triggerBlockReport(getDataNode(hostNames[hostNames.length - 1]));
    boolean satisfied=bm.isPlacementPolicySatisfied(blockInfo);
    Assert.assertFalse(satisfied);
    final DatanodeAdminManager decomManager=(DatanodeAdminManager)Whitebox.getInternalState(dm,"datanodeAdminManager");
    cluster.getNamesystem().writeLock();
    try {
      dn9.stopDecommission();
      decomManager.startDecommission(dn9);
    }
  finally {
      cluster.getNamesystem().writeUnlock();
    }
    boolean decommissioned=dn9.isDecommissioned();
    for (int i=0; i < 10 && !decommissioned; i++) {
      Thread.sleep(1000);
      decommissioned=dn9.isDecommissioned();
    }
    Assert.assertTrue(decommissioned);
    Assert.assertTrue(bm.isPlacementPolicySatisfied(blockInfo));
  }
}
