/** 
 * Test if a datanode can correctly upgrade itself 
 */
public class TestDatanodeRestart {
  @Test public void testFinalizedReplicas() throws Exception {
    Configuration conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,1024L);
    conf.setInt(HdfsClientConfigKeys.DFS_CLIENT_WRITE_PACKET_SIZE_KEY,512);
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
    cluster.waitActive();
    FileSystem fs=cluster.getFileSystem();
    try {
      final String TopDir="/test";
      DFSTestUtil util=new DFSTestUtil.Builder().setName("TestDatanodeRestart").setNumFiles(2).build();
      util.createFiles(fs,TopDir,(short)3);
      util.waitReplication(fs,TopDir,(short)3);
      util.checkFiles(fs,TopDir);
      cluster.restartDataNodes();
      cluster.waitActive();
      util.checkFiles(fs,TopDir);
    }
  finally {
      cluster.shutdown();
    }
  }
  public void testRbwReplicas() throws IOException {
    Configuration conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,1024L);
    conf.setInt(HdfsClientConfigKeys.DFS_CLIENT_WRITE_PACKET_SIZE_KEY,512);
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(2).build();
    cluster.waitActive();
    try {
      testRbwReplicas(cluster,false);
      testRbwReplicas(cluster,true);
    }
  finally {
      cluster.shutdown();
    }
  }
  private void testRbwReplicas(  MiniDFSCluster cluster,  boolean isCorrupt) throws IOException {
    FSDataOutputStream out=null;
    FileSystem fs=cluster.getFileSystem();
    final Path src=new Path("/test.txt");
    try {
      final int fileLen=515;
      byte[] writeBuf=new byte[fileLen];
      new Random().nextBytes(writeBuf);
      out=fs.create(src);
      out.write(writeBuf);
      out.hflush();
      DataNode dn=cluster.getDataNodes().get(0);
      try (FsDatasetSpi.FsVolumeReferences volumes=dataset(dn).getFsVolumeReferences()){
        for (        FsVolumeSpi vol : volumes) {
          final FsVolumeImpl volume=(FsVolumeImpl)vol;
          File currentDir=volume.getCurrentDir().getParentFile().getParentFile();
          File rbwDir=new File(currentDir,"rbw");
          for (          File file : rbwDir.listFiles()) {
            if (isCorrupt && Block.isBlockFilename(file)) {
              new RandomAccessFile(file,"rw").setLength(fileLen - 1);
            }
          }
        }
      }
       cluster.restartDataNodes();
      cluster.waitActive();
      dn=cluster.getDataNodes().get(0);
      String bpid=cluster.getNamesystem().getBlockPoolId();
      ReplicaMap replicas=dataset(dn).volumeMap;
      Assert.assertEquals(1,replicas.size(bpid));
      ReplicaInfo replica=replicas.replicas(bpid).iterator().next();
      Assert.assertEquals(ReplicaState.RWR,replica.getState());
      if (isCorrupt) {
        Assert.assertEquals((fileLen - 1) / 512 * 512,replica.getNumBytes());
      }
 else {
        Assert.assertEquals(fileLen,replica.getNumBytes());
      }
      dataset(dn).invalidate(bpid,new Block[]{replica});
    }
  finally {
      IOUtils.closeStream(out);
      if (fs.exists(src)) {
        fs.delete(src,false);
      }
      fs.close();
    }
  }
  private static FsDatasetImpl dataset(  DataNode dn){
    return (FsDatasetImpl)DataNodeTestUtils.getFSDataset(dn);
  }
  @Test public void testWaitForRegistrationOnRestart() throws Exception {
    Configuration conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_DATANODE_BP_READY_TIMEOUT_KEY,5);
    conf.setInt(HdfsClientConfigKeys.DFS_CLIENT_SOCKET_TIMEOUT_KEY,5000);
    DataNodeFaultInjector dnFaultInjector=new DataNodeFaultInjector(){
      @Override public void noRegistration() throws IOException {
        throw new IOException("no reg found for testing");
      }
    }
;
    DataNodeFaultInjector oldDnInjector=DataNodeFaultInjector.get();
    DataNodeFaultInjector.set(dnFaultInjector);
    MiniDFSCluster cluster=null;
    long start=0;
    Path file=new Path("/reg");
    try {
      int numDNs=1;
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(numDNs).build();
      cluster.waitActive();
      start=Time.monotonicNow();
      FileSystem fileSys=cluster.getFileSystem();
      try {
        DFSTestUtil.createFile(fileSys,file,10240L,(short)1,0L);
        throw new IOException("Did not fail!");
      }
 catch (      org.apache.hadoop.ipc.RemoteException e) {
        long elapsed=Time.monotonicNow() - start;
        if (elapsed < 5000 || elapsed > 10000) {
          throw new IOException(elapsed + " milliseconds passed.",e);
        }
      }
      DataNodeFaultInjector.set(oldDnInjector);
      DFSTestUtil.createFile(fileSys,file,10240L,(short)1,0L);
      fileSys.append(file);
      DataNodeFaultInjector.set(dnFaultInjector);
      byte[] buffer=new byte[8];
      start=Time.monotonicNow();
      try {
        fileSys.open(file).read(0L,buffer,0,1);
        throw new IOException("Did not fail!");
      }
 catch (      IOException e) {
        long elapsed=Time.monotonicNow() - start;
        if (e.getMessage().contains("readBlockLength")) {
          throw new IOException("Failed, but with unexpected exception:",e);
        }
        if (elapsed < 5000 || elapsed > 10000) {
          throw new IOException(elapsed + " milliseconds passed.",e);
        }
      }
      DataNodeFaultInjector.set(oldDnInjector);
      fileSys.open(file).read(0L,buffer,0,1);
    }
  finally {
      DataNodeFaultInjector.set(oldDnInjector);
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
}
