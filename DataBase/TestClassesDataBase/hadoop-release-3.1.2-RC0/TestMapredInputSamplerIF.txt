static class TestMapredInputSamplerIF extends TestInputSamplerIF implements org.apache.hadoop.mapred.InputFormat<IntWritable,NullWritable> {
  TestMapredInputSamplerIF(  int maxDepth,  int numSplits,  int... splitInit){
    super(maxDepth,numSplits,splitInit);
  }
  @Override public org.apache.hadoop.mapred.InputSplit[] getSplits(  JobConf job,  int numSplits) throws IOException {
    List<InputSplit> splits=null;
    try {
      splits=getSplits(Job.getInstance(job));
    }
 catch (    InterruptedException e) {
      throw new IOException(e);
    }
    org.apache.hadoop.mapred.InputSplit[] retVals=new org.apache.hadoop.mapred.InputSplit[splits.size()];
    for (int i=0; i < splits.size(); ++i) {
      MapredSequentialSplit split=new MapredSequentialSplit(((SequentialSplit)splits.get(i)).getInit());
      retVals[i]=split;
    }
    return retVals;
  }
  @Override public org.apache.hadoop.mapred.RecordReader<IntWritable,NullWritable> getRecordReader(  final org.apache.hadoop.mapred.InputSplit split,  JobConf job,  Reporter reporter) throws IOException {
    return new org.apache.hadoop.mapred.RecordReader<IntWritable,NullWritable>(){
      private final IntWritable i=new IntWritable(((MapredSequentialSplit)split).getInit());
      private int maxVal=i.get() + maxDepth + 1;
      @Override public boolean next(      IntWritable key,      NullWritable value) throws IOException {
        i.set(i.get() + 1);
        return i.get() < maxVal;
      }
      @Override public IntWritable createKey(){
        return new IntWritable(i.get());
      }
      @Override public NullWritable createValue(){
        return NullWritable.get();
      }
      @Override public long getPos() throws IOException {
        return 0;
      }
      @Override public void close() throws IOException {
      }
      @Override public float getProgress() throws IOException {
        return 0;
      }
    }
;
  }
}
