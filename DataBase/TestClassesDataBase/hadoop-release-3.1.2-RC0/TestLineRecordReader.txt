public class TestLineRecordReader {
  private static Path workDir=new Path(new Path(System.getProperty("test.build.data","target"),"data"),"TestTextInputFormat");
  private static Path inputDir=new Path(workDir,"input");
  private void testSplitRecords(  String testFileName,  long firstSplitLength) throws IOException {
    URL testFileUrl=getClass().getClassLoader().getResource(testFileName);
    assertNotNull("Cannot find " + testFileName,testFileUrl);
    File testFile=new File(testFileUrl.getFile());
    long testFileSize=testFile.length();
    Path testFilePath=new Path(testFile.getAbsolutePath());
    Configuration conf=new Configuration();
    testSplitRecordsForFile(conf,firstSplitLength,testFileSize,testFilePath);
  }
  private void testSplitRecordsForFile(  Configuration conf,  long firstSplitLength,  long testFileSize,  Path testFilePath) throws IOException {
    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH,Integer.MAX_VALUE);
    assertTrue("unexpected test data at " + testFilePath,testFileSize > firstSplitLength);
    String delimiter=conf.get("textinputformat.record.delimiter");
    byte[] recordDelimiterBytes=null;
    if (null != delimiter) {
      recordDelimiterBytes=delimiter.getBytes(StandardCharsets.UTF_8);
    }
    TaskAttemptContext context=new TaskAttemptContextImpl(conf,new TaskAttemptID());
    FileSplit split=new FileSplit(testFilePath,0,testFileSize,(String[])null);
    LineRecordReader reader=new LineRecordReader(recordDelimiterBytes);
    reader.initialize(split,context);
    int numRecordsNoSplits=0;
    while (reader.nextKeyValue()) {
      ++numRecordsNoSplits;
    }
    reader.close();
    split=new FileSplit(testFilePath,0,firstSplitLength,(String[])null);
    reader=new LineRecordReader(recordDelimiterBytes);
    reader.initialize(split,context);
    int numRecordsFirstSplit=0;
    while (reader.nextKeyValue()) {
      ++numRecordsFirstSplit;
    }
    reader.close();
    split=new FileSplit(testFilePath,firstSplitLength,testFileSize - firstSplitLength,(String[])null);
    reader=new LineRecordReader(recordDelimiterBytes);
    reader.initialize(split,context);
    int numRecordsRemainingSplits=0;
    while (reader.nextKeyValue()) {
      ++numRecordsRemainingSplits;
    }
    reader.close();
    assertEquals("Unexpected number of records in split ",numRecordsNoSplits,numRecordsFirstSplit + numRecordsRemainingSplits);
  }
  @Test public void testBzip2SplitEndsAtCR() throws IOException {
    testSplitRecords("blockEndingInCR.txt.bz2",136498);
  }
  @Test public void testBzip2SplitEndsAtCRThenLF() throws IOException {
    testSplitRecords("blockEndingInCRThenLF.txt.bz2",136498);
  }
  @Test(expected=IOException.class) public void testSafeguardSplittingUnsplittableFiles() throws IOException {
    testSplitRecords("TestSafeguardSplittingUnsplittableFiles.txt.gz",2);
  }
  public ArrayList<String> readRecords(  URL testFileUrl,  int splitSize) throws IOException {
    File testFile=new File(testFileUrl.getFile());
    long testFileSize=testFile.length();
    Path testFilePath=new Path(testFile.getAbsolutePath());
    Configuration conf=new Configuration();
    conf.setInt("io.file.buffer.size",1);
    TaskAttemptContext context=new TaskAttemptContextImpl(conf,new TaskAttemptID());
    ArrayList<String> records=new ArrayList<String>();
    long offset=0;
    while (offset < testFileSize) {
      FileSplit split=new FileSplit(testFilePath,offset,splitSize,null);
      LineRecordReader reader=new LineRecordReader();
      reader.initialize(split,context);
      while (reader.nextKeyValue()) {
        records.add(reader.getCurrentValue().toString());
      }
      offset+=splitSize;
    }
    return records;
  }
  public String[] readRecordsDirectly(  URL testFileUrl,  boolean bzip) throws IOException {
    int MAX_DATA_SIZE=1024 * 1024;
    byte[] data=new byte[MAX_DATA_SIZE];
    FileInputStream fis=new FileInputStream(testFileUrl.getFile());
    int count;
    if (bzip) {
      BZip2CompressorInputStream bzIn=new BZip2CompressorInputStream(fis);
      count=bzIn.read(data);
      bzIn.close();
    }
 else {
      count=fis.read(data);
    }
    fis.close();
    assertTrue("Test file data too big for buffer",count < data.length);
    return new String(data,0,count,"UTF-8").split("\n");
  }
  public void checkRecordSpanningMultipleSplits(  String testFile,  int splitSize,  boolean bzip) throws IOException {
    URL testFileUrl=getClass().getClassLoader().getResource(testFile);
    ArrayList<String> records=readRecords(testFileUrl,splitSize);
    String[] actuals=readRecordsDirectly(testFileUrl,bzip);
    assertEquals("Wrong number of records",actuals.length,records.size());
    boolean hasLargeRecord=false;
    for (int i=0; i < actuals.length; ++i) {
      assertEquals(actuals[i],records.get(i));
      if (actuals[i].length() > 2 * splitSize) {
        hasLargeRecord=true;
      }
    }
    assertTrue("Invalid test data. Doesn't have a large enough record",hasLargeRecord);
  }
  @Test public void testRecordSpanningMultipleSplits() throws IOException {
    checkRecordSpanningMultipleSplits("recordSpanningMultipleSplits.txt",10,false);
  }
  @Test public void testRecordSpanningMultipleSplitsCompressed() throws IOException {
    checkRecordSpanningMultipleSplits("recordSpanningMultipleSplits.txt.bz2",200 * 1000,true);
  }
  @Test public void testStripBOM() throws IOException {
    String UTF8_BOM="\uFEFF";
    URL testFileUrl=getClass().getClassLoader().getResource("testBOM.txt");
    assertNotNull("Cannot find testBOM.txt",testFileUrl);
    File testFile=new File(testFileUrl.getFile());
    Path testFilePath=new Path(testFile.getAbsolutePath());
    long testFileSize=testFile.length();
    Configuration conf=new Configuration();
    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH,Integer.MAX_VALUE);
    TaskAttemptContext context=new TaskAttemptContextImpl(conf,new TaskAttemptID());
    FileSplit split=new FileSplit(testFilePath,0,testFileSize,(String[])null);
    LineRecordReader reader=new LineRecordReader();
    reader.initialize(split,context);
    int numRecords=0;
    boolean firstLine=true;
    boolean skipBOM=true;
    while (reader.nextKeyValue()) {
      if (firstLine) {
        firstLine=false;
        if (reader.getCurrentValue().toString().startsWith(UTF8_BOM)) {
          skipBOM=false;
        }
      }
      ++numRecords;
    }
    reader.close();
    assertTrue("BOM is not skipped",skipBOM);
  }
  @Test public void testMultipleClose() throws IOException {
    URL testFileUrl=getClass().getClassLoader().getResource("recordSpanningMultipleSplits.txt.bz2");
    assertNotNull("Cannot find recordSpanningMultipleSplits.txt.bz2",testFileUrl);
    File testFile=new File(testFileUrl.getFile());
    Path testFilePath=new Path(testFile.getAbsolutePath());
    long testFileSize=testFile.length();
    Configuration conf=new Configuration();
    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH,Integer.MAX_VALUE);
    TaskAttemptContext context=new TaskAttemptContextImpl(conf,new TaskAttemptID());
    FileSplit split=new FileSplit(testFilePath,0,testFileSize,null);
    LineRecordReader reader=new LineRecordReader();
    reader.initialize(split,context);
    while (reader.nextKeyValue())     ;
    reader.close();
    reader.close();
    BZip2Codec codec=new BZip2Codec();
    codec.setConf(conf);
    Set<Decompressor> decompressors=new HashSet<Decompressor>();
    for (int i=0; i < 10; ++i) {
      decompressors.add(CodecPool.getDecompressor(codec));
    }
    assertEquals(10,decompressors.size());
  }
  /** 
 * Writes the input test file
 * @param conf
 * @return Path of the file created
 * @throws IOException
 */
  private Path createInputFile(  Configuration conf,  String data) throws IOException {
    FileSystem localFs=FileSystem.getLocal(conf);
    Path file=new Path(inputDir,"test.txt");
    Writer writer=new OutputStreamWriter(localFs.create(file));
    try {
      writer.write(data);
    }
  finally {
      writer.close();
    }
    return file;
  }
  @Test public void testUncompressedInput() throws Exception {
    Configuration conf=new Configuration();
    String inputData="abc+def+ghi+jkl+mno+pqr+stu+vw +xyz";
    Path inputFile=createInputFile(conf,inputData);
    conf.set("textinputformat.record.delimiter","+");
    for (int bufferSize=1; bufferSize <= inputData.length(); bufferSize++) {
      for (int splitSize=1; splitSize < inputData.length(); splitSize++) {
        conf.setInt("io.file.buffer.size",bufferSize);
        testSplitRecordsForFile(conf,splitSize,inputData.length(),inputFile);
      }
    }
    inputData="abc|+|def|+|ghi|+|jkl|+|mno|+|pqr|+|stu|+|vw |+|xyz";
    inputFile=createInputFile(conf,inputData);
    conf.set("textinputformat.record.delimiter","|+|");
    for (int bufferSize=1; bufferSize <= inputData.length(); bufferSize++) {
      for (int splitSize=1; splitSize < inputData.length(); splitSize++) {
        conf.setInt("io.file.buffer.size",bufferSize);
        testSplitRecordsForFile(conf,splitSize,inputData.length(),inputFile);
      }
    }
    inputData="abc+def++ghi+jkl++mno+pqr++stu+vw ++xyz";
    inputFile=createInputFile(conf,inputData);
    conf.set("textinputformat.record.delimiter","+");
    for (int bufferSize=1; bufferSize <= inputData.length(); bufferSize++) {
      for (int splitSize=1; splitSize < inputData.length(); splitSize++) {
        conf.setInt("io.file.buffer.size",bufferSize);
        testSplitRecordsForFile(conf,splitSize,inputData.length(),inputFile);
      }
    }
    inputData="abc|+||+|defghi|+|jkl|+||+|mno|+|pqr|+||+|stu|+|vw |+||+|xyz";
    inputFile=createInputFile(conf,inputData);
    conf.set("textinputformat.record.delimiter","|+|");
    for (int bufferSize=1; bufferSize <= inputData.length(); bufferSize++) {
      for (int splitSize=1; splitSize < inputData.length(); splitSize++) {
        conf.setInt("io.file.buffer.size",bufferSize);
        testSplitRecordsForFile(conf,splitSize,inputData.length(),inputFile);
      }
    }
    inputData="abc+def+-ghi+jkl+-mno+pqr+-stu+vw +-xyz";
    inputFile=createInputFile(conf,inputData);
    conf.set("textinputformat.record.delimiter","+-");
    for (int bufferSize=1; bufferSize <= inputData.length(); bufferSize++) {
      for (int splitSize=1; splitSize < inputData.length(); splitSize++) {
        conf.setInt("io.file.buffer.size",bufferSize);
        testSplitRecordsForFile(conf,splitSize,inputData.length(),inputFile);
      }
    }
    inputData="abc\n+def\n+ghi\n+jkl\n+mno";
    inputFile=createInputFile(conf,inputData);
    conf.set("textinputformat.record.delimiter","\n+");
    for (int bufferSize=1; bufferSize <= inputData.length(); bufferSize++) {
      for (int splitSize=1; splitSize < inputData.length(); splitSize++) {
        conf.setInt("io.file.buffer.size",bufferSize);
        testSplitRecordsForFile(conf,splitSize,inputData.length(),inputFile);
      }
    }
    inputData="abc\ndef+\nghi+\njkl\nmno";
    inputFile=createInputFile(conf,inputData);
    conf.set("textinputformat.record.delimiter","+\n");
    for (int bufferSize=1; bufferSize <= inputData.length(); bufferSize++) {
      for (int splitSize=1; splitSize < inputData.length(); splitSize++) {
        conf.setInt("io.file.buffer.size",bufferSize);
        testSplitRecordsForFile(conf,splitSize,inputData.length(),inputFile);
      }
    }
  }
  @Test public void testUncompressedInputContainingCRLF() throws Exception {
    Configuration conf=new Configuration();
    String inputData="a\r\nb\rc\nd\r\n";
    Path inputFile=createInputFile(conf,inputData);
    for (int bufferSize=1; bufferSize <= inputData.length(); bufferSize++) {
      for (int splitSize=1; splitSize < inputData.length(); splitSize++) {
        conf.setInt("io.file.buffer.size",bufferSize);
        testSplitRecordsForFile(conf,splitSize,inputData.length(),inputFile);
      }
    }
  }
  @Test public void testUncompressedInputCustomDelimiterPosValue() throws Exception {
    Configuration conf=new Configuration();
    conf.setInt("io.file.buffer.size",10);
    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH,Integer.MAX_VALUE);
    String inputData="abcdefghij++kl++mno";
    Path inputFile=createInputFile(conf,inputData);
    String delimiter="++";
    byte[] recordDelimiterBytes=delimiter.getBytes(StandardCharsets.UTF_8);
    int splitLength=15;
    FileSplit split=new FileSplit(inputFile,0,splitLength,(String[])null);
    TaskAttemptContext context=new TaskAttemptContextImpl(conf,new TaskAttemptID());
    LineRecordReader reader=new LineRecordReader(recordDelimiterBytes);
    reader.initialize(split,context);
    assertTrue("Expected record got nothing",reader.nextKeyValue());
    LongWritable key=reader.getCurrentKey();
    Text value=reader.getCurrentValue();
    assertEquals("Wrong length for record value",10,value.getLength());
    assertEquals("Wrong position after record read",0,key.get());
    assertTrue("Expected record got nothing",reader.nextKeyValue());
    assertEquals("Wrong length for record value",2,value.getLength());
    assertEquals("Wrong position after record read",12,key.get());
    assertTrue("Expected record got nothing",reader.nextKeyValue());
    assertEquals("Wrong length for record value",3,value.getLength());
    assertEquals("Wrong position after record read",16,key.get());
    assertFalse(reader.nextKeyValue());
    assertEquals("Wrong position after record read",19,key.get());
    key=reader.getCurrentKey();
    assertNull("Unexpected key returned",key);
    reader.close();
    split=new FileSplit(inputFile,splitLength,inputData.length() - splitLength,(String[])null);
    reader=new LineRecordReader(recordDelimiterBytes);
    reader.initialize(split,context);
    assertFalse("Unexpected record returned",reader.nextKeyValue());
    key=reader.getCurrentKey();
    assertNull("Unexpected key returned",key);
    reader.close();
    inputData="abcd+efgh++ijk++mno";
    inputFile=createInputFile(conf,inputData);
    splitLength=5;
    split=new FileSplit(inputFile,0,splitLength,(String[])null);
    reader=new LineRecordReader(recordDelimiterBytes);
    reader.initialize(split,context);
    assertTrue("Expected record got nothing",reader.nextKeyValue());
    key=reader.getCurrentKey();
    value=reader.getCurrentValue();
    assertEquals("Wrong position after record read",0,key.get());
    assertEquals("Wrong length for record value",9,value.getLength());
    assertFalse(reader.nextKeyValue());
    assertEquals("Wrong position after record read",11,key.get());
    key=reader.getCurrentKey();
    assertNull("Unexpected key returned",key);
    reader.close();
    split=new FileSplit(inputFile,splitLength,inputData.length() - splitLength,(String[])null);
    reader=new LineRecordReader(recordDelimiterBytes);
    reader.initialize(split,context);
    assertTrue("Expected record got nothing",reader.nextKeyValue());
    key=reader.getCurrentKey();
    value=reader.getCurrentValue();
    assertEquals("Wrong position after record read",11,key.get());
    assertEquals("Wrong length for record value",3,value.getLength());
    assertTrue("Expected record got nothing",reader.nextKeyValue());
    assertEquals("Wrong position after record read",16,key.get());
    assertEquals("Wrong length for record value",3,value.getLength());
    assertFalse(reader.nextKeyValue());
    assertEquals("Wrong position after record read",19,key.get());
    reader.close();
    inputData="abcd|efgh|+|ij|kl|+|mno|pqr";
    inputFile=createInputFile(conf,inputData);
    delimiter="|+|";
    recordDelimiterBytes=delimiter.getBytes(StandardCharsets.UTF_8);
    for (int bufferSize=1; bufferSize <= inputData.length(); bufferSize++) {
      for (int splitSize=1; splitSize < inputData.length(); splitSize++) {
        int keyPosition=0;
        conf.setInt("io.file.buffer.size",bufferSize);
        split=new FileSplit(inputFile,0,bufferSize,(String[])null);
        reader=new LineRecordReader(recordDelimiterBytes);
        reader.initialize(split,context);
        assertTrue("Expected record got nothing",reader.nextKeyValue());
        key=reader.getCurrentKey();
        value=reader.getCurrentValue();
        assertTrue("abcd|efgh".equals(value.toString()));
        assertEquals("Wrong position after record read",keyPosition,key.get());
        keyPosition=12;
        if (reader.nextKeyValue()) {
          assertTrue("ij|kl".equals(value.toString()));
          assertEquals("Wrong position after record read",keyPosition,key.get());
          keyPosition=20;
        }
        if (reader.nextKeyValue()) {
          assertTrue("mno|pqr".equals(value.toString()));
          assertEquals("Wrong position after record read",keyPosition,key.get());
          keyPosition=inputData.length();
        }
        assertFalse("Unexpected record returned",reader.nextKeyValue());
        assertEquals("Wrong position after record read",keyPosition,key.get());
        key=reader.getCurrentKey();
        assertNull("Unexpected key returned",key);
        reader.close();
      }
    }
  }
  @Test public void testUncompressedInputDefaultDelimiterPosValue() throws Exception {
    Configuration conf=new Configuration();
    String inputData="1234567890\r\n12\r\n345";
    Path inputFile=createInputFile(conf,inputData);
    conf.setInt("io.file.buffer.size",10);
    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH,Integer.MAX_VALUE);
    FileSplit split=new FileSplit(inputFile,0,15,(String[])null);
    TaskAttemptContext context=new TaskAttemptContextImpl(conf,new TaskAttemptID());
    LineRecordReader reader=new LineRecordReader(null);
    reader.initialize(split,context);
    LongWritable key;
    Text value;
    reader.nextKeyValue();
    key=reader.getCurrentKey();
    value=reader.getCurrentValue();
    assertEquals(10,value.getLength());
    assertEquals(0,key.get());
    reader.nextKeyValue();
    assertEquals(2,value.getLength());
    assertEquals(12,key.get());
    assertFalse(reader.nextKeyValue());
    assertEquals(16,key.get());
    split=new FileSplit(inputFile,15,4,(String[])null);
    reader=new LineRecordReader(null);
    reader.initialize(split,context);
    reader.nextKeyValue();
    key=reader.getCurrentKey();
    value=reader.getCurrentValue();
    assertEquals(3,value.getLength());
    assertEquals(16,key.get());
    assertFalse(reader.nextKeyValue());
    assertEquals(19,key.get());
    inputData="123456789\r\r\n";
    inputFile=createInputFile(conf,inputData);
    split=new FileSplit(inputFile,0,12,(String[])null);
    reader=new LineRecordReader(null);
    reader.initialize(split,context);
    reader.nextKeyValue();
    key=reader.getCurrentKey();
    value=reader.getCurrentValue();
    assertEquals(9,value.getLength());
    assertEquals(0,key.get());
    reader.nextKeyValue();
    assertEquals(0,value.getLength());
    assertEquals(10,key.get());
    assertFalse(reader.nextKeyValue());
    assertEquals(12,key.get());
  }
  @Test public void testBzipWithMultibyteDelimiter() throws IOException {
    String testFileName="compressedMultibyteDelimiter.txt.bz2";
    int firstSplitLength=100;
    URL testFileUrl=getClass().getClassLoader().getResource(testFileName);
    assertNotNull("Cannot find " + testFileName,testFileUrl);
    File testFile=new File(testFileUrl.getFile());
    long testFileSize=testFile.length();
    Path testFilePath=new Path(testFile.getAbsolutePath());
    assertTrue("Split size is smaller than header length",firstSplitLength > 9);
    assertTrue("Split size is larger than compressed file size " + testFilePath,testFileSize > firstSplitLength);
    Configuration conf=new Configuration();
    conf.setInt(org.apache.hadoop.mapreduce.lib.input.LineRecordReader.MAX_LINE_LENGTH,Integer.MAX_VALUE);
    String delimiter="<E-LINE>\r\r\n";
    conf.set("textinputformat.record.delimiter",delimiter);
    testSplitRecordsForFile(conf,firstSplitLength,testFileSize,testFilePath);
  }
}
