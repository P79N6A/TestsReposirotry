/** 
 * Test for HCatOutputFormat. Writes a partition using HCatOutputFormat and reads it back using HCatInputFormat, checks the column values and counts. This class can be tested to test different partitioning schemes. This is a parameterized test that tests HCatOutputFormat and HCatInputFormat against Hive's native storage formats enumerated using  {@link org.apache.hive.hcatalog.mapreduce.StorageFormats}.
 */
@RunWith(Parameterized.class) public abstract class HCatMapReduceTest extends HCatBaseTest {
  private static final Logger LOG=LoggerFactory.getLogger(HCatMapReduceTest.class);
  protected static String dbName=MetaStoreUtils.DEFAULT_DATABASE_NAME;
  protected static final String TABLE_NAME="testHCatMapReduceTable";
  private static List<HCatRecord> writeRecords=new ArrayList<HCatRecord>();
  private static List<HCatRecord> readRecords=new ArrayList<HCatRecord>();
  private static FileSystem fs;
  private String externalTableLocation=null;
  protected String tableName;
  protected String serdeClass;
  protected String inputFormatClass;
  protected String outputFormatClass;
  /** 
 * List of SerDe classes that the HCatalog core tests will not be run against.
 */
  public static final Set<String> DISABLED_SERDES=ImmutableSet.of(AvroSerDe.class.getName(),ParquetHiveSerDe.class.getName());
  @Parameterized.Parameters public static Collection<Object[]> generateParameters(){
    return StorageFormats.asParameters();
  }
  /** 
 * Test constructor that sets the storage format class names provided by the test parameter.
 */
  public HCatMapReduceTest(  String name,  String serdeClass,  String inputFormatClass,  String outputFormatClass) throws Exception {
    this.serdeClass=serdeClass;
    this.inputFormatClass=inputFormatClass;
    this.outputFormatClass=outputFormatClass;
    this.tableName=TABLE_NAME + "_" + name;
  }
  protected abstract List<FieldSchema> getPartitionKeys();
  protected abstract List<FieldSchema> getTableColumns();
  protected Boolean isTableExternal(){
    return false;
  }
  protected boolean isTableImmutable(){
    return true;
  }
  @BeforeClass public static void setUpOneTime() throws Exception {
    fs=new LocalFileSystem();
    fs.initialize(fs.getWorkingDirectory().toUri(),new Configuration());
    HiveConf hiveConf=new HiveConf();
    hiveConf.setInt(HCatConstants.HCAT_HIVE_CLIENT_EXPIRY_TIME,0);
    HCatUtil.getHiveMetastoreClient(hiveConf);
    MapCreate.writeCount=0;
    MapRead.readCount=0;
  }
  @After public void deleteTable() throws Exception {
    try {
      String databaseName=(dbName == null) ? MetaStoreUtils.DEFAULT_DATABASE_NAME : dbName;
      client.dropTable(databaseName,tableName);
      if (isTableExternal() && (externalTableLocation != null)) {
        if (fs.exists(new Path(externalTableLocation))) {
          fs.delete(new Path(externalTableLocation),true);
        }
      }
    }
 catch (    Exception e) {
      e.printStackTrace();
      throw e;
    }
  }
  @Before public void createTable() throws Exception {
    Assume.assumeTrue(!DISABLED_SERDES.contains(serdeClass));
    String databaseName=(dbName == null) ? MetaStoreUtils.DEFAULT_DATABASE_NAME : dbName;
    try {
      client.dropTable(databaseName,tableName);
    }
 catch (    Exception e) {
    }
    Table tbl=new Table();
    tbl.setDbName(databaseName);
    tbl.setTableName(tableName);
    if (isTableExternal()) {
      tbl.setTableType(TableType.EXTERNAL_TABLE.toString());
    }
 else {
      tbl.setTableType(TableType.MANAGED_TABLE.toString());
    }
    StorageDescriptor sd=new StorageDescriptor();
    sd.setCols(getTableColumns());
    tbl.setPartitionKeys(getPartitionKeys());
    tbl.setSd(sd);
    sd.setBucketCols(new ArrayList<String>(2));
    sd.setSerdeInfo(new SerDeInfo());
    sd.getSerdeInfo().setName(tbl.getTableName());
    sd.getSerdeInfo().setParameters(new HashMap<String,String>());
    sd.getSerdeInfo().getParameters().put(serdeConstants.SERIALIZATION_FORMAT,"1");
    if (isTableExternal()) {
      sd.getSerdeInfo().getParameters().put("EXTERNAL","TRUE");
    }
    sd.getSerdeInfo().setSerializationLib(serdeClass);
    sd.setInputFormat(inputFormatClass);
    sd.setOutputFormat(outputFormatClass);
    Map<String,String> tableParams=new HashMap<String,String>();
    if (isTableExternal()) {
      tableParams.put("EXTERNAL","TRUE");
    }
    if (isTableImmutable()) {
      tableParams.put(hive_metastoreConstants.IS_IMMUTABLE,"true");
    }
    StatsSetupConst.setBasicStatsState(tableParams,StatsSetupConst.TRUE);
    tbl.setParameters(tableParams);
    client.createTable(tbl);
  }
  private void createInputFile(  Path path,  int rowCount) throws IOException {
    if (fs.exists(path)) {
      fs.delete(path,true);
    }
    FSDataOutputStream os=fs.create(path);
    for (int i=0; i < rowCount; i++) {
      os.writeChars(i + "\n");
    }
    os.close();
  }
public static class MapCreate extends Mapper<LongWritable,Text,BytesWritable,HCatRecord> {
    static int writeCount=0;
    @Override public void map(    LongWritable key,    Text value,    Context context) throws IOException, InterruptedException {
      try {
        HCatRecord rec=writeRecords.get(writeCount);
        context.write(null,rec);
        writeCount++;
      }
 catch (      Exception e) {
        e.printStackTrace(System.err);
        throw new IOException(e);
      }
    }
  }
public static class MapRead extends Mapper<WritableComparable,HCatRecord,BytesWritable,Text> {
    static int readCount=0;
    @Override public void map(    WritableComparable key,    HCatRecord value,    Context context) throws IOException, InterruptedException {
      try {
        readRecords.add(value);
        readCount++;
      }
 catch (      Exception e) {
        e.printStackTrace();
        throw new IOException(e);
      }
    }
  }
  Job runMRCreate(  Map<String,String> partitionValues,  List<HCatFieldSchema> partitionColumns,  List<HCatRecord> records,  int writeCount,  boolean assertWrite) throws Exception {
    return runMRCreate(partitionValues,partitionColumns,records,writeCount,assertWrite,true,null);
  }
  /** 
 * Run a local map reduce job to load data from in memory records to an HCatalog Table
 * @param partitionValues
 * @param partitionColumns
 * @param records data to be written to HCatalog table
 * @param writeCount
 * @param assertWrite
 * @param asSingleMapTask
 * @return
 * @throws Exception
 */
  Job runMRCreate(  Map<String,String> partitionValues,  List<HCatFieldSchema> partitionColumns,  List<HCatRecord> records,  int writeCount,  boolean assertWrite,  boolean asSingleMapTask,  String customDynamicPathPattern) throws Exception {
    writeRecords=records;
    MapCreate.writeCount=0;
    Configuration conf=new Configuration();
    Job job=new Job(conf,"hcat mapreduce write test");
    job.setJarByClass(this.getClass());
    job.setMapperClass(HCatMapReduceTest.MapCreate.class);
    job.setInputFormatClass(TextInputFormat.class);
    if (asSingleMapTask) {
      Path path=new Path(fs.getWorkingDirectory(),"mapred/testHCatMapReduceInput");
      createInputFile(path,writeCount);
      TextInputFormat.setInputPaths(job,path);
    }
 else {
      Path path=new Path(fs.getWorkingDirectory(),"mapred/testHCatMapReduceInput");
      createInputFile(path,writeCount / 2);
      Path path2=new Path(fs.getWorkingDirectory(),"mapred/testHCatMapReduceInput2");
      createInputFile(path2,(writeCount - writeCount / 2));
      TextInputFormat.setInputPaths(job,path,path2);
    }
    job.setOutputFormatClass(HCatOutputFormat.class);
    OutputJobInfo outputJobInfo=OutputJobInfo.create(dbName,tableName,partitionValues);
    if (customDynamicPathPattern != null) {
      job.getConfiguration().set(HCatConstants.HCAT_DYNAMIC_CUSTOM_PATTERN,customDynamicPathPattern);
    }
    HCatOutputFormat.setOutput(job,outputJobInfo);
    job.setMapOutputKeyClass(BytesWritable.class);
    job.setMapOutputValueClass(DefaultHCatRecord.class);
    job.setNumReduceTasks(0);
    HCatOutputFormat.setSchema(job,new HCatSchema(partitionColumns));
    boolean success=job.waitForCompletion(true);
    if (partitionValues != null) {
      assertTrue(job.getCounters().getGroup("FileSystemCounters").findCounter("FILE_BYTES_READ").getValue() > 0);
    }
    if (!HCatUtil.isHadoop23()) {
      if (success) {
        new FileOutputCommitterContainer(job,null).commitJob(job);
      }
 else {
        new FileOutputCommitterContainer(job,null).abortJob(job,JobStatus.State.FAILED);
      }
    }
    if (assertWrite) {
      Assert.assertEquals(writeCount,MapCreate.writeCount);
    }
    if (isTableExternal()) {
      externalTableLocation=outputJobInfo.getTableInfo().getTableLocation();
    }
    return job;
  }
  List<HCatRecord> runMRRead(  int readCount) throws Exception {
    return runMRRead(readCount,null);
  }
  /** 
 * Run a local map reduce job to read records from HCatalog table and verify if the count is as expected
 * @param readCount
 * @param filter
 * @return
 * @throws Exception
 */
  List<HCatRecord> runMRRead(  int readCount,  String filter) throws Exception {
    MapRead.readCount=0;
    readRecords.clear();
    Configuration conf=new Configuration();
    conf.set(HiveConf.ConfVars.METASTORE_INTEGER_JDO_PUSHDOWN.varname,"true");
    Job job=new Job(conf,"hcat mapreduce read test");
    job.setJarByClass(this.getClass());
    job.setMapperClass(HCatMapReduceTest.MapRead.class);
    job.setInputFormatClass(HCatInputFormat.class);
    job.setOutputFormatClass(TextOutputFormat.class);
    HCatInputFormat.setInput(job,dbName,tableName,filter);
    job.setMapOutputKeyClass(BytesWritable.class);
    job.setMapOutputValueClass(Text.class);
    job.setNumReduceTasks(0);
    Path path=new Path(fs.getWorkingDirectory(),"mapred/testHCatMapReduceOutput");
    if (fs.exists(path)) {
      fs.delete(path,true);
    }
    TextOutputFormat.setOutputPath(job,path);
    job.waitForCompletion(true);
    Assert.assertEquals(readCount,MapRead.readCount);
    return readRecords;
  }
  protected HCatSchema getTableSchema() throws Exception {
    Configuration conf=new Configuration();
    Job job=new Job(conf,"hcat mapreduce read schema test");
    job.setJarByClass(this.getClass());
    job.setInputFormatClass(HCatInputFormat.class);
    job.setOutputFormatClass(TextOutputFormat.class);
    HCatInputFormat.setInput(job,dbName,tableName);
    return HCatInputFormat.getTableSchema(job.getConfiguration());
  }
}
