public class TestDruidRecordWriter {
  private ObjectMapper objectMapper=DruidStorageHandlerUtils.JSON_MAPPER;
  private static final Interval INTERVAL_FULL=new Interval("2014-10-22T00:00:00Z/P1D");
  @Rule public TemporaryFolder temporaryFolder=new TemporaryFolder();
  private DruidRecordWriter druidRecordWriter;
  final List<ImmutableMap<String,Object>> expectedRows=ImmutableList.of(ImmutableMap.<String,Object>of(DruidTable.DEFAULT_TIMESTAMP_COLUMN,DateTime.parse("2014-10-22T00:00:00.000Z").getMillis(),"host",ImmutableList.of("a.example.com"),"visited_sum",190L,"unique_hosts",1.0d),ImmutableMap.<String,Object>of(DruidTable.DEFAULT_TIMESTAMP_COLUMN,DateTime.parse("2014-10-22T01:00:00.000Z").getMillis(),"host",ImmutableList.of("b.example.com"),"visited_sum",175L,"unique_hosts",1.0d),ImmutableMap.<String,Object>of(DruidTable.DEFAULT_TIMESTAMP_COLUMN,DateTime.parse("2014-10-22T02:00:00.000Z").getMillis(),"host",ImmutableList.of("c.example.com"),"visited_sum",270L,"unique_hosts",1.0d));
  @Ignore @Test public void testWrite() throws IOException, SegmentLoadingException {
    final String dataSourceName="testDataSource";
    final File segmentOutputDir=temporaryFolder.newFolder();
    final File workingDir=temporaryFolder.newFolder();
    Configuration config=new Configuration();
    final InputRowParser inputRowParser=new MapInputRowParser(new TimeAndDimsParseSpec(new TimestampSpec(DruidTable.DEFAULT_TIMESTAMP_COLUMN,"auto",null),new DimensionsSpec(ImmutableList.<DimensionSchema>of(new StringDimensionSchema("host")),null,null)));
    final Map<String,Object> parserMap=objectMapper.convertValue(inputRowParser,Map.class);
    DataSchema dataSchema=new DataSchema(dataSourceName,parserMap,new AggregatorFactory[]{new LongSumAggregatorFactory("visited_sum","visited_sum"),new HyperUniquesAggregatorFactory("unique_hosts","unique_hosts")},new UniformGranularitySpec(Granularity.DAY,QueryGranularities.NONE,ImmutableList.of(INTERVAL_FULL)),objectMapper);
    RealtimeTuningConfig tuningConfig=RealtimeTuningConfig.makeDefaultTuningConfig(temporaryFolder.newFolder());
    LocalFileSystem localFileSystem=FileSystem.getLocal(config);
    DataSegmentPusher dataSegmentPusher=new LocalDataSegmentPusher(new LocalDataSegmentPusherConfig(){
      @Override public File getStorageDirectory(){
        return segmentOutputDir;
      }
    }
,objectMapper);
    Path segmentDescriptroPath=new Path(workingDir.getAbsolutePath(),DruidStorageHandler.SEGMENTS_DESCRIPTOR_DIR_NAME);
    druidRecordWriter=new DruidRecordWriter(dataSchema,tuningConfig,dataSegmentPusher,20,segmentDescriptroPath,localFileSystem);
    List<DruidWritable> druidWritables=Lists.transform(expectedRows,new Function<ImmutableMap<String,Object>,DruidWritable>(){
      @Nullable @Override public DruidWritable apply(      @Nullable ImmutableMap<String,Object> input){
        return new DruidWritable(ImmutableMap.<String,Object>builder().putAll(input).put(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME,Granularity.DAY.truncate(new DateTime((long)input.get(DruidTable.DEFAULT_TIMESTAMP_COLUMN))).getMillis()).build());
      }
    }
);
    for (    DruidWritable druidWritable : druidWritables) {
      druidRecordWriter.write(druidWritable);
    }
    druidRecordWriter.close(false);
    List<DataSegment> dataSegmentList=DruidStorageHandlerUtils.getPublishedSegments(segmentDescriptroPath,config);
    Assert.assertEquals(1,dataSegmentList.size());
    File tmpUnzippedSegmentDir=temporaryFolder.newFolder();
    new LocalDataSegmentPuller().getSegmentFiles(dataSegmentList.get(0),tmpUnzippedSegmentDir);
    final QueryableIndex queryableIndex=DruidStorageHandlerUtils.INDEX_IO.loadIndex(tmpUnzippedSegmentDir);
    QueryableIndexStorageAdapter adapter=new QueryableIndexStorageAdapter(queryableIndex);
    Firehose firehose=new IngestSegmentFirehose(ImmutableList.of(new WindowedStorageAdapter(adapter,adapter.getInterval())),ImmutableList.of("host"),ImmutableList.of("visited_sum","unique_hosts"),null,QueryGranularities.NONE);
    List<InputRow> rows=Lists.newArrayList();
    while (firehose.hasMore()) {
      rows.add(firehose.nextRow());
    }
    verifyRows(expectedRows,rows);
  }
  private void verifyRows(  List<ImmutableMap<String,Object>> expectedRows,  List<InputRow> actualRows){
    System.out.println("actualRows = " + actualRows);
    Assert.assertEquals(expectedRows.size(),actualRows.size());
    for (int i=0; i < expectedRows.size(); i++) {
      Map<String,Object> expected=expectedRows.get(i);
      InputRow actual=actualRows.get(i);
      Assert.assertEquals(ImmutableList.of("host"),actual.getDimensions());
      Assert.assertEquals(expected.get(DruidTable.DEFAULT_TIMESTAMP_COLUMN),actual.getTimestamp().getMillis());
      Assert.assertEquals(expected.get("host"),actual.getDimension("host"));
      Assert.assertEquals(expected.get("visited_sum"),actual.getLongMetric("visited_sum"));
      Assert.assertEquals((Double)expected.get("unique_hosts"),(Double)HyperUniquesAggregatorFactory.estimateCardinality(actual.getRaw("unique_hosts")),0.001);
    }
  }
  @Test public void testSerDesr() throws IOException {
    String segment="{\"dataSource\":\"datasource2015\",\"interval\":\"2015-06-01T00:00:00.000-04:00/2015-06-02T00:00:00.000-04:00\",\"version\":\"2016-11-04T19:24:01.732-04:00\",\"loadSpec\":{\"type\":\"hdfs\",\"path\":\"hdfs://cn105-10.l42scl.hortonworks.com:8020/apps/hive/warehouse/druid.db/.hive-staging_hive_2016-11-04_19-23-50_168_1550339856804207572-1/_task_tmp.-ext-10002/_tmp.000000_0/datasource2015/20150601T000000.000-0400_20150602T000000.000-0400/2016-11-04T19_24_01.732-04_00/0/index.zip\"},\"dimensions\":\"dimension1\",\"metrics\":\"bigint\",\"shardSpec\":{\"type\":\"linear\",\"partitionNum\":0},\"binaryVersion\":9,\"size\":1765,\"identifier\":\"datasource2015_2015-06-01T00:00:00.000-04:00_2015-06-02T00:00:00.000-04:00_2016-11-04T19:24:01.732-04:00\"}";
    DataSegment dataSegment=objectMapper.reader(DataSegment.class).readValue(segment);
    Assert.assertTrue(dataSegment.getDataSource().equals("datasource2015"));
  }
}
