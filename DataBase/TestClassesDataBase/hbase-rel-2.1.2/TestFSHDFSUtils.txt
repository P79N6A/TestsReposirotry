/** 
 * Test our recoverLease loop against mocked up filesystem.
 */
@Category({MiscTests.class,MediumTests.class}) public class TestFSHDFSUtils {
  @ClassRule public static final HBaseClassTestRule CLASS_RULE=HBaseClassTestRule.forClass(TestFSHDFSUtils.class);
  private static final Logger LOG=LoggerFactory.getLogger(TestFSHDFSUtils.class);
  private static final HBaseTestingUtility HTU=new HBaseTestingUtility();
static {
    Configuration conf=HTU.getConfiguration();
    conf.setInt("hbase.lease.recovery.first.pause",10);
    conf.setInt("hbase.lease.recovery.pause",10);
  }
  private FSHDFSUtils fsHDFSUtils=new FSHDFSUtils();
  private static Path FILE=new Path(HTU.getDataTestDir(),"file.txt");
  long startTime=-1;
  @Before public void setup(){
    this.startTime=EnvironmentEdgeManager.currentTime();
  }
  /** 
 * Test recover lease eventually succeeding.
 */
  @Test public void testRecoverLease() throws IOException {
    HTU.getConfiguration().setInt("hbase.lease.recovery.dfs.timeout",1000);
    CancelableProgressable reporter=Mockito.mock(CancelableProgressable.class);
    Mockito.when(reporter.progress()).thenReturn(true);
    DistributedFileSystem dfs=Mockito.mock(DistributedFileSystem.class);
    Mockito.when(dfs.recoverLease(FILE)).thenReturn(false).thenReturn(false).thenReturn(false).thenReturn(false).thenReturn(true);
    assertTrue(this.fsHDFSUtils.recoverDFSFileLease(dfs,FILE,HTU.getConfiguration(),reporter));
    Mockito.verify(dfs,Mockito.times(5)).recoverLease(FILE);
    assertTrue((EnvironmentEdgeManager.currentTime() - this.startTime) > (3 * HTU.getConfiguration().getInt("hbase.lease.recovery.dfs.timeout",61000)));
  }
  /** 
 * Test that isFileClosed makes us recover lease faster.
 */
  @Test public void testIsFileClosed() throws IOException {
    HTU.getConfiguration().setInt("hbase.lease.recovery.dfs.timeout",100000);
    CancelableProgressable reporter=Mockito.mock(CancelableProgressable.class);
    Mockito.when(reporter.progress()).thenReturn(true);
    IsFileClosedDistributedFileSystem dfs=Mockito.mock(IsFileClosedDistributedFileSystem.class);
    Mockito.when(dfs.recoverLease(FILE)).thenReturn(false).thenReturn(false).thenReturn(true);
    Mockito.when(dfs.isFileClosed(FILE)).thenReturn(true);
    assertTrue(this.fsHDFSUtils.recoverDFSFileLease(dfs,FILE,HTU.getConfiguration(),reporter));
    Mockito.verify(dfs,Mockito.times(2)).recoverLease(FILE);
    Mockito.verify(dfs,Mockito.times(1)).isFileClosed(FILE);
  }
  void testIsSameHdfs(  int nnport) throws IOException {
    try {
      Class dfsUtilClazz=Class.forName("org.apache.hadoop.hdfs.DFSUtil");
      dfsUtilClazz.getMethod("getNNServiceRpcAddresses",Configuration.class);
    }
 catch (    Exception e) {
      LOG.info("Skip testIsSameHdfs test case because of the no-HA hadoop version.");
      return;
    }
    Configuration conf=HBaseConfiguration.create();
    Path srcPath=new Path("hdfs://localhost:" + nnport + "/");
    Path desPath=new Path("hdfs://127.0.0.1/");
    FileSystem srcFs=srcPath.getFileSystem(conf);
    FileSystem desFs=desPath.getFileSystem(conf);
    assertTrue(FSHDFSUtils.isSameHdfs(conf,srcFs,desFs));
    desPath=new Path("hdfs://127.0.0.1:8070/");
    desFs=desPath.getFileSystem(conf);
    assertTrue(!FSHDFSUtils.isSameHdfs(conf,srcFs,desFs));
    desPath=new Path("hdfs://127.0.1.1:" + nnport + "/");
    desFs=desPath.getFileSystem(conf);
    assertTrue(!FSHDFSUtils.isSameHdfs(conf,srcFs,desFs));
    conf.set("fs.defaultFS","hdfs://haosong-hadoop");
    conf.set("dfs.nameservices","haosong-hadoop");
    conf.set("dfs.ha.namenodes.haosong-hadoop","nn1,nn2");
    conf.set("dfs.client.failover.proxy.provider.haosong-hadoop","org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider");
    conf.set("dfs.namenode.rpc-address.haosong-hadoop.nn1","127.0.0.1:" + nnport);
    conf.set("dfs.namenode.rpc-address.haosong-hadoop.nn2","127.10.2.1:8000");
    desPath=new Path("/");
    desFs=desPath.getFileSystem(conf);
    assertTrue(FSHDFSUtils.isSameHdfs(conf,srcFs,desFs));
    conf.set("dfs.namenode.rpc-address.haosong-hadoop.nn1","127.10.2.1:" + nnport);
    conf.set("dfs.namenode.rpc-address.haosong-hadoop.nn2","127.0.0.1:8000");
    desPath=new Path("/");
    desFs=desPath.getFileSystem(conf);
    assertTrue(!FSHDFSUtils.isSameHdfs(conf,srcFs,desFs));
  }
  @Test public void testIsSameHdfs() throws IOException {
    String hadoopVersion=org.apache.hadoop.util.VersionInfo.getVersion();
    LOG.info("hadoop version is: " + hadoopVersion);
    boolean isHadoop3_0_0=hadoopVersion.startsWith("3.0.0");
    if (isHadoop3_0_0) {
      testIsSameHdfs(9820);
    }
 else {
      testIsSameHdfs(8020);
    }
  }
  /** 
 * Version of DFS that has HDFS-4525 in it.
 */
static class IsFileClosedDistributedFileSystem extends DistributedFileSystem {
    /** 
 * Close status of a file. Copied over from HDFS-4525
 * @return true if file is already closed
 */
    @Override public boolean isFileClosed(    Path f) throws IOException {
      return false;
    }
  }
}
