/** 
 * Tests of HRegion methods for replaying flush, compaction, region open, etc events for secondary region replicas
 */
@Category(LargeTests.class) public class TestHRegionReplayEvents {
  @ClassRule public static final HBaseClassTestRule CLASS_RULE=HBaseClassTestRule.forClass(TestHRegionReplayEvents.class);
  private static final Logger LOG=LoggerFactory.getLogger(TestHRegion.class);
  @Rule public TestName name=new TestName();
  private static HBaseTestingUtility TEST_UTIL;
  public static Configuration CONF;
  private String dir;
  private byte[][] families=new byte[][]{Bytes.toBytes("cf1"),Bytes.toBytes("cf2"),Bytes.toBytes("cf3")};
  protected byte[] tableName;
  protected String method;
  protected final byte[] row=Bytes.toBytes("rowA");
  protected final byte[] row2=Bytes.toBytes("rowB");
  protected byte[] cq=Bytes.toBytes("cq");
  private Path rootDir;
  private TableDescriptor htd;
  private RegionServerServices rss;
  private RegionInfo primaryHri, secondaryHri;
  private HRegion primaryRegion, secondaryRegion;
  private WAL walPrimary, walSecondary;
  private WAL.Reader reader;
  @BeforeClass public static void setUpBeforeClass() throws Exception {
    TEST_UTIL=new HBaseTestingUtility();
    TEST_UTIL.startMiniDFSCluster(1);
  }
  @AfterClass public static void tearDownAfterClass() throws Exception {
    LOG.info("Cleaning test directory: " + TEST_UTIL.getDataTestDir());
    TEST_UTIL.cleanupTestDir();
    TEST_UTIL.shutdownMiniDFSCluster();
  }
  @Before public void setUp() throws Exception {
    CONF=TEST_UTIL.getConfiguration();
    dir=TEST_UTIL.getDataTestDir("TestHRegionReplayEvents").toString();
    method=name.getMethodName();
    tableName=Bytes.toBytes(name.getMethodName());
    rootDir=new Path(dir + method);
    TEST_UTIL.getConfiguration().set(HConstants.HBASE_DIR,rootDir.toString());
    method=name.getMethodName();
    TableDescriptorBuilder builder=TableDescriptorBuilder.newBuilder(TableName.valueOf(method));
    for (    byte[] family : families) {
      builder.setColumnFamily(ColumnFamilyDescriptorBuilder.of(family));
    }
    htd=builder.build();
    long time=System.currentTimeMillis();
    ChunkCreator.initialize(MemStoreLABImpl.CHUNK_SIZE_DEFAULT,false,0,0,0,null);
    primaryHri=RegionInfoBuilder.newBuilder(htd.getTableName()).setRegionId(time).setReplicaId(0).build();
    secondaryHri=RegionInfoBuilder.newBuilder(htd.getTableName()).setRegionId(time).setReplicaId(1).build();
    WALFactory wals=TestHRegion.createWALFactory(CONF,rootDir);
    walPrimary=wals.getWAL(primaryHri);
    walSecondary=wals.getWAL(secondaryHri);
    rss=mock(RegionServerServices.class);
    when(rss.getServerName()).thenReturn(ServerName.valueOf("foo",1,1));
    when(rss.getConfiguration()).thenReturn(CONF);
    when(rss.getRegionServerAccounting()).thenReturn(new RegionServerAccounting(CONF));
    String string=org.apache.hadoop.hbase.executor.EventType.RS_COMPACTED_FILES_DISCHARGER.toString();
    ExecutorService es=new ExecutorService(string);
    es.startExecutorService(string + "-" + string,1);
    when(rss.getExecutorService()).thenReturn(es);
    primaryRegion=HRegion.createHRegion(primaryHri,rootDir,CONF,htd,walPrimary);
    primaryRegion.close();
    List<HRegion> regions=new ArrayList<>();
    regions.add(primaryRegion);
    Mockito.doReturn(regions).when(rss).getRegions();
    primaryRegion=HRegion.openHRegion(rootDir,primaryHri,htd,walPrimary,CONF,rss,null);
    secondaryRegion=HRegion.openHRegion(secondaryHri,htd,null,CONF,rss,null);
    reader=null;
  }
  @After public void tearDown() throws Exception {
    if (reader != null) {
      reader.close();
    }
    if (primaryRegion != null) {
      HBaseTestingUtility.closeRegionAndWAL(primaryRegion);
    }
    if (secondaryRegion != null) {
      HBaseTestingUtility.closeRegionAndWAL(secondaryRegion);
    }
    EnvironmentEdgeManagerTestHelper.reset();
  }
  String getName(){
    return name.getMethodName();
  }
  @Test public void testRegionReplicaSecondaryCannotFlush() throws IOException {
    putDataByReplay(secondaryRegion,0,1000,cq,families);
    verifyData(secondaryRegion,0,1000,cq,families);
    FlushResultImpl flush=(FlushResultImpl)secondaryRegion.flush(true);
    assertEquals(FlushResultImpl.Result.CANNOT_FLUSH,flush.result);
    verifyData(secondaryRegion,0,1000,cq,families);
    Map<byte[],List<HStoreFile>> files=secondaryRegion.close(false);
    for (    List<HStoreFile> f : files.values()) {
      assertTrue(f.isEmpty());
    }
  }
  /** 
 * Tests a case where we replay only a flush start marker, then the region is closed. This region should not block indefinitely
 */
  @Test public void testOnlyReplayingFlushStartDoesNotHoldUpRegionClose() throws IOException {
    int start=0;
    LOG.info("-- Writing some data to primary from " + start + " to "+ (start + 100));
    putData(primaryRegion,Durability.SYNC_WAL,start,100,cq,families);
    LOG.info("-- Flushing primary, creating 3 files for 3 stores");
    primaryRegion.flush(true);
    reader=createWALReaderForPrimary();
    LOG.info("-- Replaying edits and flush events in secondary");
    while (true) {
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      FlushDescriptor flushDesc=WALEdit.getFlushDescriptor(entry.getEdit().getCells().get(0));
      if (flushDesc != null) {
        if (flushDesc.getAction() == FlushAction.START_FLUSH) {
          LOG.info("-- Replaying flush start in secondary");
          secondaryRegion.replayWALFlushStartMarker(flushDesc);
        }
 else         if (flushDesc.getAction() == FlushAction.COMMIT_FLUSH) {
          LOG.info("-- NOT Replaying flush commit in secondary");
        }
      }
 else {
        replayEdit(secondaryRegion,entry);
      }
    }
    assertTrue(rss.getRegionServerAccounting().getGlobalMemStoreDataSize() > 0);
    secondaryRegion.close();
    assertEquals(0,rss.getRegionServerAccounting().getGlobalMemStoreDataSize());
  }
  static int replayEdit(  HRegion region,  WAL.Entry entry) throws IOException {
    if (WALEdit.isMetaEditFamily(entry.getEdit().getCells().get(0))) {
      return 0;
    }
    Put put=new Put(CellUtil.cloneRow(entry.getEdit().getCells().get(0)));
    for (    Cell cell : entry.getEdit().getCells())     put.add(cell);
    put.setDurability(Durability.SKIP_WAL);
    MutationReplay mutation=new MutationReplay(MutationType.PUT,put,0,0);
    region.batchReplay(new MutationReplay[]{mutation},entry.getKey().getSequenceId());
    return Integer.parseInt(Bytes.toString(put.getRow()));
  }
  WAL.Reader createWALReaderForPrimary() throws FileNotFoundException, IOException {
    return WALFactory.createReader(TEST_UTIL.getTestFileSystem(),AbstractFSWALProvider.getCurrentFileName(walPrimary),TEST_UTIL.getConfiguration());
  }
  @Test public void testBatchReplayWithMultipleNonces() throws IOException {
    try {
      MutationReplay[] mutations=new MutationReplay[100];
      for (int i=0; i < 100; i++) {
        Put put=new Put(Bytes.toBytes(i));
        put.setDurability(Durability.SYNC_WAL);
        for (        byte[] familly : this.families) {
          put.addColumn(familly,this.cq,null);
          long nonceNum=i / 10;
          mutations[i]=new MutationReplay(MutationType.PUT,put,nonceNum,nonceNum);
        }
      }
      primaryRegion.batchReplay(mutations,20);
    }
 catch (    Exception e) {
      String msg="Error while replay of batch with multiple nonces. ";
      LOG.error(msg,e);
      fail(msg + e.getMessage());
    }
  }
  @Test public void testReplayFlushesAndCompactions() throws IOException {
    putDataWithFlushes(primaryRegion,100,300,100);
    LOG.info("-- Compacting primary, only 1 store");
    primaryRegion.compactStore(Bytes.toBytes("cf1"),NoLimitThroughputController.INSTANCE);
    reader=createWALReaderForPrimary();
    LOG.info("-- Replaying edits and flush events in secondary");
    int lastReplayed=0;
    int expectedStoreFileCount=0;
    while (true) {
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      FlushDescriptor flushDesc=WALEdit.getFlushDescriptor(entry.getEdit().getCells().get(0));
      CompactionDescriptor compactionDesc=WALEdit.getCompaction(entry.getEdit().getCells().get(0));
      if (flushDesc != null) {
        verifyData(secondaryRegion,0,lastReplayed,cq,families);
        HStore store=secondaryRegion.getStore(Bytes.toBytes("cf1"));
        long storeMemstoreSize=store.getMemStoreSize().getHeapSize();
        long regionMemstoreSize=secondaryRegion.getMemStoreDataSize();
        MemStoreSize mss=store.getFlushableSize();
        long storeSize=store.getSize();
        long storeSizeUncompressed=store.getStoreSizeUncompressed();
        if (flushDesc.getAction() == FlushAction.START_FLUSH) {
          LOG.info("-- Replaying flush start in secondary");
          PrepareFlushResult result=secondaryRegion.replayWALFlushStartMarker(flushDesc);
          assertNull(result.result);
          assertEquals(result.flushOpSeqId,flushDesc.getFlushSequenceNumber());
          long newStoreMemstoreSize=store.getMemStoreSize().getHeapSize();
          LOG.info("Memstore size reduced by:" + StringUtils.humanReadableInt(newStoreMemstoreSize - storeMemstoreSize));
          assertTrue(storeMemstoreSize > newStoreMemstoreSize);
        }
 else         if (flushDesc.getAction() == FlushAction.COMMIT_FLUSH) {
          LOG.info("-- Replaying flush commit in secondary");
          secondaryRegion.replayWALFlushCommitMarker(flushDesc);
          expectedStoreFileCount++;
          for (          HStore s : secondaryRegion.getStores()) {
            assertEquals(expectedStoreFileCount,s.getStorefilesCount());
          }
          MemStoreSize newMss=store.getFlushableSize();
          assertTrue(mss.getHeapSize() > newMss.getHeapSize());
          long newRegionMemstoreSize=secondaryRegion.getMemStoreDataSize();
          assertTrue(regionMemstoreSize > newRegionMemstoreSize);
          assertTrue(store.getSize() > storeSize);
          assertTrue(store.getStoreSizeUncompressed() > storeSizeUncompressed);
          assertEquals(store.getSize(),store.getStorefilesSize());
        }
        verifyData(secondaryRegion,0,lastReplayed + 1,cq,families);
      }
 else       if (compactionDesc != null) {
        secondaryRegion.replayWALCompactionMarker(compactionDesc,true,false,Long.MAX_VALUE);
        for (        HStore store : secondaryRegion.getStores()) {
          if (store.getColumnFamilyName().equals("cf1")) {
            assertEquals(1,store.getStorefilesCount());
          }
 else {
            assertEquals(expectedStoreFileCount,store.getStorefilesCount());
          }
        }
      }
 else {
        lastReplayed=replayEdit(secondaryRegion,entry);
        ;
      }
    }
    assertEquals(400 - 1,lastReplayed);
    LOG.info("-- Verifying edits from secondary");
    verifyData(secondaryRegion,0,400,cq,families);
    LOG.info("-- Verifying edits from primary. Ensuring that files are not deleted");
    verifyData(primaryRegion,0,lastReplayed,cq,families);
    for (    HStore store : primaryRegion.getStores()) {
      if (store.getColumnFamilyName().equals("cf1")) {
        assertEquals(1,store.getStorefilesCount());
      }
 else {
        assertEquals(expectedStoreFileCount,store.getStorefilesCount());
      }
    }
  }
  /** 
 * Tests cases where we prepare a flush with some seqId and we receive other flush start markers equal to, greater or less than the previous flush start marker.
 */
  @Test public void testReplayFlushStartMarkers() throws IOException {
    putDataWithFlushes(primaryRegion,100,100,100);
    int numRows=200;
    reader=createWALReaderForPrimary();
    LOG.info("-- Replaying edits and flush events in secondary");
    FlushDescriptor startFlushDesc=null;
    int lastReplayed=0;
    while (true) {
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      FlushDescriptor flushDesc=WALEdit.getFlushDescriptor(entry.getEdit().getCells().get(0));
      if (flushDesc != null) {
        HStore store=secondaryRegion.getStore(Bytes.toBytes("cf1"));
        long storeMemstoreSize=store.getMemStoreSize().getHeapSize();
        long regionMemstoreSize=secondaryRegion.getMemStoreDataSize();
        MemStoreSize mss=store.getFlushableSize();
        if (flushDesc.getAction() == FlushAction.START_FLUSH) {
          startFlushDesc=flushDesc;
          LOG.info("-- Replaying flush start in secondary");
          PrepareFlushResult result=secondaryRegion.replayWALFlushStartMarker(startFlushDesc);
          assertNull(result.result);
          assertEquals(result.flushOpSeqId,startFlushDesc.getFlushSequenceNumber());
          assertTrue(regionMemstoreSize > 0);
          assertTrue(mss.getHeapSize() > 0);
          long newStoreMemstoreSize=store.getMemStoreSize().getHeapSize();
          LOG.info("Memstore size reduced by:" + StringUtils.humanReadableInt(newStoreMemstoreSize - storeMemstoreSize));
          assertTrue(storeMemstoreSize > newStoreMemstoreSize);
          verifyData(secondaryRegion,0,lastReplayed + 1,cq,families);
        }
        verifyData(secondaryRegion,0,lastReplayed + 1,cq,families);
      }
 else {
        lastReplayed=replayEdit(secondaryRegion,entry);
      }
    }
    verifyData(secondaryRegion,0,numRows,cq,families);
    LOG.info("-- Replaying same flush start in secondary again");
    PrepareFlushResult result=secondaryRegion.replayWALFlushStartMarker(startFlushDesc);
    assertNull(result);
    assertNotNull(secondaryRegion.getPrepareFlushResult());
    assertEquals(secondaryRegion.getPrepareFlushResult().flushOpSeqId,startFlushDesc.getFlushSequenceNumber());
    assertTrue(secondaryRegion.getMemStoreDataSize() > 0);
    verifyData(secondaryRegion,0,numRows,cq,families);
    FlushDescriptor startFlushDescSmallerSeqId=clone(startFlushDesc,startFlushDesc.getFlushSequenceNumber() - 50);
    LOG.info("-- Replaying same flush start in secondary again " + startFlushDescSmallerSeqId);
    result=secondaryRegion.replayWALFlushStartMarker(startFlushDescSmallerSeqId);
    assertNull(result);
    assertNotNull(secondaryRegion.getPrepareFlushResult());
    assertEquals(secondaryRegion.getPrepareFlushResult().flushOpSeqId,startFlushDesc.getFlushSequenceNumber());
    assertTrue(secondaryRegion.getMemStoreDataSize() > 0);
    verifyData(secondaryRegion,0,numRows,cq,families);
    FlushDescriptor startFlushDescLargerSeqId=clone(startFlushDesc,startFlushDesc.getFlushSequenceNumber() + 50);
    LOG.info("-- Replaying same flush start in secondary again " + startFlushDescLargerSeqId);
    result=secondaryRegion.replayWALFlushStartMarker(startFlushDescLargerSeqId);
    assertNull(result);
    assertNotNull(secondaryRegion.getPrepareFlushResult());
    assertEquals(secondaryRegion.getPrepareFlushResult().flushOpSeqId,startFlushDesc.getFlushSequenceNumber());
    assertTrue(secondaryRegion.getMemStoreDataSize() > 0);
    verifyData(secondaryRegion,0,numRows,cq,families);
    LOG.info("-- Verifying edits from secondary");
    verifyData(secondaryRegion,0,numRows,cq,families);
    LOG.info("-- Verifying edits from primary.");
    verifyData(primaryRegion,0,numRows,cq,families);
  }
  /** 
 * Tests the case where we prepare a flush with some seqId and we receive a flush commit marker less than the previous flush start marker.
 */
  @Test public void testReplayFlushCommitMarkerSmallerThanFlushStartMarker() throws IOException {
    putDataWithFlushes(primaryRegion,100,200,100);
    int numRows=300;
    reader=createWALReaderForPrimary();
    LOG.info("-- Replaying edits and flush events in secondary");
    FlushDescriptor startFlushDesc=null;
    FlushDescriptor commitFlushDesc=null;
    int lastReplayed=0;
    while (true) {
      System.out.println(lastReplayed);
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      FlushDescriptor flushDesc=WALEdit.getFlushDescriptor(entry.getEdit().getCells().get(0));
      if (flushDesc != null) {
        if (flushDesc.getAction() == FlushAction.START_FLUSH) {
          if (startFlushDesc == null) {
            startFlushDesc=flushDesc;
          }
 else {
            LOG.info("-- Replaying flush start in secondary");
            startFlushDesc=flushDesc;
            PrepareFlushResult result=secondaryRegion.replayWALFlushStartMarker(startFlushDesc);
            assertNull(result.result);
          }
        }
 else         if (flushDesc.getAction() == FlushAction.COMMIT_FLUSH) {
          if (commitFlushDesc == null) {
            commitFlushDesc=flushDesc;
          }
        }
        verifyData(secondaryRegion,0,lastReplayed + 1,cq,families);
      }
 else {
        lastReplayed=replayEdit(secondaryRegion,entry);
      }
    }
    verifyData(secondaryRegion,0,numRows,cq,families);
    int expectedStoreFileCount=0;
    for (    HStore s : secondaryRegion.getStores()) {
      assertEquals(expectedStoreFileCount,s.getStorefilesCount());
    }
    long regionMemstoreSize=secondaryRegion.getMemStoreDataSize();
    LOG.info("Testing replaying flush COMMIT " + commitFlushDesc + " on top of flush START"+ startFlushDesc);
    assertTrue(commitFlushDesc.getFlushSequenceNumber() < startFlushDesc.getFlushSequenceNumber());
    LOG.info("-- Replaying flush commit in secondary" + commitFlushDesc);
    secondaryRegion.replayWALFlushCommitMarker(commitFlushDesc);
    expectedStoreFileCount++;
    for (    HStore s : secondaryRegion.getStores()) {
      assertEquals(expectedStoreFileCount,s.getStorefilesCount());
    }
    HStore store=secondaryRegion.getStore(Bytes.toBytes("cf1"));
    MemStoreSize mss=store.getFlushableSize();
    assertTrue(mss.getHeapSize() > 0);
    long newRegionMemstoreSize=secondaryRegion.getMemStoreDataSize();
    assertEquals(regionMemstoreSize,newRegionMemstoreSize);
    assertNotNull(secondaryRegion.getPrepareFlushResult());
    LOG.info("-- Verifying edits from secondary");
    verifyData(secondaryRegion,0,numRows,cq,families);
    LOG.info("-- Verifying edits from primary.");
    verifyData(primaryRegion,0,numRows,cq,families);
  }
  /** 
 * Tests the case where we prepare a flush with some seqId and we receive a flush commit marker larger than the previous flush start marker.
 */
  @Test public void testReplayFlushCommitMarkerLargerThanFlushStartMarker() throws IOException {
    putDataWithFlushes(primaryRegion,100,100,100);
    int numRows=200;
    reader=createWALReaderForPrimary();
    LOG.info("-- Replaying edits and flush events in secondary");
    FlushDescriptor startFlushDesc=null;
    FlushDescriptor commitFlushDesc=null;
    int lastReplayed=0;
    while (true) {
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      FlushDescriptor flushDesc=WALEdit.getFlushDescriptor(entry.getEdit().getCells().get(0));
      if (flushDesc != null) {
        if (flushDesc.getAction() == FlushAction.START_FLUSH) {
          if (startFlushDesc == null) {
            LOG.info("-- Replaying flush start in secondary");
            startFlushDesc=flushDesc;
            PrepareFlushResult result=secondaryRegion.replayWALFlushStartMarker(startFlushDesc);
            assertNull(result.result);
          }
        }
 else         if (flushDesc.getAction() == FlushAction.COMMIT_FLUSH) {
          commitFlushDesc=FlushDescriptor.newBuilder(flushDesc).setFlushSequenceNumber(flushDesc.getFlushSequenceNumber() + 50).build();
        }
        verifyData(secondaryRegion,0,lastReplayed + 1,cq,families);
      }
 else {
        lastReplayed=replayEdit(secondaryRegion,entry);
      }
    }
    verifyData(secondaryRegion,0,numRows,cq,families);
    int expectedStoreFileCount=0;
    for (    HStore s : secondaryRegion.getStores()) {
      assertEquals(expectedStoreFileCount,s.getStorefilesCount());
    }
    long regionMemstoreSize=secondaryRegion.getMemStoreDataSize();
    LOG.info("Testing replaying flush COMMIT " + commitFlushDesc + " on top of flush START"+ startFlushDesc);
    assertTrue(commitFlushDesc.getFlushSequenceNumber() > startFlushDesc.getFlushSequenceNumber());
    LOG.info("-- Replaying flush commit in secondary" + commitFlushDesc);
    secondaryRegion.replayWALFlushCommitMarker(commitFlushDesc);
    expectedStoreFileCount++;
    for (    HStore s : secondaryRegion.getStores()) {
      assertEquals(expectedStoreFileCount,s.getStorefilesCount());
    }
    HStore store=secondaryRegion.getStore(Bytes.toBytes("cf1"));
    MemStoreSize mss=store.getFlushableSize();
    assertTrue(mss.getHeapSize() > 0);
    long newRegionMemstoreSize=secondaryRegion.getMemStoreDataSize();
    assertTrue(newRegionMemstoreSize > 0);
    assertTrue(regionMemstoreSize > newRegionMemstoreSize);
    assertNull(secondaryRegion.getPrepareFlushResult());
    LOG.info("-- Verifying edits from secondary");
    verifyData(secondaryRegion,0,numRows,cq,families);
    LOG.info("-- Verifying edits from primary.");
    verifyData(primaryRegion,0,numRows,cq,families);
  }
  /** 
 * Tests the case where we receive a flush commit before receiving any flush prepare markers. The memstore edits should be dropped after the flush commit replay since they should be in flushed files
 */
  @Test public void testReplayFlushCommitMarkerWithoutFlushStartMarkerDroppableMemstore() throws IOException {
    testReplayFlushCommitMarkerWithoutFlushStartMarker(true);
  }
  /** 
 * Tests the case where we receive a flush commit before receiving any flush prepare markers. The memstore edits should be not dropped after the flush commit replay since not every edit will be in flushed files (based on seqId)
 */
  @Test public void testReplayFlushCommitMarkerWithoutFlushStartMarkerNonDroppableMemstore() throws IOException {
    testReplayFlushCommitMarkerWithoutFlushStartMarker(false);
  }
  /** 
 * Tests the case where we receive a flush commit before receiving any flush prepare markers
 */
  public void testReplayFlushCommitMarkerWithoutFlushStartMarker(  boolean droppableMemstore) throws IOException {
    putDataWithFlushes(primaryRegion,100,100,droppableMemstore ? 0 : 100);
    int numRows=droppableMemstore ? 100 : 200;
    reader=createWALReaderForPrimary();
    LOG.info("-- Replaying edits and flush events in secondary");
    FlushDescriptor commitFlushDesc=null;
    int lastReplayed=0;
    while (true) {
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      FlushDescriptor flushDesc=WALEdit.getFlushDescriptor(entry.getEdit().getCells().get(0));
      if (flushDesc != null) {
        if (flushDesc.getAction() == FlushAction.START_FLUSH) {
        }
 else         if (flushDesc.getAction() == FlushAction.COMMIT_FLUSH) {
          commitFlushDesc=flushDesc;
        }
        verifyData(secondaryRegion,0,lastReplayed + 1,cq,families);
      }
 else {
        lastReplayed=replayEdit(secondaryRegion,entry);
      }
    }
    verifyData(secondaryRegion,0,numRows,cq,families);
    int expectedStoreFileCount=0;
    for (    HStore s : secondaryRegion.getStores()) {
      assertEquals(expectedStoreFileCount,s.getStorefilesCount());
    }
    long regionMemstoreSize=secondaryRegion.getMemStoreDataSize();
    assertNull(secondaryRegion.getPrepareFlushResult());
    assertTrue(commitFlushDesc.getFlushSequenceNumber() > 0);
    for (    HStore store : secondaryRegion.getStores()) {
      assertTrue(store.getMaxSequenceId().orElse(0L) <= secondaryRegion.getReadPoint(null));
    }
    LOG.info("-- Replaying flush commit in secondary" + commitFlushDesc);
    secondaryRegion.replayWALFlushCommitMarker(commitFlushDesc);
    expectedStoreFileCount++;
    for (    HStore s : secondaryRegion.getStores()) {
      assertEquals(expectedStoreFileCount,s.getStorefilesCount());
    }
    HStore store=secondaryRegion.getStore(Bytes.toBytes("cf1"));
    MemStoreSize mss=store.getFlushableSize();
    if (droppableMemstore) {
      assertTrue(mss.getHeapSize() == MutableSegment.DEEP_OVERHEAD);
    }
 else {
      assertTrue(mss.getHeapSize() > 0);
    }
    long newRegionMemstoreSize=secondaryRegion.getMemStoreDataSize();
    if (droppableMemstore) {
      assertTrue(0 == newRegionMemstoreSize);
    }
 else {
      assertTrue(regionMemstoreSize == newRegionMemstoreSize);
    }
    LOG.info("-- Verifying edits from secondary");
    verifyData(secondaryRegion,0,numRows,cq,families);
    LOG.info("-- Verifying edits from primary.");
    verifyData(primaryRegion,0,numRows,cq,families);
  }
  private FlushDescriptor clone(  FlushDescriptor flush,  long flushSeqId){
    return FlushDescriptor.newBuilder(flush).setFlushSequenceNumber(flushSeqId).build();
  }
  /** 
 * Tests replaying region open markers from primary region. Checks whether the files are picked up
 */
  @Test public void testReplayRegionOpenEvent() throws IOException {
    putDataWithFlushes(primaryRegion,100,0,100);
    int numRows=100;
    primaryRegion.close();
    primaryRegion=HRegion.openHRegion(rootDir,primaryHri,htd,walPrimary,CONF,rss,null);
    reader=createWALReaderForPrimary();
    List<RegionEventDescriptor> regionEvents=Lists.newArrayList();
    LOG.info("-- Replaying edits and region events in secondary");
    while (true) {
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      FlushDescriptor flushDesc=WALEdit.getFlushDescriptor(entry.getEdit().getCells().get(0));
      RegionEventDescriptor regionEventDesc=WALEdit.getRegionEventDescriptor(entry.getEdit().getCells().get(0));
      if (flushDesc != null) {
      }
 else       if (regionEventDesc != null) {
        regionEvents.add(regionEventDesc);
      }
 else {
      }
    }
    assertEquals(3,regionEvents.size());
    secondaryRegion.replayWALRegionEventMarker(regionEvents.get(0));
    secondaryRegion.replayWALRegionEventMarker(regionEvents.get(1));
    int expectedStoreFileCount=0;
    for (    HStore s : secondaryRegion.getStores()) {
      assertEquals(expectedStoreFileCount,s.getStorefilesCount());
    }
    long regionMemstoreSize=secondaryRegion.getMemStoreDataSize();
    assertTrue(regionMemstoreSize == 0);
    LOG.info("Testing replaying region open event " + regionEvents.get(2));
    secondaryRegion.replayWALRegionEventMarker(regionEvents.get(2));
    expectedStoreFileCount++;
    for (    HStore s : secondaryRegion.getStores()) {
      assertEquals(expectedStoreFileCount,s.getStorefilesCount());
    }
    Store store=secondaryRegion.getStore(Bytes.toBytes("cf1"));
    MemStoreSize mss=store.getFlushableSize();
    assertTrue(mss.getHeapSize() == MutableSegment.DEEP_OVERHEAD);
    long newRegionMemstoreSize=secondaryRegion.getMemStoreDataSize();
    assertTrue(newRegionMemstoreSize == 0);
    assertNull(secondaryRegion.getPrepareFlushResult());
    LOG.info("-- Verifying edits from secondary");
    verifyData(secondaryRegion,0,numRows,cq,families);
    LOG.info("-- Verifying edits from primary.");
    verifyData(primaryRegion,0,numRows,cq,families);
  }
  /** 
 * Tests the case where we replay a region open event after a flush start but before receiving flush commit
 */
  @Test public void testReplayRegionOpenEventAfterFlushStart() throws IOException {
    putDataWithFlushes(primaryRegion,100,100,100);
    int numRows=200;
    primaryRegion.close();
    primaryRegion=HRegion.openHRegion(rootDir,primaryHri,htd,walPrimary,CONF,rss,null);
    reader=createWALReaderForPrimary();
    List<RegionEventDescriptor> regionEvents=Lists.newArrayList();
    LOG.info("-- Replaying edits and region events in secondary");
    while (true) {
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      FlushDescriptor flushDesc=WALEdit.getFlushDescriptor(entry.getEdit().getCells().get(0));
      RegionEventDescriptor regionEventDesc=WALEdit.getRegionEventDescriptor(entry.getEdit().getCells().get(0));
      if (flushDesc != null) {
        if (flushDesc.getAction() == FlushAction.START_FLUSH) {
          secondaryRegion.replayWALFlushStartMarker(flushDesc);
        }
      }
 else       if (regionEventDesc != null) {
        regionEvents.add(regionEventDesc);
      }
 else {
        replayEdit(secondaryRegion,entry);
      }
    }
    verifyData(secondaryRegion,0,numRows,cq,families);
    assertEquals(3,regionEvents.size());
    int expectedStoreFileCount=0;
    for (    HStore s : secondaryRegion.getStores()) {
      assertEquals(expectedStoreFileCount,s.getStorefilesCount());
    }
    LOG.info("Testing replaying region open event " + regionEvents.get(2));
    secondaryRegion.replayWALRegionEventMarker(regionEvents.get(2));
    expectedStoreFileCount=2;
    for (    HStore s : secondaryRegion.getStores()) {
      assertEquals(expectedStoreFileCount,s.getStorefilesCount());
    }
    HStore store=secondaryRegion.getStore(Bytes.toBytes("cf1"));
    MemStoreSize newSnapshotSize=store.getSnapshotSize();
    assertTrue(newSnapshotSize.getDataSize() == 0);
    long newRegionMemstoreSize=secondaryRegion.getMemStoreDataSize();
    assertTrue(newRegionMemstoreSize == 0);
    assertNull(secondaryRegion.getPrepareFlushResult());
    LOG.info("-- Verifying edits from secondary");
    verifyData(secondaryRegion,0,numRows,cq,families);
    LOG.info("-- Verifying edits from primary.");
    verifyData(primaryRegion,0,numRows,cq,families);
  }
  /** 
 * Tests whether edits coming in for replay are skipped which have smaller seq id than the seqId of the last replayed region open event.
 */
  @Test public void testSkippingEditsWithSmallerSeqIdAfterRegionOpenEvent() throws IOException {
    putDataWithFlushes(primaryRegion,100,100,0);
    int numRows=100;
    primaryRegion.close();
    primaryRegion=HRegion.openHRegion(rootDir,primaryHri,htd,walPrimary,CONF,rss,null);
    reader=createWALReaderForPrimary();
    List<RegionEventDescriptor> regionEvents=Lists.newArrayList();
    List<WAL.Entry> edits=Lists.newArrayList();
    LOG.info("-- Replaying edits and region events in secondary");
    while (true) {
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      FlushDescriptor flushDesc=WALEdit.getFlushDescriptor(entry.getEdit().getCells().get(0));
      RegionEventDescriptor regionEventDesc=WALEdit.getRegionEventDescriptor(entry.getEdit().getCells().get(0));
      if (flushDesc != null) {
      }
 else       if (regionEventDesc != null) {
        regionEvents.add(regionEventDesc);
      }
 else {
        edits.add(entry);
      }
    }
    secondaryRegion.replayWALRegionEventMarker(RegionEventDescriptor.newBuilder(regionEvents.get(0)).setLogSequenceNumber(regionEvents.get(2).getLogSequenceNumber()).build());
    for (    WAL.Entry entry : edits) {
      replayEdit(secondaryRegion,entry);
    }
    boolean expectedFail=false;
    try {
      verifyData(secondaryRegion,0,numRows,cq,families);
    }
 catch (    AssertionError e) {
      expectedFail=true;
    }
    if (!expectedFail) {
      fail("Should have failed this verification");
    }
  }
  @Test public void testReplayFlushSeqIds() throws IOException {
    int start=0;
    LOG.info("-- Writing some data to primary from " + start + " to "+ (start + 100));
    putData(primaryRegion,Durability.SYNC_WAL,start,100,cq,families);
    LOG.info("-- Flushing primary, creating 3 files for 3 stores");
    primaryRegion.flush(true);
    reader=createWALReaderForPrimary();
    long flushSeqId=-1;
    LOG.info("-- Replaying flush events in secondary");
    while (true) {
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      FlushDescriptor flushDesc=WALEdit.getFlushDescriptor(entry.getEdit().getCells().get(0));
      if (flushDesc != null) {
        if (flushDesc.getAction() == FlushAction.START_FLUSH) {
          LOG.info("-- Replaying flush start in secondary");
          secondaryRegion.replayWALFlushStartMarker(flushDesc);
          flushSeqId=flushDesc.getFlushSequenceNumber();
        }
 else         if (flushDesc.getAction() == FlushAction.COMMIT_FLUSH) {
          LOG.info("-- Replaying flush commit in secondary");
          secondaryRegion.replayWALFlushCommitMarker(flushDesc);
          assertEquals(flushSeqId,flushDesc.getFlushSequenceNumber());
        }
      }
    }
    long readPoint=secondaryRegion.getMVCC().getReadPoint();
    assertEquals(flushSeqId,readPoint);
    verifyData(secondaryRegion,0,100,cq,families);
  }
  @Test public void testSeqIdsFromReplay() throws IOException {
    String method=name.getMethodName();
    byte[] tableName=Bytes.toBytes(method);
    byte[] family=Bytes.toBytes("family");
    HRegion region=initHRegion(tableName,method,family);
    try {
      long readPoint=region.getMVCC().getReadPoint();
      long origSeqId=readPoint + 100;
      Put put=new Put(row).addColumn(family,row,row);
      put.setDurability(Durability.SKIP_WAL);
      replay(region,put,origSeqId);
      assertGet(region,family,row);
      assertEquals(origSeqId,region.getReadPoint(null));
      put=new Put(row2).addColumn(family,row2,row2);
      put.setDurability(Durability.SKIP_WAL);
      replay(region,put,origSeqId - 50);
      assertGet(region,family,row2);
    }
  finally {
      region.close();
    }
  }
  /** 
 * Tests that a region opened in secondary mode would not write region open / close events to its WAL.
 * @throws IOException
 */
  @Test public void testSecondaryRegionDoesNotWriteRegionEventsToWAL() throws IOException {
    secondaryRegion.close();
    walSecondary=spy(walSecondary);
    secondaryRegion=HRegion.openHRegion(secondaryHri,htd,walSecondary,CONF,rss,null);
    verify(walSecondary,times(0)).append(any(RegionInfo.class),any(WALKeyImpl.class),any(WALEdit.class),anyBoolean());
    putDataByReplay(secondaryRegion,0,10,cq,families);
    secondaryRegion.replayWALFlushStartMarker(FlushDescriptor.newBuilder().setFlushSequenceNumber(10).setTableName(UnsafeByteOperations.unsafeWrap(primaryRegion.getTableDescriptor().getTableName().getName())).setAction(FlushAction.START_FLUSH).setEncodedRegionName(UnsafeByteOperations.unsafeWrap(primaryRegion.getRegionInfo().getEncodedNameAsBytes())).setRegionName(UnsafeByteOperations.unsafeWrap(primaryRegion.getRegionInfo().getRegionName())).build());
    verify(walSecondary,times(0)).append(any(RegionInfo.class),any(WALKeyImpl.class),any(WALEdit.class),anyBoolean());
    secondaryRegion.close();
    verify(walSecondary,times(0)).append(any(RegionInfo.class),any(WALKeyImpl.class),any(WALEdit.class),anyBoolean());
  }
  /** 
 * Tests the reads enabled flag for the region. When unset all reads should be rejected
 */
  @Test public void testRegionReadsEnabledFlag() throws IOException {
    putDataByReplay(secondaryRegion,0,100,cq,families);
    verifyData(secondaryRegion,0,100,cq,families);
    secondaryRegion.setReadsEnabled(false);
    try {
      verifyData(secondaryRegion,0,100,cq,families);
      fail("Should have failed with IOException");
    }
 catch (    IOException ex) {
    }
    putDataByReplay(secondaryRegion,100,100,cq,families);
    secondaryRegion.setReadsEnabled(true);
    verifyData(secondaryRegion,0,200,cq,families);
  }
  /** 
 * Tests the case where a request for flush cache is sent to the region, but region cannot flush. It should write the flush request marker instead.
 */
  @Test public void testWriteFlushRequestMarker() throws IOException {
    FlushResultImpl result=primaryRegion.flushcache(true,false,FlushLifeCycleTracker.DUMMY);
    assertNotNull(result);
    assertEquals(FlushResultImpl.Result.CANNOT_FLUSH_MEMSTORE_EMPTY,result.result);
    assertFalse(result.wroteFlushWalMarker);
    result=primaryRegion.flushcache(true,true,FlushLifeCycleTracker.DUMMY);
    assertNotNull(result);
    assertEquals(FlushResultImpl.Result.CANNOT_FLUSH_MEMSTORE_EMPTY,result.result);
    assertTrue(result.wroteFlushWalMarker);
    List<FlushDescriptor> flushes=Lists.newArrayList();
    reader=createWALReaderForPrimary();
    while (true) {
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      FlushDescriptor flush=WALEdit.getFlushDescriptor(entry.getEdit().getCells().get(0));
      if (flush != null) {
        flushes.add(flush);
      }
    }
    assertEquals(1,flushes.size());
    assertNotNull(flushes.get(0));
    assertEquals(FlushDescriptor.FlushAction.CANNOT_FLUSH,flushes.get(0).getAction());
  }
  /** 
 * Test the case where the secondary region replica is not in reads enabled state because it is waiting for a flush or region open marker from primary region. Replaying CANNOT_FLUSH flush marker entry should restore the reads enabled status in the region and allow the reads to continue.
 */
  @Test public void testReplayingFlushRequestRestoresReadsEnabledState() throws IOException {
    disableReads(secondaryRegion);
    primaryRegion.flushcache(true,true,FlushLifeCycleTracker.DUMMY);
    reader=createWALReaderForPrimary();
    while (true) {
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      FlushDescriptor flush=WALEdit.getFlushDescriptor(entry.getEdit().getCells().get(0));
      if (flush != null) {
        secondaryRegion.replayWALFlushMarker(flush,entry.getKey().getSequenceId());
      }
    }
    secondaryRegion.get(new Get(Bytes.toBytes(0)));
  }
  /** 
 * Test the case where the secondary region replica is not in reads enabled state because it is waiting for a flush or region open marker from primary region. Replaying flush start and commit entries should restore the reads enabled status in the region and allow the reads to continue.
 */
  @Test public void testReplayingFlushRestoresReadsEnabledState() throws IOException {
    disableReads(secondaryRegion);
    putData(primaryRegion,Durability.SYNC_WAL,0,100,cq,families);
    primaryRegion.flush(true);
    putData(primaryRegion,Durability.SYNC_WAL,0,100,cq,families);
    reader=createWALReaderForPrimary();
    while (true) {
      WAL.Entry entry=reader.next();
      LOG.info(Objects.toString(entry));
      if (entry == null) {
        break;
      }
      FlushDescriptor flush=WALEdit.getFlushDescriptor(entry.getEdit().getCells().get(0));
      if (flush != null) {
        secondaryRegion.replayWALFlushMarker(flush,entry.getKey().getSequenceId());
      }
 else {
        replayEdit(secondaryRegion,entry);
      }
    }
    verifyData(secondaryRegion,0,100,cq,families);
  }
  /** 
 * Test the case where the secondary region replica is not in reads enabled state because it is waiting for a flush or region open marker from primary region. Replaying flush start and commit entries should restore the reads enabled status in the region and allow the reads to continue.
 */
  @Test public void testReplayingFlushWithEmptyMemstoreRestoresReadsEnabledState() throws IOException {
    disableReads(secondaryRegion);
    putData(primaryRegion,Durability.SYNC_WAL,0,100,cq,families);
    primaryRegion.flush(true);
    reader=createWALReaderForPrimary();
    while (true) {
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      FlushDescriptor flush=WALEdit.getFlushDescriptor(entry.getEdit().getCells().get(0));
      if (flush != null) {
        secondaryRegion.replayWALFlushMarker(flush,entry.getKey().getSequenceId());
      }
    }
    verifyData(secondaryRegion,0,100,cq,families);
  }
  /** 
 * Test the case where the secondary region replica is not in reads enabled state because it is waiting for a flush or region open marker from primary region. Replaying region open event entry from primary should restore the reads enabled status in the region and allow the reads to continue.
 */
  @Test public void testReplayingRegionOpenEventRestoresReadsEnabledState() throws IOException {
    disableReads(secondaryRegion);
    primaryRegion.close();
    primaryRegion=HRegion.openHRegion(rootDir,primaryHri,htd,walPrimary,CONF,rss,null);
    reader=createWALReaderForPrimary();
    while (true) {
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      RegionEventDescriptor regionEventDesc=WALEdit.getRegionEventDescriptor(entry.getEdit().getCells().get(0));
      if (regionEventDesc != null) {
        secondaryRegion.replayWALRegionEventMarker(regionEventDesc);
      }
    }
    secondaryRegion.get(new Get(Bytes.toBytes(0)));
  }
  @Test public void testRefresStoreFiles() throws IOException {
    assertEquals(0,primaryRegion.getStoreFileList(families).size());
    assertEquals(0,secondaryRegion.getStoreFileList(families).size());
    secondaryRegion.refreshStoreFiles();
    assertEquals(0,secondaryRegion.getStoreFileList(families).size());
    putDataWithFlushes(primaryRegion,100,100,0);
    int numRows=100;
    secondaryRegion.refreshStoreFiles();
    assertPathListsEqual(primaryRegion.getStoreFileList(families),secondaryRegion.getStoreFileList(families));
    assertEquals(families.length,secondaryRegion.getStoreFileList(families).size());
    LOG.info("-- Verifying edits from secondary");
    verifyData(secondaryRegion,0,numRows,cq,families);
    putDataWithFlushes(primaryRegion,100,300,0);
    numRows=300;
    secondaryRegion.refreshStoreFiles();
    assertPathListsEqual(primaryRegion.getStoreFileList(families),secondaryRegion.getStoreFileList(families));
    assertEquals(families.length * 4,secondaryRegion.getStoreFileList(families).size());
    LOG.info("-- Verifying edits from secondary");
    verifyData(secondaryRegion,0,numRows,cq,families);
    if (FSUtils.WINDOWS) {
      return;
    }
    primaryRegion.compactStores();
    List<HRegion> regions=new ArrayList<>();
    regions.add(primaryRegion);
    Mockito.doReturn(regions).when(rss).getRegions();
    CompactedHFilesDischarger cleaner=new CompactedHFilesDischarger(100,null,rss,false);
    cleaner.chore();
    secondaryRegion.refreshStoreFiles();
    assertPathListsEqual(primaryRegion.getStoreFileList(families),secondaryRegion.getStoreFileList(families));
    assertEquals(families.length,secondaryRegion.getStoreFileList(families).size());
    LOG.info("-- Verifying edits from secondary");
    verifyData(secondaryRegion,0,numRows,cq,families);
    LOG.info("-- Replaying edits in secondary");
    assertTrue(secondaryRegion.getMemStoreDataSize() == 0);
    putDataWithFlushes(primaryRegion,400,400,0);
    numRows=400;
    reader=createWALReaderForPrimary();
    while (true) {
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      FlushDescriptor flush=WALEdit.getFlushDescriptor(entry.getEdit().getCells().get(0));
      if (flush != null) {
      }
 else {
        replayEdit(secondaryRegion,entry);
      }
    }
    assertTrue(secondaryRegion.getMemStoreDataSize() > 0);
    secondaryRegion.refreshStoreFiles();
    assertTrue(secondaryRegion.getMemStoreDataSize() == 0);
    LOG.info("-- Verifying edits from primary");
    verifyData(primaryRegion,0,numRows,cq,families);
    LOG.info("-- Verifying edits from secondary");
    verifyData(secondaryRegion,0,numRows,cq,families);
  }
  /** 
 * Paths can be qualified or not. This does the assertion using String->Path conversion.
 */
  private void assertPathListsEqual(  List<String> list1,  List<String> list2){
    List<Path> l1=new ArrayList<>(list1.size());
    for (    String path : list1) {
      l1.add(Path.getPathWithoutSchemeAndAuthority(new Path(path)));
    }
    List<Path> l2=new ArrayList<>(list2.size());
    for (    String path : list2) {
      l2.add(Path.getPathWithoutSchemeAndAuthority(new Path(path)));
    }
    assertEquals(l1,l2);
  }
  private void disableReads(  HRegion region){
    region.setReadsEnabled(false);
    try {
      verifyData(region,0,1,cq,families);
      fail("Should have failed with IOException");
    }
 catch (    IOException ex) {
    }
  }
  private void replay(  HRegion region,  Put put,  long replaySeqId) throws IOException {
    put.setDurability(Durability.SKIP_WAL);
    MutationReplay mutation=new MutationReplay(MutationType.PUT,put,0,0);
    region.batchReplay(new MutationReplay[]{mutation},replaySeqId);
  }
  /** 
 * Tests replaying region open markers from primary region. Checks whether the files are picked up
 */
  @Test public void testReplayBulkLoadEvent() throws IOException {
    LOG.info("testReplayBulkLoadEvent starts");
    putDataWithFlushes(primaryRegion,100,0,100);
    primaryRegion.close();
    primaryRegion=HRegion.openHRegion(rootDir,primaryHri,htd,walPrimary,CONF,rss,null);
    Random random=new Random();
    byte[] randomValues=new byte[20];
    random.nextBytes(randomValues);
    Path testPath=TEST_UTIL.getDataTestDirOnTestFS();
    List<Pair<byte[],String>> familyPaths=new ArrayList<>();
    int expectedLoadFileCount=0;
    for (    byte[] family : families) {
      familyPaths.add(new Pair<>(family,createHFileForFamilies(testPath,family,randomValues)));
      expectedLoadFileCount++;
    }
    primaryRegion.bulkLoadHFiles(familyPaths,false,null);
    reader=createWALReaderForPrimary();
    LOG.info("-- Replaying edits and region events in secondary");
    BulkLoadDescriptor bulkloadEvent=null;
    while (true) {
      WAL.Entry entry=reader.next();
      if (entry == null) {
        break;
      }
      bulkloadEvent=WALEdit.getBulkLoadDescriptor(entry.getEdit().getCells().get(0));
      if (bulkloadEvent != null) {
        break;
      }
    }
    assertTrue(bulkloadEvent != null);
    assertEquals(expectedLoadFileCount,bulkloadEvent.getStoresCount());
    secondaryRegion.replayWALBulkLoadEventMarker(bulkloadEvent);
    List<String> storeFileName=new ArrayList<>();
    for (    StoreDescriptor storeDesc : bulkloadEvent.getStoresList()) {
      storeFileName.addAll(storeDesc.getStoreFileList());
    }
    for (    HStore s : secondaryRegion.getStores()) {
      for (      HStoreFile sf : s.getStorefiles()) {
        storeFileName.remove(sf.getPath().getName());
      }
    }
    assertTrue("Found some store file isn't loaded:" + storeFileName,storeFileName.isEmpty());
    LOG.info("-- Verifying edits from secondary");
    for (    byte[] family : families) {
      assertGet(secondaryRegion,family,randomValues);
    }
  }
  @Test public void testReplayingFlushCommitWithFileAlreadyDeleted() throws IOException {
    secondaryRegion.replayWALFlushCommitMarker(FlushDescriptor.newBuilder().setFlushSequenceNumber(Long.MAX_VALUE).setTableName(UnsafeByteOperations.unsafeWrap(primaryRegion.getTableDescriptor().getTableName().getName())).setAction(FlushAction.COMMIT_FLUSH).setEncodedRegionName(UnsafeByteOperations.unsafeWrap(primaryRegion.getRegionInfo().getEncodedNameAsBytes())).setRegionName(UnsafeByteOperations.unsafeWrap(primaryRegion.getRegionInfo().getRegionName())).addStoreFlushes(StoreFlushDescriptor.newBuilder().setFamilyName(UnsafeByteOperations.unsafeWrap(families[0])).setStoreHomeDir("/store_home_dir").addFlushOutput("/foo/baz/123").build()).build());
  }
  @Test public void testReplayingCompactionWithFileAlreadyDeleted() throws IOException {
    secondaryRegion.replayWALCompactionMarker(CompactionDescriptor.newBuilder().setTableName(UnsafeByteOperations.unsafeWrap(primaryRegion.getTableDescriptor().getTableName().getName())).setEncodedRegionName(UnsafeByteOperations.unsafeWrap(primaryRegion.getRegionInfo().getEncodedNameAsBytes())).setFamilyName(UnsafeByteOperations.unsafeWrap(families[0])).addCompactionInput("/123").addCompactionOutput("/456").setStoreHomeDir("/store_home_dir").setRegionName(UnsafeByteOperations.unsafeWrap(primaryRegion.getRegionInfo().getRegionName())).build(),true,true,Long.MAX_VALUE);
  }
  @Test public void testReplayingRegionOpenEventWithFileAlreadyDeleted() throws IOException {
    secondaryRegion.replayWALRegionEventMarker(RegionEventDescriptor.newBuilder().setTableName(UnsafeByteOperations.unsafeWrap(primaryRegion.getTableDescriptor().getTableName().getName())).setEncodedRegionName(UnsafeByteOperations.unsafeWrap(primaryRegion.getRegionInfo().getEncodedNameAsBytes())).setRegionName(UnsafeByteOperations.unsafeWrap(primaryRegion.getRegionInfo().getRegionName())).setEventType(EventType.REGION_OPEN).setServer(ProtobufUtil.toServerName(ServerName.valueOf("foo",1,1))).setLogSequenceNumber(Long.MAX_VALUE).addStores(StoreDescriptor.newBuilder().setFamilyName(UnsafeByteOperations.unsafeWrap(families[0])).setStoreHomeDir("/store_home_dir").addStoreFile("/123").build()).build());
  }
  @Test public void testReplayingBulkLoadEventWithFileAlreadyDeleted() throws IOException {
    secondaryRegion.replayWALBulkLoadEventMarker(BulkLoadDescriptor.newBuilder().setTableName(ProtobufUtil.toProtoTableName(primaryRegion.getTableDescriptor().getTableName())).setEncodedRegionName(UnsafeByteOperations.unsafeWrap(primaryRegion.getRegionInfo().getEncodedNameAsBytes())).setBulkloadSeqNum(Long.MAX_VALUE).addStores(StoreDescriptor.newBuilder().setFamilyName(UnsafeByteOperations.unsafeWrap(families[0])).setStoreHomeDir("/store_home_dir").addStoreFile("/123").build()).build());
  }
  private String createHFileForFamilies(  Path testPath,  byte[] family,  byte[] valueBytes) throws IOException {
    HFile.WriterFactory hFileFactory=HFile.getWriterFactoryNoCache(TEST_UTIL.getConfiguration());
    Path testFile=new Path(testPath,TEST_UTIL.getRandomUUID().toString());
    FSDataOutputStream out=TEST_UTIL.getTestFileSystem().create(testFile);
    try {
      hFileFactory.withOutputStream(out);
      hFileFactory.withFileContext(new HFileContext());
      HFile.Writer writer=hFileFactory.create();
      try {
        writer.append(new KeyValue(CellUtil.createCell(valueBytes,family,valueBytes,0L,KeyValue.Type.Put.getCode(),valueBytes)));
      }
  finally {
        writer.close();
      }
    }
  finally {
      out.close();
    }
    return testFile.toString();
  }
  /** 
 * Puts a total of numRows + numRowsAfterFlush records indexed with numeric row keys. Does a flush every flushInterval number of records. Then it puts numRowsAfterFlush number of more rows but does not execute flush after
 * @throws IOException 
 */
  private void putDataWithFlushes(  HRegion region,  int flushInterval,  int numRows,  int numRowsAfterFlush) throws IOException {
    int start=0;
    for (; start < numRows; start+=flushInterval) {
      LOG.info("-- Writing some data to primary from " + start + " to "+ (start + flushInterval));
      putData(region,Durability.SYNC_WAL,start,flushInterval,cq,families);
      LOG.info("-- Flushing primary, creating 3 files for 3 stores");
      region.flush(true);
    }
    LOG.info("-- Writing some more data to primary, not flushing");
    putData(region,Durability.SYNC_WAL,start,numRowsAfterFlush,cq,families);
  }
  private void putDataByReplay(  HRegion region,  int startRow,  int numRows,  byte[] qf,  byte[]... families) throws IOException {
    for (int i=startRow; i < startRow + numRows; i++) {
      Put put=new Put(Bytes.toBytes("" + i));
      put.setDurability(Durability.SKIP_WAL);
      for (      byte[] family : families) {
        put.addColumn(family,qf,EnvironmentEdgeManager.currentTime(),null);
      }
      replay(region,put,i + 1);
    }
  }
  private static HRegion initHRegion(  byte[] tableName,  String callingMethod,  byte[]... families) throws IOException {
    return initHRegion(tableName,HConstants.EMPTY_START_ROW,HConstants.EMPTY_END_ROW,callingMethod,TEST_UTIL.getConfiguration(),false,Durability.SYNC_WAL,null,families);
  }
  private static HRegion initHRegion(  byte[] tableName,  byte[] startKey,  byte[] stopKey,  String callingMethod,  Configuration conf,  boolean isReadOnly,  Durability durability,  WAL wal,  byte[]... families) throws IOException {
    return TEST_UTIL.createLocalHRegion(tableName,startKey,stopKey,callingMethod,conf,isReadOnly,durability,wal,families);
  }
}
