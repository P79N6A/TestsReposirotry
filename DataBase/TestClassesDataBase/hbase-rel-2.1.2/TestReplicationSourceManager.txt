/** 
 * An abstract class that tests ReplicationSourceManager. Classes that extend this class should set up the proper config for this class and initialize the proper cluster using HBaseTestingUtility.
 */
@Category({ReplicationTests.class,MediumTests.class}) public abstract class TestReplicationSourceManager {
  @ClassRule public static final HBaseClassTestRule CLASS_RULE=HBaseClassTestRule.forClass(TestReplicationSourceManager.class);
  protected static final Logger LOG=LoggerFactory.getLogger(TestReplicationSourceManager.class);
  protected static Configuration conf;
  protected static HBaseTestingUtility utility;
  protected static Replication replication;
  protected static ReplicationSourceManager manager;
  protected static ReplicationSourceManager managerOfCluster;
  protected static ZKWatcher zkw;
  protected static HTableDescriptor htd;
  protected static HRegionInfo hri;
  protected static final byte[] r1=Bytes.toBytes("r1");
  protected static final byte[] r2=Bytes.toBytes("r2");
  protected static final byte[] f1=Bytes.toBytes("f1");
  protected static final byte[] f2=Bytes.toBytes("f2");
  protected static final TableName test=TableName.valueOf("test");
  protected static final String slaveId="1";
  protected static FileSystem fs;
  protected static Path oldLogDir;
  protected static Path logDir;
  protected static CountDownLatch latch;
  protected static List<String> files=new ArrayList<>();
  protected static NavigableMap<byte[],Integer> scopes;
  protected static void setupZkAndReplication() throws Exception {
    assertNotNull(conf);
    zkw=new ZKWatcher(conf,"test",null);
    ZKUtil.createWithParents(zkw,"/hbase/replication");
    ZKUtil.createWithParents(zkw,"/hbase/replication/peers/1");
    ZKUtil.setData(zkw,"/hbase/replication/peers/1",Bytes.toBytes(conf.get(HConstants.ZOOKEEPER_QUORUM) + ":" + conf.get(HConstants.ZOOKEEPER_CLIENT_PORT)+ ":/1"));
    ZKUtil.createWithParents(zkw,"/hbase/replication/peers/1/peer-state");
    ZKUtil.setData(zkw,"/hbase/replication/peers/1/peer-state",ZKReplicationPeerStorage.ENABLED_ZNODE_BYTES);
    ZKUtil.createWithParents(zkw,"/hbase/replication/state");
    ZKUtil.setData(zkw,"/hbase/replication/state",ZKReplicationPeerStorage.ENABLED_ZNODE_BYTES);
    ZKClusterId.setClusterId(zkw,new ClusterId());
    FSUtils.setRootDir(utility.getConfiguration(),utility.getDataTestDir());
    fs=FileSystem.get(conf);
    oldLogDir=new Path(utility.getDataTestDir(),HConstants.HREGION_OLDLOGDIR_NAME);
    logDir=new Path(utility.getDataTestDir(),HConstants.HREGION_LOGDIR_NAME);
    replication=new Replication();
    replication.initialize(new DummyServer(),fs,logDir,oldLogDir,null);
    managerOfCluster=getManagerFromCluster();
    if (managerOfCluster != null) {
      managerOfCluster.addPeer(slaveId);
    }
    manager=replication.getReplicationManager();
    manager.addSource(slaveId);
    if (managerOfCluster != null) {
      waitPeer(slaveId,managerOfCluster,true);
    }
    waitPeer(slaveId,manager,true);
    htd=new HTableDescriptor(test);
    HColumnDescriptor col=new HColumnDescriptor(f1);
    col.setScope(HConstants.REPLICATION_SCOPE_GLOBAL);
    htd.addFamily(col);
    col=new HColumnDescriptor(f2);
    col.setScope(HConstants.REPLICATION_SCOPE_LOCAL);
    htd.addFamily(col);
    scopes=new TreeMap<>(Bytes.BYTES_COMPARATOR);
    for (    byte[] fam : htd.getFamiliesKeys()) {
      scopes.put(fam,0);
    }
    hri=new HRegionInfo(htd.getTableName(),r1,r2);
  }
  private static ReplicationSourceManager getManagerFromCluster(){
    if (utility.getMiniHBaseCluster() == null) {
      return null;
    }
    return utility.getMiniHBaseCluster().getRegionServerThreads().stream().map(JVMClusterUtil.RegionServerThread::getRegionServer).findAny().map(HRegionServer::getReplicationSourceService).map(r -> (Replication)r).map(Replication::getReplicationManager).get();
  }
  @AfterClass public static void tearDownAfterClass() throws Exception {
    if (manager != null) {
      manager.join();
    }
    utility.shutdownMiniCluster();
  }
  @Rule public TestName testName=new TestName();
  private void cleanLogDir() throws IOException {
    fs.delete(logDir,true);
    fs.delete(oldLogDir,true);
  }
  @Before public void setUp() throws Exception {
    LOG.info("Start " + testName.getMethodName());
    cleanLogDir();
  }
  @After public void tearDown() throws Exception {
    LOG.info("End " + testName.getMethodName());
    cleanLogDir();
    List<String> ids=manager.getSources().stream().map(ReplicationSourceInterface::getPeerId).collect(Collectors.toList());
    for (    String id : ids) {
      if (slaveId.equals(id)) {
        continue;
      }
      removePeerAndWait(id);
    }
  }
  @Test public void testLogRoll() throws Exception {
    long baseline=1000;
    long time=baseline;
    MultiVersionConcurrencyControl mvcc=new MultiVersionConcurrencyControl();
    KeyValue kv=new KeyValue(r1,f1,r1);
    WALEdit edit=new WALEdit();
    edit.add(kv);
    WALFactory wals=new WALFactory(utility.getConfiguration(),URLEncoder.encode("regionserver:60020","UTF8"));
    ReplicationSourceManager replicationManager=replication.getReplicationManager();
    wals.getWALProvider().addWALActionsListener(new ReplicationSourceWALActionListener(conf,replicationManager));
    final WAL wal=wals.getWAL(hri);
    manager.init();
    HTableDescriptor htd=new HTableDescriptor(TableName.valueOf("tableame"));
    htd.addFamily(new HColumnDescriptor(f1));
    NavigableMap<byte[],Integer> scopes=new TreeMap<>(Bytes.BYTES_COMPARATOR);
    for (    byte[] fam : htd.getFamiliesKeys()) {
      scopes.put(fam,0);
    }
    for (long i=1; i < 101; i++) {
      if (i > 1 && i % 20 == 0) {
        wal.rollWriter();
      }
      LOG.info(Long.toString(i));
      final long txid=wal.append(hri,new WALKeyImpl(hri.getEncodedNameAsBytes(),test,System.currentTimeMillis(),mvcc,scopes),edit,true);
      wal.sync(txid);
    }
    LOG.info(baseline + " and " + time);
    baseline+=101;
    time=baseline;
    LOG.info(baseline + " and " + time);
    for (int i=0; i < 3; i++) {
      wal.append(hri,new WALKeyImpl(hri.getEncodedNameAsBytes(),test,System.currentTimeMillis(),mvcc,scopes),edit,true);
    }
    wal.sync();
    int logNumber=0;
    for (    Map.Entry<String,NavigableSet<String>> entry : manager.getWALs().get(slaveId).entrySet()) {
      logNumber+=entry.getValue().size();
    }
    assertEquals(6,logNumber);
    wal.rollWriter();
    manager.logPositionAndCleanOldLogs("1",false,new WALEntryBatch(0,manager.getSources().get(0).getCurrentPath()));
    wal.append(hri,new WALKeyImpl(hri.getEncodedNameAsBytes(),test,System.currentTimeMillis(),mvcc,scopes),edit,true);
    wal.sync();
    assertEquals(1,manager.getWALs().size());
  }
  @Test public void testClaimQueues() throws Exception {
    Server server=new DummyServer("hostname0.example.org");
    ReplicationQueueStorage rq=ReplicationStorageFactory.getReplicationQueueStorage(server.getZooKeeper(),server.getConfiguration());
    files.add("log1");
    files.add("log2");
    for (    String file : files) {
      rq.addWAL(server.getServerName(),"1",file);
    }
    Server s1=new DummyServer("dummyserver1.example.org");
    Server s2=new DummyServer("dummyserver2.example.org");
    Server s3=new DummyServer("dummyserver3.example.org");
    DummyNodeFailoverWorker w1=new DummyNodeFailoverWorker(server.getServerName(),s1);
    DummyNodeFailoverWorker w2=new DummyNodeFailoverWorker(server.getServerName(),s2);
    DummyNodeFailoverWorker w3=new DummyNodeFailoverWorker(server.getServerName(),s3);
    latch=new CountDownLatch(3);
    w1.start();
    w2.start();
    w3.start();
    int populatedMap=0;
    latch.await();
    populatedMap+=w1.isLogZnodesMapPopulated() + w2.isLogZnodesMapPopulated() + w3.isLogZnodesMapPopulated();
    assertEquals(1,populatedMap);
    server.abort("",null);
  }
  @Test public void testCleanupFailoverQueues() throws Exception {
    Server server=new DummyServer("hostname1.example.org");
    ReplicationQueueStorage rq=ReplicationStorageFactory.getReplicationQueueStorage(server.getZooKeeper(),server.getConfiguration());
    SortedSet<String> files=new TreeSet<>();
    String group="testgroup";
    String file1=group + ".log1";
    String file2=group + ".log2";
    files.add(file1);
    files.add(file2);
    for (    String file : files) {
      rq.addWAL(server.getServerName(),"1",file);
    }
    Server s1=new DummyServer("dummyserver1.example.org");
    ReplicationPeers rp1=ReplicationFactory.getReplicationPeers(s1.getZooKeeper(),s1.getConfiguration());
    rp1.init();
    NodeFailoverWorker w1=manager.new NodeFailoverWorker(server.getServerName());
    w1.run();
    assertEquals(1,manager.getWalsByIdRecoveredQueues().size());
    String id="1-" + server.getServerName().getServerName();
    assertEquals(files,manager.getWalsByIdRecoveredQueues().get(id).get(group));
    manager.cleanOldLogs(file2,false,id,true);
    assertEquals(Sets.newHashSet(file2),manager.getWalsByIdRecoveredQueues().get(id).get(group));
  }
  @Test public void testCleanupUnknownPeerZNode() throws Exception {
    Server server=new DummyServer("hostname2.example.org");
    ReplicationQueueStorage rq=ReplicationStorageFactory.getReplicationQueueStorage(server.getZooKeeper(),server.getConfiguration());
    String group="testgroup";
    rq.addWAL(server.getServerName(),"2",group + ".log1");
    rq.addWAL(server.getServerName(),"2",group + ".log2");
    NodeFailoverWorker w1=manager.new NodeFailoverWorker(server.getServerName());
    w1.run();
    for (    String peer : manager.getAllQueues()) {
      assertTrue(peer.startsWith("1"));
    }
  }
  /** 
 * Test for HBASE-9038, Replication.scopeWALEdits would NPE if it wasn't filtering out the compaction WALEdit.
 */
  @Test public void testCompactionWALEdits() throws Exception {
    TableName tableName=TableName.valueOf("testCompactionWALEdits");
    WALProtos.CompactionDescriptor compactionDescriptor=WALProtos.CompactionDescriptor.getDefaultInstance();
    RegionInfo hri=RegionInfoBuilder.newBuilder(tableName).setStartKey(HConstants.EMPTY_START_ROW).setEndKey(HConstants.EMPTY_END_ROW).build();
    WALEdit edit=WALEdit.createCompaction(hri,compactionDescriptor);
    ReplicationSourceWALActionListener.scopeWALEdits(new WALKeyImpl(),edit,conf);
  }
  @Test public void testBulkLoadWALEditsWithoutBulkLoadReplicationEnabled() throws Exception {
    NavigableMap<byte[],Integer> scope=new TreeMap<>(Bytes.BYTES_COMPARATOR);
    WALEdit logEdit=getBulkLoadWALEdit(scope);
    WALKeyImpl logKey=new WALKeyImpl(scope);
    ReplicationSourceWALActionListener.scopeWALEdits(logKey,logEdit,conf);
    assertNull("No bulk load entries scope should be added if bulk load replication is disabled.",logKey.getReplicationScopes());
  }
  @Test public void testBulkLoadWALEdits() throws Exception {
    NavigableMap<byte[],Integer> scope=new TreeMap<>(Bytes.BYTES_COMPARATOR);
    WALEdit logEdit=getBulkLoadWALEdit(scope);
    WALKeyImpl logKey=new WALKeyImpl(scope);
    Configuration bulkLoadConf=HBaseConfiguration.create(conf);
    bulkLoadConf.setBoolean(HConstants.REPLICATION_BULKLOAD_ENABLE_KEY,true);
    ReplicationSourceWALActionListener.scopeWALEdits(logKey,logEdit,bulkLoadConf);
    NavigableMap<byte[],Integer> scopes=logKey.getReplicationScopes();
    assertTrue("This family scope is set to global, should be part of replication key scopes.",scopes.containsKey(f1));
    assertFalse("This family scope is set to local, should not be part of replication key scopes",scopes.containsKey(f2));
  }
  /** 
 * Test whether calling removePeer() on a ReplicationSourceManager that failed on initializing the corresponding ReplicationSourceInterface correctly cleans up the corresponding replication queue and ReplicationPeer. See HBASE-16096.
 * @throws Exception
 */
  @Test public void testPeerRemovalCleanup() throws Exception {
    String replicationSourceImplName=conf.get("replication.replicationsource.implementation");
    final String peerId="FakePeer";
    final ReplicationPeerConfig peerConfig=new ReplicationPeerConfig().setClusterKey("localhost:" + utility.getZkCluster().getClientPort() + ":/hbase");
    try {
      DummyServer server=new DummyServer();
      ReplicationQueueStorage rq=ReplicationStorageFactory.getReplicationQueueStorage(server.getZooKeeper(),server.getConfiguration());
      conf.set("replication.replicationsource.implementation",FailInitializeDummyReplicationSource.class.getName());
      final ReplicationPeers rp=manager.getReplicationPeers();
      addPeerAndWait(peerId,peerConfig,false);
      assertNull(manager.getSource(peerId));
      rq.addWAL(server.getServerName(),peerId,"FakeFile");
      removePeerAndWait(peerId);
      assertFalse(rq.getAllQueues(server.getServerName()).contains(peerId));
    }
  finally {
      conf.set("replication.replicationsource.implementation",replicationSourceImplName);
      removePeerAndWait(peerId);
    }
  }
  private static MetricsReplicationSourceSource getGlobalSource() throws Exception {
    ReplicationSourceInterface source=manager.getSource(slaveId);
    Field f=MetricsSource.class.getDeclaredField("globalSourceSource");
    f.setAccessible(true);
    return (MetricsReplicationSourceSource)f.get(source.getSourceMetrics());
  }
  private static long getSizeOfLatestPath(){
    if (utility.getMiniHBaseCluster() == null) {
      return 0;
    }
    return utility.getMiniHBaseCluster().getRegionServerThreads().stream().map(JVMClusterUtil.RegionServerThread::getRegionServer).map(HRegionServer::getReplicationSourceService).map(r -> (Replication)r).map(Replication::getReplicationManager).mapToLong(ReplicationSourceManager::getSizeOfLatestPath).sum();
  }
  @Test public void testRemovePeerMetricsCleanup() throws Exception {
    final String peerId="DummyPeer";
    final ReplicationPeerConfig peerConfig=new ReplicationPeerConfig().setClusterKey("localhost:" + utility.getZkCluster().getClientPort() + ":/hbase");
    try {
      MetricsReplicationSourceSource globalSource=getGlobalSource();
      final int globalLogQueueSizeInitial=globalSource.getSizeOfLogQueue();
      final long sizeOfLatestPath=getSizeOfLatestPath();
      addPeerAndWait(peerId,peerConfig,true);
      assertEquals(sizeOfLatestPath + globalLogQueueSizeInitial,globalSource.getSizeOfLogQueue());
      ReplicationSourceInterface source=manager.getSource(peerId);
      assertNotNull(source);
      final int sizeOfSingleLogQueue=source.getSourceMetrics().getSizeOfLogQueue();
      source.enqueueLog(new Path("abc"));
      assertEquals(1 + sizeOfSingleLogQueue,source.getSourceMetrics().getSizeOfLogQueue());
      assertEquals(source.getSourceMetrics().getSizeOfLogQueue() + globalLogQueueSizeInitial,globalSource.getSizeOfLogQueue());
      removePeerAndWait(peerId);
      assertEquals(globalLogQueueSizeInitial,globalSource.getSizeOfLogQueue());
      addPeerAndWait(peerId,peerConfig,true);
      source=manager.getSource(peerId);
      assertNotNull(source);
      assertEquals(source.getSourceMetrics().getSizeOfLogQueue() + globalLogQueueSizeInitial,globalSource.getSizeOfLogQueue());
    }
  finally {
      removePeerAndWait(peerId);
    }
  }
  /** 
 * Add a peer and wait for it to initialize
 * @param peerId
 * @param peerConfig
 * @param waitForSource Whether to wait for replication source to initialize
 * @throws Exception
 */
  private void addPeerAndWait(  final String peerId,  final ReplicationPeerConfig peerConfig,  final boolean waitForSource) throws Exception {
    final ReplicationPeers rp=manager.getReplicationPeers();
    rp.getPeerStorage().addPeer(peerId,peerConfig,true);
    try {
      manager.addPeer(peerId);
    }
 catch (    Exception e) {
    }
    waitPeer(peerId,manager,waitForSource);
    if (managerOfCluster != null) {
      managerOfCluster.addPeer(peerId);
      waitPeer(peerId,managerOfCluster,waitForSource);
    }
  }
  private static void waitPeer(  final String peerId,  ReplicationSourceManager manager,  final boolean waitForSource){
    ReplicationPeers rp=manager.getReplicationPeers();
    Waiter.waitFor(conf,20000,() -> {
      if (waitForSource) {
        ReplicationSourceInterface rs=manager.getSource(peerId);
        if (rs == null) {
          return false;
        }
        if (rs instanceof ReplicationSourceDummy) {
          return ((ReplicationSourceDummy)rs).isStartup();
        }
        return true;
      }
 else {
        return (rp.getPeer(peerId) != null);
      }
    }
);
  }
  /** 
 * Remove a peer and wait for it to get cleaned up
 * @param peerId
 * @throws Exception
 */
  private void removePeerAndWait(  final String peerId) throws Exception {
    final ReplicationPeers rp=manager.getReplicationPeers();
    if (rp.getPeerStorage().listPeerIds().contains(peerId)) {
      rp.getPeerStorage().removePeer(peerId);
      try {
        manager.removePeer(peerId);
      }
 catch (      Exception e) {
      }
    }
    Waiter.waitFor(conf,20000,new Waiter.Predicate<Exception>(){
      @Override public boolean evaluate() throws Exception {
        Collection<String> peers=rp.getPeerStorage().listPeerIds();
        return (!manager.getAllQueues().contains(peerId)) && (rp.getPeer(peerId) == null) && (!peers.contains(peerId))&& manager.getSource(peerId) == null;
      }
    }
);
  }
  private WALEdit getBulkLoadWALEdit(  NavigableMap<byte[],Integer> scope){
    Map<byte[],List<Path>> storeFiles=new HashMap<>(1);
    Map<String,Long> storeFilesSize=new HashMap<>(1);
    List<Path> p=new ArrayList<>(1);
    Path hfilePath1=new Path(Bytes.toString(f1));
    p.add(hfilePath1);
    try {
      storeFilesSize.put(hfilePath1.getName(),fs.getFileStatus(hfilePath1).getLen());
    }
 catch (    IOException e) {
      LOG.debug("Failed to calculate the size of hfile " + hfilePath1);
      storeFilesSize.put(hfilePath1.getName(),0L);
    }
    storeFiles.put(f1,p);
    scope.put(f1,1);
    p=new ArrayList<>(1);
    Path hfilePath2=new Path(Bytes.toString(f2));
    p.add(hfilePath2);
    try {
      storeFilesSize.put(hfilePath2.getName(),fs.getFileStatus(hfilePath2).getLen());
    }
 catch (    IOException e) {
      LOG.debug("Failed to calculate the size of hfile " + hfilePath2);
      storeFilesSize.put(hfilePath2.getName(),0L);
    }
    storeFiles.put(f2,p);
    BulkLoadDescriptor desc=ProtobufUtil.toBulkLoadDescriptor(hri.getTable(),UnsafeByteOperations.unsafeWrap(hri.getEncodedNameAsBytes()),storeFiles,storeFilesSize,1);
    WALEdit logEdit=WALEdit.createBulkLoadEvent(hri,desc);
    return logEdit;
  }
static class DummyNodeFailoverWorker extends Thread {
    private Map<String,Set<String>> logZnodesMap;
    Server server;
    private ServerName deadRS;
    ReplicationQueueStorage rq;
    public DummyNodeFailoverWorker(    ServerName deadRS,    Server s) throws Exception {
      this.deadRS=deadRS;
      this.server=s;
      this.rq=ReplicationStorageFactory.getReplicationQueueStorage(server.getZooKeeper(),server.getConfiguration());
    }
    @Override public void run(){
      try {
        logZnodesMap=new HashMap<>();
        List<String> queues=rq.getAllQueues(deadRS);
        for (        String queue : queues) {
          Pair<String,SortedSet<String>> pair=rq.claimQueue(deadRS,queue,server.getServerName());
          if (pair != null) {
            logZnodesMap.put(pair.getFirst(),pair.getSecond());
          }
        }
        server.abort("Done with testing",null);
      }
 catch (      Exception e) {
        LOG.error("Got exception while running NodeFailoverWorker",e);
      }
 finally {
        latch.countDown();
      }
    }
    /** 
 * @return 1 when the map is not empty.
 */
    private int isLogZnodesMapPopulated(){
      Collection<Set<String>> sets=logZnodesMap.values();
      if (sets.size() > 1) {
        throw new RuntimeException("unexpected size of logZnodesMap: " + sets.size());
      }
      if (sets.size() == 1) {
        Set<String> s=sets.iterator().next();
        for (        String file : files) {
          if (!s.contains(file)) {
            return 0;
          }
        }
        return 1;
      }
      return 0;
    }
  }
static class FailInitializeDummyReplicationSource extends ReplicationSourceDummy {
    @Override public void init(    Configuration conf,    FileSystem fs,    ReplicationSourceManager manager,    ReplicationQueueStorage rq,    ReplicationPeer rp,    Server server,    String peerClusterId,    UUID clusterId,    WALFileLengthProvider walFileLengthProvider,    MetricsSource metrics) throws IOException {
      throw new IOException("Failing deliberately");
    }
  }
static class DummyServer implements Server {
    String hostname;
    DummyServer(){
      hostname="hostname.example.org";
    }
    DummyServer(    String hostname){
      this.hostname=hostname;
    }
    @Override public Configuration getConfiguration(){
      return conf;
    }
    @Override public ZKWatcher getZooKeeper(){
      return zkw;
    }
    @Override public CoordinatedStateManager getCoordinatedStateManager(){
      return null;
    }
    @Override public ClusterConnection getConnection(){
      return null;
    }
    @Override public MetaTableLocator getMetaTableLocator(){
      return null;
    }
    @Override public ServerName getServerName(){
      return ServerName.valueOf(hostname,1234,1L);
    }
    @Override public void abort(    String why,    Throwable e){
    }
    @Override public boolean isAborted(){
      return false;
    }
    @Override public void stop(    String why){
    }
    @Override public boolean isStopped(){
      return false;
    }
    @Override public ChoreService getChoreService(){
      return null;
    }
    @Override public ClusterConnection getClusterConnection(){
      return null;
    }
    @Override public FileSystem getFileSystem(){
      return null;
    }
    @Override public boolean isStopping(){
      return false;
    }
    @Override public Connection createConnection(    Configuration conf) throws IOException {
      return null;
    }
  }
}
