/** 
 * Tests for the hdfs fix from HBASE-6435. Please don't add new subtest which involves starting / stopping MiniDFSCluster in this class. When stopping MiniDFSCluster, shutdown hooks would be cleared in hadoop's ShutdownHookManager in hadoop 3. This leads to 'Failed suppression of fs shutdown hook' error in region server.
 */
@Category({MiscTests.class,LargeTests.class}) public class TestBlockReorderBlockLocation {
  @ClassRule public static final HBaseClassTestRule CLASS_RULE=HBaseClassTestRule.forClass(TestBlockReorderBlockLocation.class);
  private Configuration conf;
  private MiniDFSCluster cluster;
  private HBaseTestingUtility htu;
  private DistributedFileSystem dfs;
  private static final String host1="host1";
  private static final String host2="host2";
  private static final String host3="host3";
  @Rule public TestName name=new TestName();
  @Before public void setUp() throws Exception {
    htu=new HBaseTestingUtility();
    htu.getConfiguration().setInt("dfs.blocksize",1024);
    htu.getConfiguration().setInt("dfs.replication",3);
    htu.startMiniDFSCluster(3,new String[]{"/r1","/r2","/r3"},new String[]{host1,host2,host3});
    conf=htu.getConfiguration();
    cluster=htu.getDFSCluster();
    dfs=(DistributedFileSystem)FileSystem.get(conf);
  }
  @After public void tearDownAfterClass() throws Exception {
    htu.shutdownMiniCluster();
  }
  private static ClientProtocol getNamenode(  DFSClient dfsc) throws Exception {
    Field nf=DFSClient.class.getDeclaredField("namenode");
    nf.setAccessible(true);
    return (ClientProtocol)nf.get(dfsc);
  }
  /** 
 * Test that the reorder algo works as we expect.
 */
  @Test public void testBlockLocation() throws Exception {
    htu.startMiniZKCluster();
    MiniHBaseCluster hbm=htu.startMiniHBaseCluster(1,1);
    conf=hbm.getConfiguration();
    final String fileName="/helloWorld";
    Path p=new Path(fileName);
    final int repCount=3;
    Assert.assertTrue((short)cluster.getDataNodes().size() >= repCount);
    FSDataOutputStream fop=dfs.create(p,(short)repCount);
    final double toWrite=875.5613;
    fop.writeDouble(toWrite);
    fop.close();
    for (int i=0; i < 10; i++) {
      LocatedBlocks l;
      final long max=System.currentTimeMillis() + 10000;
      do {
        l=getNamenode(dfs.getClient()).getBlockLocations(fileName,0,1);
        Assert.assertNotNull(l.getLocatedBlocks());
        Assert.assertEquals(1,l.getLocatedBlocks().size());
        Assert.assertTrue("Expecting " + repCount + " , got "+ l.get(0).getLocations().length,System.currentTimeMillis() < max);
      }
 while (l.get(0).getLocations().length != repCount);
      Object originalList[]=l.getLocatedBlocks().toArray();
      HFileSystem.ReorderWALBlocks lrb=new HFileSystem.ReorderWALBlocks();
      lrb.reorderBlocks(conf,l,fileName);
      Assert.assertArrayEquals(originalList,l.getLocatedBlocks().toArray());
      Assert.assertNotNull(conf.get(HConstants.HBASE_DIR));
      Assert.assertFalse(conf.get(HConstants.HBASE_DIR).isEmpty());
      String pseudoLogFile=conf.get(HConstants.HBASE_DIR) + "/" + HConstants.HREGION_LOGDIR_NAME+ "/"+ host1+ ",6977,6576"+ "/mylogfile";
      Assert.assertNotNull("log= " + pseudoLogFile,AbstractFSWALProvider.getServerNameFromWALDirectoryName(dfs.getConf(),pseudoLogFile));
      lrb.reorderBlocks(conf,l,pseudoLogFile);
      Assert.assertEquals(host1,l.get(0).getLocations()[2].getHostName());
      lrb.reorderBlocks(conf,l,pseudoLogFile);
      Assert.assertEquals(host1,l.get(0).getLocations()[2].getHostName());
    }
  }
}
