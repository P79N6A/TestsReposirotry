/** 
 * Test  {@link FSUtils}.
 */
@Category({MiscTests.class,MediumTests.class}) public class TestFSUtils {
  @ClassRule public static final HBaseClassTestRule CLASS_RULE=HBaseClassTestRule.forClass(TestFSUtils.class);
  private static final Logger LOG=LoggerFactory.getLogger(TestFSUtils.class);
  private HBaseTestingUtility htu;
  private FileSystem fs;
  private Configuration conf;
  @Before public void setUp() throws IOException {
    htu=new HBaseTestingUtility();
    fs=htu.getTestFileSystem();
    conf=htu.getConfiguration();
  }
  @Test public void testIsHDFS() throws Exception {
    assertFalse(FSUtils.isHDFS(conf));
    MiniDFSCluster cluster=null;
    try {
      cluster=htu.startMiniDFSCluster(1);
      assertTrue(FSUtils.isHDFS(conf));
    }
  finally {
      if (cluster != null)       cluster.shutdown();
    }
  }
  private void WriteDataToHDFS(  FileSystem fs,  Path file,  int dataSize) throws Exception {
    FSDataOutputStream out=fs.create(file);
    byte[] data=new byte[dataSize];
    out.write(data,0,dataSize);
    out.close();
  }
  @Test public void testcomputeHDFSBlocksDistribution() throws Exception {
    final int DEFAULT_BLOCK_SIZE=1024;
    conf.setLong("dfs.blocksize",DEFAULT_BLOCK_SIZE);
    MiniDFSCluster cluster=null;
    Path testFile=null;
    try {
      String hosts[]=new String[]{"host1","host2","host3"};
      cluster=htu.startMiniDFSCluster(hosts);
      cluster.waitActive();
      FileSystem fs=cluster.getFileSystem();
      testFile=new Path("/test1.txt");
      WriteDataToHDFS(fs,testFile,2 * DEFAULT_BLOCK_SIZE);
      final long maxTime=System.currentTimeMillis() + 2000;
      boolean ok;
      do {
        ok=true;
        FileStatus status=fs.getFileStatus(testFile);
        HDFSBlocksDistribution blocksDistribution=FSUtils.computeHDFSBlocksDistribution(fs,status,0,status.getLen());
        long uniqueBlocksTotalWeight=blocksDistribution.getUniqueBlocksTotalWeight();
        for (        String host : hosts) {
          long weight=blocksDistribution.getWeight(host);
          ok=(ok && uniqueBlocksTotalWeight == weight);
        }
      }
 while (!ok && System.currentTimeMillis() < maxTime);
      assertTrue(ok);
    }
  finally {
      htu.shutdownMiniDFSCluster();
    }
    try {
      String hosts[]=new String[]{"host1","host2","host3","host4"};
      cluster=htu.startMiniDFSCluster(hosts);
      cluster.waitActive();
      FileSystem fs=cluster.getFileSystem();
      testFile=new Path("/test2.txt");
      WriteDataToHDFS(fs,testFile,3 * DEFAULT_BLOCK_SIZE);
      final long maxTime=System.currentTimeMillis() + 2000;
      long weight;
      long uniqueBlocksTotalWeight;
      do {
        FileStatus status=fs.getFileStatus(testFile);
        HDFSBlocksDistribution blocksDistribution=FSUtils.computeHDFSBlocksDistribution(fs,status,0,status.getLen());
        uniqueBlocksTotalWeight=blocksDistribution.getUniqueBlocksTotalWeight();
        String tophost=blocksDistribution.getTopHosts().get(0);
        weight=blocksDistribution.getWeight(tophost);
      }
 while (uniqueBlocksTotalWeight != weight && System.currentTimeMillis() < maxTime);
      assertTrue(uniqueBlocksTotalWeight == weight);
    }
  finally {
      htu.shutdownMiniDFSCluster();
    }
    try {
      String hosts[]=new String[]{"host1","host2","host3","host4"};
      cluster=htu.startMiniDFSCluster(hosts);
      cluster.waitActive();
      FileSystem fs=cluster.getFileSystem();
      testFile=new Path("/test3.txt");
      WriteDataToHDFS(fs,testFile,DEFAULT_BLOCK_SIZE);
      final long maxTime=System.currentTimeMillis() + 2000;
      HDFSBlocksDistribution blocksDistribution;
      do {
        FileStatus status=fs.getFileStatus(testFile);
        blocksDistribution=FSUtils.computeHDFSBlocksDistribution(fs,status,0,status.getLen());
      }
 while (blocksDistribution.getTopHosts().size() != 3 && System.currentTimeMillis() < maxTime);
      assertEquals("Wrong number of hosts distributing blocks.",3,blocksDistribution.getTopHosts().size());
    }
  finally {
      htu.shutdownMiniDFSCluster();
    }
  }
  @Test public void testVersion() throws DeserializationException, IOException {
    final Path rootdir=htu.getDataTestDir();
    final FileSystem fs=rootdir.getFileSystem(conf);
    assertNull(FSUtils.getVersion(fs,rootdir));
    Path versionFile=new Path(rootdir,HConstants.VERSION_FILE_NAME);
    FSDataOutputStream s=fs.create(versionFile);
    final String version=HConstants.FILE_SYSTEM_VERSION;
    s.writeUTF(version);
    s.close();
    assertTrue(fs.exists(versionFile));
    FileStatus[] status=fs.listStatus(versionFile);
    assertNotNull(status);
    assertTrue(status.length > 0);
    String newVersion=FSUtils.getVersion(fs,rootdir);
    assertEquals(version.length(),newVersion.length());
    assertEquals(version,newVersion);
    assertEquals(version,FSUtils.getVersion(fs,rootdir));
    FSUtils.checkVersion(fs,rootdir,true);
  }
  @Test public void testPermMask() throws Exception {
    final Path rootdir=htu.getDataTestDir();
    final FileSystem fs=rootdir.getFileSystem(conf);
    FsPermission defaultFsPerm=FSUtils.getFilePermissions(fs,conf,HConstants.DATA_FILE_UMASK_KEY);
    assertEquals(FsPermission.getFileDefault(),defaultFsPerm);
    conf.setBoolean(HConstants.ENABLE_DATA_FILE_UMASK,true);
    FsPermission defaultStartPerm=FSUtils.getFilePermissions(fs,conf,HConstants.DATA_FILE_UMASK_KEY);
    assertEquals(new FsPermission(FSUtils.FULL_RWX_PERMISSIONS),defaultStartPerm);
    conf.setStrings(HConstants.DATA_FILE_UMASK_KEY,"077");
    FsPermission filePerm=FSUtils.getFilePermissions(fs,conf,HConstants.DATA_FILE_UMASK_KEY);
    assertEquals(new FsPermission("700"),filePerm);
    Path p=new Path("target" + File.separator + htu.getRandomUUID().toString());
    try {
      FSDataOutputStream out=FSUtils.create(conf,fs,p,filePerm,null);
      out.close();
      FileStatus stat=fs.getFileStatus(p);
      assertEquals(new FsPermission("700"),stat.getPermission());
    }
  finally {
      fs.delete(p,true);
    }
  }
  @Test public void testDeleteAndExists() throws Exception {
    final Path rootdir=htu.getDataTestDir();
    final FileSystem fs=rootdir.getFileSystem(conf);
    conf.setBoolean(HConstants.ENABLE_DATA_FILE_UMASK,true);
    FsPermission perms=FSUtils.getFilePermissions(fs,conf,HConstants.DATA_FILE_UMASK_KEY);
    String file=htu.getRandomUUID().toString();
    Path p=new Path(htu.getDataTestDir(),"temptarget" + File.separator + file);
    Path p1=new Path(htu.getDataTestDir(),"temppath" + File.separator + file);
    try {
      FSDataOutputStream out=FSUtils.create(conf,fs,p,perms,null);
      out.close();
      assertTrue("The created file should be present",FSUtils.isExists(fs,p));
      FSUtils.delete(fs,p,false);
      FSDataOutputStream out1=FSUtils.create(conf,fs,p1,perms,null);
      out1.close();
      FSUtils.delete(fs,p1,true);
      assertFalse("The created file should be present",FSUtils.isExists(fs,p1));
    }
  finally {
      FSUtils.delete(fs,p,true);
      FSUtils.delete(fs,p1,true);
    }
  }
  @Test public void testFilteredStatusDoesNotThrowOnNotFound() throws Exception {
    MiniDFSCluster cluster=htu.startMiniDFSCluster(1);
    try {
      assertNull(FSUtils.listStatusWithStatusFilter(cluster.getFileSystem(),new Path("definitely/doesn't/exist"),null));
    }
  finally {
      cluster.shutdown();
    }
  }
  @Test public void testRenameAndSetModifyTime() throws Exception {
    MiniDFSCluster cluster=htu.startMiniDFSCluster(1);
    assertTrue(FSUtils.isHDFS(conf));
    FileSystem fs=FileSystem.get(conf);
    Path testDir=htu.getDataTestDirOnTestFS("testArchiveFile");
    String file=htu.getRandomUUID().toString();
    Path p=new Path(testDir,file);
    FSDataOutputStream out=fs.create(p);
    out.close();
    assertTrue("The created file should be present",FSUtils.isExists(fs,p));
    long expect=System.currentTimeMillis() + 1000;
    assertNotEquals(expect,fs.getFileStatus(p).getModificationTime());
    ManualEnvironmentEdge mockEnv=new ManualEnvironmentEdge();
    mockEnv.setValue(expect);
    EnvironmentEdgeManager.injectEdge(mockEnv);
    try {
      String dstFile=htu.getRandomUUID().toString();
      Path dst=new Path(testDir,dstFile);
      assertTrue(FSUtils.renameAndSetModifyTime(fs,p,dst));
      assertFalse("The moved file should not be present",FSUtils.isExists(fs,p));
      assertTrue("The dst file should be present",FSUtils.isExists(fs,dst));
      assertEquals(expect,fs.getFileStatus(dst).getModificationTime());
      cluster.shutdown();
    }
  finally {
      EnvironmentEdgeManager.reset();
    }
  }
  @Test public void testSetStoragePolicyDefault() throws Exception {
    verifyNoHDFSApiInvocationForDefaultPolicy();
    verifyFileInDirWithStoragePolicy(HConstants.DEFAULT_WAL_STORAGE_POLICY);
  }
  /** 
 * Note: currently the default policy is set to defer to HDFS and this case is to verify the logic, will need to remove the check if the default policy is changed
 */
  private void verifyNoHDFSApiInvocationForDefaultPolicy(){
    FileSystem testFs=new AlwaysFailSetStoragePolicyFileSystem();
    try {
      FSUtils.setStoragePolicy(testFs,new Path("non-exist"),HConstants.DEFAULT_WAL_STORAGE_POLICY,true);
    }
 catch (    IOException e) {
      Assert.fail("Should have bypassed the FS API when setting default storage policy");
    }
    try {
      FSUtils.setStoragePolicy(testFs,new Path("non-exist"),"HOT",true);
      Assert.fail("Should have invoked the FS API but haven't");
    }
 catch (    IOException e) {
    }
  }
class AlwaysFailSetStoragePolicyFileSystem extends DistributedFileSystem {
    @Override public void setStoragePolicy(    final Path src,    final String policyName) throws IOException {
      throw new IOException("The setStoragePolicy method is invoked");
    }
  }
  @Test public void testSetStoragePolicyValidButMaybeNotPresent() throws Exception {
    verifyFileInDirWithStoragePolicy("ALL_SSD");
  }
  final String INVALID_STORAGE_POLICY="1772";
  @Test public void testSetStoragePolicyInvalid() throws Exception {
    verifyFileInDirWithStoragePolicy(INVALID_STORAGE_POLICY);
  }
  private void verifyFileInDirWithStoragePolicy(  final String policy) throws Exception {
    conf.set(HConstants.WAL_STORAGE_POLICY,policy);
    MiniDFSCluster cluster=htu.startMiniDFSCluster(1);
    try {
      assertTrue(FSUtils.isHDFS(conf));
      FileSystem fs=FileSystem.get(conf);
      Path testDir=htu.getDataTestDirOnTestFS("testArchiveFile");
      fs.mkdirs(testDir);
      String storagePolicy=conf.get(HConstants.WAL_STORAGE_POLICY,HConstants.DEFAULT_WAL_STORAGE_POLICY);
      FSUtils.setStoragePolicy(fs,testDir,storagePolicy);
      String file=htu.getRandomUUID().toString();
      Path p=new Path(testDir,file);
      WriteDataToHDFS(fs,p,4096);
      HFileSystem hfs=new HFileSystem(fs);
      String policySet=hfs.getStoragePolicyName(p);
      LOG.debug("The storage policy of path " + p + " is "+ policySet);
      if (policy.equals(HConstants.DEFER_TO_HDFS_STORAGE_POLICY) || policy.equals(INVALID_STORAGE_POLICY)) {
        String hdfsDefaultPolicy=hfs.getStoragePolicyName(hfs.getHomeDirectory());
        LOG.debug("The default hdfs storage policy (indicated by home path: " + hfs.getHomeDirectory() + ") is "+ hdfsDefaultPolicy);
        Assert.assertEquals(hdfsDefaultPolicy,policySet);
      }
 else {
        Assert.assertEquals(policy,policySet);
      }
      cleanupFile(fs,testDir);
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Ugly test that ensures we can get at the hedged read counters in dfsclient. Does a bit of preading with hedged reads enabled using code taken from hdfs TestPread.
 * @throws Exception
 */
  @Test public void testDFSHedgedReadMetrics() throws Exception {
    conf.setInt(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THREADPOOL_SIZE,5);
    conf.setLong(DFSConfigKeys.DFS_DFSCLIENT_HEDGED_READ_THRESHOLD_MILLIS,0);
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,4096);
    conf.setLong(DFSConfigKeys.DFS_CLIENT_READ_PREFETCH_SIZE_KEY,4096);
    conf.setInt(DFSConfigKeys.DFS_CLIENT_RETRY_WINDOW_BASE,0);
    conf.setBoolean("dfs.datanode.transferTo.allowed",false);
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
    DFSHedgedReadMetrics metrics=FSUtils.getDFSHedgedReadMetrics(conf);
    assertEquals(0,metrics.getHedgedReadOps());
    FileSystem fileSys=cluster.getFileSystem();
    try {
      Path p=new Path("preadtest.dat");
      DFSTestUtil.createFile(fileSys,p,12 * blockSize,12 * blockSize,blockSize,(short)3,seed);
      pReadFile(fileSys,p);
      cleanupFile(fileSys,p);
      assertTrue(metrics.getHedgedReadOps() > 0);
    }
  finally {
      fileSys.close();
      cluster.shutdown();
    }
  }
  @Test public void testCopyFilesParallel() throws Exception {
    MiniDFSCluster cluster=htu.startMiniDFSCluster(1);
    cluster.waitActive();
    FileSystem fs=cluster.getFileSystem();
    Path src=new Path("/src");
    fs.mkdirs(src);
    for (int i=0; i < 50; i++) {
      WriteDataToHDFS(fs,new Path(src,String.valueOf(i)),1024);
    }
    Path sub=new Path(src,"sub");
    fs.mkdirs(sub);
    for (int i=0; i < 50; i++) {
      WriteDataToHDFS(fs,new Path(sub,String.valueOf(i)),1024);
    }
    Path dst=new Path("/dst");
    List<Path> allFiles=FSUtils.copyFilesParallel(fs,src,fs,dst,conf,4);
    assertEquals(102,allFiles.size());
    FileStatus[] list=fs.listStatus(dst);
    assertEquals(51,list.length);
    FileStatus[] sublist=fs.listStatus(new Path(dst,"sub"));
    assertEquals(50,sublist.length);
  }
  static final int blockSize=4096;
  static final long seed=0xDEADBEEFL;
  private void pReadFile(  FileSystem fileSys,  Path name) throws IOException {
    FSDataInputStream stm=fileSys.open(name);
    byte[] expected=new byte[12 * blockSize];
    Random rand=new Random(seed);
    rand.nextBytes(expected);
    byte[] actual=new byte[4096];
    stm.readFully(actual);
    checkAndEraseData(actual,0,expected,"Read Sanity Test");
    actual=new byte[8192];
    doPread(stm,0L,actual,0,8192);
    checkAndEraseData(actual,0,expected,"Pread Test 1");
    actual=new byte[4096];
    stm.readFully(actual);
    checkAndEraseData(actual,4096,expected,"Pread Test 2");
    stm.readFully(blockSize - 2048,actual,0,4096);
    checkAndEraseData(actual,(blockSize - 2048),expected,"Pread Test 3");
    actual=new byte[blockSize + 4096];
    stm.readFully(blockSize - 2048,actual);
    checkAndEraseData(actual,(blockSize - 2048),expected,"Pread Test 4");
    actual=new byte[blockSize + 4096];
    stm.readFully(10 * blockSize - 2048,actual);
    checkAndEraseData(actual,(10 * blockSize - 2048),expected,"Pread Test 5");
    actual=new byte[4096];
    stm.readFully(actual);
    checkAndEraseData(actual,8192,expected,"Pread Test 6");
    stm.close();
    stm=fileSys.open(name);
    stm.readFully(1,actual,0,4096);
    stm.readFully(4 * blockSize,actual,0,4096);
    stm.readFully(7 * blockSize,actual,0,4096);
    actual=new byte[3 * 4096];
    stm.readFully(0 * blockSize,actual,0,3 * 4096);
    checkAndEraseData(actual,0,expected,"Pread Test 7");
    actual=new byte[8 * 4096];
    stm.readFully(3 * blockSize,actual,0,8 * 4096);
    checkAndEraseData(actual,3 * blockSize,expected,"Pread Test 8");
    stm.readFully(11 * blockSize + blockSize / 2,actual,0,blockSize / 2);
    IOException res=null;
    try {
      stm.readFully(11 * blockSize + blockSize / 2,actual,0,blockSize);
    }
 catch (    IOException e) {
      res=e;
    }
    assertTrue("Error reading beyond file boundary.",res != null);
    stm.close();
  }
  private void checkAndEraseData(  byte[] actual,  int from,  byte[] expected,  String message){
    for (int idx=0; idx < actual.length; idx++) {
      assertEquals(message + " byte " + (from + idx)+ " differs. expected "+ expected[from + idx]+ " actual "+ actual[idx],actual[idx],expected[from + idx]);
      actual[idx]=0;
    }
  }
  private void doPread(  FSDataInputStream stm,  long position,  byte[] buffer,  int offset,  int length) throws IOException {
    int nread=0;
    while (nread < length) {
      int nbytes=stm.read(position + nread,buffer,offset + nread,length - nread);
      assertTrue("Error in pread",nbytes > 0);
      nread+=nbytes;
    }
  }
  private void cleanupFile(  FileSystem fileSys,  Path name) throws IOException {
    assertTrue(fileSys.exists(name));
    assertTrue(fileSys.delete(name,true));
    assertTrue(!fileSys.exists(name));
  }
  private static final boolean STREAM_CAPABILITIES_IS_PRESENT;
static {
    boolean tmp=false;
    try {
      Class.forName("org.apache.hadoop.fs.StreamCapabilities");
      tmp=true;
      LOG.debug("Test thought StreamCapabilities class was present.");
    }
 catch (    ClassNotFoundException exception) {
      LOG.debug("Test didn't think StreamCapabilities class was present.");
    }
 finally {
      STREAM_CAPABILITIES_IS_PRESENT=tmp;
    }
  }
  @Test public void checkStreamCapabilitiesOnHdfsDataOutputStream() throws Exception {
    MiniDFSCluster cluster=htu.startMiniDFSCluster(1);
    try (FileSystem filesystem=cluster.getFileSystem()){
      FSDataOutputStream stream=filesystem.create(new Path("/tmp/foobar"));
      assertTrue(FSUtils.hasCapability(stream,"hsync"));
      assertTrue(FSUtils.hasCapability(stream,"hflush"));
      assertNotEquals("We expect HdfsDataOutputStream to say it has a dummy capability iff the " + "StreamCapabilities class is not defined.",STREAM_CAPABILITIES_IS_PRESENT,FSUtils.hasCapability(stream,"a capability that hopefully HDFS doesn't add."));
    }
  finally {
      cluster.shutdown();
    }
  }
}
