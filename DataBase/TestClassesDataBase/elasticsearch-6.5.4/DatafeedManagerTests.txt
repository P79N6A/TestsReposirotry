public class DatafeedManagerTests extends ESTestCase {
  private ClusterService clusterService;
  private ThreadPool threadPool;
  private DatafeedJob datafeedJob;
  private DatafeedManager datafeedManager;
  private long currentTime=120000;
  private Auditor auditor;
  private ArgumentCaptor<ClusterStateListener> capturedClusterStateListener=ArgumentCaptor.forClass(ClusterStateListener.class);
  @Before @SuppressWarnings("unchecked") public void setUpTests(){
    MlMetadata.Builder mlMetadata=new MlMetadata.Builder();
    Job job=createDatafeedJob().build(new Date());
    mlMetadata.putJob(job,false);
    DatafeedConfig datafeed=createDatafeedConfig("datafeed_id",job.getId()).build();
    mlMetadata.putDatafeed(datafeed,Collections.emptyMap());
    PersistentTasksCustomMetaData.Builder tasksBuilder=PersistentTasksCustomMetaData.builder();
    addJobTask(job.getId(),"node_id",JobState.OPENED,tasksBuilder);
    PersistentTasksCustomMetaData tasks=tasksBuilder.build();
    DiscoveryNodes nodes=DiscoveryNodes.builder().add(new DiscoveryNode("node_name","node_id",new TransportAddress(InetAddress.getLoopbackAddress(),9300),Collections.emptyMap(),Collections.emptySet(),Version.CURRENT)).build();
    ClusterState.Builder cs=ClusterState.builder(new ClusterName("cluster_name")).metaData(new MetaData.Builder().putCustom(MlMetadata.TYPE,mlMetadata.build()).putCustom(PersistentTasksCustomMetaData.TYPE,tasks)).nodes(nodes);
    clusterService=mock(ClusterService.class);
    when(clusterService.state()).thenReturn(cs.build());
    ArgumentCaptor<XContentBuilder> argumentCaptor=ArgumentCaptor.forClass(XContentBuilder.class);
    Client client=new MockClientBuilder("foo").prepareIndex(AuditorField.NOTIFICATIONS_INDEX,AuditMessage.TYPE.getPreferredName(),"responseId",argumentCaptor).build();
    DiscoveryNode dNode=mock(DiscoveryNode.class);
    when(dNode.getName()).thenReturn("this_node_has_a_name");
    when(clusterService.localNode()).thenReturn(dNode);
    auditor=mock(Auditor.class);
    auditor=mock(Auditor.class);
    threadPool=mock(ThreadPool.class);
    when(threadPool.getThreadContext()).thenReturn(new ThreadContext(Settings.EMPTY));
    ExecutorService executorService=mock(ExecutorService.class);
    doAnswer(invocation -> {
      ((Runnable)invocation.getArguments()[0]).run();
      return null;
    }
).when(executorService).submit(any(Runnable.class));
    when(threadPool.executor(MachineLearning.DATAFEED_THREAD_POOL_NAME)).thenReturn(executorService);
    when(threadPool.executor(ThreadPool.Names.GENERIC)).thenReturn(executorService);
    datafeedJob=mock(DatafeedJob.class);
    when(datafeedJob.isRunning()).thenReturn(true);
    when(datafeedJob.stop()).thenReturn(true);
    DatafeedJobBuilder datafeedJobBuilder=mock(DatafeedJobBuilder.class);
    doAnswer(invocationOnMock -> {
      @SuppressWarnings("rawtypes") ActionListener listener=(ActionListener)invocationOnMock.getArguments()[2];
      listener.onResponse(datafeedJob);
      return null;
    }
).when(datafeedJobBuilder).build(any(),any(),any());
    datafeedManager=new DatafeedManager(threadPool,client,clusterService,datafeedJobBuilder,() -> currentTime,auditor);
    verify(clusterService).addListener(capturedClusterStateListener.capture());
  }
  public void testLookbackOnly_WarnsWhenNoDataIsRetrieved() throws Exception {
    when(datafeedJob.runLookBack(0L,60000L)).thenThrow(new DatafeedJob.EmptyDataCountException(0L));
    Consumer<Exception> handler=mockConsumer();
    DatafeedTask task=createDatafeedTask("datafeed_id",0L,60000L);
    datafeedManager.run(task,handler);
    verify(threadPool,times(1)).executor(MachineLearning.DATAFEED_THREAD_POOL_NAME);
    verify(threadPool,never()).schedule(any(),any(),any());
    verify(auditor).warning("job_id","Datafeed lookback retrieved no data");
  }
  public void testStart_GivenNewlyCreatedJobLookback() throws Exception {
    when(datafeedJob.runLookBack(0L,60000L)).thenReturn(null);
    Consumer<Exception> handler=mockConsumer();
    DatafeedTask task=createDatafeedTask("datafeed_id",0L,60000L);
    datafeedManager.run(task,handler);
    verify(threadPool,times(1)).executor(MachineLearning.DATAFEED_THREAD_POOL_NAME);
    verify(threadPool,never()).schedule(any(),any(),any());
  }
  public void testStart_extractionProblem() throws Exception {
    when(datafeedJob.runLookBack(0,60000L)).thenThrow(new DatafeedJob.ExtractionProblemException(0L,new RuntimeException("dummy")));
    Consumer<Exception> handler=mockConsumer();
    DatafeedTask task=createDatafeedTask("datafeed_id",0L,60000L);
    datafeedManager.run(task,handler);
    verify(threadPool,times(1)).executor(MachineLearning.DATAFEED_THREAD_POOL_NAME);
    verify(threadPool,never()).schedule(any(),any(),any());
    verify(auditor,times(1)).error(eq("job_id"),anyString());
  }
  public void testStart_emptyDataCountException() throws Exception {
    currentTime=6000000;
    int[] counter=new int[]{0};
    doAnswer(invocationOnMock -> {
      if (counter[0]++ < 10) {
        Runnable r=(Runnable)invocationOnMock.getArguments()[2];
        currentTime+=600000;
        r.run();
      }
      return mock(ScheduledFuture.class);
    }
).when(threadPool).schedule(any(),any(),any());
    when(datafeedJob.runLookBack(anyLong(),anyLong())).thenThrow(new DatafeedJob.EmptyDataCountException(0L));
    when(datafeedJob.runRealtime()).thenThrow(new DatafeedJob.EmptyDataCountException(0L));
    Consumer<Exception> handler=mockConsumer();
    DatafeedTask task=createDatafeedTask("datafeed_id",0L,null);
    datafeedManager.run(task,handler);
    verify(threadPool,times(11)).schedule(any(),eq(MachineLearning.DATAFEED_THREAD_POOL_NAME),any());
    verify(auditor,times(1)).warning(eq("job_id"),anyString());
  }
  public void testRealTime_GivenStoppingAnalysisProblem() throws Exception {
    Exception cause=new RuntimeException("stopping");
    when(datafeedJob.runLookBack(anyLong(),anyLong())).thenThrow(new DatafeedJob.AnalysisProblemException(0L,true,cause));
    Consumer<Exception> handler=mockConsumer();
    StartDatafeedAction.DatafeedParams params=new StartDatafeedAction.DatafeedParams("datafeed_id",0L);
    DatafeedTask task=TransportStartDatafeedActionTests.createDatafeedTask(1,"type","action",null,params,datafeedManager);
    task=spyDatafeedTask(task);
    datafeedManager.run(task,handler);
    ArgumentCaptor<DatafeedJob.AnalysisProblemException> analysisProblemCaptor=ArgumentCaptor.forClass(DatafeedJob.AnalysisProblemException.class);
    verify(handler).accept(analysisProblemCaptor.capture());
    assertThat(analysisProblemCaptor.getValue().getCause(),equalTo(cause));
    verify(auditor).error("job_id","Datafeed is encountering errors submitting data for analysis: stopping");
    assertThat(datafeedManager.isRunning(task.getAllocationId()),is(false));
  }
  public void testRealTime_GivenNonStoppingAnalysisProblem() throws Exception {
    Exception cause=new RuntimeException("non-stopping");
    when(datafeedJob.runLookBack(anyLong(),anyLong())).thenThrow(new DatafeedJob.AnalysisProblemException(0L,false,cause));
    Consumer<Exception> handler=mockConsumer();
    StartDatafeedAction.DatafeedParams params=new StartDatafeedAction.DatafeedParams("datafeed_id",0L);
    DatafeedTask task=TransportStartDatafeedActionTests.createDatafeedTask(1,"type","action",null,params,datafeedManager);
    task=spyDatafeedTask(task);
    datafeedManager.run(task,handler);
    verify(auditor).error("job_id","Datafeed is encountering errors submitting data for analysis: non-stopping");
    assertThat(datafeedManager.isRunning(task.getAllocationId()),is(true));
  }
  public void testStart_GivenNewlyCreatedJobLoopBackAndRealtime() throws Exception {
    when(datafeedJob.runLookBack(anyLong(),anyLong())).thenReturn(1L);
    when(datafeedJob.runRealtime()).thenReturn(1L);
    Consumer<Exception> handler=mockConsumer();
    boolean cancelled=randomBoolean();
    StartDatafeedAction.DatafeedParams params=new StartDatafeedAction.DatafeedParams("datafeed_id",0L);
    DatafeedTask task=TransportStartDatafeedActionTests.createDatafeedTask(1,"type","action",null,params,datafeedManager);
    task=spyDatafeedTask(task);
    datafeedManager.run(task,handler);
    verify(threadPool,times(1)).executor(MachineLearning.DATAFEED_THREAD_POOL_NAME);
    if (cancelled) {
      task.stop("test",StopDatafeedAction.DEFAULT_TIMEOUT);
      verify(handler).accept(null);
      assertThat(datafeedManager.isRunning(task.getAllocationId()),is(false));
    }
 else {
      verify(threadPool,times(1)).schedule(eq(new TimeValue(1)),eq(MachineLearning.DATAFEED_THREAD_POOL_NAME),any());
      assertThat(datafeedManager.isRunning(task.getAllocationId()),is(true));
    }
  }
  public void testDatafeedTaskWaitsUntilJobIsOpened(){
    PersistentTasksCustomMetaData.Builder tasksBuilder=PersistentTasksCustomMetaData.builder();
    addJobTask("job_id","node_id",JobState.OPENING,tasksBuilder);
    ClusterState.Builder cs=ClusterState.builder(clusterService.state()).metaData(new MetaData.Builder().putCustom(MlMetadata.TYPE,MlMetadata.getMlMetadata(clusterService.state())).putCustom(PersistentTasksCustomMetaData.TYPE,tasksBuilder.build()));
    when(clusterService.state()).thenReturn(cs.build());
    Consumer<Exception> handler=mockConsumer();
    DatafeedTask task=createDatafeedTask("datafeed_id",0L,60000L);
    datafeedManager.run(task,handler);
    verify(threadPool,never()).executor(MachineLearning.DATAFEED_THREAD_POOL_NAME);
    tasksBuilder=PersistentTasksCustomMetaData.builder();
    addJobTask("job_id","node_id",JobState.OPENING,tasksBuilder);
    addJobTask("another_job","node_id",JobState.OPENED,tasksBuilder);
    ClusterState.Builder anotherJobCs=ClusterState.builder(clusterService.state()).metaData(new MetaData.Builder().putCustom(MlMetadata.TYPE,MlMetadata.getMlMetadata(clusterService.state())).putCustom(PersistentTasksCustomMetaData.TYPE,tasksBuilder.build()));
    capturedClusterStateListener.getValue().clusterChanged(new ClusterChangedEvent("_source",anotherJobCs.build(),cs.build()));
    verify(threadPool,never()).executor(MachineLearning.DATAFEED_THREAD_POOL_NAME);
    tasksBuilder=PersistentTasksCustomMetaData.builder();
    addJobTask("job_id","node_id",JobState.OPENED,tasksBuilder);
    ClusterState.Builder jobOpenedCs=ClusterState.builder(clusterService.state()).metaData(new MetaData.Builder().putCustom(MlMetadata.TYPE,MlMetadata.getMlMetadata(clusterService.state())).putCustom(PersistentTasksCustomMetaData.TYPE,tasksBuilder.build()));
    capturedClusterStateListener.getValue().clusterChanged(new ClusterChangedEvent("_source",jobOpenedCs.build(),anotherJobCs.build()));
    verify(threadPool,times(1)).executor(MachineLearning.DATAFEED_THREAD_POOL_NAME);
  }
  public void testDatafeedTaskStopsBecauseJobFailedWhileOpening(){
    PersistentTasksCustomMetaData.Builder tasksBuilder=PersistentTasksCustomMetaData.builder();
    addJobTask("job_id","node_id",JobState.OPENING,tasksBuilder);
    ClusterState.Builder cs=ClusterState.builder(clusterService.state()).metaData(new MetaData.Builder().putCustom(MlMetadata.TYPE,MlMetadata.getMlMetadata(clusterService.state())).putCustom(PersistentTasksCustomMetaData.TYPE,tasksBuilder.build()));
    when(clusterService.state()).thenReturn(cs.build());
    Consumer<Exception> handler=mockConsumer();
    DatafeedTask task=createDatafeedTask("datafeed_id",0L,60000L);
    datafeedManager.run(task,handler);
    verify(threadPool,never()).executor(MachineLearning.DATAFEED_THREAD_POOL_NAME);
    tasksBuilder=PersistentTasksCustomMetaData.builder();
    addJobTask("job_id","node_id",JobState.FAILED,tasksBuilder);
    ClusterState.Builder updatedCs=ClusterState.builder(clusterService.state()).metaData(new MetaData.Builder().putCustom(MlMetadata.TYPE,MlMetadata.getMlMetadata(clusterService.state())).putCustom(PersistentTasksCustomMetaData.TYPE,tasksBuilder.build()));
    capturedClusterStateListener.getValue().clusterChanged(new ClusterChangedEvent("_source",updatedCs.build(),cs.build()));
    verify(threadPool,never()).executor(MachineLearning.DATAFEED_THREAD_POOL_NAME);
    verify(task).stop("job_never_opened",TimeValue.timeValueSeconds(20));
  }
  public void testDatafeedGetsStoppedWhileWaitingForJobToOpen(){
    PersistentTasksCustomMetaData.Builder tasksBuilder=PersistentTasksCustomMetaData.builder();
    addJobTask("job_id","node_id",JobState.OPENING,tasksBuilder);
    ClusterState.Builder cs=ClusterState.builder(clusterService.state()).metaData(new MetaData.Builder().putCustom(MlMetadata.TYPE,MlMetadata.getMlMetadata(clusterService.state())).putCustom(PersistentTasksCustomMetaData.TYPE,tasksBuilder.build()));
    when(clusterService.state()).thenReturn(cs.build());
    Consumer<Exception> handler=mockConsumer();
    DatafeedTask task=createDatafeedTask("datafeed_id",0L,60000L);
    datafeedManager.run(task,handler);
    verify(threadPool,never()).executor(MachineLearning.DATAFEED_THREAD_POOL_NAME);
    datafeedManager.stopDatafeed(task,"test",StopDatafeedAction.DEFAULT_TIMEOUT);
    tasksBuilder=PersistentTasksCustomMetaData.builder();
    addJobTask("job_id","node_id",JobState.OPENED,tasksBuilder);
    ClusterState.Builder updatedCs=ClusterState.builder(clusterService.state()).metaData(new MetaData.Builder().putCustom(MlMetadata.TYPE,MlMetadata.getMlMetadata(clusterService.state())).putCustom(PersistentTasksCustomMetaData.TYPE,tasksBuilder.build()));
    capturedClusterStateListener.getValue().clusterChanged(new ClusterChangedEvent("_source",cs.build(),updatedCs.build()));
    verify(threadPool,never()).executor(MachineLearning.DATAFEED_THREAD_POOL_NAME);
  }
  public static DatafeedConfig.Builder createDatafeedConfig(  String datafeedId,  String jobId){
    DatafeedConfig.Builder datafeedConfig=new DatafeedConfig.Builder(datafeedId,jobId);
    datafeedConfig.setIndices(Collections.singletonList("myIndex"));
    datafeedConfig.setTypes(Collections.singletonList("myType"));
    return datafeedConfig;
  }
  public static Job.Builder createDatafeedJob(){
    AnalysisConfig.Builder acBuilder=new AnalysisConfig.Builder(Collections.singletonList(new Detector.Builder("metric","field").build()));
    acBuilder.setBucketSpan(TimeValue.timeValueHours(1));
    acBuilder.setDetectors(Collections.singletonList(new Detector.Builder("metric","field").build()));
    Job.Builder builder=new Job.Builder("job_id");
    builder.setAnalysisConfig(acBuilder);
    builder.setDataDescription(new DataDescription.Builder());
    return builder;
  }
  private static DatafeedTask createDatafeedTask(  String datafeedId,  long startTime,  Long endTime){
    DatafeedTask task=mock(DatafeedTask.class);
    when(task.getDatafeedId()).thenReturn(datafeedId);
    when(task.getDatafeedStartTime()).thenReturn(startTime);
    when(task.getEndTime()).thenReturn(endTime);
    doAnswer(invocationOnMock -> {
      @SuppressWarnings("rawtypes") ActionListener listener=(ActionListener)invocationOnMock.getArguments()[1];
      listener.onResponse(mock(PersistentTask.class));
      return null;
    }
).when(task).updatePersistentTaskState(any(),any());
    return task;
  }
  @SuppressWarnings("unchecked") private Consumer<Exception> mockConsumer(){
    return mock(Consumer.class);
  }
  private DatafeedTask spyDatafeedTask(  DatafeedTask task){
    task=spy(task);
    doAnswer(invocationOnMock -> {
      @SuppressWarnings("rawtypes") ActionListener listener=(ActionListener)invocationOnMock.getArguments()[1];
      listener.onResponse(mock(PersistentTask.class));
      return null;
    }
).when(task).updatePersistentTaskState(any(),any());
    return task;
  }
}
