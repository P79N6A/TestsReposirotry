public class ProducerBatchTest {
  private final long now=1488748346917L;
  private final MemoryRecordsBuilder memoryRecordsBuilder=MemoryRecords.builder(ByteBuffer.allocate(128),CompressionType.NONE,TimestampType.CREATE_TIME,128);
  @Test public void testChecksumNullForMagicV2(){
    ProducerBatch batch=new ProducerBatch(new TopicPartition("topic",1),memoryRecordsBuilder,now);
    FutureRecordMetadata future=batch.tryAppend(now,null,new byte[10],Record.EMPTY_HEADERS,null,now);
    assertNotNull(future);
    assertNull(future.checksumOrNull());
  }
  @Test public void testBatchAbort() throws Exception {
    ProducerBatch batch=new ProducerBatch(new TopicPartition("topic",1),memoryRecordsBuilder,now);
    MockCallback callback=new MockCallback();
    FutureRecordMetadata future=batch.tryAppend(now,null,new byte[10],Record.EMPTY_HEADERS,callback,now);
    KafkaException exception=new KafkaException();
    batch.abort(exception);
    assertTrue(future.isDone());
    assertEquals(1,callback.invocations);
    assertEquals(exception,callback.exception);
    assertNull(callback.metadata);
    assertFalse(batch.done(500L,2342342341L,null));
    assertFalse(batch.done(-1,-1,new KafkaException()));
    assertEquals(1,callback.invocations);
    assertTrue(future.isDone());
    try {
      future.get();
      fail("Future should have thrown");
    }
 catch (    ExecutionException e) {
      assertEquals(exception,e.getCause());
    }
  }
  @Test public void testBatchCannotAbortTwice() throws Exception {
    ProducerBatch batch=new ProducerBatch(new TopicPartition("topic",1),memoryRecordsBuilder,now);
    MockCallback callback=new MockCallback();
    FutureRecordMetadata future=batch.tryAppend(now,null,new byte[10],Record.EMPTY_HEADERS,callback,now);
    KafkaException exception=new KafkaException();
    batch.abort(exception);
    assertEquals(1,callback.invocations);
    assertEquals(exception,callback.exception);
    assertNull(callback.metadata);
    try {
      batch.abort(new KafkaException());
      fail("Expected exception from abort");
    }
 catch (    IllegalStateException e) {
    }
    assertEquals(1,callback.invocations);
    assertTrue(future.isDone());
    try {
      future.get();
      fail("Future should have thrown");
    }
 catch (    ExecutionException e) {
      assertEquals(exception,e.getCause());
    }
  }
  @Test public void testBatchCannotCompleteTwice() throws Exception {
    ProducerBatch batch=new ProducerBatch(new TopicPartition("topic",1),memoryRecordsBuilder,now);
    MockCallback callback=new MockCallback();
    FutureRecordMetadata future=batch.tryAppend(now,null,new byte[10],Record.EMPTY_HEADERS,callback,now);
    batch.done(500L,10L,null);
    assertEquals(1,callback.invocations);
    assertNull(callback.exception);
    assertNotNull(callback.metadata);
    try {
      batch.done(1000L,20L,null);
      fail("Expected exception from done");
    }
 catch (    IllegalStateException e) {
    }
    RecordMetadata recordMetadata=future.get();
    assertEquals(500L,recordMetadata.offset());
    assertEquals(10L,recordMetadata.timestamp());
  }
  @Test public void testAppendedChecksumMagicV0AndV1(){
    for (    byte magic : Arrays.asList(MAGIC_VALUE_V0,MAGIC_VALUE_V1)) {
      MemoryRecordsBuilder builder=MemoryRecords.builder(ByteBuffer.allocate(128),magic,CompressionType.NONE,TimestampType.CREATE_TIME,0L);
      ProducerBatch batch=new ProducerBatch(new TopicPartition("topic",1),builder,now);
      byte[] key="hi".getBytes();
      byte[] value="there".getBytes();
      FutureRecordMetadata future=batch.tryAppend(now,key,value,Record.EMPTY_HEADERS,null,now);
      assertNotNull(future);
      byte attributes=LegacyRecord.computeAttributes(magic,CompressionType.NONE,TimestampType.CREATE_TIME);
      long expectedChecksum=LegacyRecord.computeChecksum(magic,attributes,now,key,value);
      assertEquals(expectedChecksum,future.checksumOrNull().longValue());
    }
  }
  @Test public void testSplitPreservesHeaders(){
    for (    CompressionType compressionType : CompressionType.values()) {
      MemoryRecordsBuilder builder=MemoryRecords.builder(ByteBuffer.allocate(1024),MAGIC_VALUE_V2,compressionType,TimestampType.CREATE_TIME,0L);
      ProducerBatch batch=new ProducerBatch(new TopicPartition("topic",1),builder,now);
      Header header=new RecordHeader("header-key","header-value".getBytes());
      while (true) {
        FutureRecordMetadata future=batch.tryAppend(now,"hi".getBytes(),"there".getBytes(),new Header[]{header},null,now);
        if (future == null) {
          break;
        }
      }
      Deque<ProducerBatch> batches=batch.split(200);
      assertTrue("This batch should be split to multiple small batches.",batches.size() >= 2);
      for (      ProducerBatch splitProducerBatch : batches) {
        for (        RecordBatch splitBatch : splitProducerBatch.records().batches()) {
          for (          Record record : splitBatch) {
            assertTrue("Header size should be 1.",record.headers().length == 1);
            assertTrue("Header key should be 'header-key'.",record.headers()[0].key().equals("header-key"));
            assertTrue("Header value should be 'header-value'.",new String(record.headers()[0].value()).equals("header-value"));
          }
        }
      }
    }
  }
  @Test public void testSplitPreservesMagicAndCompressionType(){
    for (    byte magic : Arrays.asList(MAGIC_VALUE_V0,MAGIC_VALUE_V1,MAGIC_VALUE_V2)) {
      for (      CompressionType compressionType : CompressionType.values()) {
        if (compressionType == CompressionType.NONE && magic < MAGIC_VALUE_V2)         continue;
        if (compressionType == CompressionType.ZSTD && magic < MAGIC_VALUE_V2)         continue;
        MemoryRecordsBuilder builder=MemoryRecords.builder(ByteBuffer.allocate(1024),magic,compressionType,TimestampType.CREATE_TIME,0L);
        ProducerBatch batch=new ProducerBatch(new TopicPartition("topic",1),builder,now);
        while (true) {
          FutureRecordMetadata future=batch.tryAppend(now,"hi".getBytes(),"there".getBytes(),Record.EMPTY_HEADERS,null,now);
          if (future == null)           break;
        }
        Deque<ProducerBatch> batches=batch.split(512);
        assertTrue(batches.size() >= 2);
        for (        ProducerBatch splitProducerBatch : batches) {
          assertEquals(magic,splitProducerBatch.magic());
          assertTrue(splitProducerBatch.isSplitBatch());
          for (          RecordBatch splitBatch : splitProducerBatch.records().batches()) {
            assertEquals(magic,splitBatch.magic());
            assertEquals(0L,splitBatch.baseOffset());
            assertEquals(compressionType,splitBatch.compressionType());
          }
        }
      }
    }
  }
  /** 
 * A  {@link ProducerBatch} configured using a timestamp preceding its create time is interpreted correctlyas not expired by  {@link ProducerBatch#hasReachedDeliveryTimeout(long,long)}.
 */
  @Test public void testBatchExpiration(){
    long deliveryTimeoutMs=10240;
    ProducerBatch batch=new ProducerBatch(new TopicPartition("topic",1),memoryRecordsBuilder,now);
    assertFalse(batch.hasReachedDeliveryTimeout(deliveryTimeoutMs,now - 2));
    assertTrue(batch.hasReachedDeliveryTimeout(deliveryTimeoutMs,now + deliveryTimeoutMs));
  }
  /** 
 * A  {@link ProducerBatch} configured using a timestamp preceding its create time is interpreted correctly* as not expired by  {@link ProducerBatch#hasReachedDeliveryTimeout(long,long)}.
 */
  @Test public void testBatchExpirationAfterReenqueue(){
    ProducerBatch batch=new ProducerBatch(new TopicPartition("topic",1),memoryRecordsBuilder,now);
    batch.reenqueued(now);
    assertFalse(batch.hasReachedDeliveryTimeout(10240,now - 2L));
  }
  @Test public void testShouldNotAttemptAppendOnceRecordsBuilderIsClosedForAppends(){
    ProducerBatch batch=new ProducerBatch(new TopicPartition("topic",1),memoryRecordsBuilder,now);
    FutureRecordMetadata result0=batch.tryAppend(now,null,new byte[10],Record.EMPTY_HEADERS,null,now);
    assertNotNull(result0);
    assertTrue(memoryRecordsBuilder.hasRoomFor(now,null,new byte[10],Record.EMPTY_HEADERS));
    memoryRecordsBuilder.closeForRecordAppends();
    assertFalse(memoryRecordsBuilder.hasRoomFor(now,null,new byte[10],Record.EMPTY_HEADERS));
    assertEquals(null,batch.tryAppend(now + 1,null,new byte[10],Record.EMPTY_HEADERS,null,now + 1));
  }
private static class MockCallback implements Callback {
    private int invocations=0;
    private RecordMetadata metadata;
    private Exception exception;
    @Override public void onCompletion(    RecordMetadata metadata,    Exception exception){
      invocations++;
      this.metadata=metadata;
      this.exception=exception;
    }
  }
}
