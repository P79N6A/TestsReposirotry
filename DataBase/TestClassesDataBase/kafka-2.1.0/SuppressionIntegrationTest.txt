@Category({IntegrationTest.class}) public class SuppressionIntegrationTest {
  @ClassRule public static final EmbeddedKafkaCluster CLUSTER=new EmbeddedKafkaCluster(1,mkProperties(mkMap()),0L);
  private static final StringDeserializer STRING_DESERIALIZER=new StringDeserializer();
  private static final StringSerializer STRING_SERIALIZER=new StringSerializer();
  private static final Serde<String> STRING_SERDE=Serdes.String();
  private static final LongDeserializer LONG_DESERIALIZER=new LongDeserializer();
  private static final int COMMIT_INTERVAL=100;
  private static final long TIMEOUT_MS=30_000L;
  @Test public void shouldSuppressIntermediateEventsWithEmitAfter(){
    final String testId="-shouldSuppressIntermediateEventsWithEmitAfter";
    final String appId=getClass().getSimpleName().toLowerCase(Locale.getDefault()) + testId;
    final String input="input" + testId;
    final String outputSuppressed="output-suppressed" + testId;
    final String outputRaw="output-raw" + testId;
    cleanStateBeforeTest(CLUSTER,input,outputSuppressed,outputRaw);
    final StreamsBuilder builder=new StreamsBuilder();
    final KTable<String,Long> valueCounts=buildCountsTable(input,builder);
    valueCounts.suppress(untilTimeLimit(ofMillis(scaledTime(2L)),unbounded())).toStream().to(outputSuppressed,Produced.with(STRING_SERDE,Serdes.Long()));
    valueCounts.toStream().to(outputRaw,Produced.with(STRING_SERDE,Serdes.Long()));
    final Properties streamsConfig=getStreamsConfig(appId);
    final KafkaStreams driver=IntegrationTestUtils.getStartedStreams(streamsConfig,builder,true);
    try {
      produceSynchronously(input,asList(new KeyValueTimestamp<>("k1","v1",scaledTime(0L)),new KeyValueTimestamp<>("k1","v2",scaledTime(1L)),new KeyValueTimestamp<>("k2","v1",scaledTime(2L)),new KeyValueTimestamp<>("tick","tick",scaledTime(5L))));
      verifyOutput(outputRaw,asList(new KeyValueTimestamp<>("v1",1L,scaledTime(0L)),new KeyValueTimestamp<>("v1",0L,scaledTime(1L)),new KeyValueTimestamp<>("v2",1L,scaledTime(1L)),new KeyValueTimestamp<>("v1",1L,scaledTime(2L)),new KeyValueTimestamp<>("tick",1L,scaledTime(5L))));
      verifyOutput(outputSuppressed,asList(new KeyValueTimestamp<>("v1",1L,scaledTime(2L)),new KeyValueTimestamp<>("v2",1L,scaledTime(1L))));
    }
  finally {
      driver.close();
      cleanStateAfterTest(CLUSTER,driver);
    }
  }
  private KTable<String,Long> buildCountsTable(  final String input,  final StreamsBuilder builder){
    return builder.table(input,Consumed.with(STRING_SERDE,STRING_SERDE),Materialized.<String,String,KeyValueStore<Bytes,byte[]>>with(STRING_SERDE,STRING_SERDE).withCachingDisabled().withLoggingDisabled()).groupBy((k,v) -> new KeyValue<>(v,k),Grouped.with(STRING_SERDE,STRING_SERDE)).count(Materialized.<String,Long,KeyValueStore<Bytes,byte[]>>as("counts").withCachingDisabled());
  }
  @Test public void shouldNotSuppressIntermediateEventsWithZeroEmitAfter(){
    final String testId="-shouldNotSuppressIntermediateEventsWithZeroEmitAfter";
    final String appId=getClass().getSimpleName().toLowerCase(Locale.getDefault()) + testId;
    final String input="input" + testId;
    final String outputSuppressed="output-suppressed" + testId;
    final String outputRaw="output-raw" + testId;
    cleanStateBeforeTest(CLUSTER,input,outputSuppressed,outputRaw);
    final StreamsBuilder builder=new StreamsBuilder();
    final KTable<String,Long> valueCounts=buildCountsTable(input,builder);
    valueCounts.suppress(untilTimeLimit(Duration.ZERO,unbounded())).toStream().to(outputSuppressed,Produced.with(STRING_SERDE,Serdes.Long()));
    valueCounts.toStream().to(outputRaw,Produced.with(STRING_SERDE,Serdes.Long()));
    final Properties streamsConfig=getStreamsConfig(appId);
    final KafkaStreams driver=IntegrationTestUtils.getStartedStreams(streamsConfig,builder,true);
    try {
      produceSynchronously(input,asList(new KeyValueTimestamp<>("k1","v1",scaledTime(0L)),new KeyValueTimestamp<>("k1","v2",scaledTime(1L)),new KeyValueTimestamp<>("k2","v1",scaledTime(2L)),new KeyValueTimestamp<>("x","x",scaledTime(4L))));
      verifyOutput(outputRaw,asList(new KeyValueTimestamp<>("v1",1L,scaledTime(0L)),new KeyValueTimestamp<>("v1",0L,scaledTime(1L)),new KeyValueTimestamp<>("v2",1L,scaledTime(1L)),new KeyValueTimestamp<>("v1",1L,scaledTime(2L)),new KeyValueTimestamp<>("x",1L,scaledTime(4L))));
      verifyOutput(outputSuppressed,asList(new KeyValueTimestamp<>("v1",1L,scaledTime(0L)),new KeyValueTimestamp<>("v1",0L,scaledTime(1L)),new KeyValueTimestamp<>("v2",1L,scaledTime(1L)),new KeyValueTimestamp<>("v1",1L,scaledTime(2L)),new KeyValueTimestamp<>("x",1L,scaledTime(4L))));
    }
  finally {
      driver.close();
      cleanStateAfterTest(CLUSTER,driver);
    }
  }
  @Test public void shouldSuppressIntermediateEventsWithRecordLimit(){
    final String testId="-shouldSuppressIntermediateEventsWithRecordLimit";
    final String appId=getClass().getSimpleName().toLowerCase(Locale.getDefault()) + testId;
    final String input="input" + testId;
    final String outputSuppressed="output-suppressed" + testId;
    final String outputRaw="output-raw" + testId;
    cleanStateBeforeTest(CLUSTER,input,outputRaw,outputSuppressed);
    final StreamsBuilder builder=new StreamsBuilder();
    final KTable<String,Long> valueCounts=buildCountsTable(input,builder);
    valueCounts.suppress(untilTimeLimit(ofMillis(MAX_VALUE),maxRecords(1L).emitEarlyWhenFull())).toStream().to(outputSuppressed,Produced.with(STRING_SERDE,Serdes.Long()));
    valueCounts.toStream().to(outputRaw,Produced.with(STRING_SERDE,Serdes.Long()));
    final Properties streamsConfig=getStreamsConfig(appId);
    final KafkaStreams driver=IntegrationTestUtils.getStartedStreams(streamsConfig,builder,true);
    try {
      produceSynchronously(input,asList(new KeyValueTimestamp<>("k1","v1",scaledTime(0L)),new KeyValueTimestamp<>("k1","v2",scaledTime(1L)),new KeyValueTimestamp<>("k2","v1",scaledTime(2L)),new KeyValueTimestamp<>("x","x",scaledTime(3L))));
      verifyOutput(outputRaw,asList(new KeyValueTimestamp<>("v1",1L,scaledTime(0L)),new KeyValueTimestamp<>("v1",0L,scaledTime(1L)),new KeyValueTimestamp<>("v2",1L,scaledTime(1L)),new KeyValueTimestamp<>("v1",1L,scaledTime(2L)),new KeyValueTimestamp<>("x",1L,scaledTime(3L))));
      verifyOutput(outputSuppressed,asList(new KeyValueTimestamp<>("v1",0L,scaledTime(1L)),new KeyValueTimestamp<>("v2",1L,scaledTime(1L)),new KeyValueTimestamp<>("v1",1L,scaledTime(2L))));
    }
  finally {
      driver.close();
      cleanStateAfterTest(CLUSTER,driver);
    }
  }
  @Test public void shouldShutdownWhenRecordConstraintIsViolated() throws InterruptedException {
    final String testId="-shouldShutdownWhenRecordConstraintIsViolated";
    final String appId=getClass().getSimpleName().toLowerCase(Locale.getDefault()) + testId;
    final String input="input" + testId;
    final String outputSuppressed="output-suppressed" + testId;
    final String outputRaw="output-raw" + testId;
    cleanStateBeforeTest(CLUSTER,input,outputRaw,outputSuppressed);
    final StreamsBuilder builder=new StreamsBuilder();
    final KTable<String,Long> valueCounts=buildCountsTable(input,builder);
    valueCounts.suppress(untilTimeLimit(ofMillis(MAX_VALUE),maxRecords(1L).shutDownWhenFull())).toStream().to(outputSuppressed,Produced.with(STRING_SERDE,Serdes.Long()));
    valueCounts.toStream().to(outputRaw,Produced.with(STRING_SERDE,Serdes.Long()));
    final Properties streamsConfig=getStreamsConfig(appId);
    final KafkaStreams driver=IntegrationTestUtils.getStartedStreams(streamsConfig,builder,true);
    try {
      produceSynchronously(input,asList(new KeyValueTimestamp<>("k1","v1",scaledTime(0L)),new KeyValueTimestamp<>("k1","v2",scaledTime(1L)),new KeyValueTimestamp<>("k2","v1",scaledTime(2L)),new KeyValueTimestamp<>("x","x",scaledTime(3L))));
      verifyErrorShutdown(driver);
    }
  finally {
      driver.close();
      cleanStateAfterTest(CLUSTER,driver);
    }
  }
  @Test public void shouldSuppressIntermediateEventsWithBytesLimit(){
    final String testId="-shouldSuppressIntermediateEventsWithBytesLimit";
    final String appId=getClass().getSimpleName().toLowerCase(Locale.getDefault()) + testId;
    final String input="input" + testId;
    final String outputSuppressed="output-suppressed" + testId;
    final String outputRaw="output-raw" + testId;
    cleanStateBeforeTest(CLUSTER,input,outputRaw,outputSuppressed);
    final StreamsBuilder builder=new StreamsBuilder();
    final KTable<String,Long> valueCounts=buildCountsTable(input,builder);
    valueCounts.suppress(untilTimeLimit(Duration.ofMillis(MAX_VALUE),maxBytes(200L).emitEarlyWhenFull())).toStream().to(outputSuppressed,Produced.with(STRING_SERDE,Serdes.Long()));
    valueCounts.toStream().to(outputRaw,Produced.with(STRING_SERDE,Serdes.Long()));
    final Properties streamsConfig=getStreamsConfig(appId);
    final KafkaStreams driver=IntegrationTestUtils.getStartedStreams(streamsConfig,builder,true);
    try {
      produceSynchronously(input,asList(new KeyValueTimestamp<>("k1","v1",scaledTime(0L)),new KeyValueTimestamp<>("k1","v2",scaledTime(1L)),new KeyValueTimestamp<>("k2","v1",scaledTime(2L)),new KeyValueTimestamp<>("x","x",scaledTime(3L))));
      verifyOutput(outputRaw,asList(new KeyValueTimestamp<>("v1",1L,scaledTime(0L)),new KeyValueTimestamp<>("v1",0L,scaledTime(1L)),new KeyValueTimestamp<>("v2",1L,scaledTime(1L)),new KeyValueTimestamp<>("v1",1L,scaledTime(2L)),new KeyValueTimestamp<>("x",1L,scaledTime(3L))));
      verifyOutput(outputSuppressed,asList(new KeyValueTimestamp<>("v1",0L,scaledTime(1L)),new KeyValueTimestamp<>("v2",1L,scaledTime(1L)),new KeyValueTimestamp<>("v1",1L,scaledTime(2L))));
    }
  finally {
      driver.close();
      cleanStateAfterTest(CLUSTER,driver);
    }
  }
  @Test public void shouldShutdownWhenBytesConstraintIsViolated() throws InterruptedException {
    final String testId="-shouldShutdownWhenBytesConstraintIsViolated";
    final String appId=getClass().getSimpleName().toLowerCase(Locale.getDefault()) + testId;
    final String input="input" + testId;
    final String outputSuppressed="output-suppressed" + testId;
    final String outputRaw="output-raw" + testId;
    cleanStateBeforeTest(CLUSTER,input,outputRaw,outputSuppressed);
    final StreamsBuilder builder=new StreamsBuilder();
    final KTable<String,Long> valueCounts=buildCountsTable(input,builder);
    valueCounts.suppress(untilTimeLimit(Duration.ofMillis(MAX_VALUE),maxBytes(200L).shutDownWhenFull())).toStream().to(outputSuppressed,Produced.with(STRING_SERDE,Serdes.Long()));
    valueCounts.toStream().to(outputRaw,Produced.with(STRING_SERDE,Serdes.Long()));
    final Properties streamsConfig=getStreamsConfig(appId);
    final KafkaStreams driver=IntegrationTestUtils.getStartedStreams(streamsConfig,builder,true);
    try {
      produceSynchronously(input,asList(new KeyValueTimestamp<>("k1","v1",scaledTime(0L)),new KeyValueTimestamp<>("k1","v2",scaledTime(1L)),new KeyValueTimestamp<>("k2","v1",scaledTime(2L)),new KeyValueTimestamp<>("x","x",scaledTime(3L))));
      verifyErrorShutdown(driver);
    }
  finally {
      driver.close();
      cleanStateAfterTest(CLUSTER,driver);
    }
  }
  @Test public void shouldSupportFinalResultsForTimeWindows(){
    final String testId="-shouldSupportFinalResultsForTimeWindows";
    final String appId=getClass().getSimpleName().toLowerCase(Locale.getDefault()) + testId;
    final String input="input" + testId;
    final String outputSuppressed="output-suppressed" + testId;
    final String outputRaw="output-raw" + testId;
    cleanStateBeforeTest(CLUSTER,input,outputRaw,outputSuppressed);
    final StreamsBuilder builder=new StreamsBuilder();
    final KTable<Windowed<String>,Long> valueCounts=builder.stream(input,Consumed.with(STRING_SERDE,STRING_SERDE)).groupBy((    String k1,    String v1) -> k1,Grouped.with(STRING_SERDE,STRING_SERDE)).windowedBy(TimeWindows.of(ofMillis(scaledTime(2L))).grace(ofMillis(scaledTime(1L)))).count(Materialized.<String,Long,WindowStore<Bytes,byte[]>>as("counts").withCachingDisabled().withLoggingDisabled());
    valueCounts.suppress(untilWindowCloses(unbounded())).toStream().map((    final Windowed<String> k,    final Long v) -> new KeyValue<>(k.toString(),v)).to(outputSuppressed,Produced.with(STRING_SERDE,Serdes.Long()));
    valueCounts.toStream().map((    final Windowed<String> k,    final Long v) -> new KeyValue<>(k.toString(),v)).to(outputRaw,Produced.with(STRING_SERDE,Serdes.Long()));
    final Properties streamsConfig=getStreamsConfig(appId);
    final KafkaStreams driver=IntegrationTestUtils.getStartedStreams(streamsConfig,builder,true);
    try {
      produceSynchronously(input,asList(new KeyValueTimestamp<>("k1","v1",scaledTime(0L)),new KeyValueTimestamp<>("k1","v1",scaledTime(1L)),new KeyValueTimestamp<>("k1","v1",scaledTime(2L)),new KeyValueTimestamp<>("k1","v1",scaledTime(1L)),new KeyValueTimestamp<>("k1","v1",scaledTime(0L)),new KeyValueTimestamp<>("k1","v1",scaledTime(4L)),new KeyValueTimestamp<>("k1","v1",scaledTime(0L))));
      verifyOutput(outputRaw,asList(new KeyValueTimestamp<>(scaledWindowKey("k1",0L,2L),1L,scaledTime(0L)),new KeyValueTimestamp<>(scaledWindowKey("k1",0L,2L),2L,scaledTime(1L)),new KeyValueTimestamp<>(scaledWindowKey("k1",2L,4L),1L,scaledTime(2L)),new KeyValueTimestamp<>(scaledWindowKey("k1",0L,2L),3L,scaledTime(1L)),new KeyValueTimestamp<>(scaledWindowKey("k1",0L,2L),4L,scaledTime(0L)),new KeyValueTimestamp<>(scaledWindowKey("k1",4L,6L),1L,scaledTime(4L))));
      verifyOutput(outputSuppressed,singletonList(new KeyValueTimestamp<>(scaledWindowKey("k1",0L,2L),4L,scaledTime(0L))));
    }
  finally {
      driver.close();
      cleanStateAfterTest(CLUSTER,driver);
    }
  }
  private Properties getStreamsConfig(  final String appId){
    return mkProperties(mkMap(mkEntry(StreamsConfig.APPLICATION_ID_CONFIG,appId),mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG,CLUSTER.bootstrapServers()),mkEntry(StreamsConfig.POLL_MS_CONFIG,Integer.toString(COMMIT_INTERVAL)),mkEntry(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG,Integer.toString(COMMIT_INTERVAL)),mkEntry(StreamsConfig.PROCESSING_GUARANTEE_CONFIG,AT_LEAST_ONCE)));
  }
  private String scaledWindowKey(  final String key,  final long unscaledStart,  final long unscaledEnd){
    return new Windowed<>(key,new TimeWindow(scaledTime(unscaledStart),scaledTime(unscaledEnd))).toString();
  }
  /** 
 * scaling to ensure that there are commits in between the various test events, just to exercise that everything works properly in the presence of commits.
 */
  private long scaledTime(  final long unscaledTime){
    return COMMIT_INTERVAL * 2 * unscaledTime;
  }
  private void produceSynchronously(  final String topic,  final List<KeyValueTimestamp<String,String>> toProduce){
    final Properties producerConfig=mkProperties(mkMap(mkEntry(ProducerConfig.CLIENT_ID_CONFIG,"anything"),mkEntry(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,((Serializer<String>)STRING_SERIALIZER).getClass().getName()),mkEntry(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,((Serializer<String>)STRING_SERIALIZER).getClass().getName()),mkEntry(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,CLUSTER.bootstrapServers())));
    IntegrationTestUtils.produceSynchronously(producerConfig,false,topic,toProduce);
  }
  private void verifyErrorShutdown(  final KafkaStreams driver) throws InterruptedException {
    waitForCondition(() -> !driver.state().isRunning(),TIMEOUT_MS,"Streams didn't shut down.");
    assertThat(driver.state(),is(KafkaStreams.State.ERROR));
  }
  private void verifyOutput(  final String topic,  final List<KeyValueTimestamp<String,Long>> keyValueTimestamps){
    final Properties properties=mkProperties(mkMap(mkEntry(ConsumerConfig.GROUP_ID_CONFIG,"test-group"),mkEntry(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,CLUSTER.bootstrapServers()),mkEntry(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,((Deserializer<String>)STRING_DESERIALIZER).getClass().getName()),mkEntry(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,((Deserializer<Long>)LONG_DESERIALIZER).getClass().getName())));
    IntegrationTestUtils.verifyKeyValueTimestamps(properties,topic,keyValueTimestamps);
  }
}
