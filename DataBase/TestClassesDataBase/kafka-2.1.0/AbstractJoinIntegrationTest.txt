/** 
 * Tests all available joins of Kafka Streams DSL.
 */
@Category({IntegrationTest.class}) @RunWith(value=Parameterized.class) public abstract class AbstractJoinIntegrationTest {
  @ClassRule public static final EmbeddedKafkaCluster CLUSTER=new EmbeddedKafkaCluster(1);
  @Rule public final TemporaryFolder testFolder=new TemporaryFolder(TestUtils.tempDirectory());
  @Parameterized.Parameters(name="caching enabled = {0}") public static Collection<Object[]> data(){
    final List<Object[]> values=new ArrayList<>();
    for (    final boolean cacheEnabled : Arrays.asList(true,false))     values.add(new Object[]{cacheEnabled});
    return values;
  }
  static String appID;
  private static final Long COMMIT_INTERVAL=100L;
  static final Properties STREAMS_CONFIG=new Properties();
  static final String INPUT_TOPIC_RIGHT="inputTopicRight";
  static final String INPUT_TOPIC_LEFT="inputTopicLeft";
  static final String OUTPUT_TOPIC="outputTopic";
  private final long anyUniqueKey=0L;
  private final static Properties PRODUCER_CONFIG=new Properties();
  private final static Properties RESULT_CONSUMER_CONFIG=new Properties();
  private KafkaProducer<Long,String> producer;
  private KafkaStreams streams;
  StreamsBuilder builder;
  int numRecordsExpected=0;
  AtomicBoolean finalResultReached=new AtomicBoolean(false);
  private final List<Input<String>> input=Arrays.asList(new Input<>(INPUT_TOPIC_LEFT,(String)null),new Input<>(INPUT_TOPIC_RIGHT,(String)null),new Input<>(INPUT_TOPIC_LEFT,"A"),new Input<>(INPUT_TOPIC_RIGHT,"a"),new Input<>(INPUT_TOPIC_LEFT,"B"),new Input<>(INPUT_TOPIC_RIGHT,"b"),new Input<>(INPUT_TOPIC_LEFT,(String)null),new Input<>(INPUT_TOPIC_RIGHT,(String)null),new Input<>(INPUT_TOPIC_LEFT,"C"),new Input<>(INPUT_TOPIC_RIGHT,"c"),new Input<>(INPUT_TOPIC_RIGHT,(String)null),new Input<>(INPUT_TOPIC_LEFT,(String)null),new Input<>(INPUT_TOPIC_RIGHT,(String)null),new Input<>(INPUT_TOPIC_RIGHT,"d"),new Input<>(INPUT_TOPIC_LEFT,"D"));
  final ValueJoiner<String,String,String> valueJoiner=new ValueJoiner<String,String,String>(){
    @Override public String apply(    final String value1,    final String value2){
      return value1 + "-" + value2;
    }
  }
;
  final boolean cacheEnabled;
  AbstractJoinIntegrationTest(  final boolean cacheEnabled){
    this.cacheEnabled=cacheEnabled;
  }
  @BeforeClass public static void setupConfigsAndUtils(){
    PRODUCER_CONFIG.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,CLUSTER.bootstrapServers());
    PRODUCER_CONFIG.put(ProducerConfig.ACKS_CONFIG,"all");
    PRODUCER_CONFIG.put(ProducerConfig.RETRIES_CONFIG,0);
    PRODUCER_CONFIG.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,LongSerializer.class);
    PRODUCER_CONFIG.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class);
    RESULT_CONSUMER_CONFIG.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,CLUSTER.bootstrapServers());
    RESULT_CONSUMER_CONFIG.put(ConsumerConfig.GROUP_ID_CONFIG,appID + "-result-consumer");
    RESULT_CONSUMER_CONFIG.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"earliest");
    RESULT_CONSUMER_CONFIG.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,LongDeserializer.class);
    RESULT_CONSUMER_CONFIG.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class);
    STREAMS_CONFIG.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"earliest");
    STREAMS_CONFIG.put(IntegrationTestUtils.INTERNAL_LEAVE_GROUP_ON_CLOSE,true);
    STREAMS_CONFIG.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG,CLUSTER.bootstrapServers());
    STREAMS_CONFIG.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG,Serdes.Long().getClass());
    STREAMS_CONFIG.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,Serdes.String().getClass());
    STREAMS_CONFIG.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG,COMMIT_INTERVAL);
  }
  void prepareEnvironment() throws InterruptedException {
    CLUSTER.createTopics(INPUT_TOPIC_LEFT,INPUT_TOPIC_RIGHT,OUTPUT_TOPIC);
    if (!cacheEnabled)     STREAMS_CONFIG.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG,0);
    STREAMS_CONFIG.put(StreamsConfig.STATE_DIR_CONFIG,testFolder.getRoot().getPath());
    producer=new KafkaProducer<>(PRODUCER_CONFIG);
  }
  @After public void cleanup() throws InterruptedException {
    producer.close(0,TimeUnit.MILLISECONDS);
    CLUSTER.deleteAllTopicsAndWait(120000);
  }
  private void checkResult(  final String outputTopic,  final List<String> expectedResult) throws InterruptedException {
    final List<String> result=IntegrationTestUtils.waitUntilMinValuesRecordsReceived(RESULT_CONSUMER_CONFIG,outputTopic,expectedResult.size(),30 * 1000L);
    assertThat(result,is(expectedResult));
  }
  private void checkResult(  final String outputTopic,  final String expectedFinalResult,  final int expectedTotalNumRecords) throws InterruptedException {
    final List<String> result=IntegrationTestUtils.waitUntilMinValuesRecordsReceived(RESULT_CONSUMER_CONFIG,outputTopic,expectedTotalNumRecords,30 * 1000L);
    assertThat(result.get(result.size() - 1),is(expectedFinalResult));
  }
  void runTest(  final List<List<String>> expectedResult) throws Exception {
    runTest(expectedResult,null);
  }
  void runTest(  final List<List<String>> expectedResult,  final String storeName) throws Exception {
    assert expectedResult.size() == input.size();
    IntegrationTestUtils.purgeLocalStreamsState(STREAMS_CONFIG);
    streams=new KafkaStreams(builder.build(),STREAMS_CONFIG);
    String expectedFinalResult=null;
    try {
      streams.start();
      long ts=System.currentTimeMillis();
      final Iterator<List<String>> resultIterator=expectedResult.iterator();
      for (      final Input<String> singleInput : input) {
        producer.send(new ProducerRecord<>(singleInput.topic,null,++ts,singleInput.record.key,singleInput.record.value)).get();
        final List<String> expected=resultIterator.next();
        if (expected != null) {
          checkResult(OUTPUT_TOPIC,expected);
          expectedFinalResult=expected.get(expected.size() - 1);
        }
      }
      if (storeName != null) {
        checkQueryableStore(storeName,expectedFinalResult);
      }
    }
  finally {
      streams.close();
    }
  }
  void runTest(  final String expectedFinalResult) throws Exception {
    runTest(expectedFinalResult,null);
  }
  void runTest(  final String expectedFinalResult,  final String storeName) throws Exception {
    IntegrationTestUtils.purgeLocalStreamsState(STREAMS_CONFIG);
    streams=new KafkaStreams(builder.build(),STREAMS_CONFIG);
    try {
      streams.start();
      long ts=System.currentTimeMillis();
      for (      final Input<String> singleInput : input) {
        producer.send(new ProducerRecord<>(singleInput.topic,null,++ts,singleInput.record.key,singleInput.record.value)).get();
      }
      TestUtils.waitForCondition(new TestCondition(){
        @Override public boolean conditionMet(){
          return finalResultReached.get();
        }
      }
,"Never received expected final result.");
      checkResult(OUTPUT_TOPIC,expectedFinalResult,numRecordsExpected);
      if (storeName != null) {
        checkQueryableStore(storeName,expectedFinalResult);
      }
    }
  finally {
      streams.close();
    }
  }
  private void checkQueryableStore(  final String queryableName,  final String expectedFinalResult){
    final ReadOnlyKeyValueStore<Long,String> store=streams.store(queryableName,QueryableStoreTypes.<Long,String>keyValueStore());
    final KeyValueIterator<Long,String> all=store.all();
    final KeyValue<Long,String> onlyEntry=all.next();
    try {
      assertThat(onlyEntry.key,is(anyUniqueKey));
      assertThat(onlyEntry.value,is(expectedFinalResult));
      assertThat(all.hasNext(),is(false));
    }
  finally {
      all.close();
    }
  }
private final class Input<V> {
    String topic;
    KeyValue<Long,V> record;
    Input(    final String topic,    final V value){
      this.topic=topic;
      record=KeyValue.pair(anyUniqueKey,value);
    }
  }
}
