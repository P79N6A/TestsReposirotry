@Category({IntegrationTest.class}) public class EosIntegrationTest {
  private static final int NUM_BROKERS=3;
  private static final int MAX_POLL_INTERVAL_MS=5 * 1000;
  private static final int MAX_WAIT_TIME_MS=60 * 1000;
  @ClassRule public static final EmbeddedKafkaCluster CLUSTER=new EmbeddedKafkaCluster(NUM_BROKERS,new Properties(){
{
      put("auto.create.topics.enable",false);
    }
  }
);
  private static String applicationId;
  private final static int NUM_TOPIC_PARTITIONS=2;
  private final static String CONSUMER_GROUP_ID="readCommitted";
  private final static String SINGLE_PARTITION_INPUT_TOPIC="singlePartitionInputTopic";
  private final static String SINGLE_PARTITION_THROUGH_TOPIC="singlePartitionThroughTopic";
  private final static String SINGLE_PARTITION_OUTPUT_TOPIC="singlePartitionOutputTopic";
  private final static String MULTI_PARTITION_INPUT_TOPIC="multiPartitionInputTopic";
  private final static String MULTI_PARTITION_THROUGH_TOPIC="multiPartitionThroughTopic";
  private final static String MULTI_PARTITION_OUTPUT_TOPIC="multiPartitionOutputTopic";
  private final String storeName="store";
  private AtomicBoolean errorInjected;
  private AtomicBoolean gcInjected;
  private volatile boolean doGC=true;
  private AtomicInteger commitRequested;
  private Throwable uncaughtException;
  private int testNumber=0;
  @Before public void createTopics() throws InterruptedException {
    applicationId="appId-" + ++testNumber;
    CLUSTER.deleteTopicsAndWait(SINGLE_PARTITION_INPUT_TOPIC,MULTI_PARTITION_INPUT_TOPIC,SINGLE_PARTITION_THROUGH_TOPIC,MULTI_PARTITION_THROUGH_TOPIC,SINGLE_PARTITION_OUTPUT_TOPIC,MULTI_PARTITION_OUTPUT_TOPIC);
    CLUSTER.createTopics(SINGLE_PARTITION_INPUT_TOPIC,SINGLE_PARTITION_THROUGH_TOPIC,SINGLE_PARTITION_OUTPUT_TOPIC);
    CLUSTER.createTopic(MULTI_PARTITION_INPUT_TOPIC,NUM_TOPIC_PARTITIONS,1);
    CLUSTER.createTopic(MULTI_PARTITION_THROUGH_TOPIC,NUM_TOPIC_PARTITIONS,1);
    CLUSTER.createTopic(MULTI_PARTITION_OUTPUT_TOPIC,NUM_TOPIC_PARTITIONS,1);
  }
  @Test public void shouldBeAbleToRunWithEosEnabled() throws Exception {
    runSimpleCopyTest(1,SINGLE_PARTITION_INPUT_TOPIC,null,SINGLE_PARTITION_OUTPUT_TOPIC);
  }
  @Test public void shouldBeAbleToRestartAfterClose() throws Exception {
    runSimpleCopyTest(2,SINGLE_PARTITION_INPUT_TOPIC,null,SINGLE_PARTITION_OUTPUT_TOPIC);
  }
  @Test public void shouldBeAbleToCommitToMultiplePartitions() throws Exception {
    runSimpleCopyTest(1,SINGLE_PARTITION_INPUT_TOPIC,null,MULTI_PARTITION_OUTPUT_TOPIC);
  }
  @Test public void shouldBeAbleToCommitMultiplePartitionOffsets() throws Exception {
    runSimpleCopyTest(1,MULTI_PARTITION_INPUT_TOPIC,null,SINGLE_PARTITION_OUTPUT_TOPIC);
  }
  @Test public void shouldBeAbleToRunWithTwoSubtopologies() throws Exception {
    runSimpleCopyTest(1,SINGLE_PARTITION_INPUT_TOPIC,SINGLE_PARTITION_THROUGH_TOPIC,SINGLE_PARTITION_OUTPUT_TOPIC);
  }
  @Test public void shouldBeAbleToRunWithTwoSubtopologiesAndMultiplePartitions() throws Exception {
    runSimpleCopyTest(1,MULTI_PARTITION_INPUT_TOPIC,MULTI_PARTITION_THROUGH_TOPIC,MULTI_PARTITION_OUTPUT_TOPIC);
  }
  private void runSimpleCopyTest(  final int numberOfRestarts,  final String inputTopic,  final String throughTopic,  final String outputTopic) throws Exception {
    final StreamsBuilder builder=new StreamsBuilder();
    final KStream<Long,Long> input=builder.stream(inputTopic);
    KStream<Long,Long> output=input;
    if (throughTopic != null) {
      output=input.through(throughTopic);
    }
    output.to(outputTopic);
    for (int i=0; i < numberOfRestarts; ++i) {
      final KafkaStreams streams=new KafkaStreams(builder.build(),StreamsTestUtils.getStreamsConfig(applicationId,CLUSTER.bootstrapServers(),Serdes.LongSerde.class.getName(),Serdes.LongSerde.class.getName(),new Properties(){
{
          put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG,StreamsConfig.EXACTLY_ONCE);
          put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG,0);
          put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG,100);
          put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_RECORDS_CONFIG),1);
          put(StreamsConfig.consumerPrefix(ConsumerConfig.METADATA_MAX_AGE_CONFIG),"1000");
          put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG),"earliest");
          put(IntegrationTestUtils.INTERNAL_LEAVE_GROUP_ON_CLOSE,true);
        }
      }
));
      try {
        streams.start();
        final List<KeyValue<Long,Long>> inputData=prepareData(i * 100,i * 100 + 10L,0L,1L);
        IntegrationTestUtils.produceKeyValuesSynchronously(inputTopic,inputData,TestUtils.producerConfig(CLUSTER.bootstrapServers(),LongSerializer.class,LongSerializer.class),CLUSTER.time);
        final List<KeyValue<Long,Long>> committedRecords=IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(TestUtils.consumerConfig(CLUSTER.bootstrapServers(),CONSUMER_GROUP_ID,LongDeserializer.class,LongDeserializer.class,new Properties(){
{
            put(ConsumerConfig.ISOLATION_LEVEL_CONFIG,IsolationLevel.READ_COMMITTED.name().toLowerCase(Locale.ROOT));
          }
        }
),outputTopic,inputData.size());
        checkResultPerKey(committedRecords,inputData);
      }
  finally {
        streams.close();
      }
    }
  }
  private void checkResultPerKey(  final List<KeyValue<Long,Long>> result,  final List<KeyValue<Long,Long>> expectedResult){
    final Set<Long> allKeys=new HashSet<>();
    addAllKeys(allKeys,result);
    addAllKeys(allKeys,expectedResult);
    for (    final Long key : allKeys) {
      assertThat(getAllRecordPerKey(key,result),equalTo(getAllRecordPerKey(key,expectedResult)));
    }
  }
  private void addAllKeys(  final Set<Long> allKeys,  final List<KeyValue<Long,Long>> records){
    for (    final KeyValue<Long,Long> record : records) {
      allKeys.add(record.key);
    }
  }
  private List<KeyValue<Long,Long>> getAllRecordPerKey(  final Long key,  final List<KeyValue<Long,Long>> records){
    final List<KeyValue<Long,Long>> recordsPerKey=new ArrayList<>(records.size());
    for (    final KeyValue<Long,Long> record : records) {
      if (record.key.equals(key)) {
        recordsPerKey.add(record);
      }
    }
    return recordsPerKey;
  }
  @Test public void shouldBeAbleToPerformMultipleTransactions() throws Exception {
    final StreamsBuilder builder=new StreamsBuilder();
    builder.stream(SINGLE_PARTITION_INPUT_TOPIC).to(SINGLE_PARTITION_OUTPUT_TOPIC);
    final KafkaStreams streams=new KafkaStreams(builder.build(),StreamsTestUtils.getStreamsConfig(applicationId,CLUSTER.bootstrapServers(),Serdes.LongSerde.class.getName(),Serdes.LongSerde.class.getName(),new Properties(){
{
        put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG,StreamsConfig.EXACTLY_ONCE);
        put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG,0);
        put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG,100);
        put(ConsumerConfig.METADATA_MAX_AGE_CONFIG,"1000");
        put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,"earliest");
      }
    }
));
    try {
      streams.start();
      final List<KeyValue<Long,Long>> firstBurstOfData=prepareData(0L,5L,0L);
      final List<KeyValue<Long,Long>> secondBurstOfData=prepareData(5L,8L,0L);
      IntegrationTestUtils.produceKeyValuesSynchronously(SINGLE_PARTITION_INPUT_TOPIC,firstBurstOfData,TestUtils.producerConfig(CLUSTER.bootstrapServers(),LongSerializer.class,LongSerializer.class),CLUSTER.time);
      final List<KeyValue<Long,Long>> firstCommittedRecords=IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(TestUtils.consumerConfig(CLUSTER.bootstrapServers(),CONSUMER_GROUP_ID,LongDeserializer.class,LongDeserializer.class,new Properties(){
{
          put(ConsumerConfig.ISOLATION_LEVEL_CONFIG,IsolationLevel.READ_COMMITTED.name().toLowerCase(Locale.ROOT));
        }
      }
),SINGLE_PARTITION_OUTPUT_TOPIC,firstBurstOfData.size());
      assertThat(firstCommittedRecords,equalTo(firstBurstOfData));
      IntegrationTestUtils.produceKeyValuesSynchronously(SINGLE_PARTITION_INPUT_TOPIC,secondBurstOfData,TestUtils.producerConfig(CLUSTER.bootstrapServers(),LongSerializer.class,LongSerializer.class),CLUSTER.time);
      final List<KeyValue<Long,Long>> secondCommittedRecords=IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(TestUtils.consumerConfig(CLUSTER.bootstrapServers(),CONSUMER_GROUP_ID,LongDeserializer.class,LongDeserializer.class,new Properties(){
{
          put(ConsumerConfig.ISOLATION_LEVEL_CONFIG,IsolationLevel.READ_COMMITTED.name().toLowerCase(Locale.ROOT));
        }
      }
),SINGLE_PARTITION_OUTPUT_TOPIC,secondBurstOfData.size());
      assertThat(secondCommittedRecords,equalTo(secondBurstOfData));
    }
  finally {
      streams.close();
    }
  }
  @Test public void shouldNotViolateEosIfOneTaskFails() throws Exception {
    final KafkaStreams streams=getKafkaStreams(false,"appDir",2);
    try {
      streams.start();
      final List<KeyValue<Long,Long>> committedDataBeforeFailure=prepareData(0L,10L,0L,1L);
      final List<KeyValue<Long,Long>> uncommittedDataBeforeFailure=prepareData(10L,15L,0L,1L);
      final List<KeyValue<Long,Long>> dataBeforeFailure=new ArrayList<>();
      dataBeforeFailure.addAll(committedDataBeforeFailure);
      dataBeforeFailure.addAll(uncommittedDataBeforeFailure);
      final List<KeyValue<Long,Long>> dataAfterFailure=prepareData(15L,20L,0L,1L);
      writeInputData(committedDataBeforeFailure);
      TestUtils.waitForCondition(new TestCondition(){
        @Override public boolean conditionMet(){
          return commitRequested.get() == 2;
        }
      }
,MAX_WAIT_TIME_MS,"SteamsTasks did not request commit.");
      writeInputData(uncommittedDataBeforeFailure);
      final List<KeyValue<Long,Long>> uncommittedRecords=readResult(dataBeforeFailure.size(),null);
      final List<KeyValue<Long,Long>> committedRecords=readResult(committedDataBeforeFailure.size(),CONSUMER_GROUP_ID);
      checkResultPerKey(committedRecords,committedDataBeforeFailure);
      checkResultPerKey(uncommittedRecords,dataBeforeFailure);
      errorInjected.set(true);
      writeInputData(dataAfterFailure);
      TestUtils.waitForCondition(new TestCondition(){
        @Override public boolean conditionMet(){
          return uncaughtException != null;
        }
      }
,MAX_WAIT_TIME_MS,"Should receive uncaught exception from one StreamThread.");
      final List<KeyValue<Long,Long>> allCommittedRecords=readResult(committedDataBeforeFailure.size() + uncommittedDataBeforeFailure.size() + dataAfterFailure.size(),CONSUMER_GROUP_ID + "_ALL");
      final List<KeyValue<Long,Long>> committedRecordsAfterFailure=readResult(uncommittedDataBeforeFailure.size() + dataAfterFailure.size(),CONSUMER_GROUP_ID);
      final List<KeyValue<Long,Long>> allExpectedCommittedRecordsAfterRecovery=new ArrayList<>();
      allExpectedCommittedRecordsAfterRecovery.addAll(committedDataBeforeFailure);
      allExpectedCommittedRecordsAfterRecovery.addAll(uncommittedDataBeforeFailure);
      allExpectedCommittedRecordsAfterRecovery.addAll(dataAfterFailure);
      final List<KeyValue<Long,Long>> expectedCommittedRecordsAfterRecovery=new ArrayList<>();
      expectedCommittedRecordsAfterRecovery.addAll(uncommittedDataBeforeFailure);
      expectedCommittedRecordsAfterRecovery.addAll(dataAfterFailure);
      checkResultPerKey(allCommittedRecords,allExpectedCommittedRecordsAfterRecovery);
      checkResultPerKey(committedRecordsAfterFailure,expectedCommittedRecordsAfterRecovery);
    }
  finally {
      streams.close();
    }
  }
  @Test public void shouldNotViolateEosIfOneTaskFailsWithState() throws Exception {
    final KafkaStreams streams=getKafkaStreams(true,"appDir",2);
    try {
      streams.start();
      final List<KeyValue<Long,Long>> committedDataBeforeFailure=prepareData(0L,10L,0L,1L);
      final List<KeyValue<Long,Long>> uncommittedDataBeforeFailure=prepareData(10L,15L,0L,1L,2L,3L);
      final List<KeyValue<Long,Long>> dataBeforeFailure=new ArrayList<>();
      dataBeforeFailure.addAll(committedDataBeforeFailure);
      dataBeforeFailure.addAll(uncommittedDataBeforeFailure);
      final List<KeyValue<Long,Long>> dataAfterFailure=prepareData(15L,20L,0L,1L);
      writeInputData(committedDataBeforeFailure);
      TestUtils.waitForCondition(new TestCondition(){
        @Override public boolean conditionMet(){
          return commitRequested.get() == 2;
        }
      }
,MAX_WAIT_TIME_MS,"SteamsTasks did not request commit.");
      writeInputData(uncommittedDataBeforeFailure);
      final List<KeyValue<Long,Long>> uncommittedRecords=readResult(dataBeforeFailure.size(),null);
      final List<KeyValue<Long,Long>> committedRecords=readResult(committedDataBeforeFailure.size(),CONSUMER_GROUP_ID);
      final List<KeyValue<Long,Long>> expectedResultBeforeFailure=computeExpectedResult(dataBeforeFailure);
      checkResultPerKey(committedRecords,computeExpectedResult(committedDataBeforeFailure));
      checkResultPerKey(uncommittedRecords,expectedResultBeforeFailure);
      verifyStateStore(streams,getMaxPerKey(expectedResultBeforeFailure));
      errorInjected.set(true);
      writeInputData(dataAfterFailure);
      TestUtils.waitForCondition(new TestCondition(){
        @Override public boolean conditionMet(){
          return uncaughtException != null;
        }
      }
,MAX_WAIT_TIME_MS,"Should receive uncaught exception from one StreamThread.");
      final List<KeyValue<Long,Long>> allCommittedRecords=readResult(committedDataBeforeFailure.size() + uncommittedDataBeforeFailure.size() + dataAfterFailure.size(),CONSUMER_GROUP_ID + "_ALL");
      final List<KeyValue<Long,Long>> committedRecordsAfterFailure=readResult(uncommittedDataBeforeFailure.size() + dataAfterFailure.size(),CONSUMER_GROUP_ID);
      final List<KeyValue<Long,Long>> allExpectedCommittedRecordsAfterRecovery=new ArrayList<>();
      allExpectedCommittedRecordsAfterRecovery.addAll(committedDataBeforeFailure);
      allExpectedCommittedRecordsAfterRecovery.addAll(uncommittedDataBeforeFailure);
      allExpectedCommittedRecordsAfterRecovery.addAll(dataAfterFailure);
      final List<KeyValue<Long,Long>> expectedResult=computeExpectedResult(allExpectedCommittedRecordsAfterRecovery);
      checkResultPerKey(allCommittedRecords,expectedResult);
      checkResultPerKey(committedRecordsAfterFailure,expectedResult.subList(committedDataBeforeFailure.size(),expectedResult.size()));
      verifyStateStore(streams,getMaxPerKey(expectedResult));
    }
  finally {
      streams.close();
    }
  }
  @Test public void shouldNotViolateEosIfOneTaskGetsFencedUsingIsolatedAppInstances() throws Exception {
    final KafkaStreams streams1=getKafkaStreams(false,"appDir1",1);
    final KafkaStreams streams2=getKafkaStreams(false,"appDir2",1);
    try {
      streams1.start();
      streams2.start();
      final List<KeyValue<Long,Long>> committedDataBeforeGC=prepareData(0L,10L,0L,1L);
      final List<KeyValue<Long,Long>> uncommittedDataBeforeGC=prepareData(10L,15L,0L,1L);
      final List<KeyValue<Long,Long>> dataBeforeGC=new ArrayList<>();
      dataBeforeGC.addAll(committedDataBeforeGC);
      dataBeforeGC.addAll(uncommittedDataBeforeGC);
      final List<KeyValue<Long,Long>> dataToTriggerFirstRebalance=prepareData(15L,20L,0L,1L);
      final List<KeyValue<Long,Long>> dataAfterSecondRebalance=prepareData(20L,30L,0L,1L);
      writeInputData(committedDataBeforeGC);
      TestUtils.waitForCondition(new TestCondition(){
        @Override public boolean conditionMet(){
          return commitRequested.get() == 2;
        }
      }
,MAX_WAIT_TIME_MS,"SteamsTasks did not request commit.");
      writeInputData(uncommittedDataBeforeGC);
      final List<KeyValue<Long,Long>> uncommittedRecords=readResult(dataBeforeGC.size(),null);
      final List<KeyValue<Long,Long>> committedRecords=readResult(committedDataBeforeGC.size(),CONSUMER_GROUP_ID);
      checkResultPerKey(committedRecords,committedDataBeforeGC);
      checkResultPerKey(uncommittedRecords,dataBeforeGC);
      gcInjected.set(true);
      writeInputData(dataToTriggerFirstRebalance);
      TestUtils.waitForCondition(new TestCondition(){
        @Override public boolean conditionMet(){
          return streams1.allMetadata().size() == 1 && streams2.allMetadata().size() == 1 && (streams1.allMetadata().iterator().next().topicPartitions().size() == 2 || streams2.allMetadata().iterator().next().topicPartitions().size() == 2);
        }
      }
,MAX_WAIT_TIME_MS,"Should have rebalanced.");
      final List<KeyValue<Long,Long>> committedRecordsAfterRebalance=readResult(uncommittedDataBeforeGC.size() + dataToTriggerFirstRebalance.size(),CONSUMER_GROUP_ID);
      final List<KeyValue<Long,Long>> expectedCommittedRecordsAfterRebalance=new ArrayList<>();
      expectedCommittedRecordsAfterRebalance.addAll(uncommittedDataBeforeGC);
      expectedCommittedRecordsAfterRebalance.addAll(dataToTriggerFirstRebalance);
      checkResultPerKey(committedRecordsAfterRebalance,expectedCommittedRecordsAfterRebalance);
      doGC=false;
      TestUtils.waitForCondition(new TestCondition(){
        @Override public boolean conditionMet(){
          return streams1.allMetadata().size() == 1 && streams2.allMetadata().size() == 1 && streams1.allMetadata().iterator().next().topicPartitions().size() == 1 && streams2.allMetadata().iterator().next().topicPartitions().size() == 1;
        }
      }
,MAX_WAIT_TIME_MS,"Should have rebalanced.");
      writeInputData(dataAfterSecondRebalance);
      final List<KeyValue<Long,Long>> allCommittedRecords=readResult(committedDataBeforeGC.size() + uncommittedDataBeforeGC.size() + dataToTriggerFirstRebalance.size()+ dataAfterSecondRebalance.size(),CONSUMER_GROUP_ID + "_ALL");
      final List<KeyValue<Long,Long>> allExpectedCommittedRecordsAfterRecovery=new ArrayList<>();
      allExpectedCommittedRecordsAfterRecovery.addAll(committedDataBeforeGC);
      allExpectedCommittedRecordsAfterRecovery.addAll(uncommittedDataBeforeGC);
      allExpectedCommittedRecordsAfterRecovery.addAll(dataToTriggerFirstRebalance);
      allExpectedCommittedRecordsAfterRecovery.addAll(dataAfterSecondRebalance);
      checkResultPerKey(allCommittedRecords,allExpectedCommittedRecordsAfterRecovery);
    }
  finally {
      streams1.close();
      streams2.close();
    }
  }
  private List<KeyValue<Long,Long>> prepareData(  final long fromInclusive,  final long toExclusive,  final Long... keys){
    final List<KeyValue<Long,Long>> data=new ArrayList<>();
    for (    final Long k : keys) {
      for (long v=fromInclusive; v < toExclusive; ++v) {
        data.add(new KeyValue<>(k,v));
      }
    }
    return data;
  }
  private KafkaStreams getKafkaStreams(  final boolean withState,  final String appDir,  final int numberOfStreamsThreads){
    commitRequested=new AtomicInteger(0);
    errorInjected=new AtomicBoolean(false);
    gcInjected=new AtomicBoolean(false);
    final StreamsBuilder builder=new StreamsBuilder();
    String[] storeNames=null;
    if (withState) {
      storeNames=new String[]{storeName};
      final StoreBuilder<KeyValueStore<Long,Long>> storeBuilder=Stores.keyValueStoreBuilder(Stores.persistentKeyValueStore(storeName),Serdes.Long(),Serdes.Long()).withCachingEnabled();
      builder.addStateStore(storeBuilder);
    }
    final KStream<Long,Long> input=builder.stream(MULTI_PARTITION_INPUT_TOPIC);
    input.transform(new TransformerSupplier<Long,Long,KeyValue<Long,Long>>(){
      @SuppressWarnings("unchecked") @Override public Transformer<Long,Long,KeyValue<Long,Long>> get(){
        return new Transformer<Long,Long,KeyValue<Long,Long>>(){
          ProcessorContext context;
          KeyValueStore<Long,Long> state=null;
          @Override public void init(          final ProcessorContext context){
            this.context=context;
            if (withState) {
              state=(KeyValueStore<Long,Long>)context.getStateStore(storeName);
            }
          }
          @Override public KeyValue<Long,Long> transform(          final Long key,          final Long value){
            if (gcInjected.compareAndSet(true,false)) {
              while (doGC) {
                try {
                  Thread.sleep(100);
                }
 catch (                final InterruptedException e) {
                  throw new RuntimeException(e);
                }
              }
            }
            if ((value + 1) % 10 == 0) {
              context.commit();
              commitRequested.incrementAndGet();
            }
            if (state != null) {
              Long sum=state.get(key);
              if (sum == null) {
                sum=value;
              }
 else {
                sum+=value;
              }
              state.put(key,sum);
              state.flush();
            }
            if (errorInjected.compareAndSet(true,false)) {
              throw new RuntimeException("Injected test exception.");
            }
            if (state != null) {
              return new KeyValue<>(key,state.get(key));
            }
 else {
              return new KeyValue<>(key,value);
            }
          }
          @Override public void close(){
          }
        }
;
      }
    }
,storeNames).to(SINGLE_PARTITION_OUTPUT_TOPIC);
    final KafkaStreams streams=new KafkaStreams(builder.build(),StreamsTestUtils.getStreamsConfig(applicationId,CLUSTER.bootstrapServers(),Serdes.LongSerde.class.getName(),Serdes.LongSerde.class.getName(),new Properties(){
{
        put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG,StreamsConfig.EXACTLY_ONCE);
        put(StreamsConfig.NUM_STREAM_THREADS_CONFIG,numberOfStreamsThreads);
        put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG,-1);
        put(StreamsConfig.consumerPrefix(ConsumerConfig.METADATA_MAX_AGE_CONFIG),"1000");
        put(StreamsConfig.consumerPrefix(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG),"earliest");
        put(StreamsConfig.consumerPrefix(ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG),5 * 1000);
        put(StreamsConfig.consumerPrefix(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG),5 * 1000 - 1);
        put(StreamsConfig.consumerPrefix(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG),MAX_POLL_INTERVAL_MS);
        put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG,0);
        put(StreamsConfig.STATE_DIR_CONFIG,TestUtils.tempDirectory().getPath() + File.separator + appDir);
        put(StreamsConfig.APPLICATION_SERVER_CONFIG,"dummy:2142");
      }
    }
));
    streams.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler(){
      @Override public void uncaughtException(      final Thread t,      final Throwable e){
        if (uncaughtException != null) {
          e.printStackTrace(System.err);
          fail("Should only get one uncaught exception from Streams.");
        }
        uncaughtException=e;
      }
    }
);
    return streams;
  }
  private void writeInputData(  final List<KeyValue<Long,Long>> records) throws Exception {
    IntegrationTestUtils.produceKeyValuesSynchronously(MULTI_PARTITION_INPUT_TOPIC,records,TestUtils.producerConfig(CLUSTER.bootstrapServers(),LongSerializer.class,LongSerializer.class),CLUSTER.time);
  }
  private List<KeyValue<Long,Long>> readResult(  final int numberOfRecords,  final String groupId) throws InterruptedException {
    if (groupId != null) {
      return IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(TestUtils.consumerConfig(CLUSTER.bootstrapServers(),groupId,LongDeserializer.class,LongDeserializer.class,new Properties(){
{
          put(ConsumerConfig.ISOLATION_LEVEL_CONFIG,IsolationLevel.READ_COMMITTED.name().toLowerCase(Locale.ROOT));
        }
      }
),SINGLE_PARTITION_OUTPUT_TOPIC,numberOfRecords);
    }
    return IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(TestUtils.consumerConfig(CLUSTER.bootstrapServers(),LongDeserializer.class,LongDeserializer.class),SINGLE_PARTITION_OUTPUT_TOPIC,numberOfRecords);
  }
  private List<KeyValue<Long,Long>> computeExpectedResult(  final List<KeyValue<Long,Long>> input){
    final List<KeyValue<Long,Long>> expectedResult=new ArrayList<>(input.size());
    final HashMap<Long,Long> sums=new HashMap<>();
    for (    final KeyValue<Long,Long> record : input) {
      Long sum=sums.get(record.key);
      if (sum == null) {
        sum=record.value;
      }
 else {
        sum+=record.value;
      }
      sums.put(record.key,sum);
      expectedResult.add(new KeyValue<>(record.key,sum));
    }
    return expectedResult;
  }
  private Set<KeyValue<Long,Long>> getMaxPerKey(  final List<KeyValue<Long,Long>> input){
    final Set<KeyValue<Long,Long>> expectedResult=new HashSet<>(input.size());
    final HashMap<Long,Long> maxPerKey=new HashMap<>();
    for (    final KeyValue<Long,Long> record : input) {
      final Long max=maxPerKey.get(record.key);
      if (max == null || record.value > max) {
        maxPerKey.put(record.key,record.value);
      }
    }
    for (    final Map.Entry<Long,Long> max : maxPerKey.entrySet()) {
      expectedResult.add(new KeyValue<>(max.getKey(),max.getValue()));
    }
    return expectedResult;
  }
  private void verifyStateStore(  final KafkaStreams streams,  final Set<KeyValue<Long,Long>> expectedStoreContent){
    ReadOnlyKeyValueStore<Long,Long> store=null;
    final long maxWaitingTime=System.currentTimeMillis() + 300000L;
    while (System.currentTimeMillis() < maxWaitingTime) {
      try {
        store=streams.store(storeName,QueryableStoreTypes.<Long,Long>keyValueStore());
        break;
      }
 catch (      final InvalidStateStoreException okJustRetry) {
        try {
          Thread.sleep(5000L);
        }
 catch (        final Exception ignore) {
        }
      }
    }
    assertNotNull(store);
    final KeyValueIterator<Long,Long> it=store.all();
    while (it.hasNext()) {
      assertTrue(expectedStoreContent.remove(it.next()));
    }
    assertTrue(expectedStoreContent.isEmpty());
  }
}
