public class KafkaProducerTest {
  @Test public void testMetricsReporterAutoGeneratedClientId(){
    Properties props=new Properties();
    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9999");
    props.setProperty(ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG,MockMetricsReporter.class.getName());
    KafkaProducer<String,String> producer=new KafkaProducer<>(props,new StringSerializer(),new StringSerializer());
    MockMetricsReporter mockMetricsReporter=(MockMetricsReporter)producer.metrics.reporters().get(0);
    Assert.assertEquals(producer.getClientId(),mockMetricsReporter.clientId);
    producer.close();
  }
  @Test public void testConstructorWithSerializers(){
    Properties producerProps=new Properties();
    producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9000");
    new KafkaProducer<>(producerProps,new ByteArraySerializer(),new ByteArraySerializer()).close();
  }
  @Test(expected=ConfigException.class) public void testNoSerializerProvided(){
    Properties producerProps=new Properties();
    producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9000");
    new KafkaProducer(producerProps);
  }
  @Test public void testConstructorFailureCloseResource(){
    Properties props=new Properties();
    props.setProperty(ProducerConfig.CLIENT_ID_CONFIG,"testConstructorClose");
    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"some.invalid.hostname.foo.bar.local:9999");
    props.setProperty(ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG,MockMetricsReporter.class.getName());
    final int oldInitCount=MockMetricsReporter.INIT_COUNT.get();
    final int oldCloseCount=MockMetricsReporter.CLOSE_COUNT.get();
    try (KafkaProducer<byte[],byte[]> ignored=new KafkaProducer<>(props,new ByteArraySerializer(),new ByteArraySerializer())){
      fail("should have caught an exception and returned");
    }
 catch (    KafkaException e) {
      assertEquals(oldInitCount + 1,MockMetricsReporter.INIT_COUNT.get());
      assertEquals(oldCloseCount + 1,MockMetricsReporter.CLOSE_COUNT.get());
      assertEquals("Failed to construct kafka producer",e.getMessage());
    }
  }
  @Test public void testSerializerClose(){
    Map<String,Object> configs=new HashMap<>();
    configs.put(ProducerConfig.CLIENT_ID_CONFIG,"testConstructorClose");
    configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9999");
    configs.put(ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG,MockMetricsReporter.class.getName());
    configs.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,CommonClientConfigs.DEFAULT_SECURITY_PROTOCOL);
    final int oldInitCount=MockSerializer.INIT_COUNT.get();
    final int oldCloseCount=MockSerializer.CLOSE_COUNT.get();
    KafkaProducer<byte[],byte[]> producer=new KafkaProducer<>(configs,new MockSerializer(),new MockSerializer());
    assertEquals(oldInitCount + 2,MockSerializer.INIT_COUNT.get());
    assertEquals(oldCloseCount,MockSerializer.CLOSE_COUNT.get());
    producer.close();
    assertEquals(oldInitCount + 2,MockSerializer.INIT_COUNT.get());
    assertEquals(oldCloseCount + 2,MockSerializer.CLOSE_COUNT.get());
  }
  @Test public void testInterceptorConstructClose(){
    try {
      Properties props=new Properties();
      props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9999");
      props.setProperty(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,MockProducerInterceptor.class.getName());
      props.setProperty(MockProducerInterceptor.APPEND_STRING_PROP,"something");
      KafkaProducer<String,String> producer=new KafkaProducer<>(props,new StringSerializer(),new StringSerializer());
      assertEquals(1,MockProducerInterceptor.INIT_COUNT.get());
      assertEquals(0,MockProducerInterceptor.CLOSE_COUNT.get());
      Assert.assertNull(MockProducerInterceptor.CLUSTER_META.get());
      producer.close();
      assertEquals(1,MockProducerInterceptor.INIT_COUNT.get());
      assertEquals(1,MockProducerInterceptor.CLOSE_COUNT.get());
    }
  finally {
      MockProducerInterceptor.resetCounters();
    }
  }
  @Test public void testPartitionerClose(){
    try {
      Properties props=new Properties();
      props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9999");
      MockPartitioner.resetCounters();
      props.setProperty(ProducerConfig.PARTITIONER_CLASS_CONFIG,MockPartitioner.class.getName());
      KafkaProducer<String,String> producer=new KafkaProducer<>(props,new StringSerializer(),new StringSerializer());
      assertEquals(1,MockPartitioner.INIT_COUNT.get());
      assertEquals(0,MockPartitioner.CLOSE_COUNT.get());
      producer.close();
      assertEquals(1,MockPartitioner.INIT_COUNT.get());
      assertEquals(1,MockPartitioner.CLOSE_COUNT.get());
    }
  finally {
      MockPartitioner.resetCounters();
    }
  }
  @Test public void shouldCloseProperlyAndThrowIfInterrupted() throws Exception {
    Properties props=new Properties();
    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9999");
    props.setProperty(ProducerConfig.PARTITIONER_CLASS_CONFIG,MockPartitioner.class.getName());
    props.setProperty(ProducerConfig.BATCH_SIZE_CONFIG,"1");
    Time time=new MockTime();
    Cluster cluster=TestUtils.singletonCluster("topic",1);
    Node node=cluster.nodes().get(0);
    Metadata metadata=new Metadata(0,Long.MAX_VALUE,true);
    metadata.update(cluster,Collections.emptySet(),time.milliseconds());
    MockClient client=new MockClient(time,metadata);
    client.setNode(node);
    final Producer<String,String> producer=new KafkaProducer<>(new ProducerConfig(ProducerConfig.addSerializerToConfig(props,new StringSerializer(),new StringSerializer())),new StringSerializer(),new StringSerializer(),metadata,client,null,time);
    ExecutorService executor=Executors.newSingleThreadExecutor();
    final AtomicReference<Exception> closeException=new AtomicReference<>();
    try {
      Future<?> future=executor.submit(() -> {
        producer.send(new ProducerRecord<>("topic","key","value"));
        try {
          producer.close();
          fail("Close should block and throw.");
        }
 catch (        Exception e) {
          closeException.set(e);
        }
      }
);
      try {
        future.get(100,TimeUnit.MILLISECONDS);
        fail("Close completed without waiting for send");
      }
 catch (      java.util.concurrent.TimeoutException expected) {
      }
      client.waitForRequests(1,1000);
      assertTrue("Close terminated prematurely",future.cancel(true));
      TestUtils.waitForCondition(() -> closeException.get() != null,"InterruptException did not occur within timeout.");
      assertTrue("Expected exception not thrown " + closeException,closeException.get() instanceof InterruptException);
    }
  finally {
      executor.shutdownNow();
    }
  }
  @Test public void testOsDefaultSocketBufferSizes(){
    Map<String,Object> config=new HashMap<>();
    config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9999");
    config.put(ProducerConfig.SEND_BUFFER_CONFIG,Selectable.USE_DEFAULT_BUFFER_SIZE);
    config.put(ProducerConfig.RECEIVE_BUFFER_CONFIG,Selectable.USE_DEFAULT_BUFFER_SIZE);
    new KafkaProducer<>(config,new ByteArraySerializer(),new ByteArraySerializer()).close();
  }
  @Test(expected=KafkaException.class) public void testInvalidSocketSendBufferSize(){
    Map<String,Object> config=new HashMap<>();
    config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9999");
    config.put(ProducerConfig.SEND_BUFFER_CONFIG,-2);
    new KafkaProducer<>(config,new ByteArraySerializer(),new ByteArraySerializer());
  }
  @Test(expected=KafkaException.class) public void testInvalidSocketReceiveBufferSize(){
    Map<String,Object> config=new HashMap<>();
    config.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9999");
    config.put(ProducerConfig.RECEIVE_BUFFER_CONFIG,-2);
    new KafkaProducer<>(config,new ByteArraySerializer(),new ByteArraySerializer());
  }
  @Test public void testMetadataFetch() throws InterruptedException {
    Properties props=new Properties();
    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9999");
    ProducerConfig config=new ProducerConfig(ProducerConfig.addSerializerToConfig(props,new StringSerializer(),new StringSerializer()));
    String topic="topic";
    Collection<Node> nodes=Collections.singletonList(new Node(0,"host1",1000));
    final Cluster emptyCluster=new Cluster(null,nodes,Collections.emptySet(),Collections.emptySet(),Collections.emptySet());
    final Cluster cluster=new Cluster("dummy",Collections.singletonList(new Node(0,"host1",1000)),Collections.singletonList(new PartitionInfo(topic,0,null,null,null)),Collections.emptySet(),Collections.emptySet());
    Metadata metadata=mock(Metadata.class);
    when(metadata.fetch()).thenReturn(emptyCluster,emptyCluster,emptyCluster,emptyCluster,cluster);
    KafkaProducer<String,String> producer=new KafkaProducer<String,String>(config,null,null,metadata,new MockClient(Time.SYSTEM,metadata),null,Time.SYSTEM){
      @Override Sender newSender(      LogContext logContext,      KafkaClient kafkaClient,      Metadata metadata){
        return super.newSender(logContext,kafkaClient,new Metadata(0,100_000,true));
      }
    }
;
    ProducerRecord<String,String> record=new ProducerRecord<>(topic,"value");
    producer.send(record);
    verify(metadata,times(4)).requestUpdate();
    verify(metadata,times(4)).awaitUpdate(anyInt(),anyLong());
    verify(metadata,times(5)).fetch();
    producer.send(record,null);
    verify(metadata,times(4)).requestUpdate();
    verify(metadata,times(4)).awaitUpdate(anyInt(),anyLong());
    verify(metadata,times(6)).fetch();
    producer.partitionsFor(topic);
    verify(metadata,times(4)).requestUpdate();
    verify(metadata,times(4)).awaitUpdate(anyInt(),anyLong());
    verify(metadata,times(7)).fetch();
    producer.close(0,TimeUnit.MILLISECONDS);
  }
  @Test public void testMetadataFetchOnStaleMetadata() throws Exception {
    Properties props=new Properties();
    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9999");
    ProducerConfig config=new ProducerConfig(ProducerConfig.addSerializerToConfig(props,new StringSerializer(),new StringSerializer()));
    String topic="topic";
    ProducerRecord<String,String> initialRecord=new ProducerRecord<>(topic,"value");
    ProducerRecord<String,String> extendedRecord=new ProducerRecord<>(topic,2,null,"value");
    Collection<Node> nodes=Collections.singletonList(new Node(0,"host1",1000));
    final Cluster emptyCluster=new Cluster(null,nodes,Collections.emptySet(),Collections.emptySet(),Collections.emptySet());
    final Cluster initialCluster=new Cluster("dummy",Collections.singletonList(new Node(0,"host1",1000)),Collections.singletonList(new PartitionInfo(topic,0,null,null,null)),Collections.emptySet(),Collections.emptySet());
    final Cluster extendedCluster=new Cluster("dummy",Collections.singletonList(new Node(0,"host1",1000)),Arrays.asList(new PartitionInfo(topic,0,null,null,null),new PartitionInfo(topic,1,null,null,null),new PartitionInfo(topic,2,null,null,null)),Collections.emptySet(),Collections.emptySet());
    Metadata metadata=mock(Metadata.class);
    AtomicInteger invocationCount=new AtomicInteger(0);
    when(metadata.fetch()).then(invocation -> {
      invocationCount.incrementAndGet();
      if (invocationCount.get() > 9)       return extendedCluster;
 else       if (invocationCount.get() > 4)       return initialCluster;
      return emptyCluster;
    }
);
    KafkaProducer<String,String> producer=new KafkaProducer<String,String>(config,null,null,metadata,new MockClient(Time.SYSTEM,metadata),null,Time.SYSTEM){
      @Override Sender newSender(      LogContext logContext,      KafkaClient kafkaClient,      Metadata metadata){
        return super.newSender(logContext,kafkaClient,new Metadata(0,100_000,true));
      }
    }
;
    producer.send(initialRecord);
    verify(metadata,times(4)).requestUpdate();
    verify(metadata,times(4)).awaitUpdate(anyInt(),anyLong());
    verify(metadata,times(5)).fetch();
    producer.send(initialRecord);
    verify(metadata,times(4)).requestUpdate();
    verify(metadata,times(4)).awaitUpdate(anyInt(),anyLong());
    verify(metadata,times(6)).fetch();
    try {
      producer.send(extendedRecord);
      fail("Expected KafkaException to be raised");
    }
 catch (    KafkaException e) {
    }
    verify(metadata,times(5)).requestUpdate();
    verify(metadata,times(5)).awaitUpdate(anyInt(),anyLong());
    verify(metadata,times(8)).fetch();
    producer.send(extendedRecord);
    verify(metadata,times(6)).requestUpdate();
    verify(metadata,times(6)).awaitUpdate(anyInt(),anyLong());
    verify(metadata,times(10)).fetch();
    producer.close(0,TimeUnit.MILLISECONDS);
  }
  @Test public void testTopicRefreshInMetadata() throws InterruptedException {
    Properties props=new Properties();
    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9999");
    props.setProperty(ProducerConfig.MAX_BLOCK_MS_CONFIG,"600000");
    ProducerConfig config=new ProducerConfig(ProducerConfig.addSerializerToConfig(props,new StringSerializer(),new StringSerializer()));
    long refreshBackoffMs=500L;
    long metadataExpireMs=60000L;
    final Metadata metadata=new Metadata(refreshBackoffMs,metadataExpireMs,true,true,new ClusterResourceListeners());
    final Time time=new MockTime();
    final String topic="topic";
    try (KafkaProducer<String,String> producer=new KafkaProducer<>(config,null,null,metadata,null,null,time)){
      Thread t=new Thread(() -> {
        long startTimeMs=System.currentTimeMillis();
        for (int i=0; i < 10; i++) {
          while (!metadata.updateRequested() && System.currentTimeMillis() - startTimeMs < 1000)           Thread.yield();
          metadata.update(Cluster.empty(),Collections.singleton(topic),time.milliseconds());
          time.sleep(60 * 1000L);
        }
      }
);
      t.start();
      try {
        producer.partitionsFor(topic);
        fail("Expect TimeoutException");
      }
 catch (      TimeoutException e) {
      }
      t.join();
    }
     assertTrue("Topic should still exist in metadata",metadata.containsTopic(topic));
  }
  @Test @Deprecated public void testHeadersWithExtendedClasses(){
    doTestHeaders(ExtendedSerializer.class);
  }
  @Test public void testHeaders(){
    doTestHeaders(Serializer.class);
  }
  private <T extends Serializer<String>>void doTestHeaders(  Class<T> serializerClassToMock){
    Properties props=new Properties();
    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9999");
    @SuppressWarnings("unchecked") Serializer<String> keySerializer=mock(serializerClassToMock);
    @SuppressWarnings("unchecked") Serializer<String> valueSerializer=mock(serializerClassToMock);
    ProducerConfig config=new ProducerConfig(ProducerConfig.addSerializerToConfig(props,keySerializer,valueSerializer));
    String topic="topic";
    final Cluster cluster=new Cluster("dummy",Collections.singletonList(new Node(0,"host1",1000)),Collections.singletonList(new PartitionInfo(topic,0,null,new Node[0],new Node[0])),Collections.emptySet(),Collections.emptySet());
    Metadata metadata=new Metadata(0,90000,true);
    metadata.update(cluster,Collections.emptySet(),Time.SYSTEM.milliseconds());
    KafkaProducer<String,String> producer=new KafkaProducer<>(config,keySerializer,valueSerializer,metadata,null,null,Time.SYSTEM);
    when(keySerializer.serialize(any(),any(),any())).then(invocation -> invocation.<String>getArgument(2).getBytes());
    when(valueSerializer.serialize(any(),any(),any())).then(invocation -> invocation.<String>getArgument(2).getBytes());
    String value="value";
    String key="key";
    ProducerRecord<String,String> record=new ProducerRecord<>(topic,key,value);
    record.headers().add(new RecordHeader("test","header2".getBytes()));
    producer.send(record,null);
    try {
      record.headers().add(new RecordHeader("test","test".getBytes()));
      fail("Expected IllegalStateException to be raised");
    }
 catch (    IllegalStateException ise) {
    }
    assertArrayEquals(record.headers().lastHeader("test").value(),"header2".getBytes());
    verify(valueSerializer).serialize(topic,record.headers(),value);
    verify(keySerializer).serialize(topic,record.headers(),key);
    producer.close(0,TimeUnit.MILLISECONDS);
  }
  @Test public void closeShouldBeIdempotent(){
    Properties producerProps=new Properties();
    producerProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9000");
    Producer producer=new KafkaProducer<>(producerProps,new ByteArraySerializer(),new ByteArraySerializer());
    producer.close();
    producer.close();
  }
  @Test public void testMetricConfigRecordingLevel(){
    Properties props=new Properties();
    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9000");
    try (KafkaProducer producer=new KafkaProducer<>(props,new ByteArraySerializer(),new ByteArraySerializer())){
      assertEquals(Sensor.RecordingLevel.INFO,producer.metrics.config().recordLevel());
    }
     props.put(ProducerConfig.METRICS_RECORDING_LEVEL_CONFIG,"DEBUG");
    try (KafkaProducer producer=new KafkaProducer<>(props,new ByteArraySerializer(),new ByteArraySerializer())){
      assertEquals(Sensor.RecordingLevel.DEBUG,producer.metrics.config().recordLevel());
    }
   }
  @Test public void testInterceptorPartitionSetOnTooLargeRecord(){
    Properties props=new Properties();
    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9999");
    props.setProperty(ProducerConfig.MAX_REQUEST_SIZE_CONFIG,"1");
    ProducerConfig config=new ProducerConfig(ProducerConfig.addSerializerToConfig(props,new StringSerializer(),new StringSerializer()));
    String topic="topic";
    ProducerRecord<String,String> record=new ProducerRecord<>(topic,"value");
    Metadata metadata=new Metadata(0,90000,true);
    final Cluster cluster=new Cluster("dummy",Collections.singletonList(new Node(0,"host1",1000)),Collections.singletonList(new PartitionInfo(topic,0,null,new Node[0],new Node[0])),Collections.emptySet(),Collections.emptySet());
    metadata.update(cluster,Collections.emptySet(),Time.SYSTEM.milliseconds());
    @SuppressWarnings("unchecked") ProducerInterceptors<String,String> interceptors=mock(ProducerInterceptors.class);
    KafkaProducer<String,String> producer=new KafkaProducer<>(config,null,null,metadata,null,interceptors,Time.SYSTEM);
    when(interceptors.onSend(any())).then(invocation -> invocation.getArgument(0));
    producer.send(record);
    verify(interceptors).onSend(record);
    verify(interceptors).onSendError(eq(record),notNull(),notNull());
    producer.close(0,TimeUnit.MILLISECONDS);
  }
  @Test public void testPartitionsForWithNullTopic(){
    Properties props=new Properties();
    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9000");
    try (KafkaProducer<byte[],byte[]> producer=new KafkaProducer<>(props,new ByteArraySerializer(),new ByteArraySerializer())){
      producer.partitionsFor(null);
      fail("Expected NullPointerException to be raised");
    }
 catch (    NullPointerException e) {
    }
  }
  @Test(expected=TimeoutException.class) public void testInitTransactionTimeout(){
    Properties props=new Properties();
    props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG,"bad-transaction");
    props.put(ProducerConfig.MAX_BLOCK_MS_CONFIG,5);
    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9000");
    Time time=Time.SYSTEM;
    Cluster cluster=TestUtils.singletonCluster("topic",1);
    Node node=cluster.nodes().get(0);
    Metadata metadata=new Metadata(0,Long.MAX_VALUE,true);
    metadata.update(cluster,Collections.emptySet(),time.milliseconds());
    MockClient client=new MockClient(time,metadata);
    client.setNode(node);
    try (Producer<String,String> producer=new KafkaProducer<>(new ProducerConfig(ProducerConfig.addSerializerToConfig(props,new StringSerializer(),new StringSerializer())),null,null,metadata,client,null,time)){
      producer.initTransactions();
      fail("initTransactions() should have raised TimeoutException");
    }
   }
  @Test(expected=KafkaException.class) public void testOnlyCanExecuteCloseAfterInitTransactionsTimeout(){
    Properties props=new Properties();
    props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG,"bad-transaction");
    props.put(ProducerConfig.MAX_BLOCK_MS_CONFIG,5);
    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9000");
    Time time=new MockTime();
    Cluster cluster=TestUtils.singletonCluster("topic",1);
    Node node=cluster.nodes().get(0);
    Metadata metadata=new Metadata(0,Long.MAX_VALUE,true);
    metadata.update(cluster,Collections.emptySet(),time.milliseconds());
    MockClient client=new MockClient(time,metadata);
    client.setNode(node);
    Producer<String,String> producer=new KafkaProducer<>(new ProducerConfig(ProducerConfig.addSerializerToConfig(props,new StringSerializer(),new StringSerializer())),null,null,metadata,client,null,time);
    try {
      producer.initTransactions();
    }
 catch (    TimeoutException e) {
    }
    try {
      producer.beginTransaction();
    }
  finally {
      producer.close(0,TimeUnit.MILLISECONDS);
    }
  }
  @Test public void testSendToInvalidTopic() throws Exception {
    Properties props=new Properties();
    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9000");
    props.put(ProducerConfig.MAX_BLOCK_MS_CONFIG,"15000");
    Time time=new MockTime();
    Cluster cluster=TestUtils.singletonCluster();
    Node node=cluster.nodes().get(0);
    Metadata metadata=new Metadata(0,Long.MAX_VALUE,true);
    metadata.update(cluster,Collections.emptySet(),time.milliseconds());
    MockClient client=new MockClient(time,metadata);
    client.setNode(node);
    Producer<String,String> producer=new KafkaProducer<>(new ProducerConfig(ProducerConfig.addSerializerToConfig(props,new StringSerializer(),new StringSerializer())),null,null,metadata,client,null,time);
    String invalidTopicName="topic abc";
    ProducerRecord<String,String> record=new ProducerRecord<>(invalidTopicName,"HelloKafka");
    Set<String> invalidTopic=new HashSet<>();
    invalidTopic.add(invalidTopicName);
    Cluster metaDataUpdateResponseCluster=new Cluster(cluster.clusterResource().clusterId(),cluster.nodes(),new ArrayList<>(0),Collections.emptySet(),invalidTopic,cluster.internalTopics(),cluster.controller());
    client.prepareMetadataUpdate(metaDataUpdateResponseCluster,Collections.emptySet());
    Future<RecordMetadata> future=producer.send(record);
    assertEquals("Cluster has incorrect invalid topic list",metaDataUpdateResponseCluster.invalidTopics(),metadata.fetch().invalidTopics());
    TestUtils.assertFutureError(future,InvalidTopicException.class);
    producer.close(0,TimeUnit.MILLISECONDS);
  }
  @Test public void testCloseWhenWaitingForMetadataUpdate() throws InterruptedException {
    Properties props=new Properties();
    props.put(ProducerConfig.MAX_BLOCK_MS_CONFIG,Long.MAX_VALUE);
    props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9000");
    String topicName="test";
    Time time=new MockTime();
    Cluster cluster=TestUtils.singletonCluster();
    Node node=cluster.nodes().get(0);
    Metadata metadata=new Metadata(0,Long.MAX_VALUE,false);
    metadata.update(cluster,Collections.emptySet(),time.milliseconds());
    MockClient client=new MockClient(time,metadata);
    client.setNode(node);
    Producer<String,String> producer=new KafkaProducer<>(new ProducerConfig(ProducerConfig.addSerializerToConfig(props,new StringSerializer(),new StringSerializer())),new StringSerializer(),new StringSerializer(),metadata,client,null,time);
    ExecutorService executor=Executors.newSingleThreadExecutor();
    final AtomicReference<Exception> sendException=new AtomicReference<>();
    try {
      executor.submit(() -> {
        try {
          producer.send(new ProducerRecord<>(topicName,"key","value"));
          fail();
        }
 catch (        Exception e) {
          sendException.set(e);
        }
      }
);
      TestUtils.waitForCondition(() -> metadata.containsTopic(topicName),"Timeout when waiting for topic to be added to metadata");
      producer.close(0,TimeUnit.MILLISECONDS);
      TestUtils.waitForCondition(() -> sendException.get() != null,"No producer exception within timeout");
      assertEquals(KafkaException.class,sendException.get().getClass());
    }
  finally {
      executor.shutdownNow();
    }
  }
}
