@Category({IntegrationTest.class}) public class PurgeRepartitionTopicIntegrationTest {
  private static final int NUM_BROKERS=1;
  private static final String INPUT_TOPIC="input-stream";
  private static final String APPLICATION_ID="restore-test";
  private static final String REPARTITION_TOPIC=APPLICATION_ID + "-KSTREAM-AGGREGATE-STATE-STORE-0000000002-repartition";
  private static AdminClient adminClient;
  private static KafkaStreams kafkaStreams;
  private static Integer purgeIntervalMs=10;
  private static Integer purgeSegmentBytes=2000;
  @ClassRule public static final EmbeddedKafkaCluster CLUSTER=new EmbeddedKafkaCluster(NUM_BROKERS,new Properties(){
{
      put("log.retention.check.interval.ms",purgeIntervalMs);
      put(TopicConfig.FILE_DELETE_DELAY_MS_CONFIG,0);
    }
  }
);
  private Time time=CLUSTER.time;
private class RepartitionTopicCreatedWithExpectedConfigs implements TestCondition {
    @Override final public boolean conditionMet(){
      try {
        final Set<String> topics=adminClient.listTopics().names().get();
        if (!topics.contains(REPARTITION_TOPIC)) {
          return false;
        }
      }
 catch (      final Exception e) {
        return false;
      }
      try {
        final ConfigResource resource=new ConfigResource(ConfigResource.Type.TOPIC,REPARTITION_TOPIC);
        final Config config=adminClient.describeConfigs(Collections.singleton(resource)).values().get(resource).get();
        return config.get(TopicConfig.CLEANUP_POLICY_CONFIG).value().equals(TopicConfig.CLEANUP_POLICY_DELETE) && config.get(TopicConfig.SEGMENT_MS_CONFIG).value().equals(purgeIntervalMs.toString()) && config.get(TopicConfig.SEGMENT_BYTES_CONFIG).value().equals(purgeSegmentBytes.toString());
      }
 catch (      final Exception e) {
        return false;
      }
    }
  }
private interface TopicSizeVerifier {
    boolean verify(    long currentSize);
  }
private class RepartitionTopicVerified implements TestCondition {
    private final TopicSizeVerifier verifier;
    RepartitionTopicVerified(    final TopicSizeVerifier verifier){
      this.verifier=verifier;
    }
    @Override public final boolean conditionMet(){
      time.sleep(purgeIntervalMs);
      try {
        final Collection<DescribeLogDirsResponse.LogDirInfo> logDirInfo=adminClient.describeLogDirs(Collections.singleton(0)).values().get(0).get().values();
        for (        final DescribeLogDirsResponse.LogDirInfo partitionInfo : logDirInfo) {
          final DescribeLogDirsResponse.ReplicaInfo replicaInfo=partitionInfo.replicaInfos.get(new TopicPartition(REPARTITION_TOPIC,0));
          if (replicaInfo != null && verifier.verify(replicaInfo.size)) {
            return true;
          }
        }
      }
 catch (      final Exception e) {
      }
      return false;
    }
  }
  @BeforeClass public static void createTopics() throws InterruptedException {
    CLUSTER.createTopic(INPUT_TOPIC,1,1);
  }
  @Before public void setup(){
    final Properties adminConfig=new Properties();
    adminConfig.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG,CLUSTER.bootstrapServers());
    adminClient=AdminClient.create(adminConfig);
    final Properties streamsConfiguration=new Properties();
    streamsConfiguration.put(StreamsConfig.APPLICATION_ID_CONFIG,APPLICATION_ID);
    streamsConfiguration.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG,purgeIntervalMs);
    streamsConfiguration.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG,CLUSTER.bootstrapServers());
    streamsConfiguration.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG,Serdes.Integer().getClass());
    streamsConfiguration.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,Serdes.Integer().getClass());
    streamsConfiguration.put(StreamsConfig.STATE_DIR_CONFIG,TestUtils.tempDirectory(APPLICATION_ID).getPath());
    streamsConfiguration.put(StreamsConfig.topicPrefix(TopicConfig.SEGMENT_MS_CONFIG),purgeIntervalMs);
    streamsConfiguration.put(StreamsConfig.topicPrefix(TopicConfig.SEGMENT_BYTES_CONFIG),purgeSegmentBytes);
    streamsConfiguration.put(StreamsConfig.producerPrefix(ProducerConfig.BATCH_SIZE_CONFIG),purgeSegmentBytes / 2);
    streamsConfiguration.put(IntegrationTestUtils.INTERNAL_LEAVE_GROUP_ON_CLOSE,true);
    final StreamsBuilder builder=new StreamsBuilder();
    builder.stream(INPUT_TOPIC).groupBy(MockMapper.selectKeyKeyValueMapper()).count();
    kafkaStreams=new KafkaStreams(builder.build(),streamsConfiguration,time);
  }
  @After public void shutdown() throws IOException {
    if (kafkaStreams != null) {
      kafkaStreams.close(Duration.ofSeconds(30));
    }
  }
  @Test public void shouldRestoreState() throws InterruptedException, ExecutionException {
    final List<KeyValue<Integer,Integer>> messages=new ArrayList<>();
    for (int i=0; i < 1000; i++) {
      messages.add(new KeyValue<>(i,i));
    }
    IntegrationTestUtils.produceKeyValuesSynchronouslyWithTimestamp(INPUT_TOPIC,messages,TestUtils.producerConfig(CLUSTER.bootstrapServers(),IntegerSerializer.class,IntegerSerializer.class),time.milliseconds());
    kafkaStreams.start();
    TestUtils.waitForCondition(new RepartitionTopicCreatedWithExpectedConfigs(),60000,"Repartition topic " + REPARTITION_TOPIC + " not created with the expected configs after 60000 ms.");
    TestUtils.waitForCondition(new RepartitionTopicVerified(new TopicSizeVerifier(){
      @Override public boolean verify(      final long currentSize){
        return currentSize > 0;
      }
    }
),60000,"Repartition topic " + REPARTITION_TOPIC + " not received data after 60000 ms.");
    TestUtils.waitForCondition(new RepartitionTopicVerified(new TopicSizeVerifier(){
      @Override public boolean verify(      final long currentSize){
        return currentSize <= purgeSegmentBytes;
      }
    }
),60000,"Repartition topic " + REPARTITION_TOPIC + " not purged data after 60000 ms.");
  }
}
