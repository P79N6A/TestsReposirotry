/** 
 * Startup and format tests
 */
public class TestAllowFormat {
  public static final String NAME_NODE_HOST="localhost:";
  public static final String NAME_NODE_HTTP_HOST="0.0.0.0:";
  private static final Logger LOG=LoggerFactory.getLogger(TestAllowFormat.class.getName());
  private static final File DFS_BASE_DIR=new File(PathUtils.getTestDir(TestAllowFormat.class),"dfs");
  private static Configuration config;
  private static MiniDFSCluster cluster=null;
  @BeforeClass public static void setUp() throws Exception {
    config=new Configuration();
    if (DFS_BASE_DIR.exists() && !FileUtil.fullyDelete(DFS_BASE_DIR)) {
      throw new IOException("Could not delete hdfs directory '" + DFS_BASE_DIR + "'");
    }
    LOG.info("hdfsdir is " + DFS_BASE_DIR.getAbsolutePath());
    File nameDir1=new File(DFS_BASE_DIR,"name1");
    File nameDir2=new File(DFS_BASE_DIR,"name2");
    nameDir1.mkdirs();
    config.set(DFS_NAMENODE_NAME_DIR_KEY,nameDir1.getPath() + "," + nameDir2.getPath());
    config.set(DFS_DATANODE_DATA_DIR_KEY,new File(DFS_BASE_DIR,"data").getPath());
    config.set(DFS_NAMENODE_CHECKPOINT_DIR_KEY,new File(DFS_BASE_DIR,"secondary").getPath());
    FileSystem.setDefaultUri(config,"hdfs://" + NAME_NODE_HOST + "0");
  }
  /** 
 * clean up
 */
  @AfterClass public static void tearDown() throws Exception {
    if (cluster != null) {
      cluster.shutdown();
      LOG.info("Stopping mini cluster");
    }
    if (DFS_BASE_DIR.exists() && !FileUtil.fullyDelete(DFS_BASE_DIR)) {
      throw new IOException("Could not delete hdfs directory in tearDown '" + DFS_BASE_DIR + "'");
    }
  }
  /** 
 * start MiniDFScluster, try formatting with different settings
 * @throws IOException
 * @throws InterruptedException 
 */
  @Test public void testAllowFormat() throws IOException {
    LOG.info("--starting mini cluster");
    NameNode nn;
    config.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY,true);
    cluster=new MiniDFSCluster.Builder(config).manageDataDfsDirs(false).manageNameDfsDirs(false).build();
    cluster.waitActive();
    assertNotNull(cluster);
    nn=cluster.getNameNode();
    assertNotNull(nn);
    LOG.info("Mini cluster created OK");
    LOG.info("Verifying format will fail with allowformat false");
    config.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY,false);
    try {
      cluster.shutdown();
      NameNode.format(config);
      fail("Format succeeded, when it should have failed");
    }
 catch (    IOException e) {
      assertTrue("Exception was not about formatting Namenode",e.getMessage().startsWith("The option " + DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY));
      LOG.info("Expected failure: " + StringUtils.stringifyException(e));
      LOG.info("Done verifying format will fail with allowformat false");
    }
    LOG.info("Verifying format will succeed with allowformat true");
    config.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY,true);
    NameNode.format(config);
    LOG.info("Done verifying format will succeed with allowformat true");
  }
  /** 
 * Test to skip format for non file scheme directory configured
 * @throws Exception
 */
  @Test public void testFormatShouldBeIgnoredForNonFileBasedDirs() throws Exception {
    Configuration conf=new HdfsConfiguration();
    String logicalName="mycluster";
    String localhost="127.0.0.1";
    InetSocketAddress nnAddr1=new InetSocketAddress(localhost,8020);
    InetSocketAddress nnAddr2=new InetSocketAddress(localhost,8020);
    HATestUtil.setFailoverConfigurations(conf,logicalName,nnAddr1,nnAddr2);
    conf.set(DFS_NAMENODE_NAME_DIR_KEY,new File(DFS_BASE_DIR,"name").getAbsolutePath());
    conf.setBoolean(DFS_NAMENODE_SUPPORT_ALLOW_FORMAT_KEY,true);
    conf.set(DFSUtil.addKeySuffixes(DFSConfigKeys.DFS_NAMENODE_EDITS_PLUGIN_PREFIX,"dummy"),DummyJournalManager.class.getName());
    conf.set(DFSConfigKeys.DFS_NAMENODE_SHARED_EDITS_DIR_KEY,"dummy://" + localhost + ":2181/ledgers");
    conf.set(DFSConfigKeys.DFS_HA_NAMENODE_ID_KEY,"nn1");
    NameNode.format(conf);
  }
}
