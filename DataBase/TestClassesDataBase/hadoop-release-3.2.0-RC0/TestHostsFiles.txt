/** 
 * DFS_HOSTS and DFS_HOSTS_EXCLUDE tests
 */
@RunWith(Parameterized.class) public class TestHostsFiles {
  private static final Logger LOG=LoggerFactory.getLogger(TestHostsFiles.class.getName());
  private Class hostFileMgrClass;
  public TestHostsFiles(  Class hostFileMgrClass){
    this.hostFileMgrClass=hostFileMgrClass;
  }
  @Parameterized.Parameters public static Iterable<Object[]> data(){
    return Arrays.asList(new Object[][]{{HostFileManager.class},{CombinedHostFileManager.class}});
  }
  private Configuration getConf(){
    Configuration conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1L);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,1);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_RECONSTRUCTION_PENDING_TIMEOUT_SEC_KEY,1);
    conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY,1000L);
    conf.set(DFSConfigKeys.NET_TOPOLOGY_SCRIPT_FILE_NAME_KEY,"xyz");
    conf.setClass(DFSConfigKeys.DFS_NAMENODE_HOSTS_PROVIDER_CLASSNAME_KEY,hostFileMgrClass,HostConfigManager.class);
    return conf;
  }
  @Test public void testHostsExcludeInUI() throws Exception {
    Configuration conf=getConf();
    short REPLICATION_FACTOR=2;
    final Path filePath=new Path("/testFile");
    HostsFileWriter hostsFileWriter=new HostsFileWriter();
    hostsFileWriter.initialize(conf,"temp/decommission");
    String racks[]={"/rack1","/rack1","/rack2","/rack2"};
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(racks.length).racks(racks).build();
    final FSNamesystem ns=cluster.getNameNode().getNamesystem();
    try {
      final FileSystem fs=cluster.getFileSystem();
      DFSTestUtil.createFile(fs,filePath,1L,REPLICATION_FACTOR,1L);
      ExtendedBlock b=DFSTestUtil.getFirstBlock(fs,filePath);
      DFSTestUtil.waitForReplication(cluster,b,2,REPLICATION_FACTOR,0);
      BlockLocation locs[]=fs.getFileBlockLocations(fs.getFileStatus(filePath),0,Long.MAX_VALUE);
      String name=locs[0].getNames()[0];
      LOG.info("adding '" + name + "' to decommission");
      hostsFileWriter.initExcludeHost(name);
      ns.getBlockManager().getDatanodeManager().refreshNodes(conf);
      DFSTestUtil.waitForDecommission(fs,name);
      DFSTestUtil.waitForReplication(cluster,b,2,REPLICATION_FACTOR,0);
      MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();
      ObjectName mxbeanName=new ObjectName("Hadoop:service=NameNode,name=NameNodeInfo");
      String nodes=(String)mbs.getAttribute(mxbeanName,"LiveNodes");
      assertTrue("Live nodes should contain the decommissioned node",nodes.contains("Decommissioned"));
    }
  finally {
      if (cluster != null) {
        cluster.shutdown();
      }
      hostsFileWriter.cleanup();
    }
  }
  @Test public void testHostsIncludeForDeadCount() throws Exception {
    Configuration conf=getConf();
    HostsFileWriter hostsFileWriter=new HostsFileWriter();
    hostsFileWriter.initialize(conf,"temp/decommission");
    hostsFileWriter.initIncludeHosts(new String[]{"localhost:52","127.0.0.1:7777"});
    MiniDFSCluster cluster=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(0).build();
      final FSNamesystem ns=cluster.getNameNode().getNamesystem();
      assertTrue(ns.getNumDeadDataNodes() == 2);
      assertTrue(ns.getNumLiveDataNodes() == 0);
      MBeanServer mbs=ManagementFactory.getPlatformMBeanServer();
      ObjectName mxbeanName=new ObjectName("Hadoop:service=NameNode,name=FSNamesystemState");
      String nodes=mbs.getAttribute(mxbeanName,"NumDeadDataNodes") + "";
      assertTrue((Integer)mbs.getAttribute(mxbeanName,"NumDeadDataNodes") == 2);
      assertTrue((Integer)mbs.getAttribute(mxbeanName,"NumLiveDataNodes") == 0);
    }
  finally {
      if (cluster != null) {
        cluster.shutdown();
      }
      hostsFileWriter.cleanup();
    }
  }
}
