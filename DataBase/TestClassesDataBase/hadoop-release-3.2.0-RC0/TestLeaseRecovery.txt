public class TestLeaseRecovery {
  static final int BLOCK_SIZE=1024;
  static final short REPLICATION_NUM=(short)3;
  private static final long LEASE_PERIOD=300L;
  private MiniDFSCluster cluster;
  @After public void shutdown() throws IOException {
    if (cluster != null) {
      cluster.shutdown();
      cluster=null;
    }
  }
  static void checkMetaInfo(  ExtendedBlock b,  DataNode dn) throws IOException {
    TestInterDatanodeProtocol.checkMetaInfo(b,dn);
  }
  static int min(  Integer... x){
    int m=x[0];
    for (int i=1; i < x.length; i++) {
      if (x[i] < m) {
        m=x[i];
      }
    }
    return m;
  }
  void waitLeaseRecovery(  MiniDFSCluster cluster){
    cluster.setLeasePeriod(LEASE_PERIOD,LEASE_PERIOD);
    try {
      Thread.sleep(2 * 3000);
    }
 catch (    InterruptedException e) {
    }
  }
  /** 
 * The following test first creates a file with a few blocks. It randomly truncates the replica of the last block stored in each datanode. Finally, it triggers block synchronization to synchronize all stored block.
 */
  @Test public void testBlockSynchronization() throws Exception {
    final int ORG_FILE_SIZE=3000;
    Configuration conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,BLOCK_SIZE);
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(5).build();
    cluster.waitActive();
    DistributedFileSystem dfs=cluster.getFileSystem();
    String filestr="/foo";
    Path filepath=new Path(filestr);
    DFSTestUtil.createFile(dfs,filepath,ORG_FILE_SIZE,REPLICATION_NUM,0L);
    assertTrue(dfs.exists(filepath));
    DFSTestUtil.waitReplication(dfs,filepath,REPLICATION_NUM);
    LocatedBlock locatedblock=TestInterDatanodeProtocol.getLastLocatedBlock(dfs.dfs.getNamenode(),filestr);
    DatanodeInfo[] datanodeinfos=locatedblock.getLocations();
    assertEquals(REPLICATION_NUM,datanodeinfos.length);
    DataNode[] datanodes=new DataNode[REPLICATION_NUM];
    for (int i=0; i < REPLICATION_NUM; i++) {
      datanodes[i]=cluster.getDataNode(datanodeinfos[i].getIpcPort());
      assertTrue(datanodes[i] != null);
    }
    ExtendedBlock lastblock=locatedblock.getBlock();
    DataNode.LOG.info("newblocks=" + lastblock);
    for (int i=0; i < REPLICATION_NUM; i++) {
      checkMetaInfo(lastblock,datanodes[i]);
    }
    DataNode.LOG.info("dfs.dfs.clientName=" + dfs.dfs.clientName);
    cluster.getNameNodeRpc().append(filestr,dfs.dfs.clientName,new EnumSetWritable<>(EnumSet.of(CreateFlag.APPEND)));
    waitLeaseRecovery(cluster);
    Block[] updatedmetainfo=new Block[REPLICATION_NUM];
    long oldSize=lastblock.getNumBytes();
    lastblock=TestInterDatanodeProtocol.getLastLocatedBlock(dfs.dfs.getNamenode(),filestr).getBlock();
    long currentGS=lastblock.getGenerationStamp();
    for (int i=0; i < REPLICATION_NUM; i++) {
      updatedmetainfo[i]=DataNodeTestUtils.getFSDataset(datanodes[i]).getStoredBlock(lastblock.getBlockPoolId(),lastblock.getBlockId());
      assertEquals(lastblock.getBlockId(),updatedmetainfo[i].getBlockId());
      assertEquals(oldSize,updatedmetainfo[i].getNumBytes());
      assertEquals(currentGS,updatedmetainfo[i].getGenerationStamp());
    }
    System.out.println("Testing that lease recovery cannot happen during safemode.");
    filestr="/foo.safemode";
    filepath=new Path(filestr);
    dfs.create(filepath,(short)1);
    cluster.getNameNodeRpc().setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_ENTER,false);
    assertTrue(dfs.dfs.exists(filestr));
    DFSTestUtil.waitReplication(dfs,filepath,(short)1);
    waitLeaseRecovery(cluster);
    LeaseManager lm=NameNodeAdapter.getLeaseManager(cluster.getNamesystem());
    assertTrue("Found " + lm.countLease() + " lease, expected 1",lm.countLease() == 1);
    cluster.getNameNodeRpc().setSafeMode(HdfsConstants.SafeModeAction.SAFEMODE_LEAVE,false);
  }
  /** 
 * Block Recovery when the meta file not having crcs for all chunks in block file
 */
  @Test public void testBlockRecoveryWithLessMetafile() throws Exception {
    Configuration conf=new Configuration();
    conf.set(DFSConfigKeys.DFS_BLOCK_LOCAL_PATH_ACCESS_USER_KEY,UserGroupInformation.getCurrentUser().getShortUserName());
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).build();
    Path file=new Path("/testRecoveryFile");
    DistributedFileSystem dfs=cluster.getFileSystem();
    FSDataOutputStream out=dfs.create(file);
    final int FILE_SIZE=2 * 1024 * 1024;
    int count=0;
    while (count < FILE_SIZE) {
      out.writeBytes("Data");
      count+=4;
    }
    out.hsync();
    ((DFSOutputStream)out.getWrappedStream()).abort();
    LocatedBlocks locations=cluster.getNameNodeRpc().getBlockLocations(file.toString(),0,count);
    ExtendedBlock block=locations.get(0).getBlock();
    final int CHECKSUM_SIZE=4;
    final int bytesPerChecksum=conf.getInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_DEFAULT);
    final int metaFileSize=(FILE_SIZE + bytesPerChecksum - 1) / bytesPerChecksum * CHECKSUM_SIZE + 8;
    final int newMetaFileSize=metaFileSize - CHECKSUM_SIZE;
    cluster.truncateMeta(0,block,newMetaFileSize);
    DataNodeProperties dnProp=cluster.stopDataNode(0);
    cluster.restartDataNode(dnProp,true);
    DistributedFileSystem newdfs=(DistributedFileSystem)FileSystem.newInstance(cluster.getConfiguration(0));
    count=0;
    while (++count < 10 && !newdfs.recoverLease(file)) {
      Thread.sleep(1000);
    }
    assertTrue("File should be closed",newdfs.recoverLease(file));
    final long expectedNewFileLen=FILE_SIZE - bytesPerChecksum;
    final long newFileLen=newdfs.getFileStatus(file).getLen();
    assertEquals(newFileLen,expectedNewFileLen);
  }
  /** 
 * Block/lease recovery should be retried with failed nodes from the second stage removed to avoid perpetual recovery failures.
 */
  @Test public void testBlockRecoveryRetryAfterFailedRecovery() throws Exception {
    Configuration conf=new Configuration();
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
    Path file=new Path("/testBlockRecoveryRetryAfterFailedRecovery");
    DistributedFileSystem dfs=cluster.getFileSystem();
    FSDataOutputStream out=dfs.create(file);
    final int FILE_SIZE=128 * 1024;
    int count=0;
    while (count < FILE_SIZE) {
      out.writeBytes("DE K9SUL");
      count+=8;
    }
    out.hsync();
    ((DFSOutputStream)out.getWrappedStream()).abort();
    LocatedBlocks locations=cluster.getNameNodeRpc().getBlockLocations(file.toString(),0,count);
    ExtendedBlock block=locations.get(0).getBlock();
    cluster.getDataNodes().get(0).getFSDataset().finalizeBlock(block,false);
    cluster.deleteMeta(0,block);
    DistributedFileSystem newDfs=(DistributedFileSystem)FileSystem.newInstance(cluster.getConfiguration(0));
    count=0;
    while (count++ < 15 && !newDfs.recoverLease(file)) {
      Thread.sleep(1000);
    }
    assertTrue("File should be closed",newDfs.recoverLease(file));
  }
  /** 
 * Recover the lease on a file and append file from another client.
 */
  @Test public void testLeaseRecoveryAndAppend() throws Exception {
    Configuration conf=new Configuration();
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).build();
      Path file=new Path("/testLeaseRecovery");
      DistributedFileSystem dfs=cluster.getFileSystem();
      FSDataOutputStream out=dfs.create(file);
      out.hflush();
      out.hsync();
      ((DFSOutputStream)out.getWrappedStream()).abort();
      DistributedFileSystem newdfs=(DistributedFileSystem)FileSystem.newInstance(cluster.getConfiguration(0));
      try {
        newdfs.append(file);
        fail("Append to a file(lease is held by another client) should fail");
      }
 catch (      RemoteException e) {
        assertTrue(e.getMessage().contains("file lease is currently owned"));
      }
      boolean recoverLease=newdfs.recoverLease(file);
      assertTrue(recoverLease);
      FSDataOutputStream append=newdfs.append(file);
      append.write("test".getBytes());
      append.close();
    }
  finally {
      if (cluster != null) {
        cluster.shutdown();
        cluster=null;
      }
    }
  }
}
