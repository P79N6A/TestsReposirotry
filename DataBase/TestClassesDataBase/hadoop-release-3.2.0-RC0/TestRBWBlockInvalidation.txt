/** 
 * Test when RBW block is removed. Invalidation of the corrupted block happens and then the under replicated block gets replicated to the datanode.
 */
public class TestRBWBlockInvalidation {
  private static final Logger LOG=LoggerFactory.getLogger(TestRBWBlockInvalidation.class);
  private static NumberReplicas countReplicas(  final FSNamesystem namesystem,  ExtendedBlock block){
    final BlockManager blockManager=namesystem.getBlockManager();
    return blockManager.countNodes(blockManager.getStoredBlock(block.getLocalBlock()));
  }
  /** 
 * Test when a block's replica is removed from RBW folder in one of the datanode, namenode should ask to invalidate that corrupted block and schedule replication for one more replica for that under replicated block.
 */
  @Test(timeout=600000) public void testBlockInvalidationWhenRBWReplicaMissedInDN() throws IOException, InterruptedException {
    assumeNotWindows();
    Configuration conf=new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY,2);
    conf.setLong(DFSConfigKeys.DFS_BLOCKREPORT_INTERVAL_MSEC_KEY,300);
    conf.setLong(DFSConfigKeys.DFS_DATANODE_DIRECTORYSCAN_INTERVAL_KEY,1);
    conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1);
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(2).build();
    FSDataOutputStream out=null;
    try {
      final FSNamesystem namesystem=cluster.getNamesystem();
      FileSystem fs=cluster.getFileSystem();
      Path testPath=new Path("/tmp/TestRBWBlockInvalidation","foo1");
      out=fs.create(testPath,(short)2);
      out.writeBytes("HDFS-3157: " + testPath);
      out.hsync();
      cluster.startDataNodes(conf,1,true,null,null,null);
      ExtendedBlock blk=DFSTestUtil.getFirstBlock(fs,testPath);
      MaterializedReplica replica=cluster.getMaterializedReplica(0,blk);
      replica.deleteData();
      replica.deleteMeta();
      out.close();
      int liveReplicas=0;
      while (true) {
        if ((liveReplicas=countReplicas(namesystem,blk).liveReplicas()) < 2) {
          LOG.info("Live Replicas after corruption: " + liveReplicas);
          break;
        }
        Thread.sleep(100);
      }
      assertEquals("There should be less than 2 replicas in the " + "liveReplicasMap",1,liveReplicas);
      while (true) {
        if ((liveReplicas=countReplicas(namesystem,blk).liveReplicas()) > 1) {
          LOG.info("Live Replicas after Rereplication: " + liveReplicas);
          break;
        }
        Thread.sleep(100);
      }
      assertEquals("There should be two live replicas",2,liveReplicas);
      while (true) {
        Thread.sleep(100);
        if (countReplicas(namesystem,blk).corruptReplicas() == 0) {
          LOG.info("Corrupt Replicas becomes 0");
          break;
        }
      }
    }
  finally {
      if (out != null) {
        out.close();
      }
      cluster.shutdown();
    }
  }
  /** 
 * Regression test for HDFS-4799, a case where, upon restart, if there were RWR replicas with out-of-date genstamps, the NN could accidentally delete good replicas instead of the bad replicas.
 */
  @Test(timeout=120000) public void testRWRInvalidation() throws Exception {
    Configuration conf=new HdfsConfiguration();
    conf.setClass(DFSConfigKeys.DFS_BLOCK_REPLICATOR_CLASSNAME_KEY,RandomDeleterPolicy.class,BlockPlacementPolicy.class);
    conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1);
    int numFiles=10;
    List<Path> testPaths=Lists.newArrayList();
    for (int i=0; i < numFiles; i++) {
      testPaths.add(new Path("/test" + i));
    }
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(2).build();
    try {
      List<FSDataOutputStream> streams=Lists.newArrayList();
      try {
        for (        Path path : testPaths) {
          FSDataOutputStream out=cluster.getFileSystem().create(path,(short)2);
          streams.add(out);
          out.writeBytes("old gs data\n");
          out.hflush();
        }
        for (        Path path : testPaths) {
          DFSTestUtil.waitReplication(cluster.getFileSystem(),path,(short)2);
        }
        DataNodeProperties oldGenstampNode=cluster.stopDataNode(0);
        for (int i=0; i < streams.size(); i++) {
          Path path=testPaths.get(i);
          FSDataOutputStream out=streams.get(i);
          out.writeBytes("new gs data\n");
          out.hflush();
          cluster.getFileSystem().setReplication(path,(short)1);
          out.close();
        }
        for (        Path path : testPaths) {
          DFSTestUtil.waitReplication(cluster.getFileSystem(),path,(short)1);
        }
        LOG.info("=========================== restarting cluster");
        DataNodeProperties otherNode=cluster.stopDataNode(0);
        cluster.restartNameNode();
        cluster.restartDataNode(oldGenstampNode);
        cluster.waitActive();
        cluster.restartDataNode(otherNode);
        cluster.waitActive();
        cluster.getNameNode().getNamesystem().getBlockManager().computeInvalidateWork(2);
        cluster.triggerHeartbeats();
        HATestUtil.waitForDNDeletions(cluster);
        cluster.triggerDeletionReports();
        waitForNumTotalBlocks(cluster,numFiles);
        for (        Path path : testPaths) {
          String ret=DFSTestUtil.readFile(cluster.getFileSystem(),path);
          assertEquals("old gs data\n" + "new gs data\n",ret);
        }
      }
  finally {
        IOUtils.cleanupWithLogger(LOG,streams.toArray(new Closeable[0]));
      }
    }
  finally {
      cluster.shutdown();
    }
  }
  @Test public void testRWRShouldNotAddedOnDNRestart() throws Exception {
    Configuration conf=new HdfsConfiguration();
    conf.set("dfs.client.block.write.replace-datanode-on-failure.enable","false");
    try (MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(2).build()){
      Path path=new Path("/testRBW");
      FSDataOutputStream out=cluster.getFileSystem().create(path,(short)2);
      out.writeBytes("old gs data\n");
      out.hflush();
      DataNodeProperties dnProp=cluster.stopDataNode(0);
      String dnAddress=dnProp.getDatanode().getXferAddress().toString();
      if (dnAddress.startsWith("/")) {
        dnAddress=dnAddress.substring(1);
      }
      out.writeBytes("old gs data\n");
      out.hflush();
      cluster.restartDataNode(dnProp,true);
      Thread.sleep(3000);
      BlockLocation[] locations=cluster.getFileSystem().getFileBlockLocations(path,0,Long.MAX_VALUE);
      String[] names=locations[0].getNames();
      for (      String node : names) {
        if (node.equals(dnAddress)) {
          fail("Old GS DN should not be present in latest block locations.");
        }
      }
      out.close();
    }
   }
  private void waitForNumTotalBlocks(  final MiniDFSCluster cluster,  final int numTotalBlocks) throws Exception {
    GenericTestUtils.waitFor(new Supplier<Boolean>(){
      @Override public Boolean get(){
        try {
          cluster.triggerBlockReports();
          if (cluster.getNamesystem().getBlocksTotal() == numTotalBlocks) {
            return true;
          }
        }
 catch (        Exception ignored) {
        }
        return false;
      }
    }
,1000,60000);
  }
}
