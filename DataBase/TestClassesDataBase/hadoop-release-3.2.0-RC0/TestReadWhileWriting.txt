/** 
 * Test reading from hdfs while a file is being written. 
 */
public class TestReadWhileWriting {
{
    GenericTestUtils.setLogLevel(FSNamesystem.LOG,Level.TRACE);
    GenericTestUtils.setLogLevel(DFSClient.LOG,Level.TRACE);
  }
  private static final String DIR="/" + TestReadWhileWriting.class.getSimpleName() + "/";
  private static final int BLOCK_SIZE=8192;
  private static final long SOFT_LEASE_LIMIT=500;
  private static final long HARD_LEASE_LIMIT=1000 * 600;
  /** 
 * Test reading while writing. 
 */
  @Test public void pipeline_02_03() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1);
    final MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(4).build();
    try {
      cluster.setLeasePeriod(SOFT_LEASE_LIMIT,HARD_LEASE_LIMIT);
      cluster.waitActive();
      final FileSystem fs=cluster.getFileSystem();
      final Path p=new Path(DIR,"file1");
      final int half=BLOCK_SIZE / 2;
{
        final FSDataOutputStream out=fs.create(p,true,fs.getConf().getInt(CommonConfigurationKeys.IO_FILE_BUFFER_SIZE_KEY,4096),(short)3,BLOCK_SIZE);
        write(out,0,half);
        ((DFSOutputStream)out.getWrappedStream()).hflush();
      }
      checkFile(p,half,conf);
      AppendTestUtil.LOG.info("leasechecker.interruptAndJoin()");
      ((DistributedFileSystem)fs).dfs.getLeaseRenewer().interruptAndJoin();
{
        Thread.sleep(2 * SOFT_LEASE_LIMIT);
        final UserGroupInformation current=UserGroupInformation.getCurrentUser();
        final UserGroupInformation ugi=UserGroupInformation.createUserForTesting(current.getShortUserName() + "x",new String[]{"supergroup"});
        final DistributedFileSystem dfs=ugi.doAs(new PrivilegedExceptionAction<DistributedFileSystem>(){
          @Override public DistributedFileSystem run() throws Exception {
            return (DistributedFileSystem)FileSystem.newInstance(conf);
          }
        }
);
        final FSDataOutputStream out=append(dfs,p);
        write(out,0,half);
        out.close();
      }
      checkFile(p,2 * half,conf);
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Try openning a file for append. 
 */
  private static FSDataOutputStream append(  FileSystem fs,  Path p) throws Exception {
    for (int i=0; i < 10; i++) {
      try {
        return fs.append(p);
      }
 catch (      RemoteException re) {
        if (re.getClassName().equals(RecoveryInProgressException.class.getName())) {
          AppendTestUtil.LOG.info("Will sleep and retry, i=" + i + ", p="+ p,re);
          Thread.sleep(1000);
        }
 else         throw re;
      }
    }
    throw new IOException("Cannot append to " + p);
  }
  static private int userCount=0;
  static void checkFile(  Path p,  int expectedsize,  final Configuration conf) throws IOException, InterruptedException {
    final String username=UserGroupInformation.getCurrentUser().getShortUserName() + "_" + ++userCount;
    UserGroupInformation ugi=UserGroupInformation.createUserForTesting(username,new String[]{"supergroup"});
    final FileSystem fs=DFSTestUtil.getFileSystemAs(ugi,conf);
    final HdfsDataInputStream in=(HdfsDataInputStream)fs.open(p);
    Assert.assertTrue(in.getVisibleLength() >= expectedsize);
    for (int i=0; i < expectedsize; i++) {
      Assert.assertEquals((byte)i,(byte)in.read());
    }
    in.close();
  }
  /** 
 * Write something to a file 
 */
  private static void write(  OutputStream out,  int offset,  int length) throws IOException {
    final byte[] bytes=new byte[length];
    for (int i=0; i < length; i++) {
      bytes[i]=(byte)(offset + i);
    }
    out.write(bytes);
  }
}
