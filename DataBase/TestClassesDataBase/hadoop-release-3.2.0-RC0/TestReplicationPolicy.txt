@RunWith(Parameterized.class) public class TestReplicationPolicy extends BaseReplicationPolicyTest {
  private static final String filename="/dummyfile.txt";
  private static final long staleInterval=DFSConfigKeys.DFS_NAMENODE_STALE_DATANODE_INTERVAL_DEFAULT;
  @Rule public ExpectedException exception=ExpectedException.none();
  public TestReplicationPolicy(  String blockPlacementPolicyClassName){
    this.blockPlacementPolicy=blockPlacementPolicyClassName;
  }
  @Parameterized.Parameters public static Iterable<Object[]> data(){
    return Arrays.asList(new Object[][]{{BlockPlacementPolicyDefault.class.getName()},{BlockPlacementPolicyWithUpgradeDomain.class.getName()}});
  }
  private void updateHeartbeatForExtraStorage(  long capacity,  long dfsUsed,  long remaining,  long blockPoolUsed){
    DatanodeDescriptor dn=dataNodes[5];
    dn.getStorageInfos()[1].setUtilizationForTesting(capacity,dfsUsed,remaining,blockPoolUsed);
    dn.updateHeartbeat(BlockManagerTestUtil.getStorageReportsForDatanode(dn),0L,0L,0,0,null);
  }
  private void resetHeartbeatForStorages(){
    for (int i=0; i < dataNodes.length; i++) {
      updateHeartbeatWithUsage(dataNodes[i],2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,0L,0L,0,0);
    }
    updateHeartbeatForExtraStorage(0L,0L,0L,0L);
  }
  @Override DatanodeDescriptor[] getDatanodeDescriptors(  Configuration conf){
    final String[] racks={"/d1/r1","/d1/r1","/d1/r2","/d1/r2","/d2/r3","/d2/r3"};
    storages=DFSTestUtil.createDatanodeStorageInfos(racks);
    DatanodeStorage extraStorage=new DatanodeStorage(storages[5].getStorageID() + "-extra",DatanodeStorage.State.NORMAL,StorageType.DEFAULT);
    BlockManagerTestUtil.updateStorage(storages[5].getDatanodeDescriptor(),extraStorage);
    return DFSTestUtil.toDatanodeDescriptor(storages);
  }
  /** 
 * Test whether the remaining space per storage is individually considered.
 */
  @Test public void testChooseNodeWithMultipleStorages1() throws Exception {
    updateHeartbeatWithUsage(dataNodes[5],2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,(2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE) / 3,0L,0L,0L,0,0);
    updateHeartbeatForExtraStorage(2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,(2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE) / 3,0L);
    DatanodeStorageInfo[] targets;
    targets=chooseTarget(1,dataNodes[5],new ArrayList<DatanodeStorageInfo>(),null);
    assertEquals(1,targets.length);
    assertEquals(storages[4],targets[0]);
    resetHeartbeatForStorages();
  }
  /** 
 * Test whether all storages on the datanode are considered while choosing target to place block.
 */
  @Test public void testChooseNodeWithMultipleStorages2() throws Exception {
    updateHeartbeatWithUsage(dataNodes[5],2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,(2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE) / 3,0L,0L,0L,0,0);
    updateHeartbeatForExtraStorage(2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L);
    DatanodeStorageInfo[] targets;
    targets=chooseTarget(1,dataNodes[5],new ArrayList<DatanodeStorageInfo>(),null);
    assertEquals(1,targets.length);
    assertEquals(dataNodes[5],targets[0].getDatanodeDescriptor());
    resetHeartbeatForStorages();
  }
  /** 
 * In this testcase, client is dataNodes[0]. So the 1st replica should be placed on dataNodes[0], the 2nd replica should be placed on  different rack and third should be placed on different node of rack chosen for 2nd node. The only excpetion is when the <i>numOfReplicas</i> is 2,  the 1st is on dataNodes[0] and the 2nd is on a different rack.
 * @throws Exception
 */
  @Test public void testChooseTarget1() throws Exception {
    updateHeartbeatWithUsage(dataNodes[0],2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,0L,0L,4,0);
    DatanodeStorageInfo[] targets;
    targets=chooseTarget(0);
    assertEquals(targets.length,0);
    targets=chooseTarget(1);
    assertEquals(targets.length,1);
    assertEquals(storages[0],targets[0]);
    targets=chooseTarget(2);
    assertEquals(targets.length,2);
    assertEquals(storages[0],targets[0]);
    assertFalse(isOnSameRack(targets[0],targets[1]));
    targets=chooseTarget(3);
    assertEquals(targets.length,3);
    assertEquals(storages[0],targets[0]);
    assertFalse(isOnSameRack(targets[0],targets[1]));
    assertTrue(isOnSameRack(targets[1],targets[2]));
    targets=chooseTarget(4);
    assertEquals(targets.length,4);
    assertEquals(storages[0],targets[0]);
    assertTrue(isOnSameRack(targets[1],targets[2]) || isOnSameRack(targets[2],targets[3]));
    assertFalse(isOnSameRack(targets[0],targets[2]));
    resetHeartbeatForStorages();
  }
  /** 
 * In this testcase, client is dataNodes[0], but the dataNodes[1] is not allowed to be chosen. So the 1st replica should be placed on dataNodes[0], the 2nd replica should be placed on a different rack, the 3rd should be on same rack as the 2nd replica, and the rest should be placed on a third rack.
 * @throws Exception
 */
  @Test public void testChooseTarget2() throws Exception {
    Set<Node> excludedNodes;
    DatanodeStorageInfo[] targets;
    List<DatanodeStorageInfo> chosenNodes=new ArrayList<>();
    excludedNodes=new HashSet<>();
    excludedNodes.add(dataNodes[1]);
    targets=chooseTarget(0,chosenNodes,excludedNodes);
    assertEquals(targets.length,0);
    excludedNodes.clear();
    chosenNodes.clear();
    excludedNodes.add(dataNodes[1]);
    targets=chooseTarget(1,chosenNodes,excludedNodes);
    assertEquals(targets.length,1);
    assertEquals(storages[0],targets[0]);
    excludedNodes.clear();
    chosenNodes.clear();
    excludedNodes.add(dataNodes[1]);
    targets=chooseTarget(2,chosenNodes,excludedNodes);
    assertEquals(targets.length,2);
    assertEquals(storages[0],targets[0]);
    assertFalse(isOnSameRack(targets[0],targets[1]));
    excludedNodes.clear();
    chosenNodes.clear();
    excludedNodes.add(dataNodes[1]);
    targets=chooseTarget(3,chosenNodes,excludedNodes);
    assertEquals(targets.length,3);
    assertEquals(storages[0],targets[0]);
    assertFalse(isOnSameRack(targets[0],targets[1]));
    assertTrue(isOnSameRack(targets[1],targets[2]));
    excludedNodes.clear();
    chosenNodes.clear();
    excludedNodes.add(dataNodes[1]);
    targets=chooseTarget(4,chosenNodes,excludedNodes);
    assertEquals(targets.length,4);
    assertEquals(storages[0],targets[0]);
    for (int i=1; i < 4; i++) {
      assertFalse(isOnSameRack(targets[0],targets[i]));
    }
    assertTrue(isOnSameRack(targets[1],targets[2]) || isOnSameRack(targets[2],targets[3]));
    assertFalse(isOnSameRack(targets[1],targets[3]));
    excludedNodes.clear();
    chosenNodes.clear();
    excludedNodes.add(dataNodes[1]);
    chosenNodes.add(storages[2]);
    targets=replicator.chooseTarget(filename,1,dataNodes[0],chosenNodes,true,excludedNodes,BLOCK_SIZE,TestBlockStoragePolicy.DEFAULT_STORAGE_POLICY,null);
    System.out.println("targets=" + Arrays.asList(targets));
    assertEquals(2,targets.length);
    int i=0;
    for (; i < targets.length && !storages[2].equals(targets[i]); i++)     ;
    assertTrue(i < targets.length);
  }
  /** 
 * In this testcase, client is dataNodes[0], but dataNodes[0] is not qualified to be chosen. So the 1st replica should be placed on dataNodes[1],  the 2nd replica should be placed on a different rack, the 3rd replica should be placed on the same rack as the 2nd replica, and the rest should be placed on the third rack.
 * @throws Exception
 */
  @Test public void testChooseTarget3() throws Exception {
    updateHeartbeatWithUsage(dataNodes[0],2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,(HdfsServerConstants.MIN_BLOCKS_FOR_WRITE - 1) * BLOCK_SIZE,0L,0L,0L,0,0);
    DatanodeStorageInfo[] targets;
    targets=chooseTarget(0);
    assertEquals(targets.length,0);
    targets=chooseTarget(1);
    assertEquals(targets.length,1);
    assertEquals(storages[1],targets[0]);
    targets=chooseTarget(2);
    assertEquals(targets.length,2);
    assertEquals(storages[1],targets[0]);
    assertFalse(isOnSameRack(targets[0],targets[1]));
    targets=chooseTarget(3);
    assertEquals(targets.length,3);
    assertEquals(storages[1],targets[0]);
    assertTrue(isOnSameRack(targets[1],targets[2]));
    assertFalse(isOnSameRack(targets[0],targets[1]));
    targets=chooseTarget(4);
    assertEquals(targets.length,4);
    assertEquals(storages[1],targets[0]);
    for (int i=1; i < 4; i++) {
      assertFalse(isOnSameRack(targets[0],targets[i]));
    }
    assertTrue(isOnSameRack(targets[1],targets[2]) || isOnSameRack(targets[2],targets[3]));
    assertFalse(isOnSameRack(targets[1],targets[3]));
    resetHeartbeatForStorages();
  }
  /** 
 * In this testcase, client is dataNodes[0], but none of the nodes on rack 1 is qualified to be chosen. So the 1st replica should be placed on either rack 2 or rack 3.  the 2nd replica should be placed on a different rack, the 3rd replica should be placed on the same rack as the 1st replica,
 * @throws Exception
 */
  @Test public void testChoooseTarget4() throws Exception {
    for (int i=0; i < 2; i++) {
      updateHeartbeatWithUsage(dataNodes[i],2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,(HdfsServerConstants.MIN_BLOCKS_FOR_WRITE - 1) * BLOCK_SIZE,0L,0L,0L,0,0);
    }
    DatanodeStorageInfo[] targets;
    targets=chooseTarget(0);
    assertEquals(targets.length,0);
    targets=chooseTarget(1);
    assertEquals(targets.length,1);
    assertFalse(isOnSameRack(targets[0],dataNodes[0]));
    targets=chooseTarget(2);
    assertEquals(targets.length,2);
    assertFalse(isOnSameRack(targets[0],dataNodes[0]));
    assertFalse(isOnSameRack(targets[0],targets[1]));
    targets=chooseTarget(3);
    assertEquals(targets.length,3);
    for (int i=0; i < 3; i++) {
      assertFalse(isOnSameRack(targets[i],dataNodes[0]));
    }
    assertTrue(isOnSameRack(targets[0],targets[1]) || isOnSameRack(targets[1],targets[2]));
    assertFalse(isOnSameRack(targets[0],targets[2]));
    resetHeartbeatForStorages();
  }
  /** 
 * In this testcase, client is is a node outside of file system. So the 1st replica can be placed on any node.  the 2nd replica should be placed on a different rack, the 3rd replica should be placed on the same rack as the 2nd replica,
 * @throws Exception
 */
  @Test public void testChooseTarget5() throws Exception {
    DatanodeDescriptor writerDesc=DFSTestUtil.getDatanodeDescriptor("7.7.7.7","/d2/r4");
    DatanodeStorageInfo[] targets;
    targets=chooseTarget(0,writerDesc);
    assertEquals(targets.length,0);
    targets=chooseTarget(1,writerDesc);
    assertEquals(targets.length,1);
    targets=chooseTarget(2,writerDesc);
    assertEquals(targets.length,2);
    assertFalse(isOnSameRack(targets[0],targets[1]));
    targets=chooseTarget(3,writerDesc);
    assertEquals(targets.length,3);
    assertTrue(isOnSameRack(targets[1],targets[2]));
    assertFalse(isOnSameRack(targets[0],targets[1]));
  }
  /** 
 * In this testcase, there are enough total number of nodes, but only one rack is actually available.
 * @throws Exception
 */
  @Test public void testChooseTarget6() throws Exception {
    DatanodeStorageInfo storage=DFSTestUtil.createDatanodeStorageInfo("DS-xxxx","7.7.7.7","/d2/r3","host7");
    DatanodeDescriptor newDn=storage.getDatanodeDescriptor();
    Set<Node> excludedNodes;
    List<DatanodeStorageInfo> chosenNodes=new ArrayList<>();
    excludedNodes=new HashSet<>();
    excludedNodes.add(dataNodes[0]);
    excludedNodes.add(dataNodes[1]);
    excludedNodes.add(dataNodes[2]);
    excludedNodes.add(dataNodes[3]);
    DatanodeStorageInfo[] targets;
    targets=chooseTarget(2,chosenNodes,excludedNodes);
    assertEquals(1,targets.length);
    final BlockManager bm=namenode.getNamesystem().getBlockManager();
    bm.getDatanodeManager().getNetworkTopology().add(newDn);
    bm.getDatanodeManager().getHeartbeatManager().addDatanode(newDn);
    updateHeartbeatWithUsage(newDn,2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,0L,0L,0,0);
    excludedNodes.clear();
    excludedNodes.add(dataNodes[0]);
    excludedNodes.add(dataNodes[1]);
    excludedNodes.add(dataNodes[2]);
    excludedNodes.add(dataNodes[3]);
    chosenNodes.clear();
    try {
      targets=chooseTarget(3,chosenNodes,excludedNodes);
      assertEquals(2,targets.length);
    }
  finally {
      bm.getDatanodeManager().getNetworkTopology().remove(newDn);
    }
    resetHeartbeatForStorages();
  }
  /** 
 * In this testcase, it tries to choose more targets than available nodes and check the result, with stale node avoidance on the write path enabled.
 * @throws Exception
 */
  @Test public void testChooseTargetWithMoreThanAvailableNodesWithStaleness() throws Exception {
    try {
      namenode.getNamesystem().getBlockManager().getDatanodeManager().setNumStaleNodes(dataNodes.length);
      testChooseTargetWithMoreThanAvailableNodes();
    }
  finally {
      namenode.getNamesystem().getBlockManager().getDatanodeManager().setNumStaleNodes(0);
    }
  }
  /** 
 * In this testcase, it tries to choose more targets than available nodes and check the result. 
 * @throws Exception
 */
  @Test public void testChooseTargetWithMoreThanAvailableNodes() throws Exception {
    for (int i=0; i < 2; i++) {
      updateHeartbeatWithUsage(dataNodes[i],2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,(HdfsServerConstants.MIN_BLOCKS_FOR_WRITE - 1) * BLOCK_SIZE,0L,0L,0L,0,0);
    }
    final LogVerificationAppender appender=new LogVerificationAppender();
    final Logger logger=Logger.getRootLogger();
    logger.addAppender(appender);
    DatanodeStorageInfo[] targets=chooseTarget(dataNodes.length);
    assertEquals(targets.length,dataNodes.length - 2);
    final List<LoggingEvent> log=appender.getLog();
    assertNotNull(log);
    assertFalse(log.size() == 0);
    final LoggingEvent lastLogEntry=log.get(log.size() - 1);
    assertTrue(Level.WARN.isGreaterOrEqual(lastLogEntry.getLevel()));
    assertTrue(((String)lastLogEntry.getMessage()).contains("in need of 2"));
    resetHeartbeatForStorages();
  }
  private boolean containsWithinRange(  DatanodeStorageInfo target,  DatanodeDescriptor[] nodes,  int startIndex,  int endIndex){
    assert startIndex >= 0 && startIndex < nodes.length;
    assert endIndex >= startIndex && endIndex < nodes.length;
    for (int i=startIndex; i <= endIndex; i++) {
      if (nodes[i].equals(target.getDatanodeDescriptor())) {
        return true;
      }
    }
    return false;
  }
  private boolean containsWithinRange(  DatanodeDescriptor target,  DatanodeStorageInfo[] nodes,  int startIndex,  int endIndex){
    assert startIndex >= 0 && startIndex < nodes.length;
    assert endIndex >= startIndex && endIndex < nodes.length;
    for (int i=startIndex; i <= endIndex; i++) {
      if (nodes[i].getDatanodeDescriptor().equals(target)) {
        return true;
      }
    }
    return false;
  }
  @Test public void testChooseTargetWithStaleNodes() throws Exception {
    DFSTestUtil.resetLastUpdatesWithOffset(dataNodes[0],-(staleInterval + 1));
    namenode.getNamesystem().getBlockManager().getDatanodeManager().getHeartbeatManager().heartbeatCheck();
    assertTrue(namenode.getNamesystem().getBlockManager().getDatanodeManager().shouldAvoidStaleDataNodesForWrite());
    DatanodeStorageInfo[] targets;
    targets=chooseTarget(1);
    assertEquals(targets.length,1);
    assertEquals(storages[1],targets[0]);
    Set<Node> excludedNodes=new HashSet<>();
    excludedNodes.add(dataNodes[1]);
    List<DatanodeStorageInfo> chosenNodes=new ArrayList<>();
    targets=chooseTarget(1,chosenNodes,excludedNodes);
    assertEquals(targets.length,1);
    assertFalse(isOnSameRack(targets[0],dataNodes[0]));
    DFSTestUtil.resetLastUpdatesWithOffset(dataNodes[0],0);
    namenode.getNamesystem().getBlockManager().getDatanodeManager().getHeartbeatManager().heartbeatCheck();
  }
  /** 
 * In this testcase, we set 3 nodes (dataNodes[0] ~ dataNodes[2]) as stale, and when the number of replicas is less or equal to 3, all the healthy datanodes should be returned by the chooseTarget method. When the number  of replicas is 4, a stale node should be included.
 * @throws Exception
 */
  @Test public void testChooseTargetWithHalfStaleNodes() throws Exception {
    for (int i=0; i < 3; i++) {
      DFSTestUtil.resetLastUpdatesWithOffset(dataNodes[i],-(staleInterval + 1));
    }
    namenode.getNamesystem().getBlockManager().getDatanodeManager().getHeartbeatManager().heartbeatCheck();
    DatanodeStorageInfo[] targets=chooseTarget(0);
    assertEquals(targets.length,0);
    targets=chooseTarget(1);
    assertEquals(targets.length,1);
    assertFalse(containsWithinRange(targets[0],dataNodes,0,2));
    targets=chooseTarget(2);
    assertEquals(targets.length,2);
    assertFalse(containsWithinRange(targets[0],dataNodes,0,2));
    assertFalse(containsWithinRange(targets[1],dataNodes,0,2));
    targets=chooseTarget(3);
    assertEquals(targets.length,3);
    assertTrue(containsWithinRange(targets[0],dataNodes,3,5));
    assertTrue(containsWithinRange(targets[1],dataNodes,3,5));
    assertTrue(containsWithinRange(targets[2],dataNodes,3,5));
    targets=chooseTarget(4);
    assertEquals(targets.length,4);
    assertTrue(containsWithinRange(dataNodes[3],targets,0,3));
    assertTrue(containsWithinRange(dataNodes[4],targets,0,3));
    assertTrue(containsWithinRange(dataNodes[5],targets,0,3));
    for (int i=0; i < dataNodes.length; i++) {
      DFSTestUtil.resetLastUpdatesWithOffset(dataNodes[i],0);
    }
    namenode.getNamesystem().getBlockManager().getDatanodeManager().getHeartbeatManager().heartbeatCheck();
  }
  @Test public void testChooseTargetWithMoreThanHalfStaleNodes() throws Exception {
    HdfsConfiguration conf=new HdfsConfiguration();
    conf.setBoolean(DFSConfigKeys.DFS_NAMENODE_AVOID_STALE_DATANODE_FOR_WRITE_KEY,true);
    String[] hosts=new String[]{"host1","host2","host3","host4","host5","host6"};
    String[] racks=new String[]{"/d1/r1","/d1/r1","/d1/r2","/d1/r2","/d2/r3","/d2/r3"};
    MiniDFSCluster miniCluster=new MiniDFSCluster.Builder(conf).racks(racks).hosts(hosts).numDataNodes(hosts.length).build();
    miniCluster.waitActive();
    try {
      for (int i=0; i < 2; i++) {
        DataNode dn=miniCluster.getDataNodes().get(i);
        DataNodeTestUtils.setHeartbeatsDisabledForTests(dn,true);
        DatanodeDescriptor dnDes=miniCluster.getNameNode().getNamesystem().getBlockManager().getDatanodeManager().getDatanode(dn.getDatanodeId());
        DFSTestUtil.resetLastUpdatesWithOffset(dnDes,-(staleInterval + 1));
      }
      miniCluster.getNameNode().getNamesystem().getBlockManager().getDatanodeManager().getHeartbeatManager().heartbeatCheck();
      int numStaleNodes=miniCluster.getNameNode().getNamesystem().getBlockManager().getDatanodeManager().getNumStaleNodes();
      assertEquals(numStaleNodes,2);
      assertTrue(miniCluster.getNameNode().getNamesystem().getBlockManager().getDatanodeManager().shouldAvoidStaleDataNodesForWrite());
      DatanodeDescriptor staleNodeInfo=miniCluster.getNameNode().getNamesystem().getBlockManager().getDatanodeManager().getDatanode(miniCluster.getDataNodes().get(0).getDatanodeId());
      BlockPlacementPolicy replicator=miniCluster.getNameNode().getNamesystem().getBlockManager().getBlockPlacementPolicy();
      DatanodeStorageInfo[] targets=replicator.chooseTarget(filename,3,staleNodeInfo,new ArrayList<DatanodeStorageInfo>(),false,null,BLOCK_SIZE,TestBlockStoragePolicy.DEFAULT_STORAGE_POLICY,null);
      assertEquals(targets.length,3);
      assertFalse(isOnSameRack(targets[0],staleNodeInfo));
      for (int i=0; i < 4; i++) {
        DataNode dn=miniCluster.getDataNodes().get(i);
        DataNodeTestUtils.setHeartbeatsDisabledForTests(dn,true);
        DatanodeDescriptor dnDesc=miniCluster.getNameNode().getNamesystem().getBlockManager().getDatanodeManager().getDatanode(dn.getDatanodeId());
        DFSTestUtil.resetLastUpdatesWithOffset(dnDesc,-(staleInterval + 1));
      }
      miniCluster.getNameNode().getNamesystem().getBlockManager().getDatanodeManager().getHeartbeatManager().heartbeatCheck();
      numStaleNodes=miniCluster.getNameNode().getNamesystem().getBlockManager().getDatanodeManager().getNumStaleNodes();
      assertEquals(numStaleNodes,4);
      assertFalse(miniCluster.getNameNode().getNamesystem().getBlockManager().getDatanodeManager().shouldAvoidStaleDataNodesForWrite());
      targets=replicator.chooseTarget(filename,3,staleNodeInfo,new ArrayList<DatanodeStorageInfo>(),false,null,BLOCK_SIZE,TestBlockStoragePolicy.DEFAULT_STORAGE_POLICY,null);
      assertEquals(targets.length,3);
      assertTrue(isOnSameRack(targets[0],staleNodeInfo));
      for (int i=2; i < 4; i++) {
        DataNode dn=miniCluster.getDataNodes().get(i);
        DataNodeTestUtils.setHeartbeatsDisabledForTests(dn,false);
        DatanodeDescriptor dnDesc=miniCluster.getNameNode().getNamesystem().getBlockManager().getDatanodeManager().getDatanode(dn.getDatanodeId());
        DFSTestUtil.resetLastUpdatesWithOffset(dnDesc,0);
      }
      miniCluster.getNameNode().getNamesystem().getBlockManager().getDatanodeManager().getHeartbeatManager().heartbeatCheck();
      numStaleNodes=miniCluster.getNameNode().getNamesystem().getBlockManager().getDatanodeManager().getNumStaleNodes();
      assertEquals(numStaleNodes,2);
      assertTrue(miniCluster.getNameNode().getNamesystem().getBlockManager().getDatanodeManager().shouldAvoidStaleDataNodesForWrite());
      targets=chooseTarget(3,staleNodeInfo);
      assertEquals(targets.length,3);
      assertFalse(isOnSameRack(targets[0],staleNodeInfo));
    }
  finally {
      miniCluster.shutdown();
    }
  }
  /** 
 * This testcase tests re-replication, when dataNodes[0] is already chosen. So the 1st replica can be placed on random rack.  the 2nd replica should be placed on different node by same rack as  the 1st replica. The 3rd replica can be placed randomly.
 * @throws Exception
 */
  @Test public void testRereplicate1() throws Exception {
    List<DatanodeStorageInfo> chosenNodes=new ArrayList<>();
    chosenNodes.add(storages[0]);
    DatanodeStorageInfo[] targets;
    targets=chooseTarget(0,chosenNodes);
    assertEquals(targets.length,0);
    targets=chooseTarget(1,chosenNodes);
    assertEquals(targets.length,1);
    assertFalse(isOnSameRack(targets[0],dataNodes[0]));
    targets=chooseTarget(2,chosenNodes);
    assertEquals(targets.length,2);
    assertTrue(isOnSameRack(targets[0],dataNodes[0]));
    assertFalse(isOnSameRack(targets[0],targets[1]));
    targets=chooseTarget(3,chosenNodes);
    assertEquals(targets.length,3);
    assertTrue(isOnSameRack(targets[0],dataNodes[0]));
    assertFalse(isOnSameRack(targets[0],targets[2]));
  }
  /** 
 * This testcase tests re-replication,  when dataNodes[0] and dataNodes[1] are already chosen. So the 1st replica should be placed on a different rack than rack 1.  the rest replicas can be placed randomly,
 * @throws Exception
 */
  @Test public void testRereplicate2() throws Exception {
    List<DatanodeStorageInfo> chosenNodes=new ArrayList<>();
    chosenNodes.add(storages[0]);
    chosenNodes.add(storages[1]);
    DatanodeStorageInfo[] targets;
    targets=chooseTarget(0,chosenNodes);
    assertEquals(targets.length,0);
    targets=chooseTarget(1,chosenNodes);
    assertEquals(targets.length,1);
    assertFalse(isOnSameRack(targets[0],dataNodes[0]));
    targets=chooseTarget(2,chosenNodes);
    assertEquals(targets.length,2);
    assertFalse(isOnSameRack(targets[0],dataNodes[0]));
    assertFalse(isOnSameRack(targets[1],dataNodes[0]));
  }
  /** 
 * This testcase tests re-replication,  when dataNodes[0] and dataNodes[2] are already chosen. So the 1st replica should be placed on the rack that the writer resides.  the rest replicas can be placed randomly,
 * @throws Exception
 */
  @Test public void testRereplicate3() throws Exception {
    List<DatanodeStorageInfo> chosenNodes=new ArrayList<>();
    chosenNodes.add(storages[0]);
    chosenNodes.add(storages[2]);
    DatanodeStorageInfo[] targets;
    targets=chooseTarget(0,chosenNodes);
    assertEquals(targets.length,0);
    targets=chooseTarget(1,chosenNodes);
    assertEquals(targets.length,1);
    assertTrue(isOnSameRack(targets[0],dataNodes[0]));
    assertFalse(isOnSameRack(targets[0],dataNodes[2]));
    targets=chooseTarget(1,dataNodes[2],chosenNodes);
    assertEquals(targets.length,1);
    assertTrue(isOnSameRack(targets[0],dataNodes[2]));
    assertFalse(isOnSameRack(targets[0],dataNodes[0]));
    targets=chooseTarget(2,chosenNodes);
    assertEquals(targets.length,2);
    assertTrue(isOnSameRack(targets[0],dataNodes[0]));
    targets=chooseTarget(2,dataNodes[2],chosenNodes);
    assertEquals(targets.length,2);
    assertTrue(isOnSameRack(targets[0],dataNodes[2]));
  }
  private BlockInfo genBlockInfo(  long id){
    return new BlockInfoContiguous(new Block(id),(short)3);
  }
  /** 
 * Test for the high priority blocks are processed before the low priority blocks.
 */
  @Test(timeout=60000) public void testReplicationWithPriority() throws Exception {
    int DFS_NAMENODE_REPLICATION_INTERVAL=1000;
    int HIGH_PRIORITY=0;
    Configuration conf=new Configuration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,1);
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(2).format(true).build();
    try {
      cluster.waitActive();
      final LowRedundancyBlocks neededReconstruction=cluster.getNameNode().getNamesystem().getBlockManager().neededReconstruction;
      for (int i=0; i < 100; i++) {
        neededReconstruction.add(genBlockInfo(ThreadLocalRandom.current().nextLong()),2,0,0,3);
      }
      Thread.sleep(DFS_NAMENODE_REPLICATION_INTERVAL);
      neededReconstruction.add(genBlockInfo(ThreadLocalRandom.current().nextLong()),1,0,0,3);
      Thread.sleep(DFS_NAMENODE_REPLICATION_INTERVAL);
      assertFalse("Not able to clear the element from high priority list",neededReconstruction.iterator(HIGH_PRIORITY).hasNext());
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Test for the ChooseLowRedundancyBlocks are processed based on priority
 */
  @Test public void testChooseLowRedundancyBlocks() throws Exception {
    LowRedundancyBlocks lowRedundancyBlocks=new LowRedundancyBlocks();
    for (int i=0; i < 5; i++) {
      lowRedundancyBlocks.add(genBlockInfo(ThreadLocalRandom.current().nextLong()),1,0,0,3);
      lowRedundancyBlocks.add(genBlockInfo(ThreadLocalRandom.current().nextLong()),2,0,0,7);
      lowRedundancyBlocks.add(genBlockInfo(ThreadLocalRandom.current().nextLong()),6,0,0,6);
      lowRedundancyBlocks.add(genBlockInfo(ThreadLocalRandom.current().nextLong()),5,0,0,6);
      lowRedundancyBlocks.add(genBlockInfo(ThreadLocalRandom.current().nextLong()),0,0,0,3);
    }
    List<List<BlockInfo>> chosenBlocks=lowRedundancyBlocks.chooseLowRedundancyBlocks(6);
    assertTheChosenBlocks(chosenBlocks,5,1,0,0,0);
    chosenBlocks=lowRedundancyBlocks.chooseLowRedundancyBlocks(10);
    assertTheChosenBlocks(chosenBlocks,0,4,5,1,0);
    lowRedundancyBlocks.add(genBlockInfo(ThreadLocalRandom.current().nextLong()),0,1,0,3);
    chosenBlocks=lowRedundancyBlocks.chooseLowRedundancyBlocks(10);
    assertTheChosenBlocks(chosenBlocks,1,0,0,4);
    chosenBlocks=lowRedundancyBlocks.chooseLowRedundancyBlocks(7);
    assertTheChosenBlocks(chosenBlocks,6,1,0,0,0);
  }
  /** 
 * asserts the chosen blocks with expected priority blocks 
 */
  private void assertTheChosenBlocks(  List<List<BlockInfo>> chosenBlocks,  int... expectedSizes){
    int i=0;
    for (; i < chosenBlocks.size(); i++) {
      assertEquals("Not returned the expected number for i=" + i,expectedSizes[i],chosenBlocks.get(i).size());
    }
    for (; i < expectedSizes.length; i++) {
      assertEquals("Expected size is non-zero for i=" + i,0,expectedSizes[i]);
    }
  }
  /** 
 * Test for the chooseReplicaToDelete are processed based on  block locality and free space
 */
  @Test public void testChooseReplicaToDelete() throws Exception {
    List<DatanodeStorageInfo> replicaList=new ArrayList<>();
    final Map<String,List<DatanodeStorageInfo>> rackMap=new HashMap<String,List<DatanodeStorageInfo>>();
    storages[0].setRemainingForTests(4 * 1024 * 1024);
    dataNodes[0].setRemaining(calculateRemaining(dataNodes[0]));
    replicaList.add(storages[0]);
    storages[1].setRemainingForTests(3 * 1024 * 1024);
    dataNodes[1].setRemaining(calculateRemaining(dataNodes[1]));
    replicaList.add(storages[1]);
    storages[2].setRemainingForTests(2 * 1024 * 1024);
    dataNodes[2].setRemaining(calculateRemaining(dataNodes[2]));
    replicaList.add(storages[2]);
    storages[4].setRemainingForTests(100 * 1024 * 1024);
    storages[5].setRemainingForTests(512 * 1024);
    dataNodes[5].setRemaining(calculateRemaining(dataNodes[5]));
    replicaList.add(storages[5]);
    for (int i=0; i < dataNodes.length; i++) {
      DFSTestUtil.resetLastUpdatesWithOffset(dataNodes[i],0);
    }
    List<DatanodeStorageInfo> first=new ArrayList<>();
    List<DatanodeStorageInfo> second=new ArrayList<>();
    replicator.splitNodesWithRack(replicaList,replicaList,rackMap,first,second);
    assertEquals(2,first.size());
    assertEquals(2,second.size());
    List<StorageType> excessTypes=new ArrayList<>();
{
      excessTypes.add(StorageType.SSD);
      assertNull(((BlockPlacementPolicyDefault)replicator).chooseReplicaToDelete(first,second,excessTypes,rackMap));
    }
    excessTypes.add(StorageType.DEFAULT);
    DatanodeStorageInfo chosen=((BlockPlacementPolicyDefault)replicator).chooseReplicaToDelete(first,second,excessTypes,rackMap);
    assertEquals(chosen,storages[5]);
    replicator.adjustSetsWithChosenReplica(rackMap,first,second,chosen);
    assertEquals(2,first.size());
    assertEquals(1,second.size());
    excessTypes.add(StorageType.DEFAULT);
    chosen=((BlockPlacementPolicyDefault)replicator).chooseReplicaToDelete(first,second,excessTypes,rackMap);
    assertEquals(chosen,storages[1]);
  }
  private long calculateRemaining(  DatanodeDescriptor dataNode){
    long sum=0;
    for (    DatanodeStorageInfo storageInfo : dataNode.getStorageInfos()) {
      sum+=storageInfo.getRemaining();
    }
    return sum;
  }
  @Test public void testChooseReplicasToDelete() throws Exception {
    Collection<DatanodeStorageInfo> nonExcess=new ArrayList<>();
    nonExcess.add(storages[0]);
    nonExcess.add(storages[1]);
    nonExcess.add(storages[2]);
    nonExcess.add(storages[3]);
    List<DatanodeStorageInfo> excessReplicas;
    BlockStoragePolicySuite POLICY_SUITE=BlockStoragePolicySuite.createDefaultSuite();
    BlockStoragePolicy storagePolicy=POLICY_SUITE.getDefaultPolicy();
    DatanodeStorageInfo excessSSD=DFSTestUtil.createDatanodeStorageInfo("Storage-excess-SSD-ID","localhost",storages[0].getDatanodeDescriptor().getNetworkLocation(),"foo.com",StorageType.SSD,null);
    updateHeartbeatWithUsage(excessSSD.getDatanodeDescriptor(),2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,2 * HdfsServerConstants.MIN_BLOCKS_FOR_WRITE * BLOCK_SIZE,0L,0L,0L,0,0);
    DatanodeDescriptor delHintNode=storages[0].getDatanodeDescriptor();
    List<StorageType> excessTypes=storagePolicy.chooseExcess((short)3,DatanodeStorageInfo.toStorageTypes(nonExcess));
    excessReplicas=replicator.chooseReplicasToDelete(nonExcess,nonExcess,3,excessTypes,storages[3].getDatanodeDescriptor(),delHintNode);
    assertTrue(excessReplicas.size() == 1);
    assertTrue(excessReplicas.contains(storages[0]));
    DatanodeStorageInfo excessStorage=DFSTestUtil.createDatanodeStorageInfo("Storage-excess-ID","localhost",delHintNode.getNetworkLocation(),"foo.com",StorageType.ARCHIVE,null);
    nonExcess.add(excessStorage);
    excessTypes=storagePolicy.chooseExcess((short)3,DatanodeStorageInfo.toStorageTypes(nonExcess));
    excessReplicas=replicator.chooseReplicasToDelete(nonExcess,nonExcess,3,excessTypes,storages[3].getDatanodeDescriptor(),null);
    assertTrue(excessReplicas.contains(excessStorage));
    nonExcess.clear();
    nonExcess.add(excessSSD);
    nonExcess.add(storages[3]);
    nonExcess.add(storages[4]);
    nonExcess.add(storages[5]);
    excessTypes=storagePolicy.chooseExcess((short)3,DatanodeStorageInfo.toStorageTypes(nonExcess));
    excessReplicas=replicator.chooseReplicasToDelete(nonExcess,nonExcess,3,excessTypes,storages[3].getDatanodeDescriptor(),storages[5].getDatanodeDescriptor());
    assertEquals(1,excessReplicas.size());
    assertTrue(excessReplicas.contains(excessSSD));
    nonExcess.clear();
    nonExcess.add(excessSSD);
    nonExcess.add(storages[1]);
    nonExcess.add(storages[2]);
    nonExcess.add(storages[3]);
    excessTypes=storagePolicy.chooseExcess((short)3,DatanodeStorageInfo.toStorageTypes(nonExcess));
    excessReplicas=replicator.chooseReplicasToDelete(nonExcess,nonExcess,3,excessTypes,storages[1].getDatanodeDescriptor(),storages[3].getDatanodeDescriptor());
    assertEquals(1,excessReplicas.size());
    assertTrue(excessReplicas.contains(excessSSD));
    nonExcess.clear();
    nonExcess.add(excessSSD);
    nonExcess.add(storages[2]);
    excessTypes=storagePolicy.chooseExcess((short)1,DatanodeStorageInfo.toStorageTypes(nonExcess));
    excessReplicas=replicator.chooseReplicasToDelete(nonExcess,nonExcess,1,excessTypes,storages[2].getDatanodeDescriptor(),null);
    assertEquals(1,excessReplicas.size());
    assertTrue(excessReplicas.contains(excessSSD));
    nonExcess.clear();
    nonExcess.add(excessSSD);
    nonExcess.add(storages[4]);
    nonExcess.add(storages[5]);
    excessTypes=storagePolicy.chooseExcess((short)2,DatanodeStorageInfo.toStorageTypes(nonExcess));
    excessReplicas=replicator.chooseReplicasToDelete(nonExcess,nonExcess,2,excessTypes,null,null);
    assertEquals(0,excessReplicas.size());
  }
  @Test public void testUseDelHint() throws Exception {
    List<StorageType> excessTypes=new ArrayList<>();
    excessTypes.add(StorageType.ARCHIVE);
    BlockPlacementPolicyDefault policyDefault=(BlockPlacementPolicyDefault)replicator;
    assertFalse(policyDefault.useDelHint(null,null,null,null,null));
    assertFalse(policyDefault.useDelHint(storages[0],null,null,null,excessTypes));
    List<DatanodeStorageInfo> moreThanOne=new ArrayList<>();
    moreThanOne.add(storages[0]);
    moreThanOne.add(storages[1]);
    List<DatanodeStorageInfo> exactlyOne=new ArrayList<>();
    exactlyOne.add(storages[3]);
    exactlyOne.add(storages[5]);
    excessTypes.add(StorageType.DEFAULT);
    assertTrue(policyDefault.useDelHint(storages[0],null,moreThanOne,exactlyOne,excessTypes));
    assertTrue(policyDefault.useDelHint(storages[3],storages[5],moreThanOne,exactlyOne,excessTypes));
    assertFalse(policyDefault.useDelHint(storages[3],storages[0],moreThanOne,exactlyOne,excessTypes));
    assertFalse(policyDefault.useDelHint(storages[3],null,moreThanOne,exactlyOne,excessTypes));
  }
  @Test public void testIsMovable() throws Exception {
    List<DatanodeInfo> candidates=new ArrayList<>();
    candidates.add(dataNodes[0]);
    candidates.add(dataNodes[1]);
    candidates.add(dataNodes[2]);
    candidates.add(dataNodes[3]);
    assertTrue(replicator.isMovable(candidates,dataNodes[0],dataNodes[3]));
    candidates.clear();
    candidates.add(dataNodes[0]);
    candidates.add(dataNodes[1]);
    candidates.add(dataNodes[2]);
    candidates.add(dataNodes[4]);
    assertTrue(replicator.isMovable(candidates,dataNodes[0],dataNodes[1]));
    candidates.clear();
    candidates.add(dataNodes[0]);
    candidates.add(dataNodes[1]);
    candidates.add(dataNodes[2]);
    candidates.add(dataNodes[4]);
    assertTrue(replicator.isMovable(candidates,dataNodes[0],dataNodes[4]));
    candidates.clear();
    candidates.add(dataNodes[0]);
    candidates.add(dataNodes[2]);
    candidates.add(dataNodes[3]);
    candidates.add(dataNodes[4]);
    assertFalse(replicator.isMovable(candidates,dataNodes[0],dataNodes[3]));
  }
  /** 
 * This testcase tests whether the default value returned by DFSUtil.getInvalidateWorkPctPerIteration() is positive,  and whether an IllegalArgumentException will be thrown  when 0.0f is retrieved
 */
  @Test public void testGetInvalidateWorkPctPerIteration(){
    Configuration conf=new Configuration();
    float blocksInvalidateWorkPct=DFSUtil.getInvalidateWorkPctPerIteration(conf);
    assertTrue(blocksInvalidateWorkPct > 0);
    conf.set(DFSConfigKeys.DFS_NAMENODE_INVALIDATE_WORK_PCT_PER_ITERATION,"0.5f");
    blocksInvalidateWorkPct=DFSUtil.getInvalidateWorkPctPerIteration(conf);
    assertEquals(blocksInvalidateWorkPct,0.5f,blocksInvalidateWorkPct * 1e-7);
    conf.set(DFSConfigKeys.DFS_NAMENODE_INVALIDATE_WORK_PCT_PER_ITERATION,"1.0f");
    blocksInvalidateWorkPct=DFSUtil.getInvalidateWorkPctPerIteration(conf);
    assertEquals(blocksInvalidateWorkPct,1.0f,blocksInvalidateWorkPct * 1e-7);
    conf.set(DFSConfigKeys.DFS_NAMENODE_INVALIDATE_WORK_PCT_PER_ITERATION,"0.0f");
    exception.expect(IllegalArgumentException.class);
    blocksInvalidateWorkPct=DFSUtil.getInvalidateWorkPctPerIteration(conf);
  }
  /** 
 * This testcase tests whether an IllegalArgumentException  will be thrown when a negative value is retrieved by  DFSUtil#getInvalidateWorkPctPerIteration
 */
  @Test public void testGetInvalidateWorkPctPerIteration_NegativeValue(){
    Configuration conf=new Configuration();
    float blocksInvalidateWorkPct=DFSUtil.getInvalidateWorkPctPerIteration(conf);
    assertTrue(blocksInvalidateWorkPct > 0);
    conf.set(DFSConfigKeys.DFS_NAMENODE_INVALIDATE_WORK_PCT_PER_ITERATION,"-0.5f");
    exception.expect(IllegalArgumentException.class);
    blocksInvalidateWorkPct=DFSUtil.getInvalidateWorkPctPerIteration(conf);
  }
  /** 
 * This testcase tests whether an IllegalArgumentException  will be thrown when a value greater than 1 is retrieved by  DFSUtil#getInvalidateWorkPctPerIteration
 */
  @Test public void testGetInvalidateWorkPctPerIteration_GreaterThanOne(){
    Configuration conf=new Configuration();
    float blocksInvalidateWorkPct=DFSUtil.getInvalidateWorkPctPerIteration(conf);
    assertTrue(blocksInvalidateWorkPct > 0);
    conf.set(DFSConfigKeys.DFS_NAMENODE_INVALIDATE_WORK_PCT_PER_ITERATION,"1.5f");
    exception.expect(IllegalArgumentException.class);
    blocksInvalidateWorkPct=DFSUtil.getInvalidateWorkPctPerIteration(conf);
  }
  /** 
 * This testcase tests whether the value returned by DFSUtil.getReplWorkMultiplier() is positive, and whether an IllegalArgumentException will be thrown  when a non-positive value is retrieved
 */
  @Test public void testGetReplWorkMultiplier(){
    Configuration conf=new Configuration();
    int blocksReplWorkMultiplier=DFSUtil.getReplWorkMultiplier(conf);
    assertTrue(blocksReplWorkMultiplier > 0);
    conf.set(DFSConfigKeys.DFS_NAMENODE_REPLICATION_WORK_MULTIPLIER_PER_ITERATION,"3");
    blocksReplWorkMultiplier=DFSUtil.getReplWorkMultiplier(conf);
    assertEquals(blocksReplWorkMultiplier,3);
    conf.set(DFSConfigKeys.DFS_NAMENODE_REPLICATION_WORK_MULTIPLIER_PER_ITERATION,"-1");
    exception.expect(IllegalArgumentException.class);
    blocksReplWorkMultiplier=DFSUtil.getReplWorkMultiplier(conf);
  }
  @Test(timeout=60000) public void testUpdateDoesNotCauseSkippedReplication(){
    LowRedundancyBlocks lowRedundancyBlocks=new LowRedundancyBlocks();
    BlockInfo block1=genBlockInfo(ThreadLocalRandom.current().nextLong());
    BlockInfo block2=genBlockInfo(ThreadLocalRandom.current().nextLong());
    BlockInfo block3=genBlockInfo(ThreadLocalRandom.current().nextLong());
    final int block1CurReplicas=2;
    final int block1ExpectedReplicas=7;
    lowRedundancyBlocks.add(block1,block1CurReplicas,0,0,block1ExpectedReplicas);
    lowRedundancyBlocks.add(block2,2,0,0,7);
    lowRedundancyBlocks.add(block3,2,0,0,6);
    List<List<BlockInfo>> chosenBlocks;
    chosenBlocks=lowRedundancyBlocks.chooseLowRedundancyBlocks(1);
    assertTheChosenBlocks(chosenBlocks,0,1,0,0,0);
    lowRedundancyBlocks.update(block1,block1CurReplicas + 1,0,0,block1ExpectedReplicas,1,0);
    chosenBlocks=lowRedundancyBlocks.chooseLowRedundancyBlocks(1);
    assertTheChosenBlocks(chosenBlocks,0,1,0,0,0);
    chosenBlocks=lowRedundancyBlocks.chooseLowRedundancyBlocks(1);
    assertTheChosenBlocks(chosenBlocks,0,0,1,0,0);
  }
  @Test(timeout=60000) public void testAddStoredBlockDoesNotCauseSkippedReplication() throws IOException {
    FSNamesystem mockNS=mock(FSNamesystem.class);
    when(mockNS.hasWriteLock()).thenReturn(true);
    when(mockNS.hasReadLock()).thenReturn(true);
    BlockManager bm=new BlockManager(mockNS,false,new HdfsConfiguration());
    LowRedundancyBlocks lowRedundancyBlocks=bm.neededReconstruction;
    BlockInfo block1=genBlockInfo(ThreadLocalRandom.current().nextLong());
    BlockInfo block2=genBlockInfo(ThreadLocalRandom.current().nextLong());
    lowRedundancyBlocks.add(block1,0,0,1,1);
    lowRedundancyBlocks.add(block2,0,0,1,1);
    List<List<BlockInfo>> chosenBlocks;
    chosenBlocks=lowRedundancyBlocks.chooseLowRedundancyBlocks(1);
    assertTheChosenBlocks(chosenBlocks,1,0,0,0,0);
    BlockInfoContiguous info=new BlockInfoContiguous(block1,(short)1);
    info.convertToBlockUnderConstruction(BlockUCState.UNDER_CONSTRUCTION,null);
    info.setBlockCollectionId(1000L);
    final INodeFile file=TestINodeFile.createINodeFile(1000L);
    when(mockNS.getBlockCollection(1000L)).thenReturn(file);
    bm.addBlockCollection(info,file);
    bm.addStoredBlockUnderConstruction(new StatefulBlockInfo(info,info,ReplicaState.FINALIZED),storages[0]);
    chosenBlocks=lowRedundancyBlocks.chooseLowRedundancyBlocks(1);
    assertTheChosenBlocks(chosenBlocks,1,0,0,0,0);
  }
  @Test(timeout=60000) public void testConvertLastBlockToUnderConstructionDoesNotCauseSkippedReplication() throws IOException {
    Namesystem mockNS=mock(Namesystem.class);
    when(mockNS.hasWriteLock()).thenReturn(true);
    BlockManager bm=new BlockManager(mockNS,false,new HdfsConfiguration());
    LowRedundancyBlocks lowRedundancyBlocks=bm.neededReconstruction;
    long blkID1=ThreadLocalRandom.current().nextLong();
    if (blkID1 < 0) {
      blkID1*=-1;
    }
    long blkID2=ThreadLocalRandom.current().nextLong();
    if (blkID2 < 0) {
      blkID2*=-1;
    }
    BlockInfo block1=genBlockInfo(blkID1);
    BlockInfo block2=genBlockInfo(blkID2);
    lowRedundancyBlocks.add(block1,0,0,1,1);
    lowRedundancyBlocks.add(block2,0,0,1,1);
    List<List<BlockInfo>> chosenBlocks;
    chosenBlocks=lowRedundancyBlocks.chooseLowRedundancyBlocks(1);
    assertTheChosenBlocks(chosenBlocks,1,0,0,0,0);
    final BlockInfoContiguous info=new BlockInfoContiguous(block1,(short)1);
    final BlockCollection mbc=mock(BlockCollection.class);
    when(mbc.getId()).thenReturn(1000L);
    when(mbc.getLastBlock()).thenReturn(info);
    when(mbc.getPreferredBlockSize()).thenReturn(block1.getNumBytes() + 1);
    when(mbc.isUnderConstruction()).thenReturn(true);
    ContentSummary cs=mock(ContentSummary.class);
    when(cs.getLength()).thenReturn((long)1);
    when(mbc.computeContentSummary(bm.getStoragePolicySuite())).thenReturn(cs);
    info.setBlockCollectionId(1000);
    bm.addBlockCollection(info,mbc);
    DatanodeStorageInfo[] storageAry={new DatanodeStorageInfo(dataNodes[0],new DatanodeStorage("s1"))};
    info.convertToBlockUnderConstruction(BlockUCState.UNDER_CONSTRUCTION,storageAry);
    DatanodeStorageInfo storage=mock(DatanodeStorageInfo.class);
    DatanodeDescriptor dn=mock(DatanodeDescriptor.class);
    when(dn.isDecommissioned()).thenReturn(true);
    when(storage.getState()).thenReturn(DatanodeStorage.State.NORMAL);
    when(storage.getDatanodeDescriptor()).thenReturn(dn);
    when(storage.removeBlock(any(BlockInfo.class))).thenReturn(true);
    when(storage.addBlock(any(BlockInfo.class))).thenReturn(DatanodeStorageInfo.AddBlockResult.ADDED);
    info.addStorage(storage,info);
    BlockInfo lastBlk=mbc.getLastBlock();
    when(mbc.getLastBlock()).thenReturn(lastBlk,info);
    bm.convertLastBlockToUnderConstruction(mbc,0L);
    chosenBlocks=lowRedundancyBlocks.chooseLowRedundancyBlocks(1);
    assertTheChosenBlocks(chosenBlocks,1,0,0,0,0);
  }
  @Test(timeout=60000) public void testupdateNeededReplicationsDoesNotCauseSkippedReplication() throws IOException {
    Namesystem mockNS=mock(Namesystem.class);
    when(mockNS.hasReadLock()).thenReturn(true);
    BlockManager bm=new BlockManager(mockNS,false,new HdfsConfiguration());
    LowRedundancyBlocks lowRedundancyBlocks=bm.neededReconstruction;
    BlockInfo block1=genBlockInfo(ThreadLocalRandom.current().nextLong());
    BlockInfo block2=genBlockInfo(ThreadLocalRandom.current().nextLong());
    lowRedundancyBlocks.add(block1,0,0,1,1);
    lowRedundancyBlocks.add(block2,0,0,1,1);
    List<List<BlockInfo>> chosenBlocks;
    chosenBlocks=lowRedundancyBlocks.chooseLowRedundancyBlocks(1);
    assertTheChosenBlocks(chosenBlocks,1,0,0,0,0);
    bm.setReplication((short)0,(short)1,block1);
    chosenBlocks=lowRedundancyBlocks.chooseLowRedundancyBlocks(1);
    assertTheChosenBlocks(chosenBlocks,1,0,0,0,0);
  }
  /** 
 * In this testcase, passed 2 favored nodes dataNodes[0],dataNodes[1] Both favored nodes should be chosen as target for placing replication and then should fall into BlockPlacement policy for choosing remaining targets ie. third target as local writer rack , forth target on remote rack and fifth on same rack as second.
 * @throws Exception
 */
  @Test public void testChooseExcessReplicaApartFromFavoredNodes() throws Exception {
    DatanodeStorageInfo[] targets;
    List<DatanodeDescriptor> expectedTargets=new ArrayList<DatanodeDescriptor>();
    expectedTargets.add(dataNodes[0]);
    expectedTargets.add(dataNodes[1]);
    expectedTargets.add(dataNodes[2]);
    expectedTargets.add(dataNodes[4]);
    expectedTargets.add(dataNodes[5]);
    List<DatanodeDescriptor> favouredNodes=new ArrayList<DatanodeDescriptor>();
    favouredNodes.add(dataNodes[0]);
    favouredNodes.add(dataNodes[1]);
    targets=chooseTarget(5,dataNodes[2],null,favouredNodes);
    assertEquals(targets.length,5);
    for (int i=0; i < targets.length; i++) {
      assertTrue("Target should be a part of Expected Targets",expectedTargets.contains(targets[i].getDatanodeDescriptor()));
    }
  }
  private DatanodeStorageInfo[] chooseTarget(  int numOfReplicas,  DatanodeDescriptor writer,  Set<Node> excludedNodes,  List<DatanodeDescriptor> favoredNodes){
    return chooseTarget(numOfReplicas,writer,excludedNodes,favoredNodes,null);
  }
  private DatanodeStorageInfo[] chooseTarget(  int numOfReplicas,  DatanodeDescriptor writer,  Set<Node> excludedNodes,  List<DatanodeDescriptor> favoredNodes,  EnumSet<AddBlockFlag> flags){
    return replicator.chooseTarget(filename,numOfReplicas,writer,excludedNodes,BLOCK_SIZE,favoredNodes,TestBlockStoragePolicy.DEFAULT_STORAGE_POLICY,flags);
  }
  @Test public void testAvoidLocalWrite() throws IOException {
    DatanodeDescriptor writer=dataNodes[2];
    EnumSet<AddBlockFlag> flags=EnumSet.of(AddBlockFlag.NO_LOCAL_WRITE);
    DatanodeStorageInfo[] targets;
    targets=chooseTarget(5,writer,null,null,flags);
    for (    DatanodeStorageInfo info : targets) {
      assertNotEquals(info.getDatanodeDescriptor(),writer);
    }
  }
  @Test public void testAvoidLocalWriteNoEnoughNodes() throws IOException {
    DatanodeDescriptor writer=dataNodes[2];
    EnumSet<AddBlockFlag> flags=EnumSet.of(AddBlockFlag.NO_LOCAL_WRITE);
    DatanodeStorageInfo[] targets;
    targets=chooseTarget(6,writer,null,null,flags);
    assertEquals(6,targets.length);
    boolean found=false;
    for (    DatanodeStorageInfo info : targets) {
      if (info.getDatanodeDescriptor().equals(writer)) {
        found=true;
      }
    }
    assertTrue(found);
  }
  @Test public void testMaxLoad(){
    FSClusterStats statistics=mock(FSClusterStats.class);
    DatanodeDescriptor node=mock(DatanodeDescriptor.class);
    when(statistics.getInServiceXceiverAverage()).thenReturn(0.0);
    when(node.getXceiverCount()).thenReturn(1);
    final Configuration conf=new Configuration();
    final Class<? extends BlockPlacementPolicy> replicatorClass=conf.getClass(DFSConfigKeys.DFS_BLOCK_REPLICATOR_CLASSNAME_KEY,DFSConfigKeys.DFS_BLOCK_REPLICATOR_CLASSNAME_DEFAULT,BlockPlacementPolicy.class);
    BlockPlacementPolicy bpp=ReflectionUtils.newInstance(replicatorClass,conf);
    assertTrue(bpp instanceof BlockPlacementPolicyDefault);
    BlockPlacementPolicyDefault bppd=(BlockPlacementPolicyDefault)bpp;
    bppd.initialize(conf,statistics,null,null);
    assertFalse(bppd.excludeNodeByLoad(node));
    when(statistics.getInServiceXceiverAverage()).thenReturn(1.0);
    when(node.getXceiverCount()).thenReturn(10);
    assertTrue(bppd.excludeNodeByLoad(node));
  }
}
