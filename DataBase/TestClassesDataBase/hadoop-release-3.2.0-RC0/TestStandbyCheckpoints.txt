public class TestStandbyCheckpoints {
  private static final int NUM_DIRS_IN_LOG=200000;
  protected static int NUM_NNS=3;
  protected MiniDFSCluster cluster;
  protected NameNode[] nns=new NameNode[NUM_NNS];
  protected FileSystem fs;
  private final Random random=new Random();
  protected File tmpOivImgDir;
  private static final Logger LOG=LoggerFactory.getLogger(TestStandbyCheckpoints.class);
  @SuppressWarnings("rawtypes") @Before public void setupCluster() throws Exception {
    Configuration conf=setupCommonConfig();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_NUM_CHECKPOINTS_RETAINED_KEY,1);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_NUM_EXTRA_EDITS_RETAINED_KEY,0);
    int retryCount=0;
    while (true) {
      try {
        int basePort=10060 + random.nextInt(100) * 2;
        MiniDFSNNTopology topology=new MiniDFSNNTopology().addNameservice(new MiniDFSNNTopology.NSConf("ns1").addNN(new MiniDFSNNTopology.NNConf("nn1").setHttpPort(basePort)).addNN(new MiniDFSNNTopology.NNConf("nn2").setHttpPort(basePort + 1)).addNN(new MiniDFSNNTopology.NNConf("nn3").setHttpPort(basePort + 2)));
        cluster=new MiniDFSCluster.Builder(conf).nnTopology(topology).numDataNodes(1).build();
        cluster.waitActive();
        setNNs();
        fs=HATestUtil.configureFailoverFs(cluster,conf);
        cluster.transitionToActive(0);
        ++retryCount;
        break;
      }
 catch (      BindException e) {
        LOG.info("Set up MiniDFSCluster failed due to port conflicts, retry " + retryCount + " times");
      }
    }
  }
  protected void setNNs(){
    for (int i=0; i < NUM_NNS; i++) {
      nns[i]=cluster.getNameNode(i);
    }
  }
  protected Configuration setupCommonConfig(){
    tmpOivImgDir=GenericTestUtils.getTestDir("TestStandbyCheckpoints");
    tmpOivImgDir.mkdirs();
    Configuration conf=new Configuration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_CHECK_PERIOD_KEY,1);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY,5);
    conf.setInt(DFSConfigKeys.DFS_HA_TAILEDITS_PERIOD_KEY,1);
    conf.set(DFSConfigKeys.DFS_NAMENODE_LEGACY_OIV_IMAGE_DIR_KEY,tmpOivImgDir.getAbsolutePath());
    conf.setBoolean(DFSConfigKeys.DFS_IMAGE_COMPRESS_KEY,true);
    conf.set(DFSConfigKeys.DFS_IMAGE_COMPRESSION_CODEC_KEY,SlowCodec.class.getCanonicalName());
    CompressionCodecFactory.setCodecClasses(conf,ImmutableList.<Class>of(SlowCodec.class));
    return conf;
  }
  @After public void shutdownCluster() throws IOException {
    if (cluster != null) {
      cluster.shutdown();
      cluster=null;
    }
    if (tmpOivImgDir != null) {
      FileUtil.fullyDelete(tmpOivImgDir);
    }
  }
  @Test(timeout=300000) public void testSBNCheckpoints() throws Exception {
    JournalSet standbyJournalSet=NameNodeAdapter.spyOnJournalSet(nns[1]);
    doEdits(0,10);
    HATestUtil.waitForStandbyToCatchUp(nns[0],nns[1]);
    HATestUtil.waitForCheckpoint(cluster,1,ImmutableList.of(12));
    GenericTestUtils.waitFor(new Supplier<Boolean>(){
      @Override public Boolean get(){
        if (tmpOivImgDir.list().length > 0) {
          return true;
        }
 else {
          return false;
        }
      }
    }
,1000,60000);
    assertEquals("One file is expected",1,tmpOivImgDir.list().length);
    HATestUtil.waitForCheckpoint(cluster,0,ImmutableList.of(12));
    Mockito.verify(standbyJournalSet,Mockito.never()).purgeLogsOlderThan(Mockito.anyLong());
  }
  @Test public void testNewDirInitAfterCheckpointing() throws Exception {
    File hdfsDir=new File(PathUtils.getTestDir(TestStandbyCheckpoints.class),"testNewDirInitAfterCheckpointing");
    File nameDir=new File(hdfsDir,"name1");
    assert nameDir.mkdirs();
    String existingDir=cluster.getConfiguration(0).get(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY);
    cluster.getConfiguration(0).set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY,existingDir + "," + Util.fileAsURI(nameDir).toString());
    cluster.restartNameNode(0);
    nns[0]=cluster.getNameNode(0);
    cluster.transitionToActive(0);
    File currDir=new File(nameDir,"current");
    File versionFile=new File(currDir,"VERSION");
    assert currDir.exists();
    assert !versionFile.exists();
    doEdits(0,10);
    HATestUtil.waitForStandbyToCatchUp(nns[0],nns[1]);
    for (int i=0; i < 20; i++) {
      if (versionFile.exists()) {
        break;
      }
      Thread.sleep(500);
    }
    assert versionFile.exists();
  }
  /** 
 * Test for the case when both of the NNs in the cluster are in the standby state, and thus are both creating checkpoints and uploading them to each other. In this circumstance, they should receive the error from the other node indicating that the other node already has a checkpoint for the given txid, but this should not cause an abort, etc.
 */
  @Test(timeout=300000) public void testBothNodesInStandbyState() throws Exception {
    doEdits(0,10);
    cluster.transitionToStandby(0);
    HATestUtil.waitForCheckpoint(cluster,1,ImmutableList.of(12));
    HATestUtil.waitForCheckpoint(cluster,0,ImmutableList.of(12));
    assertEquals(12,nns[0].getNamesystem().getFSImage().getMostRecentCheckpointTxId());
    assertEquals(12,nns[1].getNamesystem().getFSImage().getMostRecentCheckpointTxId());
    List<File> dirs=Lists.newArrayList();
    dirs.addAll(FSImageTestUtil.getNameNodeCurrentDirs(cluster,0));
    dirs.addAll(FSImageTestUtil.getNameNodeCurrentDirs(cluster,1));
    FSImageTestUtil.assertParallelFilesAreIdentical(dirs,ImmutableSet.<String>of());
  }
  /** 
 * Test for the case when the SBN is configured to checkpoint based on a time period, but no transactions are happening on the active. Thus, it would want to save a second checkpoint at the same txid, which is a no-op. This test makes sure this doesn't cause any problem.
 */
  @Test(timeout=300000) public void testCheckpointWhenNoNewTransactionsHappened() throws Exception {
    cluster.getConfiguration(1).setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_KEY,0);
    cluster.restartNameNode(1);
    nns[1]=cluster.getNameNode(1);
    FSImage spyImage1=NameNodeAdapter.spyOnFsImage(nns[1]);
    Thread.sleep(1000);
    Mockito.verify(spyImage1,Mockito.never()).saveNamespace((FSNamesystem)Mockito.anyObject());
    HATestUtil.waitForStandbyToCatchUp(nns[0],nns[1]);
    Thread.sleep(2000);
    Mockito.verify(spyImage1,Mockito.times(1)).saveNamespace((FSNamesystem)Mockito.anyObject(),Mockito.eq(NameNodeFile.IMAGE),(Canceler)Mockito.anyObject());
  }
  /** 
 * Test cancellation of ongoing checkpoints when failover happens mid-checkpoint. 
 */
  @Test(timeout=120000) public void testCheckpointCancellation() throws Exception {
    cluster.transitionToStandby(0);
    URI sharedUri=cluster.getSharedEditsDir(0,1);
    File sharedDir=new File(sharedUri.getPath(),"current");
    File tmpDir=new File(MiniDFSCluster.getBaseDirectory(),"testCheckpointCancellation-tmp");
    FSNamesystem fsn=cluster.getNamesystem(0);
    FSImageTestUtil.createAbortedLogWithMkdirs(tmpDir,NUM_DIRS_IN_LOG,3,fsn.getFSDirectory().getLastInodeId() + 1);
    String fname=NNStorage.getInProgressEditsFileName(3);
    new File(tmpDir,fname).renameTo(new File(sharedDir,fname));
    cluster.getConfiguration(1).setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_KEY,0);
    cluster.restartNameNode(1);
    nns[1]=cluster.getNameNode(1);
    cluster.transitionToActive(0);
    boolean canceledOne=false;
    for (int i=0; i < 10 && !canceledOne; i++) {
      doEdits(i * 10,i * 10 + 10);
      cluster.transitionToStandby(0);
      cluster.transitionToActive(1);
      cluster.transitionToStandby(1);
      cluster.transitionToActive(0);
      canceledOne=StandbyCheckpointer.getCanceledCount() > 0;
    }
    assertTrue(canceledOne);
  }
  /** 
 * Test cancellation of ongoing checkpoints when failover happens mid-checkpoint during image upload from standby to active NN.
 */
  @Test(timeout=60000) public void testCheckpointCancellationDuringUpload() throws Exception {
    cluster.getConfiguration(0).setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY,1000);
    for (int i=0; i < NUM_NNS; i++) {
      cluster.getConfiguration(i).setBoolean(DFSConfigKeys.DFS_IMAGE_COMPRESS_KEY,false);
    }
    for (int i=1; i < NUM_NNS; i++) {
      cluster.getConfiguration(i).setLong(DFSConfigKeys.DFS_IMAGE_TRANSFER_RATE_KEY,100);
    }
    for (int i=0; i < NUM_NNS; i++) {
      cluster.restartNameNode(i);
    }
    setNNs();
    cluster.transitionToActive(0);
    doEdits(0,100);
    for (int i=1; i < NUM_NNS; i++) {
      HATestUtil.waitForStandbyToCatchUp(nns[0],nns[i]);
      HATestUtil.waitForCheckpoint(cluster,i,ImmutableList.of(104));
    }
    cluster.transitionToStandby(0);
    cluster.transitionToActive(1);
    cluster.shutdown();
    cluster=null;
    GenericTestUtils.waitFor(new Supplier<Boolean>(){
      @Override public Boolean get(){
        ThreadMXBean threadBean=ManagementFactory.getThreadMXBean();
        ThreadInfo[] threads=threadBean.getThreadInfo(threadBean.getAllThreadIds(),1);
        for (        ThreadInfo thread : threads) {
          if (thread.getThreadName().startsWith("TransferFsImageUpload")) {
            return false;
          }
        }
        return true;
      }
    }
,1000,30000);
    assertEquals(0,nns[0].getFSImage().getMostRecentCheckpointTxId());
  }
  /** 
 * Make sure that clients will receive StandbyExceptions even when a checkpoint is in progress on the SBN, and therefore the StandbyCheckpointer thread will have FSNS lock. Regression test for HDFS-4591.
 */
  @Test(timeout=300000) public void testStandbyExceptionThrownDuringCheckpoint() throws Exception {
    FSImage spyImage1=NameNodeAdapter.spyOnFsImage(nns[1]);
    DelayAnswer answerer=new DelayAnswer(LOG);
    Mockito.doAnswer(answerer).when(spyImage1).saveNamespace(Mockito.any(FSNamesystem.class),Mockito.eq(NameNodeFile.IMAGE),Mockito.any(Canceler.class));
    doEdits(0,1000);
    nns[0].getRpcServer().rollEditLog();
    answerer.waitForCall();
    assertTrue("SBN is not performing checkpoint but it should be.",answerer.getFireCount() == 1 && answerer.getResultCount() == 0);
    ThreadUtil.sleepAtLeastIgnoreInterrupts(1000);
    try {
      nns[1].getRpcServer().getFileInfo("/");
      fail("Should have thrown StandbyException, but instead succeeded.");
    }
 catch (    StandbyException se) {
      GenericTestUtils.assertExceptionContains("is not supported",se);
    }
    assertEquals(0,cluster.getNamesystem(1).getPendingDataNodeMessageCount());
    doCreate();
    Thread.sleep(1000);
    assertTrue(cluster.getNamesystem(1).getPendingDataNodeMessageCount() > 0);
    assertTrue("SBN should have still been checkpointing.",answerer.getFireCount() == 1 && answerer.getResultCount() == 0);
    answerer.proceed();
    answerer.waitForResult();
    assertTrue("SBN should have finished checkpointing.",answerer.getFireCount() == 1 && answerer.getResultCount() == 1);
  }
  @Test(timeout=300000) public void testReadsAllowedDuringCheckpoint() throws Exception {
    FSImage spyImage1=NameNodeAdapter.spyOnFsImage(nns[1]);
    DelayAnswer answerer=new DelayAnswer(LOG);
    Mockito.doAnswer(answerer).when(spyImage1).saveNamespace(Mockito.any(FSNamesystem.class),Mockito.any(NameNodeFile.class),Mockito.any(Canceler.class));
    doEdits(0,1000);
    nns[0].getRpcServer().rollEditLog();
    answerer.waitForCall();
    assertTrue("SBN is not performing checkpoint but it should be.",answerer.getFireCount() == 1 && answerer.getResultCount() == 0);
    ThreadUtil.sleepAtLeastIgnoreInterrupts(1000);
    Thread t=new Thread(){
      @Override public void run(){
        try {
          nns[1].getRpcServer().restoreFailedStorage("false");
        }
 catch (        IOException e) {
          e.printStackTrace();
        }
      }
    }
;
    t.start();
    ThreadUtil.sleepAtLeastIgnoreInterrupts(1000);
    assertFalse(nns[1].getNamesystem().getFsLockForTests().hasQueuedThreads());
    assertFalse(nns[1].getNamesystem().getFsLockForTests().isWriteLocked());
    assertTrue(nns[1].getNamesystem().getCpLockForTests().hasQueuedThreads());
    String pageContents=DFSTestUtil.urlGet(new URL("http://" + nns[1].getHttpAddress().getHostName() + ":"+ nns[1].getHttpAddress().getPort()+ "/jmx"));
    assertTrue(pageContents.contains("NumLiveDataNodes"));
    assertTrue("SBN should have still been checkpointing.",answerer.getFireCount() == 1 && answerer.getResultCount() == 0);
    answerer.proceed();
    answerer.waitForResult();
    assertTrue("SBN should have finished checkpointing.",answerer.getFireCount() == 1 && answerer.getResultCount() == 1);
    t.join();
  }
  /** 
 * Test for the case standby NNs can upload FSImage to ANN after become non-primary standby NN. HDFS-9787
 */
  @Test(timeout=300000) public void testNonPrimarySBNUploadFSImage() throws Exception {
    for (int i=1; i < NUM_NNS; i++) {
      cluster.shutdownNameNode(i);
      cluster.getConfiguration(i).setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_PERIOD_KEY,1);
    }
    doEdits(0,10);
    cluster.transitionToStandby(0);
    for (int i=1; i < NUM_NNS; i++) {
      cluster.restartNameNode(i,false);
    }
    cluster.waitClusterUp();
    for (int i=0; i < NUM_NNS; i++) {
      HATestUtil.waitForCheckpoint(cluster,i,ImmutableList.of(12));
    }
    cluster.transitionToActive(0);
    Thread.sleep(2000);
    doEdits(11,20);
    nns[0].getRpcServer().rollEditLog();
    HATestUtil.waitForCheckpoint(cluster,0,ImmutableList.of(23));
  }
  /** 
 * Test that checkpointing is still successful even if an issue was encountered while writing the legacy OIV image.
 */
  @Test(timeout=300000) public void testCheckpointSucceedsWithLegacyOIVException() throws Exception {
    FileUtil.fullyDelete(tmpOivImgDir);
    doEdits(0,10);
    HATestUtil.waitForStandbyToCatchUp(nns[0],nns[1]);
    HATestUtil.waitForCheckpoint(cluster,1,ImmutableList.of(12));
    HATestUtil.waitForCheckpoint(cluster,0,ImmutableList.of(12));
  }
  private void doEdits(  int start,  int stop) throws IOException {
    for (int i=start; i < stop; i++) {
      Path p=new Path("/test" + i);
      fs.mkdirs(p);
    }
  }
  private void doCreate() throws IOException {
    Path p=new Path("/testFile");
    fs.delete(p,false);
    FSDataOutputStream out=fs.create(p,(short)1);
    out.write(42);
    out.close();
  }
  /** 
 * A codec which just slows down the saving of the image significantly by sleeping a few milliseconds on every write. This makes it easy to catch the standby in the middle of saving a checkpoint.
 */
public static class SlowCodec extends GzipCodec {
    @Override public CompressionOutputStream createOutputStream(    OutputStream out) throws IOException {
      CompressionOutputStream ret=super.createOutputStream(out);
      CompressionOutputStream spy=Mockito.spy(ret);
      Mockito.doAnswer(new GenericTestUtils.SleepAnswer(5)).when(spy).write(Mockito.<byte[]>any(),Mockito.anyInt(),Mockito.anyInt());
      return spy;
    }
  }
}
