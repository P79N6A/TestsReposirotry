/** 
 * Tests snapshot deletion.
 */
public class TestSnapshotDeletion {
  private static final Logger LOG=LoggerFactory.getLogger(TestSnapshotDeletion.class);
  protected static final long seed=0;
  protected static final short REPLICATION=3;
  protected static final short REPLICATION_1=2;
  protected static final long BLOCKSIZE=1024;
  private final Path dir=new Path("/TestSnapshot");
  private final Path sub=new Path(dir,"sub1");
  private final Path subsub=new Path(sub,"subsub1");
  protected Configuration conf;
  protected MiniDFSCluster cluster;
  protected FSNamesystem fsn;
  protected FSDirectory fsdir;
  protected BlockManager blockmanager;
  protected DistributedFileSystem hdfs;
  @Rule public ExpectedException exception=ExpectedException.none();
  @Before public void setUp() throws Exception {
    conf=new Configuration();
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION).format(true).build();
    cluster.waitActive();
    fsn=cluster.getNamesystem();
    fsdir=fsn.getFSDirectory();
    blockmanager=fsn.getBlockManager();
    hdfs=cluster.getFileSystem();
  }
  @After public void tearDown() throws Exception {
    if (cluster != null) {
      cluster.shutdown();
      cluster=null;
    }
  }
  /** 
 * Deleting snapshottable directory with snapshots must fail.
 */
  @Test(timeout=300000) public void testDeleteDirectoryWithSnapshot() throws Exception {
    Path file0=new Path(sub,"file0");
    Path file1=new Path(sub,"file1");
    DFSTestUtil.createFile(hdfs,file0,BLOCKSIZE,REPLICATION,seed);
    DFSTestUtil.createFile(hdfs,file1,BLOCKSIZE,REPLICATION,seed);
    hdfs.allowSnapshot(sub);
    hdfs.createSnapshot(sub,"s1");
    exception.expect(RemoteException.class);
    String error="The directory " + sub.toString() + " cannot be deleted since "+ sub.toString()+ " is snapshottable and already has snapshots";
    exception.expectMessage(error);
    hdfs.delete(sub,true);
  }
  /** 
 * Test applying editlog of operation which deletes a snapshottable directory without snapshots. The snapshottable dir list in snapshot manager should be updated.
 */
  @Test(timeout=300000) public void testApplyEditLogForDeletion() throws Exception {
    final Path foo=new Path("/foo");
    final Path bar1=new Path(foo,"bar1");
    final Path bar2=new Path(foo,"bar2");
    hdfs.mkdirs(bar1);
    hdfs.mkdirs(bar2);
    hdfs.allowSnapshot(bar1);
    hdfs.allowSnapshot(bar2);
    assertEquals(2,cluster.getNamesystem().getSnapshotManager().getNumSnapshottableDirs());
    assertEquals(2,cluster.getNamesystem().getSnapshotManager().getSnapshottableDirs().length);
    hdfs.delete(foo,true);
    cluster.restartNameNode(0);
    assertEquals(0,cluster.getNamesystem().getSnapshotManager().getNumSnapshottableDirs());
    assertEquals(0,cluster.getNamesystem().getSnapshotManager().getSnapshottableDirs().length);
    hdfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);
    hdfs.saveNamespace();
    hdfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);
    cluster.restartNameNode(0);
  }
  /** 
 * Deleting directory with snapshottable descendant with snapshots must fail.
 */
  @Test(timeout=300000) public void testDeleteDirectoryWithSnapshot2() throws Exception {
    Path file0=new Path(sub,"file0");
    Path file1=new Path(sub,"file1");
    DFSTestUtil.createFile(hdfs,file0,BLOCKSIZE,REPLICATION,seed);
    DFSTestUtil.createFile(hdfs,file1,BLOCKSIZE,REPLICATION,seed);
    Path subfile1=new Path(subsub,"file0");
    Path subfile2=new Path(subsub,"file1");
    DFSTestUtil.createFile(hdfs,subfile1,BLOCKSIZE,REPLICATION,seed);
    DFSTestUtil.createFile(hdfs,subfile2,BLOCKSIZE,REPLICATION,seed);
    hdfs.allowSnapshot(subsub);
    hdfs.createSnapshot(subsub,"s1");
    exception.expect(RemoteException.class);
    String error=subsub.toString() + " is snapshottable and already has snapshots";
    exception.expectMessage(error);
    hdfs.delete(dir,true);
  }
  private static INodeDirectory getDir(  final FSDirectory fsdir,  final Path dir) throws IOException {
    final String dirStr=dir.toString();
    return INodeDirectory.valueOf(fsdir.getINode(dirStr),dirStr);
  }
  private void checkQuotaUsageComputation(  final Path dirPath,  final long expectedNs,  final long expectedDs) throws IOException {
    INodeDirectory dirNode=getDir(fsdir,dirPath);
    assertTrue(dirNode.isQuotaSet());
    QuotaCounts q=dirNode.getDirectoryWithQuotaFeature().getSpaceConsumed();
    assertEquals(dirNode.dumpTreeRecursively().toString(),expectedNs,q.getNameSpace());
    assertEquals(dirNode.dumpTreeRecursively().toString(),expectedDs,q.getStorageSpace());
    QuotaCounts counts=dirNode.computeQuotaUsage(fsdir.getBlockStoragePolicySuite(),false);
    assertEquals(dirNode.dumpTreeRecursively().toString(),expectedNs,counts.getNameSpace());
    assertEquals(dirNode.dumpTreeRecursively().toString(),expectedDs,counts.getStorageSpace());
  }
  /** 
 * Test deleting a directory which is a descendant of a snapshottable directory. In the test we need to cover the following cases: <pre> 1. Delete current INodeFile/INodeDirectory without taking any snapshot. 2. Delete current INodeFile/INodeDirectory while snapshots have been taken  on ancestor(s). 3. Delete current INodeFileWithSnapshot. 4. Delete current INodeDirectoryWithSnapshot. </pre>
 */
  @Test(timeout=300000) public void testDeleteCurrentFileDirectory() throws Exception {
    Path deleteDir=new Path(subsub,"deleteDir");
    Path deleteFile=new Path(deleteDir,"deleteFile");
    Path noChangeDirParent=new Path(sub,"noChangeDirParent");
    Path noChangeDir=new Path(noChangeDirParent,"noChangeDir");
    Path noChangeFile=new Path(noChangeDir,"noChangeFile");
    DFSTestUtil.createFile(hdfs,deleteFile,BLOCKSIZE,REPLICATION,seed);
    DFSTestUtil.createFile(hdfs,noChangeFile,BLOCKSIZE,REPLICATION,seed);
    Path metaChangeFile1=new Path(subsub,"metaChangeFile1");
    DFSTestUtil.createFile(hdfs,metaChangeFile1,BLOCKSIZE,REPLICATION,seed);
    Path metaChangeFile2=new Path(noChangeDir,"metaChangeFile2");
    DFSTestUtil.createFile(hdfs,metaChangeFile2,BLOCKSIZE,REPLICATION,seed);
    hdfs.setQuota(dir,Long.MAX_VALUE - 1,Long.MAX_VALUE - 1);
    checkQuotaUsageComputation(dir,10,BLOCKSIZE * REPLICATION * 4);
    hdfs.delete(deleteDir,true);
    checkQuotaUsageComputation(dir,8,BLOCKSIZE * REPLICATION * 3);
    SnapshotTestHelper.createSnapshot(hdfs,dir,"s0");
    Path tempDir=new Path(dir,"tempdir");
    Path tempFile=new Path(tempDir,"tempfile");
    DFSTestUtil.createFile(hdfs,tempFile,BLOCKSIZE,REPLICATION,seed);
    final INodeFile temp=TestSnapshotBlocksMap.assertBlockCollection(tempFile.toString(),1,fsdir,blockmanager);
    BlockInfo[] blocks=temp.getBlocks();
    hdfs.delete(tempDir,true);
    checkQuotaUsageComputation(dir,8,BLOCKSIZE * REPLICATION * 3);
    for (    BlockInfo b : blocks) {
      assertEquals(INVALID_INODE_ID,b.getBlockCollectionId());
    }
    Path newFileAfterS0=new Path(subsub,"newFile");
    DFSTestUtil.createFile(hdfs,newFileAfterS0,BLOCKSIZE,REPLICATION,seed);
    hdfs.setReplication(metaChangeFile1,REPLICATION_1);
    hdfs.setReplication(metaChangeFile2,REPLICATION_1);
    SnapshotTestHelper.createSnapshot(hdfs,dir,"s1");
    checkQuotaUsageComputation(dir,9L,BLOCKSIZE * REPLICATION * 4);
    Snapshot snapshot0=fsdir.getINode(dir.toString()).asDirectory().getSnapshot(DFSUtil.string2Bytes("s0"));
    Snapshot snapshot1=fsdir.getINode(dir.toString()).asDirectory().getSnapshot(DFSUtil.string2Bytes("s1"));
    hdfs.delete(noChangeDirParent,true);
    checkQuotaUsageComputation(dir,9L,BLOCKSIZE * REPLICATION * 4);
    Path snapshotNoChangeDir=SnapshotTestHelper.getSnapshotPath(dir,"s1",sub.getName() + "/" + noChangeDirParent.getName()+ "/"+ noChangeDir.getName());
    INodeDirectory snapshotNode=(INodeDirectory)fsdir.getINode(snapshotNoChangeDir.toString());
    assertEquals(INodeDirectory.class,snapshotNode.getClass());
    ReadOnlyList<INode> children=snapshotNode.getChildrenList(Snapshot.CURRENT_STATE_ID);
    assertEquals(2,children.size());
    INode noChangeFileSCopy=children.get(1);
    assertEquals(noChangeFile.getName(),noChangeFileSCopy.getLocalName());
    assertEquals(INodeFile.class,noChangeFileSCopy.getClass());
    TestSnapshotBlocksMap.assertBlockCollection(new Path(snapshotNoChangeDir,noChangeFileSCopy.getLocalName()).toString(),1,fsdir,blockmanager);
    INodeFile metaChangeFile2SCopy=children.get(0).asFile();
    assertEquals(metaChangeFile2.getName(),metaChangeFile2SCopy.getLocalName());
    assertTrue(metaChangeFile2SCopy.isWithSnapshot());
    assertFalse(metaChangeFile2SCopy.isUnderConstruction());
    TestSnapshotBlocksMap.assertBlockCollection(new Path(snapshotNoChangeDir,metaChangeFile2SCopy.getLocalName()).toString(),1,fsdir,blockmanager);
    assertEquals(REPLICATION_1,metaChangeFile2SCopy.getFileReplication(Snapshot.CURRENT_STATE_ID));
    assertEquals(REPLICATION_1,metaChangeFile2SCopy.getFileReplication(snapshot1.getId()));
    assertEquals(REPLICATION,metaChangeFile2SCopy.getFileReplication(snapshot0.getId()));
    Path newFile=new Path(sub,"newFile");
    DFSTestUtil.createFile(hdfs,newFile,BLOCKSIZE,REPLICATION,seed);
    final INodeFile newFileNode=TestSnapshotBlocksMap.assertBlockCollection(newFile.toString(),1,fsdir,blockmanager);
    blocks=newFileNode.getBlocks();
    checkQuotaUsageComputation(dir,10L,BLOCKSIZE * REPLICATION * 5);
    hdfs.delete(sub,true);
    checkQuotaUsageComputation(dir,9L,BLOCKSIZE * REPLICATION * 4);
    for (    BlockInfo b : blocks) {
      assertEquals(INVALID_INODE_ID,b.getBlockCollectionId());
    }
    Path snapshotSub=SnapshotTestHelper.getSnapshotPath(dir,"s1",sub.getName());
    INodeDirectory snapshotNode4Sub=fsdir.getINode(snapshotSub.toString()).asDirectory();
    assertTrue(snapshotNode4Sub.isWithSnapshot());
    assertEquals(1,snapshotNode4Sub.getChildrenList(Snapshot.CURRENT_STATE_ID).size());
    assertEquals(2,snapshotNode4Sub.getChildrenList(snapshot1.getId()).size());
    INode snapshotNode4Subsub=snapshotNode4Sub.getChildrenList(Snapshot.CURRENT_STATE_ID).get(0);
    assertTrue(snapshotNode4Subsub.asDirectory().isWithSnapshot());
    assertTrue(snapshotNode4Sub == snapshotNode4Subsub.getParent());
    INodeDirectory snapshotSubsubDir=(INodeDirectory)snapshotNode4Subsub;
    children=snapshotSubsubDir.getChildrenList(Snapshot.CURRENT_STATE_ID);
    assertEquals(2,children.size());
    assertEquals(children.get(0).getLocalName(),metaChangeFile1.getName());
    assertEquals(children.get(1).getLocalName(),newFileAfterS0.getName());
    children=snapshotSubsubDir.getChildrenList(snapshot0.getId());
    assertEquals(1,children.size());
    INode child=children.get(0);
    assertEquals(child.getLocalName(),metaChangeFile1.getName());
    INodeFile metaChangeFile1SCopy=child.asFile();
    assertTrue(metaChangeFile1SCopy.isWithSnapshot());
    assertFalse(metaChangeFile1SCopy.isUnderConstruction());
    assertEquals(REPLICATION_1,metaChangeFile1SCopy.getFileReplication(Snapshot.CURRENT_STATE_ID));
    assertEquals(REPLICATION_1,metaChangeFile1SCopy.getFileReplication(snapshot1.getId()));
    assertEquals(REPLICATION,metaChangeFile1SCopy.getFileReplication(snapshot0.getId()));
  }
  /** 
 * Test deleting the earliest (first) snapshot. In this simplest scenario, the  snapshots are taken on the same directory, and we do not need to combine snapshot diffs.
 */
  @Test(timeout=300000) public void testDeleteEarliestSnapshot1() throws Exception {
    Path file0=new Path(sub,"file0");
    Path file1=new Path(sub,"file1");
    DFSTestUtil.createFile(hdfs,file0,BLOCKSIZE,REPLICATION,seed);
    DFSTestUtil.createFile(hdfs,file1,BLOCKSIZE,REPLICATION,seed);
    String snapshotName="s1";
    try {
      hdfs.deleteSnapshot(sub,snapshotName);
      fail("SnapshotException expected: " + sub.toString() + " is not snapshottable yet");
    }
 catch (    Exception e) {
      GenericTestUtils.assertExceptionContains("Directory is not a snapshottable directory: " + sub,e);
    }
    hdfs.allowSnapshot(sub);
    try {
      hdfs.deleteSnapshot(sub,snapshotName);
      fail("SnapshotException expected: snapshot " + snapshotName + " does not exist for "+ sub.toString());
    }
 catch (    Exception e) {
      GenericTestUtils.assertExceptionContains("Cannot delete snapshot " + snapshotName + " from path "+ sub.toString()+ ": the snapshot does not exist.",e);
    }
    SnapshotTestHelper.createSnapshot(hdfs,sub,snapshotName);
    checkQuotaUsageComputation(sub,3,BLOCKSIZE * REPLICATION * 2);
    hdfs.deleteSnapshot(sub,snapshotName);
    checkQuotaUsageComputation(sub,3,BLOCKSIZE * REPLICATION * 2);
    hdfs.createSnapshot(sub,snapshotName);
    checkQuotaUsageComputation(sub,3,BLOCKSIZE * REPLICATION * 2);
    Path newFile=new Path(sub,"newFile");
    DFSTestUtil.createFile(hdfs,newFile,BLOCKSIZE,REPLICATION,seed);
    String snapshotName2="s2";
    hdfs.createSnapshot(sub,snapshotName2);
    checkQuotaUsageComputation(sub,4,BLOCKSIZE * REPLICATION * 3);
    Path ss=SnapshotTestHelper.getSnapshotPath(sub,snapshotName2,"newFile");
    FileStatus statusBeforeDeletion=hdfs.getFileStatus(ss);
    hdfs.deleteSnapshot(sub,snapshotName);
    checkQuotaUsageComputation(sub,4,BLOCKSIZE * REPLICATION * 3);
    FileStatus statusAfterDeletion=hdfs.getFileStatus(ss);
    System.out.println("Before deletion: " + statusBeforeDeletion.toString() + "\n"+ "After deletion: "+ statusAfterDeletion.toString());
    assertEquals(statusBeforeDeletion.toString(),statusAfterDeletion.toString());
  }
  /** 
 * Test deleting the earliest (first) snapshot. In this more complicated  scenario, the snapshots are taken across directories. <pre> The test covers the following scenarios: 1. delete the first diff in the diff list of a directory 2. delete the first diff in the diff list of a file </pre> Also, the recursive cleanTree process should cover both INodeFile and  INodeDirectory.
 */
  @Test(timeout=300000) public void testDeleteEarliestSnapshot2() throws Exception {
    Path noChangeDir=new Path(sub,"noChangeDir");
    Path noChangeFile=new Path(noChangeDir,"noChangeFile");
    Path metaChangeFile=new Path(noChangeDir,"metaChangeFile");
    Path metaChangeDir=new Path(noChangeDir,"metaChangeDir");
    Path toDeleteFile=new Path(metaChangeDir,"toDeleteFile");
    DFSTestUtil.createFile(hdfs,noChangeFile,BLOCKSIZE,REPLICATION,seed);
    DFSTestUtil.createFile(hdfs,metaChangeFile,BLOCKSIZE,REPLICATION,seed);
    DFSTestUtil.createFile(hdfs,toDeleteFile,BLOCKSIZE,REPLICATION,seed);
    final INodeFile toDeleteFileNode=TestSnapshotBlocksMap.assertBlockCollection(toDeleteFile.toString(),1,fsdir,blockmanager);
    BlockInfo[] blocks=toDeleteFileNode.getBlocks();
    SnapshotTestHelper.createSnapshot(hdfs,dir,"s0");
    checkQuotaUsageComputation(dir,7,3 * BLOCKSIZE * REPLICATION);
    hdfs.delete(toDeleteFile,true);
    checkQuotaUsageComputation(dir,7,3 * BLOCKSIZE * REPLICATION);
    hdfs.setReplication(metaChangeFile,REPLICATION_1);
    hdfs.setOwner(metaChangeDir,"unknown","unknown");
    checkQuotaUsageComputation(dir,7,3 * BLOCKSIZE * REPLICATION);
    hdfs.createSnapshot(dir,"s1");
    checkQuotaUsageComputation(dir,7,3 * BLOCKSIZE * REPLICATION);
    hdfs.deleteSnapshot(dir,"s0");
    checkQuotaUsageComputation(dir,6,2 * BLOCKSIZE * REPLICATION - BLOCKSIZE);
    for (    BlockInfo b : blocks) {
      assertEquals(INVALID_INODE_ID,b.getBlockCollectionId());
    }
    final INodeDirectory dirNode=fsdir.getINode(dir.toString()).asDirectory();
    Snapshot snapshot0=dirNode.getSnapshot(DFSUtil.string2Bytes("s0"));
    assertNull(snapshot0);
    Snapshot snapshot1=dirNode.getSnapshot(DFSUtil.string2Bytes("s1"));
    DirectoryDiffList diffList=dirNode.getDiffs();
    assertEquals(1,diffList.asList().size());
    assertEquals(snapshot1.getId(),diffList.getLast().getSnapshotId());
    diffList=fsdir.getINode(metaChangeDir.toString()).asDirectory().getDiffs();
    assertEquals(0,diffList.asList().size());
    final INodeDirectory noChangeDirNode=(INodeDirectory)fsdir.getINode(noChangeDir.toString());
    assertEquals(INodeDirectory.class,noChangeDirNode.getClass());
    final INodeFile noChangeFileNode=(INodeFile)fsdir.getINode(noChangeFile.toString());
    assertEquals(INodeFile.class,noChangeFileNode.getClass());
    TestSnapshotBlocksMap.assertBlockCollection(noChangeFile.toString(),1,fsdir,blockmanager);
    FileStatus status=hdfs.getFileStatus(metaChangeDir);
    assertEquals("unknown",status.getOwner());
    assertEquals("unknown",status.getGroup());
    status=hdfs.getFileStatus(metaChangeFile);
    assertEquals(REPLICATION_1,status.getReplication());
    TestSnapshotBlocksMap.assertBlockCollection(metaChangeFile.toString(),1,fsdir,blockmanager);
    try {
      hdfs.getFileStatus(toDeleteFile);
      fail("should throw FileNotFoundException");
    }
 catch (    FileNotFoundException e) {
      GenericTestUtils.assertExceptionContains("File does not exist: " + toDeleteFile.toString(),e);
    }
    final Path toDeleteFileInSnapshot=SnapshotTestHelper.getSnapshotPath(dir,"s0",toDeleteFile.toString().substring(dir.toString().length()));
    try {
      hdfs.getFileStatus(toDeleteFileInSnapshot);
      fail("should throw FileNotFoundException");
    }
 catch (    FileNotFoundException e) {
      GenericTestUtils.assertExceptionContains("File does not exist: " + toDeleteFileInSnapshot.toString(),e);
    }
  }
  /** 
 * Delete a snapshot that is taken before a directory deletion, directory diff list should be combined correctly.
 */
  @Test(timeout=60000) public void testDeleteSnapshot1() throws Exception {
    final Path root=new Path("/");
    Path dir=new Path("/dir1");
    Path file1=new Path(dir,"file1");
    DFSTestUtil.createFile(hdfs,file1,BLOCKSIZE,REPLICATION,seed);
    hdfs.allowSnapshot(root);
    hdfs.createSnapshot(root,"s1");
    Path file2=new Path(dir,"file2");
    DFSTestUtil.createFile(hdfs,file2,BLOCKSIZE,REPLICATION,seed);
    hdfs.createSnapshot(root,"s2");
    hdfs.delete(file1,true);
    hdfs.delete(file2,true);
    assertTrue(hdfs.delete(dir,false));
    hdfs.deleteSnapshot(root,"s2");
    NameNodeAdapter.enterSafeMode(cluster.getNameNode(),false);
    NameNodeAdapter.saveNamespace(cluster.getNameNode());
    cluster.restartNameNodes();
  }
  /** 
 * Delete a snapshot that is taken before a directory deletion (recursively), directory diff list should be combined correctly.
 */
  @Test(timeout=60000) public void testDeleteSnapshot2() throws Exception {
    final Path root=new Path("/");
    Path dir=new Path("/dir1");
    Path file1=new Path(dir,"file1");
    DFSTestUtil.createFile(hdfs,file1,BLOCKSIZE,REPLICATION,seed);
    hdfs.allowSnapshot(root);
    hdfs.createSnapshot(root,"s1");
    Path file2=new Path(dir,"file2");
    DFSTestUtil.createFile(hdfs,file2,BLOCKSIZE,REPLICATION,seed);
    INodeFile file2Node=fsdir.getINode(file2.toString()).asFile();
    long file2NodeId=file2Node.getId();
    hdfs.createSnapshot(root,"s2");
    assertTrue(hdfs.delete(dir,true));
    assertNotNull(fsdir.getInode(file2NodeId));
    hdfs.deleteSnapshot(root,"s2");
    assertTrue(fsdir.getInode(file2NodeId) == null);
    NameNodeAdapter.enterSafeMode(cluster.getNameNode(),false);
    NameNodeAdapter.saveNamespace(cluster.getNameNode());
    cluster.restartNameNodes();
  }
  /** 
 * Test deleting snapshots in a more complicated scenario: need to combine snapshot diffs, but no need to handle diffs distributed in a dir tree
 */
  @Test(timeout=300000) public void testCombineSnapshotDiff1() throws Exception {
    testCombineSnapshotDiffImpl(sub,"",1);
  }
  /** 
 * Test deleting snapshots in more complicated scenarios (snapshot diffs are distributed in the directory sub-tree)
 */
  @Test(timeout=300000) public void testCombineSnapshotDiff2() throws Exception {
    testCombineSnapshotDiffImpl(sub,"subsub1/subsubsub1/",3);
  }
  /** 
 * When combine two snapshots, make sure files/directories created after the  prior snapshot get destroyed.
 */
  @Test(timeout=300000) public void testCombineSnapshotDiff3() throws Exception {
    Path dir=new Path("/dir");
    Path subDir1=new Path(dir,"subdir1");
    Path subDir2=new Path(dir,"subdir2");
    hdfs.mkdirs(subDir2);
    Path subsubDir=new Path(subDir1,"subsubdir");
    hdfs.mkdirs(subsubDir);
    SnapshotTestHelper.createSnapshot(hdfs,dir,"s1");
    Path newDir=new Path(subsubDir,"newdir");
    Path newFile=new Path(newDir,"newfile");
    DFSTestUtil.createFile(hdfs,newFile,BLOCKSIZE,REPLICATION,seed);
    Path newFile2=new Path(subDir2,"newfile");
    DFSTestUtil.createFile(hdfs,newFile2,BLOCKSIZE,REPLICATION,seed);
    SnapshotTestHelper.createSnapshot(hdfs,dir,"s2");
    checkQuotaUsageComputation(dir,7,BLOCKSIZE * 2 * REPLICATION);
    hdfs.delete(subsubDir,true);
    hdfs.delete(subDir2,true);
    checkQuotaUsageComputation(dir,7,BLOCKSIZE * 2 * REPLICATION);
    hdfs.deleteSnapshot(dir,"s2");
    checkQuotaUsageComputation(dir,4,0);
    Path subdir1_s1=SnapshotTestHelper.getSnapshotPath(dir,"s1",subDir1.getName());
    Path subdir1_s2=SnapshotTestHelper.getSnapshotPath(dir,"s2",subDir1.getName());
    assertTrue(hdfs.exists(subdir1_s1));
    assertFalse(hdfs.exists(subdir1_s2));
  }
  /** 
 * Test snapshot deletion
 * @param snapshotRoot The dir where the snapshots are created
 * @param modDirStr The snapshotRoot itself or one of its sub-directory, where the modifications happen. It is represented as a relative  path to the snapshotRoot.
 */
  private void testCombineSnapshotDiffImpl(  Path snapshotRoot,  String modDirStr,  int dirNodeNum) throws Exception {
    Path modDir=modDirStr.isEmpty() ? snapshotRoot : new Path(snapshotRoot,modDirStr);
    Path file10=new Path(modDir,"file10");
    Path file11=new Path(modDir,"file11");
    Path file12=new Path(modDir,"file12");
    Path file13=new Path(modDir,"file13");
    Path file14=new Path(modDir,"file14");
    Path file15=new Path(modDir,"file15");
    DFSTestUtil.createFile(hdfs,file10,BLOCKSIZE,REPLICATION_1,seed);
    DFSTestUtil.createFile(hdfs,file11,BLOCKSIZE,REPLICATION_1,seed);
    DFSTestUtil.createFile(hdfs,file12,BLOCKSIZE,REPLICATION_1,seed);
    DFSTestUtil.createFile(hdfs,file13,BLOCKSIZE,REPLICATION_1,seed);
    SnapshotTestHelper.createSnapshot(hdfs,snapshotRoot,"s1");
    checkQuotaUsageComputation(snapshotRoot,dirNodeNum + 4,8 * BLOCKSIZE);
    hdfs.delete(file11,true);
    checkQuotaUsageComputation(snapshotRoot,dirNodeNum + 4,8 * BLOCKSIZE);
    hdfs.setReplication(file12,REPLICATION);
    checkQuotaUsageComputation(snapshotRoot,dirNodeNum + 4,9 * BLOCKSIZE);
    hdfs.setReplication(file13,REPLICATION);
    checkQuotaUsageComputation(snapshotRoot,dirNodeNum + 4,10 * BLOCKSIZE);
    DFSTestUtil.createFile(hdfs,file14,BLOCKSIZE,REPLICATION,seed);
    checkQuotaUsageComputation(snapshotRoot,dirNodeNum + 5,13 * BLOCKSIZE);
    DFSTestUtil.createFile(hdfs,file15,BLOCKSIZE,REPLICATION,seed);
    checkQuotaUsageComputation(snapshotRoot,dirNodeNum + 6,16 * BLOCKSIZE);
    hdfs.createSnapshot(snapshotRoot,"s2");
    checkQuotaUsageComputation(snapshotRoot,dirNodeNum + 6,16 * BLOCKSIZE);
    DFSTestUtil.createFile(hdfs,file11,BLOCKSIZE,REPLICATION,seed);
    checkQuotaUsageComputation(snapshotRoot,dirNodeNum + 7,19 * BLOCKSIZE);
    hdfs.delete(file12,true);
    checkQuotaUsageComputation(snapshotRoot,dirNodeNum + 7,19 * BLOCKSIZE);
    hdfs.setReplication(file13,(short)(REPLICATION - 2));
    checkQuotaUsageComputation(snapshotRoot,dirNodeNum + 7,19 * BLOCKSIZE);
    hdfs.delete(file14,true);
    checkQuotaUsageComputation(snapshotRoot,dirNodeNum + 7,19 * BLOCKSIZE);
    hdfs.setReplication(file15,REPLICATION_1);
    checkQuotaUsageComputation(snapshotRoot,dirNodeNum + 7,19 * BLOCKSIZE);
    hdfs.createSnapshot(snapshotRoot,"s3");
    checkQuotaUsageComputation(snapshotRoot,dirNodeNum + 7,19 * BLOCKSIZE);
    hdfs.setReplication(file10,REPLICATION);
    checkQuotaUsageComputation(snapshotRoot,dirNodeNum + 7,20 * BLOCKSIZE);
    Path file10_s1=SnapshotTestHelper.getSnapshotPath(snapshotRoot,"s1",modDirStr + "file10");
    Path file11_s1=SnapshotTestHelper.getSnapshotPath(snapshotRoot,"s1",modDirStr + "file11");
    Path file12_s1=SnapshotTestHelper.getSnapshotPath(snapshotRoot,"s1",modDirStr + "file12");
    Path file13_s1=SnapshotTestHelper.getSnapshotPath(snapshotRoot,"s1",modDirStr + "file13");
    Path file14_s2=SnapshotTestHelper.getSnapshotPath(snapshotRoot,"s2",modDirStr + "file14");
    Path file15_s2=SnapshotTestHelper.getSnapshotPath(snapshotRoot,"s2",modDirStr + "file15");
    FileStatus statusBeforeDeletion10=hdfs.getFileStatus(file10_s1);
    FileStatus statusBeforeDeletion11=hdfs.getFileStatus(file11_s1);
    FileStatus statusBeforeDeletion12=hdfs.getFileStatus(file12_s1);
    FileStatus statusBeforeDeletion13=hdfs.getFileStatus(file13_s1);
    INodeFile file14Node=TestSnapshotBlocksMap.assertBlockCollection(file14_s2.toString(),1,fsdir,blockmanager);
    BlockInfo[] blocks_14=file14Node.getBlocks();
    TestSnapshotBlocksMap.assertBlockCollection(file15_s2.toString(),1,fsdir,blockmanager);
    hdfs.deleteSnapshot(snapshotRoot,"s2");
    checkQuotaUsageComputation(snapshotRoot,dirNodeNum + 6,14 * BLOCKSIZE);
    FileStatus statusAfterDeletion10=hdfs.getFileStatus(file10_s1);
    FileStatus statusAfterDeletion11=hdfs.getFileStatus(file11_s1);
    FileStatus statusAfterDeletion12=hdfs.getFileStatus(file12_s1);
    FileStatus statusAfterDeletion13=hdfs.getFileStatus(file13_s1);
    assertEquals(statusBeforeDeletion10.toString(),statusAfterDeletion10.toString());
    assertEquals(statusBeforeDeletion11.toString(),statusAfterDeletion11.toString());
    assertEquals(statusBeforeDeletion12.toString(),statusAfterDeletion12.toString());
    assertEquals(statusBeforeDeletion13.toString(),statusAfterDeletion13.toString());
    TestSnapshotBlocksMap.assertBlockCollection(file10_s1.toString(),1,fsdir,blockmanager);
    TestSnapshotBlocksMap.assertBlockCollection(file11_s1.toString(),1,fsdir,blockmanager);
    TestSnapshotBlocksMap.assertBlockCollection(file12_s1.toString(),1,fsdir,blockmanager);
    TestSnapshotBlocksMap.assertBlockCollection(file13_s1.toString(),1,fsdir,blockmanager);
    Path file14_s1=SnapshotTestHelper.getSnapshotPath(snapshotRoot,"s1",modDirStr + "file14");
    Path file15_s1=SnapshotTestHelper.getSnapshotPath(snapshotRoot,"s1",modDirStr + "file15");
    assertFalse(hdfs.exists(file14_s1));
    assertFalse(hdfs.exists(file15_s1));
    for (    BlockInfo b : blocks_14) {
      assertEquals(INVALID_INODE_ID,b.getBlockCollectionId());
    }
    INodeFile nodeFile13=(INodeFile)fsdir.getINode(file13.toString());
    for (    BlockInfo b : nodeFile13.getBlocks()) {
      assertEquals(REPLICATION_1,b.getReplication());
    }
    TestSnapshotBlocksMap.assertBlockCollection(file13.toString(),1,fsdir,blockmanager);
    INodeFile nodeFile12=(INodeFile)fsdir.getINode(file12_s1.toString());
    for (    BlockInfo b : nodeFile12.getBlocks()) {
      assertEquals(REPLICATION_1,b.getReplication());
    }
  }
  /** 
 * Test deleting snapshots with modification on the metadata of directory 
 */
  @Test(timeout=300000) public void testDeleteSnapshotWithDirModification() throws Exception {
    Path file=new Path(sub,"file");
    DFSTestUtil.createFile(hdfs,file,BLOCKSIZE,REPLICATION,seed);
    hdfs.setOwner(sub,"user1","group1");
    SnapshotTestHelper.createSnapshot(hdfs,sub,"s1");
    checkQuotaUsageComputation(sub,2,BLOCKSIZE * 3);
    hdfs.setOwner(sub,"user2","group2");
    checkQuotaUsageComputation(sub,2,BLOCKSIZE * 3);
    hdfs.createSnapshot(sub,"s2");
    checkQuotaUsageComputation(sub,2,BLOCKSIZE * 3);
    hdfs.createSnapshot(sub,"s3");
    checkQuotaUsageComputation(sub,2,BLOCKSIZE * 3);
    hdfs.setOwner(sub,"user3","group3");
    checkQuotaUsageComputation(sub,2,BLOCKSIZE * 3);
    hdfs.deleteSnapshot(sub,"s3");
    checkQuotaUsageComputation(sub,2,BLOCKSIZE * 3);
    FileStatus statusOfS2=hdfs.getFileStatus(new Path(sub,HdfsConstants.DOT_SNAPSHOT_DIR + "/s2"));
    assertEquals("user2",statusOfS2.getOwner());
    assertEquals("group2",statusOfS2.getGroup());
    hdfs.deleteSnapshot(sub,"s2");
    checkQuotaUsageComputation(sub,2,BLOCKSIZE * 3);
    FileStatus statusOfS1=hdfs.getFileStatus(new Path(sub,HdfsConstants.DOT_SNAPSHOT_DIR + "/s1"));
    assertEquals("user1",statusOfS1.getOwner());
    assertEquals("group1",statusOfS1.getGroup());
  }
  @Test public void testDeleteSnapshotWithPermissionsDisabled() throws Exception {
    cluster.shutdown();
    Configuration newConf=new Configuration(conf);
    newConf.setBoolean(DFSConfigKeys.DFS_PERMISSIONS_ENABLED_KEY,false);
    cluster=new MiniDFSCluster.Builder(newConf).numDataNodes(0).build();
    cluster.waitActive();
    hdfs=cluster.getFileSystem();
    final Path path=new Path("/dir");
    hdfs.mkdirs(path);
    hdfs.allowSnapshot(path);
    hdfs.mkdirs(new Path(path,"/test"));
    hdfs.createSnapshot(path,"s1");
    UserGroupInformation anotherUser=UserGroupInformation.createRemoteUser("anotheruser");
    anotherUser.doAs(new PrivilegedAction<Object>(){
      @Override public Object run(){
        DistributedFileSystem anotherUserFS=null;
        try {
          anotherUserFS=cluster.getFileSystem();
          anotherUserFS.deleteSnapshot(path,"s1");
        }
 catch (        IOException e) {
          fail("Failed to delete snapshot : " + e.getLocalizedMessage());
        }
 finally {
          IOUtils.closeStream(anotherUserFS);
        }
        return null;
      }
    }
);
  }
  /** 
 * A test covering the case where the snapshot diff to be deleted is renamed  to its previous snapshot. 
 */
  @Test(timeout=300000) public void testRenameSnapshotDiff() throws Exception {
    cluster.getNamesystem().getSnapshotManager().setAllowNestedSnapshots(true);
    final Path subFile0=new Path(sub,"file0");
    final Path subsubFile0=new Path(subsub,"file0");
    DFSTestUtil.createFile(hdfs,subFile0,BLOCKSIZE,REPLICATION,seed);
    DFSTestUtil.createFile(hdfs,subsubFile0,BLOCKSIZE,REPLICATION,seed);
    hdfs.setOwner(subsub,"owner","group");
    SnapshotTestHelper.createSnapshot(hdfs,sub,"s0");
    checkQuotaUsageComputation(sub,4,BLOCKSIZE * 6);
    final Path subFile1=new Path(sub,"file1");
    final Path subsubFile1=new Path(subsub,"file1");
    DFSTestUtil.createFile(hdfs,subFile1,BLOCKSIZE,REPLICATION_1,seed);
    DFSTestUtil.createFile(hdfs,subsubFile1,BLOCKSIZE,REPLICATION,seed);
    checkQuotaUsageComputation(sub,6,BLOCKSIZE * 11);
    SnapshotTestHelper.createSnapshot(hdfs,sub,"s1");
    checkQuotaUsageComputation(sub,6,BLOCKSIZE * 11);
    SnapshotTestHelper.createSnapshot(hdfs,dir,"s2");
    checkQuotaUsageComputation(dir,7,BLOCKSIZE * 11);
    checkQuotaUsageComputation(sub,6,BLOCKSIZE * 11);
    hdfs.setOwner(subsub,"unknown","unknown");
    hdfs.setReplication(subsubFile1,REPLICATION_1);
    checkQuotaUsageComputation(dir,7,BLOCKSIZE * 11);
    checkQuotaUsageComputation(sub,6,BLOCKSIZE * 11);
    hdfs.delete(subFile1,true);
    checkQuotaUsageComputation(new Path("/"),8,BLOCKSIZE * 11);
    checkQuotaUsageComputation(dir,7,BLOCKSIZE * 11);
    checkQuotaUsageComputation(sub,6,BLOCKSIZE * 11);
    Path subsubSnapshotCopy=SnapshotTestHelper.getSnapshotPath(dir,"s2",sub.getName() + Path.SEPARATOR + subsub.getName());
    Path subsubFile1SCopy=SnapshotTestHelper.getSnapshotPath(dir,"s2",sub.getName() + Path.SEPARATOR + subsub.getName()+ Path.SEPARATOR+ subsubFile1.getName());
    Path subFile1SCopy=SnapshotTestHelper.getSnapshotPath(dir,"s2",sub.getName() + Path.SEPARATOR + subFile1.getName());
    FileStatus subsubStatus=hdfs.getFileStatus(subsubSnapshotCopy);
    assertEquals("owner",subsubStatus.getOwner());
    assertEquals("group",subsubStatus.getGroup());
    FileStatus subsubFile1Status=hdfs.getFileStatus(subsubFile1SCopy);
    assertEquals(REPLICATION,subsubFile1Status.getReplication());
    FileStatus subFile1Status=hdfs.getFileStatus(subFile1SCopy);
    assertEquals(REPLICATION_1,subFile1Status.getReplication());
    hdfs.deleteSnapshot(dir,"s2");
    checkQuotaUsageComputation(new Path("/"),8,BLOCKSIZE * 11);
    checkQuotaUsageComputation(dir,7,BLOCKSIZE * 11);
    checkQuotaUsageComputation(sub,6,BLOCKSIZE * 11);
    try {
      hdfs.getFileStatus(subsubSnapshotCopy);
      fail("should throw FileNotFoundException");
    }
 catch (    FileNotFoundException e) {
      GenericTestUtils.assertExceptionContains("File does not exist: " + subsubSnapshotCopy.toString(),e);
    }
    try {
      hdfs.getFileStatus(subsubFile1SCopy);
      fail("should throw FileNotFoundException");
    }
 catch (    FileNotFoundException e) {
      GenericTestUtils.assertExceptionContains("File does not exist: " + subsubFile1SCopy.toString(),e);
    }
    try {
      hdfs.getFileStatus(subFile1SCopy);
      fail("should throw FileNotFoundException");
    }
 catch (    FileNotFoundException e) {
      GenericTestUtils.assertExceptionContains("File does not exist: " + subFile1SCopy.toString(),e);
    }
    subsubSnapshotCopy=SnapshotTestHelper.getSnapshotPath(sub,"s1",subsub.getName());
    subsubFile1SCopy=SnapshotTestHelper.getSnapshotPath(sub,"s1",subsub.getName() + Path.SEPARATOR + subsubFile1.getName());
    subFile1SCopy=SnapshotTestHelper.getSnapshotPath(sub,"s1",subFile1.getName());
    subsubStatus=hdfs.getFileStatus(subsubSnapshotCopy);
    assertEquals("owner",subsubStatus.getOwner());
    assertEquals("group",subsubStatus.getGroup());
    subsubFile1Status=hdfs.getFileStatus(subsubFile1SCopy);
    assertEquals(REPLICATION,subsubFile1Status.getReplication());
    subFile1Status=hdfs.getFileStatus(subFile1SCopy);
    assertEquals(REPLICATION_1,subFile1Status.getReplication());
  }
  @Test public void testDeleteSnapshotCommandWithIllegalArguments() throws Exception {
    ByteArrayOutputStream out=new ByteArrayOutputStream();
    PrintStream psOut=new PrintStream(out);
    PrintStream oldOut=System.out;
    PrintStream oldErr=System.err;
    try {
      System.setOut(psOut);
      System.setErr(psOut);
      FsShell shell=new FsShell();
      shell.setConf(conf);
      String[] argv1={"-deleteSnapshot","/tmp"};
      int val=shell.run(argv1);
      assertTrue(val == -1);
      assertTrue(out.toString().contains(argv1[0] + ": Incorrect number of arguments."));
      out.reset();
      String[] argv2={"-deleteSnapshot","/tmp","s1","s2"};
      val=shell.run(argv2);
      assertTrue(val == -1);
      assertTrue(out.toString().contains(argv2[0] + ": Incorrect number of arguments."));
      psOut.close();
      out.close();
    }
  finally {
      System.setOut(oldOut);
      System.setErr(oldErr);
    }
  }
  @Test(timeout=60000) public void testHANNRestartAfterSnapshotDeletion() throws Exception {
    hdfs.close();
    cluster.shutdown();
    conf=new Configuration();
    cluster=new MiniDFSCluster.Builder(conf).nnTopology(MiniDFSNNTopology.simpleHATopology()).numDataNodes(1).build();
    cluster.transitionToActive(0);
    NameNode snn=cluster.getNameNode(1);
    snn.stop();
    hdfs=(DistributedFileSystem)HATestUtil.configureFailoverFs(cluster,conf);
    Path dir=new Path("/dir");
    Path subDir=new Path(dir,"sub");
    hdfs.mkdirs(dir);
    hdfs.allowSnapshot(dir);
    for (int i=0; i < 5; i++) {
      DFSTestUtil.createFile(hdfs,new Path(subDir,"" + i),100,(short)1,1024L);
    }
    hdfs.createSnapshot(dir,"s0");
    hdfs.delete(subDir,true);
    NameNode ann=cluster.getNameNode(0);
    ann.getRpcServer().rollEditLog();
    hdfs.deleteSnapshot(dir,"s0");
    Thread.sleep(2000);
    NameNodeAdapter.abortEditLogs(ann);
    cluster.restartNameNode(0,false);
    cluster.transitionToActive(0);
    cluster.waitClusterUp();
  }
  @Test public void testCorrectNumberOfBlocksAfterRestart() throws IOException {
    final Path foo=new Path("/foo");
    final Path bar=new Path(foo,"bar");
    final Path file=new Path(foo,"file");
    final String snapshotName="ss0";
    DFSTestUtil.createFile(hdfs,file,BLOCKSIZE,REPLICATION,seed);
    hdfs.mkdirs(bar);
    hdfs.setQuota(foo,Long.MAX_VALUE - 1,Long.MAX_VALUE - 1);
    hdfs.setQuota(bar,Long.MAX_VALUE - 1,Long.MAX_VALUE - 1);
    hdfs.allowSnapshot(foo);
    hdfs.createSnapshot(foo,snapshotName);
    hdfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);
    hdfs.saveNamespace();
    hdfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);
    hdfs.deleteSnapshot(foo,snapshotName);
    hdfs.delete(bar,true);
    hdfs.delete(foo,true);
    long numberOfBlocks=cluster.getNamesystem().getBlocksTotal();
    cluster.restartNameNode(0);
    assertEquals(numberOfBlocks,cluster.getNamesystem().getBlocksTotal());
  }
  @Test public void testFsImageCorruption() throws Exception {
    final Path st=new Path("/st");
    final Path nonst=new Path("/nonst");
    final Path stY=new Path(st,"y");
    final Path nonstTrash=new Path(nonst,"trash");
    hdfs.mkdirs(stY);
    hdfs.allowSnapshot(st);
    hdfs.createSnapshot(st,"s0");
    Path f=new Path(stY,"nn.log");
    hdfs.createNewFile(f);
    hdfs.createSnapshot(st,"s1");
    Path f2=new Path(stY,"nn2.log");
    hdfs.rename(f,f2);
    hdfs.createSnapshot(st,"s2");
    Path trashSt=new Path(nonstTrash,"st");
    hdfs.mkdirs(trashSt);
    hdfs.rename(stY,trashSt);
    hdfs.delete(nonstTrash,true);
    hdfs.deleteSnapshot(st,"s1");
    hdfs.deleteSnapshot(st,"s2");
    hdfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);
    hdfs.saveNamespace();
    hdfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);
    cluster.restartNameNodes();
  }
  @Test public void testRenameAndDelete() throws IOException {
    final Path foo=new Path("/foo");
    final Path x=new Path(foo,"x");
    final Path y=new Path(foo,"y");
    final Path trash=new Path("/trash");
    hdfs.mkdirs(x);
    hdfs.mkdirs(y);
    final long parentId=fsdir.getINode4Write(y.toString()).getId();
    hdfs.mkdirs(trash);
    hdfs.allowSnapshot(foo);
    hdfs.createSnapshot(foo,"s0");
    final Path file=new Path(x,"bar");
    DFSTestUtil.createFile(hdfs,file,BLOCKSIZE,(short)1,0L);
    final long fileId=fsdir.getINode4Write(file.toString()).getId();
    final Path newFile=new Path(y,"bar");
    hdfs.rename(file,newFile);
    hdfs.createSnapshot(foo,"s1");
    final Path deletedY=new Path(trash,"y");
    hdfs.rename(y,deletedY);
    hdfs.createSnapshot(foo,"s2");
    hdfs.delete(deletedY,true);
    hdfs.deleteSnapshot(foo,"s1");
    INode p=fsdir.getInode(parentId);
    Assert.assertNotNull(p);
    INodeDirectory pd=p.asDirectory();
    Assert.assertNotNull(pd);
    Assert.assertNull(pd.getChild("bar".getBytes(),Snapshot.CURRENT_STATE_ID));
    Assert.assertNull(fsdir.getInode(fileId));
  }
  @Test public void testSnapshotWithConcatException() throws Exception {
    final Path st=new Path("/st");
    hdfs.mkdirs(st);
    hdfs.allowSnapshot(st);
    Path[] files=new Path[3];
    for (int i=0; i < 3; i++) {
      files[i]=new Path(st,i + ".txt");
    }
    Path dest=new Path(st,"dest.txt");
    hdfs.createNewFile(dest);
    hdfs.createSnapshot(st,"ss");
    for (int j=0; j < 3; j++) {
      FileSystem fs=cluster.getFileSystem();
      DFSTestUtil.createFile(fs,files[j],false,1024,1024,512,(short)1,RandomUtils.nextLong(1,512),true);
    }
    hdfs.createSnapshot(st,"s0");
    exception.expect(RemoteException.class);
    String error="Concat: the source file /st/0.txt is in snapshot";
    exception.expectMessage(error);
    hdfs.concat(dest,files);
    hdfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);
    hdfs.saveNamespace();
    hdfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);
    cluster.restartNameNodes();
  }
  @Test public void testSnapshotDeleteWithConcat() throws Exception {
    final Path st=new Path("/st");
    hdfs.mkdirs(st);
    hdfs.allowSnapshot(st);
    Path[] files=new Path[3];
    for (int i=0; i < 3; i++) {
      files[i]=new Path(st,i + ".txt");
    }
    Path dest=new Path(st,"dest.txt");
    hdfs.createNewFile(dest);
    hdfs.createSnapshot(st,"ss");
    for (int i=0; i < 3; i++) {
      for (int j=0; j < 3; j++) {
        FileSystem fs=cluster.getFileSystem();
        DFSTestUtil.createFile(fs,files[j],false,1024,1024,512,(short)1,RandomUtils.nextLong(1,512),true);
      }
      hdfs.concat(dest,files);
      hdfs.createSnapshot(st,"s" + i);
    }
    hdfs.deleteSnapshot(st,"s1");
    hdfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);
    hdfs.saveNamespace();
    hdfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);
    cluster.restartNameNodes();
  }
  @Test public void testSnapshotDiffReportWithConcat() throws Exception {
    final Path st=new Path("/st");
    hdfs.mkdirs(st);
    hdfs.allowSnapshot(st);
    Path[] files=new Path[3];
    for (int i=0; i < 3; i++) {
      files[i]=new Path(st,i + ".txt");
    }
    Path dest=new Path(st,"dest.txt");
    hdfs.createNewFile(dest);
    hdfs.createSnapshot(st,"ss");
    for (int i=0; i < 3; i++) {
      for (int j=0; j < 3; j++) {
        FileSystem fs=cluster.getFileSystem();
        DFSTestUtil.createFile(fs,files[j],false,1024,1024,512,(short)1,RandomUtils.nextLong(1,512),true);
      }
      hdfs.concat(dest,files);
      hdfs.createSnapshot(st,"s" + i);
      SnapshotDiffReport sdr=hdfs.getSnapshotDiffReport(st,"s" + i,"ss");
      LOG.info("Snapshot Diff s{} to ss : {}",i,sdr);
      Assert.assertEquals(sdr.getDiffList().size(),1);
      Assert.assertTrue(sdr.getDiffList().get(0).getType() == SnapshotDiffReport.DiffType.MODIFY);
      Assert.assertTrue(new Path(st,DFSUtilClient.bytes2String(sdr.getDiffList().get(0).getSourcePath())).equals(dest));
    }
    hdfs.deleteSnapshot(st,"s1");
    hdfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);
    hdfs.saveNamespace();
    hdfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);
    cluster.restartNameNodes();
  }
}
