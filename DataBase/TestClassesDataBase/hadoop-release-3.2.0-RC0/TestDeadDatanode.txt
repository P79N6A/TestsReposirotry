/** 
 * Test to ensure requests from dead datnodes are rejected by namenode with appropriate exceptions/failure response
 */
public class TestDeadDatanode {
  private static final Logger LOG=LoggerFactory.getLogger(TestDeadDatanode.class);
  private MiniDFSCluster cluster;
  @After public void cleanup(){
    if (cluster != null) {
      cluster.shutdown();
      cluster=null;
    }
  }
  /** 
 * Test to ensure namenode rejects request from dead datanode - Start a cluster - Shutdown the datanode and wait for it to be marked dead at the namenode - Send datanode requests to Namenode and make sure it is rejected  appropriately.
 */
  @Test public void testDeadDatanode() throws Exception {
    Configuration conf=new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY,500);
    conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1L);
    cluster=new MiniDFSCluster.Builder(conf).build();
    cluster.waitActive();
    String poolId=cluster.getNamesystem().getBlockPoolId();
    DataNode dn=cluster.getDataNodes().get(0);
    DatanodeRegistration reg=InternalDataNodeTestUtils.getDNRegistrationForBP(cluster.getDataNodes().get(0),poolId);
    DFSTestUtil.waitForDatanodeState(cluster,reg.getDatanodeUuid(),true,20000);
    dn.shutdown();
    DFSTestUtil.waitForDatanodeState(cluster,reg.getDatanodeUuid(),false,20000);
    DatanodeProtocol dnp=cluster.getNameNodeRpc();
    ReceivedDeletedBlockInfo[] blocks={new ReceivedDeletedBlockInfo(new Block(0),ReceivedDeletedBlockInfo.BlockStatus.RECEIVED_BLOCK,null)};
    StorageReceivedDeletedBlocks[] storageBlocks={new StorageReceivedDeletedBlocks(new DatanodeStorage(reg.getDatanodeUuid()),blocks)};
    dnp.blockReceivedAndDeleted(reg,poolId,storageBlocks);
    BlockManager bm=cluster.getNamesystem().getBlockManager();
    bm.flushBlockOps();
    assertFalse(bm.getDatanodeManager().getDatanode(reg).isRegistered());
    StorageBlockReport[] report={new StorageBlockReport(new DatanodeStorage(reg.getDatanodeUuid()),BlockListAsLongs.EMPTY)};
    try {
      dnp.blockReport(reg,poolId,report,new BlockReportContext(1,0,System.nanoTime(),0L,true));
      fail("Expected IOException is not thrown");
    }
 catch (    IOException ex) {
    }
    StorageReport[] rep={new StorageReport(new DatanodeStorage(reg.getDatanodeUuid()),false,0,0,0,0,0)};
    DatanodeCommand[] cmd=dnp.sendHeartbeat(reg,rep,0L,0L,0,0,0,null,true,SlowPeerReports.EMPTY_REPORT,SlowDiskReports.EMPTY_REPORT).getCommands();
    assertEquals(1,cmd.length);
    assertEquals(cmd[0].getAction(),RegisterCommand.REGISTER.getAction());
  }
  @Test public void testDeadNodeAsBlockTarget() throws Exception {
    Configuration conf=new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY,500);
    conf.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1L);
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).build();
    cluster.waitActive();
    String poolId=cluster.getNamesystem().getBlockPoolId();
    DataNode dn=cluster.getDataNodes().get(0);
    DatanodeRegistration reg=InternalDataNodeTestUtils.getDNRegistrationForBP(cluster.getDataNodes().get(0),poolId);
    BlockManager bm=cluster.getNamesystem().getBlockManager();
    DatanodeManager dm=bm.getDatanodeManager();
    Node clientNode=dm.getDatanode(reg);
    DFSTestUtil.waitForDatanodeState(cluster,reg.getDatanodeUuid(),true,20000);
    dn.shutdown();
    DFSTestUtil.waitForDatanodeState(cluster,reg.getDatanodeUuid(),false,20000);
    DatanodeStorageInfo[] results=bm.chooseTarget4NewBlock("/hello",3,clientNode,new HashSet<>(),256 * 1024 * 1024L,null,(byte)7,BlockType.CONTIGUOUS,null,null);
    for (    DatanodeStorageInfo datanodeStorageInfo : results) {
      assertFalse("Dead node should not be chosen",datanodeStorageInfo.getDatanodeDescriptor().equals(clientNode));
    }
  }
  @Test public void testNonDFSUsedONDeadNodeReReg() throws Exception {
    Configuration conf=new HdfsConfiguration();
    conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,1);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY,3000);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_STALE_DATANODE_INTERVAL_KEY,6 * 1000);
    long CAPACITY=5000L;
    long[] capacities=new long[]{4 * CAPACITY,4 * CAPACITY};
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(2).simulatedCapacities(capacities).build();
      long initialCapacity=cluster.getNamesystem(0).getCapacityTotal();
      assertTrue(initialCapacity > 0);
      DataNode dn1=cluster.getDataNodes().get(0);
      DataNode dn2=cluster.getDataNodes().get(1);
      final DatanodeDescriptor dn2Desc=cluster.getNamesystem(0).getBlockManager().getDatanodeManager().getDatanode(dn2.getDatanodeId());
      dn1.setHeartbeatsDisabledForTests(true);
      cluster.setDataNodeDead(dn1.getDatanodeId());
      assertEquals("Capacity shouldn't include DeadNode",dn2Desc.getCapacity(),cluster.getNamesystem(0).getCapacityTotal());
      assertEquals("NonDFS-used shouldn't include DeadNode",dn2Desc.getNonDfsUsed(),cluster.getNamesystem(0).getNonDfsUsedSpace());
      dn1.setHeartbeatsDisabledForTests(false);
      final DatanodeDescriptor dn1Desc=cluster.getNamesystem(0).getBlockManager().getDatanodeManager().getDatanode(dn1.getDatanodeId());
      GenericTestUtils.waitFor(new Supplier<Boolean>(){
        @Override public Boolean get(){
          return dn1Desc.isAlive() && dn1Desc.isHeartbeatedSinceRegistration();
        }
      }
,100,5000);
      assertEquals("Capacity should be 0 after all DNs dead",initialCapacity,cluster.getNamesystem(0).getCapacityTotal());
      long nonDfsAfterReg=cluster.getNamesystem(0).getNonDfsUsedSpace();
      assertEquals("NonDFS should include actual DN NonDFSUsed",dn1Desc.getNonDfsUsed() + dn2Desc.getNonDfsUsed(),nonDfsAfterReg);
    }
  finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
}
