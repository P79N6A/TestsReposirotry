public class TestCombineFileRecordReader {
  private static Path outDir=new Path(System.getProperty("test.build.data","/tmp"),TestCombineFileRecordReader.class.getName());
private static class TextRecordReaderWrapper extends CombineFileRecordReaderWrapper<LongWritable,Text> {
    public TextRecordReaderWrapper(    org.apache.hadoop.mapreduce.lib.input.CombineFileSplit split,    TaskAttemptContext context,    Integer idx) throws IOException, InterruptedException {
      super(new TextInputFormat(),split,context,idx);
    }
  }
  @SuppressWarnings("unchecked") @Test public void testProgressIsReportedIfInputASeriesOfEmptyFiles() throws IOException, InterruptedException {
    JobConf conf=new JobConf();
    Path[] paths=new Path[3];
    File[] files=new File[3];
    long[] fileLength=new long[3];
    try {
      for (int i=0; i < 3; i++) {
        File dir=new File(outDir.toString());
        dir.mkdir();
        files[i]=new File(dir,"testfile" + i);
        FileWriter fileWriter=new FileWriter(files[i]);
        fileWriter.flush();
        fileWriter.close();
        fileLength[i]=i;
        paths[i]=new Path(outDir + "/testfile" + i);
      }
      CombineFileSplit combineFileSplit=new CombineFileSplit(paths,fileLength);
      TaskAttemptID taskAttemptID=Mockito.mock(TaskAttemptID.class);
      TaskReporter reporter=Mockito.mock(TaskReporter.class);
      TaskAttemptContextImpl taskAttemptContext=new TaskAttemptContextImpl(conf,taskAttemptID,reporter);
      CombineFileRecordReader cfrr=new CombineFileRecordReader(combineFileSplit,taskAttemptContext,TextRecordReaderWrapper.class);
      cfrr.initialize(combineFileSplit,taskAttemptContext);
      verify(reporter).progress();
      Assert.assertFalse(cfrr.nextKeyValue());
      verify(reporter,times(3)).progress();
    }
  finally {
      FileUtil.fullyDelete(new File(outDir.toString()));
    }
  }
}
