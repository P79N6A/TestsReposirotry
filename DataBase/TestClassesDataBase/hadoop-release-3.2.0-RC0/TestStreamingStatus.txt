/** 
 * Tests if mapper/reducer with empty/nonempty input works properly if reporting is done using lines like "reporter:status:" and "reporter:counter:" before map()/reduce() method is called. Validates the task's log of STDERR if messages are written to stderr before map()/reduce() is called. Also validates job output. Uses MiniMR since the local jobtracker doesn't track task status. 
 */
public class TestStreamingStatus {
  protected static String TEST_ROOT_DIR=new File(System.getProperty("test.build.data","/tmp"),TestStreamingStatus.class.getSimpleName()).toURI().toString().replace(' ','+');
  protected String INPUT_FILE=TEST_ROOT_DIR + "/input.txt";
  protected String OUTPUT_DIR=TEST_ROOT_DIR + "/out";
  protected String input="roses.are.red\nviolets.are.blue\nbunnies.are.pink\n";
  protected String map=null;
  protected String reduce=null;
  protected String scriptFile=TEST_ROOT_DIR + "/perlScript.pl";
  protected String scriptFileName=new Path(scriptFile).toUri().getPath();
  String expectedStderr="my error msg before consuming input\n" + "my error msg after consuming input\n";
  String expectedOutput=null;
  String expectedStatus="before consuming input";
  protected String script="#!/usr/bin/perl\n" + "print STDERR \"reporter:status:" + expectedStatus + "\\n\";\n"+ "print STDERR \"reporter:counter:myOwnCounterGroup,myOwnCounter,1\\n\";\n"+ "print STDERR \"my error msg before consuming input\\n\";\n"+ "for($count = 1500; $count >= 1; $count--) {print STDOUT \"$count \";}"+ "while(<STDIN>) {chomp;}\n"+ "print STDERR \"my error msg after consuming input\\n\";\n"+ "print STDERR \"reporter:counter:myOwnCounterGroup,myOwnCounter,1\\n\";\n";
  MiniMRCluster mr=null;
  FileSystem fs=null;
  JobConf conf=null;
  /** 
 * Start the cluster and create input file before running the actual test.
 * @throws IOException
 */
  @Before public void setUp() throws IOException {
    conf=new JobConf();
    conf.setBoolean(JTConfig.JT_RETIREJOBS,false);
    conf.setBoolean(JTConfig.JT_PERSIST_JOBSTATUS,false);
    mr=new MiniMRCluster(1,"file:///",3,null,null,conf);
    Path inFile=new Path(INPUT_FILE);
    fs=inFile.getFileSystem(mr.createJobConf());
    clean(fs);
    buildExpectedJobOutput();
  }
  /** 
 * Kill the cluster after the test is done.
 */
  @After public void tearDown(){
    if (fs != null) {
      clean(fs);
    }
    if (mr != null) {
      mr.shutdown();
    }
  }
  void buildExpectedJobOutput(){
    if (expectedOutput == null) {
      expectedOutput="";
      for (int i=1500; i >= 1; i--) {
        expectedOutput=expectedOutput.concat(Integer.toString(i) + " ");
      }
      expectedOutput=expectedOutput.trim();
    }
  }
  protected void createInputAndScript(  boolean isEmptyInput,  String script) throws IOException {
    makeInput(fs,isEmptyInput ? "" : input);
    DataOutputStream file=fs.create(new Path(scriptFileName));
    file.writeBytes(script);
    file.close();
  }
  protected String[] genArgs(  String jobtracker,  String mapper,  String reducer){
    return new String[]{"-input",INPUT_FILE,"-output",OUTPUT_DIR,"-mapper",mapper,"-reducer",reducer,"-jobconf",MRJobConfig.NUM_MAPS + "=1","-jobconf",MRJobConfig.NUM_REDUCES + "=1","-jobconf",MRJobConfig.PRESERVE_FAILED_TASK_FILES + "=true","-jobconf","stream.tmpdir=" + new Path(TEST_ROOT_DIR).toUri().getPath(),"-jobconf",JTConfig.JT_IPC_ADDRESS + "=" + jobtracker,"-jobconf","fs.default.name=file:///","-jobconf","mapred.jar=" + TestStreaming.STREAMING_JAR,"-jobconf","mapreduce.framework.name=yarn"};
  }
  public void makeInput(  FileSystem fs,  String input) throws IOException {
    Path inFile=new Path(INPUT_FILE);
    DataOutputStream file=fs.create(inFile);
    file.writeBytes(input);
    file.close();
  }
  protected void deleteOutDir(  FileSystem fs){
    try {
      Path outDir=new Path(OUTPUT_DIR);
      fs.delete(outDir,true);
    }
 catch (    Exception e) {
    }
  }
  public void clean(  FileSystem fs){
    deleteOutDir(fs);
    try {
      Path file=new Path(INPUT_FILE);
      if (fs.exists(file)) {
        fs.delete(file,false);
      }
      file=new Path(scriptFile);
      if (fs.exists(file)) {
        fs.delete(file,false);
      }
    }
 catch (    Exception e) {
      e.printStackTrace();
    }
  }
  /** 
 * Check if mapper/reducer with empty/nonempty input works properly if reporting is done using lines like "reporter:status:" and "reporter:counter:" before map()/reduce() method is called. Validate the task's log of STDERR if messages are written to stderr before map()/reduce() is called. Also validate job output.
 * @throws IOException
 */
  @Test public void testReporting() throws Exception {
    testStreamJob(false);
    testStreamJob(true);
  }
  /** 
 * Run a streaming job with the given script as mapper and validate. Run another streaming job with the given script as reducer and validate.
 * @param isEmptyInput Should the input to the script be empty ?
 * @param script The content of the script that will run as the streaming task
 */
  private void testStreamJob(  boolean isEmptyInput) throws IOException {
    createInputAndScript(isEmptyInput,script);
    map=scriptFileName;
    reduce="/bin/cat";
    runStreamJob(TaskType.MAP,isEmptyInput);
    deleteOutDir(fs);
    map="/bin/cat";
    reduce=scriptFileName;
    runStreamJob(TaskType.REDUCE,isEmptyInput);
    clean(fs);
  }
  void runStreamJob(  TaskType type,  boolean isEmptyInput) throws IOException {
    boolean mayExit=false;
    StreamJob job=new StreamJob(genArgs(mr.createJobConf().get(JTConfig.JT_IPC_ADDRESS),map,reduce),mayExit);
    int returnValue=job.go();
    assertEquals(0,returnValue);
    int expectedCounterValue=0;
    if (type == TaskType.MAP || !isEmptyInput) {
      validateTaskStatus(job,type);
      validateJobOutput(job.getConf());
      expectedCounterValue=2;
    }
    validateUserCounter(job,expectedCounterValue);
    validateTaskStderr(job,type);
    deleteOutDir(fs);
  }
  void validateTaskStatus(  StreamJob job,  TaskType type) throws IOException {
    String finalPhaseInTask;
    TaskReport[] reports;
    if (type == TaskType.MAP) {
      reports=job.jc_.getMapTaskReports(job.jobId_);
      finalPhaseInTask="sort";
    }
 else {
      reports=job.jc_.getReduceTaskReports(job.jobId_);
      finalPhaseInTask="reduce";
    }
    assertEquals(1,reports.length);
    assertEquals(expectedStatus + " > " + finalPhaseInTask,reports[0].getState());
  }
  void validateJobOutput(  Configuration conf) throws IOException {
    String output=MapReduceTestUtil.readOutput(new Path(OUTPUT_DIR),conf).trim();
    assertTrue(output.equals(expectedOutput));
  }
  void validateTaskStderr(  StreamJob job,  TaskType type) throws IOException {
    TaskAttemptID attemptId=new TaskAttemptID(new TaskID(job.jobId_,type,0),0);
    String log=MapReduceTestUtil.readTaskLog(TaskLog.LogName.STDERR,attemptId,false);
    assertTrue(log.equals(expectedStderr.trim()));
  }
  void validateUserCounter(  StreamJob job,  int expectedCounterValue) throws IOException {
    Counters counters=job.running_.getCounters();
    assertEquals(expectedCounterValue,counters.findCounter("myOwnCounterGroup","myOwnCounter").getValue());
  }
}
