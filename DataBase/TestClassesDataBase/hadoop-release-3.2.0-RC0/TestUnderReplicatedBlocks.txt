public class TestUnderReplicatedBlocks {
  @Test(timeout=120000) public void testSetRepIncWithUnderReplicatedBlocks() throws Exception {
    Configuration conf=new HdfsConfiguration();
    final short REPLICATION_FACTOR=2;
    final String FILE_NAME="/testFile";
    final Path FILE_PATH=new Path(FILE_NAME);
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(REPLICATION_FACTOR + 1).build();
    try {
      final FileSystem fs=cluster.getFileSystem();
      final BlockManager bm=cluster.getNamesystem().getBlockManager();
      DFSTestUtil.createFile(fs,FILE_PATH,1L,REPLICATION_FACTOR,1L);
      DFSTestUtil.waitReplication(fs,FILE_PATH,REPLICATION_FACTOR);
      BlockManagerTestUtil.updateState(bm);
      DFSTestUtil.verifyClientStats(conf,cluster);
      ExtendedBlock b=DFSTestUtil.getFirstBlock(fs,FILE_PATH);
      DatanodeDescriptor dn=bm.blocksMap.getStorages(b.getLocalBlock()).iterator().next().getDatanodeDescriptor();
      bm.addToInvalidates(b.getLocalBlock(),dn);
      BlockManagerTestUtil.computeAllPendingWork(bm);
      DataNodeTestUtils.triggerHeartbeat(cluster.getDataNode(dn.getIpcPort()));
      Thread.sleep(5000);
      BlockManagerTestUtil.updateState(bm);
      DFSTestUtil.verifyClientStats(conf,cluster);
      bm.blocksMap.removeNode(b.getLocalBlock(),dn);
      BlockManagerTestUtil.updateState(bm);
      DFSTestUtil.verifyClientStats(conf,cluster);
      FsShell shell=new FsShell(conf);
      assertEquals(0,shell.run(new String[]{"-setrep","-w",Integer.toString(1 + REPLICATION_FACTOR),FILE_NAME}));
      BlockManagerTestUtil.updateState(bm);
      DFSTestUtil.verifyClientStats(conf,cluster);
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * The test verifies the number of outstanding replication requests for a given DN shouldn't exceed the limit set by configuration property dfs.namenode.replication.max-streams-hard-limit. The test does the followings: 1. Create a mini cluster with 2 DNs. Set large heartbeat interval so that replication requests won't be picked by any DN right away. 2. Create a file with 10 blocks and replication factor 2. Thus each of the 2 DNs have one replica of each block. 3. Add a DN to the cluster for later replication. 4. Remove a DN that has data. 5. Ask BlockManager to compute the replication work. This will assign replication requests to the only DN that has data. 6. Make sure the number of pending replication requests of that DN don't exceed the limit.
 * @throws Exception
 */
  @Test(timeout=60000) public void testNumberOfBlocksToBeReplicated() throws Exception {
    Configuration conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY,0);
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,1);
    conf.setInt(DFSConfigKeys.DFS_BYTES_PER_CHECKSUM_KEY,1);
    conf.setInt(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,100);
    conf.setInt(DFSConfigKeys.DFS_NAMENODE_REPLICATION_WORK_MULTIPLIER_PER_ITERATION,5);
    int NUM_OF_BLOCKS=10;
    final short REP_FACTOR=2;
    final String FILE_NAME="/testFile";
    final Path FILE_PATH=new Path(FILE_NAME);
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(REP_FACTOR).build();
    try {
      final FileSystem fs=cluster.getFileSystem();
      DFSTestUtil.createFile(fs,FILE_PATH,NUM_OF_BLOCKS,REP_FACTOR,1L);
      DFSTestUtil.waitReplication(fs,FILE_PATH,REP_FACTOR);
      cluster.startDataNodes(conf,1,true,null,null,null,null);
      final BlockManager bm=cluster.getNamesystem().getBlockManager();
      ExtendedBlock b=DFSTestUtil.getFirstBlock(fs,FILE_PATH);
      Iterator<DatanodeStorageInfo> storageInfos=bm.blocksMap.getStorages(b.getLocalBlock()).iterator();
      DatanodeDescriptor firstDn=storageInfos.next().getDatanodeDescriptor();
      DatanodeDescriptor secondDn=storageInfos.next().getDatanodeDescriptor();
      BlockManagerTestUtil.updateState(bm);
      DFSTestUtil.verifyClientStats(conf,cluster);
      bm.getDatanodeManager().removeDatanode(firstDn);
      BlockManagerTestUtil.updateState(bm);
      assertEquals(NUM_OF_BLOCKS,bm.getUnderReplicatedNotMissingBlocks());
      DFSTestUtil.verifyClientStats(conf,cluster);
      bm.computeDatanodeWork();
      assertTrue("The number of replication work pending before targets are " + "determined should be non-negative.",(Integer)Whitebox.getInternalState(secondDn,"pendingReplicationWithoutTargets") >= 0);
      BlockManagerTestUtil.updateState(bm);
      assertTrue("The number of blocks to be replicated should be less than " + "or equal to " + bm.replicationStreamsHardLimit,secondDn.getNumberOfBlocksToBeReplicated() <= bm.replicationStreamsHardLimit);
      DFSTestUtil.verifyClientStats(conf,cluster);
    }
  finally {
      cluster.shutdown();
    }
  }
}
