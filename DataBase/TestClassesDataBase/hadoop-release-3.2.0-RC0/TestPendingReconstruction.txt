/** 
 * This class tests the internals of PendingReconstructionBlocks.java, as well as how PendingReconstructionBlocks acts in BlockManager
 */
public class TestPendingReconstruction {
  final static int TIMEOUT=3;
  private static final int DFS_REPLICATION_INTERVAL=1;
  private static final int DATANODE_COUNT=5;
  private BlockInfo genBlockInfo(  long id,  long length,  long gs){
    return new BlockInfoContiguous(new Block(id,length,gs),(short)DATANODE_COUNT);
  }
  @Test public void testPendingReconstruction(){
    PendingReconstructionBlocks pendingReconstructions;
    pendingReconstructions=new PendingReconstructionBlocks(TIMEOUT * 1000);
    pendingReconstructions.start();
    DatanodeStorageInfo[] storages=DFSTestUtil.createDatanodeStorageInfos(10);
    for (int i=0; i < storages.length; i++) {
      BlockInfo block=genBlockInfo(i,i,0);
      DatanodeStorageInfo[] targets=new DatanodeStorageInfo[i];
      System.arraycopy(storages,0,targets,0,i);
      pendingReconstructions.increment(block,DatanodeStorageInfo.toDatanodeDescriptors(targets));
    }
    assertEquals("Size of pendingReconstruction ",10,pendingReconstructions.size());
    BlockInfo blk=genBlockInfo(8,8,0);
    pendingReconstructions.decrement(blk,storages[7].getDatanodeDescriptor());
    assertEquals("pendingReconstructions.getNumReplicas ",7,pendingReconstructions.getNumReplicas(blk));
    pendingReconstructions.increment(blk,storages[0].getDatanodeDescriptor());
    assertEquals("pendingReconstructions.getNumReplicas ",7,pendingReconstructions.getNumReplicas(blk));
    for (int i=0; i < 7; i++) {
      pendingReconstructions.decrement(blk,storages[i].getDatanodeDescriptor());
    }
    assertTrue(pendingReconstructions.size() == 9);
    pendingReconstructions.increment(blk,DatanodeStorageInfo.toDatanodeDescriptors(DFSTestUtil.createDatanodeStorageInfos(8)));
    assertTrue(pendingReconstructions.size() == 10);
    for (int i=0; i < 10; i++) {
      BlockInfo block=genBlockInfo(i,i,0);
      int numReplicas=pendingReconstructions.getNumReplicas(block);
      assertTrue(numReplicas == i);
    }
    assertNull(pendingReconstructions.getTimedOutBlocks());
    assertEquals(0L,pendingReconstructions.getNumTimedOuts());
    try {
      Thread.sleep(1000);
    }
 catch (    Exception ignored) {
    }
    for (int i=10; i < 15; i++) {
      BlockInfo block=genBlockInfo(i,i,0);
      pendingReconstructions.increment(block,DatanodeStorageInfo.toDatanodeDescriptors(DFSTestUtil.createDatanodeStorageInfos(i)));
    }
    assertEquals(15,pendingReconstructions.size());
    assertEquals(0L,pendingReconstructions.getNumTimedOuts());
    int loop=0;
    while (pendingReconstructions.size() > 0) {
      try {
        Thread.sleep(1000);
      }
 catch (      Exception e) {
      }
      loop++;
    }
    System.out.println("Had to wait for " + loop + " seconds for the lot to timeout");
    assertEquals("Size of pendingReconstructions ",0,pendingReconstructions.size());
    assertEquals(15L,pendingReconstructions.getNumTimedOuts());
    Block[] timedOut=pendingReconstructions.getTimedOutBlocks();
    assertNotNull(timedOut);
    assertEquals(15,timedOut.length);
    assertEquals(15L,pendingReconstructions.getNumTimedOuts());
    for (    Block block : timedOut) {
      assertTrue(block.getBlockId() < 15);
    }
    pendingReconstructions.stop();
  }
  @Test public void testProcessPendingReconstructions() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    conf.setLong(DFS_NAMENODE_RECONSTRUCTION_PENDING_TIMEOUT_SEC_KEY,TIMEOUT);
    MiniDFSCluster cluster=null;
    Block block;
    BlockInfo blockInfo;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(DATANODE_COUNT).build();
      cluster.waitActive();
      FSNamesystem fsn=cluster.getNamesystem();
      BlockManager blkManager=fsn.getBlockManager();
      PendingReconstructionBlocks pendingReconstruction=blkManager.pendingReconstruction;
      LowRedundancyBlocks neededReconstruction=blkManager.neededReconstruction;
      BlocksMap blocksMap=blkManager.blocksMap;
      block=new Block(1,1,0);
      blockInfo=new BlockInfoContiguous(block,(short)3);
      pendingReconstruction.increment(blockInfo,DatanodeStorageInfo.toDatanodeDescriptors(DFSTestUtil.createDatanodeStorageInfos(1)));
      BlockCollection bc=Mockito.mock(BlockCollection.class);
      blockInfo.setGenerationStamp(1);
      blocksMap.addBlockCollection(blockInfo,bc);
      BlockInfo storedBlock=blockInfo;
      assertEquals("Size of pendingReconstructions ",1,pendingReconstruction.size());
      block=new Block(2,2,0);
      blockInfo=new BlockInfoContiguous(block,(short)3);
      pendingReconstruction.increment(blockInfo,DatanodeStorageInfo.toDatanodeDescriptors(DFSTestUtil.createDatanodeStorageInfos(1)));
      assertEquals("Size of pendingReconstructions ",2,pendingReconstruction.size());
      while (pendingReconstruction.size() > 0) {
        try {
          Thread.sleep(100);
        }
 catch (        Exception e) {
        }
      }
      while (neededReconstruction.size() == 0) {
        try {
          Thread.sleep(100);
        }
 catch (        Exception e) {
        }
      }
      for (      Block b : neededReconstruction) {
        assertEquals("Generation stamp is 1 ",1,b.getGenerationStamp());
      }
      assertEquals("size of neededReconstruction is 1 ",1,neededReconstruction.size());
      BlockManagerTestUtil.stopRedundancyThread(blkManager);
      pendingReconstruction.clear();
      DatanodeDescriptor desc[]={blkManager.getDatanodeManager().getDatanodes().iterator().next()};
      pendingReconstruction.increment(storedBlock,desc);
      assertEquals("Size of pendingReconstructions ",1,pendingReconstruction.size());
      fsn.writeLock();
      try {
        blkManager.addBlock(desc[0].getStorageInfos()[0],new Block(1,1,0),null);
      }
  finally {
        fsn.writeUnlock();
      }
      assertEquals("Size of pendingReconstructions ",1,pendingReconstruction.size());
      fsn.writeLock();
      try {
        blkManager.addBlock(desc[0].getStorageInfos()[0],new Block(1,1,1),null);
      }
  finally {
        fsn.writeUnlock();
      }
      assertEquals("Size of pendingReconstructions ",0,pendingReconstruction.size());
    }
  finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
  /** 
 * Test if DatanodeProtocol#blockReceivedAndDeleted can correctly update the pending reconstruction. Also make sure the blockReceivedAndDeleted call is idempotent to the pending reconstruction.
 */
  @Test public void testBlockReceived() throws Exception {
    final Configuration conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,1024);
    MiniDFSCluster cluster=null;
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(DATANODE_COUNT).build();
      cluster.waitActive();
      DistributedFileSystem hdfs=cluster.getFileSystem();
      FSNamesystem fsn=cluster.getNamesystem();
      BlockManager blkManager=fsn.getBlockManager();
      final String file="/tmp.txt";
      final Path filePath=new Path(file);
      short replFactor=1;
      DFSTestUtil.createFile(hdfs,filePath,1024L,replFactor,0);
      ArrayList<DataNode> datanodes=cluster.getDataNodes();
      for (int i=0; i < DATANODE_COUNT; i++) {
        DataNodeTestUtils.setHeartbeatsDisabledForTests(datanodes.get(i),true);
      }
      hdfs.setReplication(filePath,(short)DATANODE_COUNT);
      BlockManagerTestUtil.computeAllPendingWork(blkManager);
      assertEquals(1,blkManager.pendingReconstruction.size());
      INodeFile fileNode=fsn.getFSDirectory().getINode4Write(file).asFile();
      BlockInfo[] blocks=fileNode.getBlocks();
      assertEquals(DATANODE_COUNT - 1,blkManager.pendingReconstruction.getNumReplicas(blocks[0]));
      LocatedBlock locatedBlock=hdfs.getClient().getLocatedBlocks(file,0).get(0);
      DatanodeInfo existingDn=(locatedBlock.getLocations())[0];
      int reportDnNum=0;
      String poolId=cluster.getNamesystem().getBlockPoolId();
      for (int i=0; i < DATANODE_COUNT && reportDnNum < 2; i++) {
        if (!datanodes.get(i).getDatanodeId().equals(existingDn)) {
          DatanodeRegistration dnR=datanodes.get(i).getDNRegistrationForBP(poolId);
          StorageReceivedDeletedBlocks[] report={new StorageReceivedDeletedBlocks(new DatanodeStorage("Fake-storage-ID-Ignored"),new ReceivedDeletedBlockInfo[]{new ReceivedDeletedBlockInfo(blocks[0],BlockStatus.RECEIVED_BLOCK,"")})};
          cluster.getNameNodeRpc().blockReceivedAndDeleted(dnR,poolId,report);
          reportDnNum++;
        }
      }
      cluster.getNamesystem().getBlockManager().flushBlockOps();
      assertEquals(DATANODE_COUNT - 3,blkManager.pendingReconstruction.getNumReplicas(blocks[0]));
      for (int i=0; i < DATANODE_COUNT && reportDnNum < 2; i++) {
        if (!datanodes.get(i).getDatanodeId().equals(existingDn)) {
          DatanodeRegistration dnR=datanodes.get(i).getDNRegistrationForBP(poolId);
          StorageReceivedDeletedBlocks[] report={new StorageReceivedDeletedBlocks(new DatanodeStorage("Fake-storage-ID-Ignored"),new ReceivedDeletedBlockInfo[]{new ReceivedDeletedBlockInfo(blocks[0],BlockStatus.RECEIVED_BLOCK,"")})};
          cluster.getNameNodeRpc().blockReceivedAndDeleted(dnR,poolId,report);
          reportDnNum++;
        }
      }
      cluster.getNamesystem().getBlockManager().flushBlockOps();
      assertEquals(DATANODE_COUNT - 3,blkManager.pendingReconstruction.getNumReplicas(blocks[0]));
      for (int i=0; i < DATANODE_COUNT; i++) {
        DataNodeTestUtils.setHeartbeatsDisabledForTests(datanodes.get(i),false);
        DataNodeTestUtils.triggerHeartbeat(datanodes.get(i));
      }
      Thread.sleep(5000);
      assertEquals(0,blkManager.pendingReconstruction.size());
    }
  finally {
      if (cluster != null) {
        cluster.shutdown();
      }
    }
  }
  /** 
 * Test if BlockManager can correctly remove corresponding pending records when a file is deleted
 * @throws Exception
 */
  @Test public void testPendingAndInvalidate() throws Exception {
    final Configuration CONF=new HdfsConfiguration();
    CONF.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,1024);
    CONF.setLong(DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY,DFS_REPLICATION_INTERVAL);
    CONF.setInt(DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,DFS_REPLICATION_INTERVAL);
    MiniDFSCluster cluster=new MiniDFSCluster.Builder(CONF).numDataNodes(DATANODE_COUNT).build();
    cluster.waitActive();
    FSNamesystem namesystem=cluster.getNamesystem();
    BlockManager bm=namesystem.getBlockManager();
    DistributedFileSystem fs=cluster.getFileSystem();
    try {
      Path filePath=new Path("/tmp.txt");
      DFSTestUtil.createFile(fs,filePath,1024,(short)3,0L);
      for (      DataNode dn : cluster.getDataNodes()) {
        DataNodeTestUtils.setHeartbeatsDisabledForTests(dn,true);
      }
      LocatedBlock block=NameNodeAdapter.getBlockLocations(cluster.getNameNode(),filePath.toString(),0,1).get(0);
      cluster.getNamesystem().writeLock();
      try {
        bm.findAndMarkBlockAsCorrupt(block.getBlock(),block.getLocations()[0],"STORAGE_ID","TEST");
        bm.findAndMarkBlockAsCorrupt(block.getBlock(),block.getLocations()[1],"STORAGE_ID","TEST");
        BlockManagerTestUtil.computeAllPendingWork(bm);
        BlockManagerTestUtil.updateState(bm);
        assertEquals(bm.getPendingReconstructionBlocksCount(),1L);
        BlockInfo storedBlock=bm.getStoredBlock(block.getBlock().getLocalBlock());
        assertEquals(bm.pendingReconstruction.getNumReplicas(storedBlock),2);
      }
  finally {
        cluster.getNamesystem().writeUnlock();
      }
      fs.delete(filePath,true);
      int retries=10;
      long pendingNum=bm.getPendingReconstructionBlocksCount();
      while (pendingNum != 0 && retries-- > 0) {
        Thread.sleep(1000);
        BlockManagerTestUtil.updateState(bm);
        pendingNum=bm.getPendingReconstructionBlocksCount();
      }
      assertEquals(pendingNum,0L);
    }
  finally {
      cluster.shutdown();
    }
  }
  /** 
 * Test the metric counters of the re-replication process.
 * @throws IOException
 * @throws InterruptedException
 * @throws TimeoutException
 */
  @Test(timeout=300000) public void testReplicationCounter() throws IOException, InterruptedException, TimeoutException {
    HdfsConfiguration conf=new HdfsConfiguration();
    conf.setInt(DFS_NAMENODE_REDUNDANCY_INTERVAL_SECONDS_KEY,1);
    conf.setInt(DFS_NAMENODE_RECONSTRUCTION_PENDING_TIMEOUT_SEC_KEY,1);
    MiniDFSCluster tmpCluster=new MiniDFSCluster.Builder(conf).numDataNodes(DATANODE_COUNT).build();
    tmpCluster.waitActive();
    FSNamesystem fsn=tmpCluster.getNamesystem(0);
    fsn.writeLock();
    try {
      BlockManager bm=fsn.getBlockManager();
      BlocksMap blocksMap=bm.blocksMap;
      BlockCollection bc0=Mockito.mock(BlockCollection.class);
      BlockInfo blockInfo0=new BlockInfoContiguous((short)3);
      blockInfo0.setBlockId(0);
      BlockCollection bc1=Mockito.mock(BlockCollection.class);
      BlockInfo blockInfo1=new BlockInfoContiguous((short)3);
      blockInfo1.setBlockId(1);
      BlockCollection bc2=Mockito.mock(BlockCollection.class);
      BlockInfo blockInfo2=new BlockInfoContiguous((short)3);
      blockInfo2.setBlockId(2);
      blocksMap.addBlockCollection(blockInfo0,bc0);
      blocksMap.addBlockCollection(blockInfo1,bc1);
      blocksMap.addBlockCollection(blockInfo2,bc2);
      PendingReconstructionBlocks pending=bm.pendingReconstruction;
      MetricsRecordBuilder rb=getMetrics("NameNodeActivity");
      assertCounter("SuccessfulReReplications",0L,rb);
      assertCounter("NumTimesReReplicationNotScheduled",0L,rb);
      assertCounter("TimeoutReReplications",0L,rb);
      pending.increment(blockInfo0);
      pending.increment(blockInfo1);
      DatanodeStorageInfo[] storageInfos=DFSTestUtil.createDatanodeStorageInfos(1);
      bm.addBlock(storageInfos[0],blockInfo0,null);
      bm.scheduleReconstruction(blockInfo2,0);
      GenericTestUtils.waitFor(new Supplier<Boolean>(){
        @Override public Boolean get(){
          MetricsRecordBuilder rb=getMetrics("NameNodeActivity");
          return getLongCounter("SuccessfulReReplications",rb) == 1 && getLongCounter("NumTimesReReplicationNotScheduled",rb) == 1 && getLongCounter("TimeoutReReplications",rb) == 1;
        }
      }
,100,60000);
    }
  finally {
      tmpCluster.shutdown();
      fsn.writeUnlock();
    }
  }
}
