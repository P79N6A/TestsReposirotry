public class TestFSMainOperationsWebHdfs extends FSMainOperationsBaseTest {
{
    GenericTestUtils.setLogLevel(ExceptionHandler.LOG,Level.TRACE);
  }
  private static MiniDFSCluster cluster=null;
  private static Path defaultWorkingDirectory;
  private static FileSystem fileSystem;
  public TestFSMainOperationsWebHdfs(){
    super("/tmp/TestFSMainOperationsWebHdfs");
  }
  @Override protected FileSystem createFileSystem() throws Exception {
    return fileSystem;
  }
  @BeforeClass public static void setupCluster(){
    final Configuration conf=new Configuration();
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,1024);
    try {
      cluster=new MiniDFSCluster.Builder(conf).numDataNodes(2).build();
      cluster.waitActive();
      cluster.getFileSystem().setPermission(new Path("/"),new FsPermission((short)0777));
      final String uri=WebHdfsConstants.WEBHDFS_SCHEME + "://" + conf.get(DFSConfigKeys.DFS_NAMENODE_HTTP_ADDRESS_KEY);
      final UserGroupInformation current=UserGroupInformation.getCurrentUser();
      final UserGroupInformation ugi=UserGroupInformation.createUserForTesting(current.getShortUserName() + "x",new String[]{"user"});
      fileSystem=ugi.doAs(new PrivilegedExceptionAction<FileSystem>(){
        @Override public FileSystem run() throws Exception {
          return FileSystem.get(new URI(uri),conf);
        }
      }
);
      defaultWorkingDirectory=fileSystem.getWorkingDirectory();
    }
 catch (    Exception e) {
      throw new RuntimeException(e);
    }
  }
  @AfterClass public static void shutdownCluster(){
    if (cluster != null) {
      cluster.shutdown();
      cluster=null;
    }
  }
  @Override protected Path getDefaultWorkingDirectory(){
    return defaultWorkingDirectory;
  }
  @Test public void testConcat() throws Exception {
    Path[] paths={new Path("/test/hadoop/file1"),new Path("/test/hadoop/file2"),new Path("/test/hadoop/file3")};
    DFSTestUtil.createFile(fSys,paths[0],1024,(short)3,0);
    DFSTestUtil.createFile(fSys,paths[1],1024,(short)3,0);
    DFSTestUtil.createFile(fSys,paths[2],1024,(short)3,0);
    Path catPath=new Path("/test/hadoop/catFile");
    DFSTestUtil.createFile(fSys,catPath,1024,(short)3,0);
    Assert.assertTrue(exists(fSys,catPath));
    fSys.concat(catPath,paths);
    Assert.assertFalse(exists(fSys,paths[0]));
    Assert.assertFalse(exists(fSys,paths[1]));
    Assert.assertFalse(exists(fSys,paths[2]));
    FileStatus fileStatus=fSys.getFileStatus(catPath);
    Assert.assertEquals(1024 * 4,fileStatus.getLen());
  }
  @Test public void testTruncate() throws Exception {
    final short repl=3;
    final int blockSize=1024;
    final int numOfBlocks=2;
    Path dir=getTestRootPath(fSys,"test/hadoop");
    Path file=getTestRootPath(fSys,"test/hadoop/file");
    final byte[] data=getFileData(numOfBlocks,blockSize);
    createFile(fSys,file,data,blockSize,repl);
    final int newLength=blockSize;
    boolean isReady=fSys.truncate(file,newLength);
    Assert.assertTrue("Recovery is not expected.",isReady);
    FileStatus fileStatus=fSys.getFileStatus(file);
    Assert.assertEquals(fileStatus.getLen(),newLength);
    AppendTestUtil.checkFullFile(fSys,file,newLength,data,file.toString());
    ContentSummary cs=fSys.getContentSummary(dir);
    Assert.assertEquals("Bad disk space usage",cs.getSpaceConsumed(),newLength * repl);
    Assert.assertTrue("Deleted",fSys.delete(dir,true));
  }
  boolean closedInputStream=false;
  @Test public void testJsonParseClosesInputStream() throws Exception {
    final WebHdfsFileSystem webhdfs=(WebHdfsFileSystem)fileSystem;
    Path file=getTestRootPath(fSys,"test/hadoop/file");
    createFile(file);
    final HttpOpParam.Op op=GetOpParam.Op.GETHOMEDIRECTORY;
    final URL url=webhdfs.toUrl(op,file);
    final HttpURLConnection conn=(HttpURLConnection)url.openConnection();
    conn.setRequestMethod(op.getType().toString());
    conn.connect();
    InputStream myIn=new InputStream(){
      private HttpURLConnection localConn=conn;
      @Override public void close() throws IOException {
        closedInputStream=true;
        localConn.getInputStream().close();
      }
      @Override public int read() throws IOException {
        return localConn.getInputStream().read();
      }
    }
;
    final HttpURLConnection spyConn=spy(conn);
    doReturn(myIn).when(spyConn).getInputStream();
    try {
      Assert.assertFalse(closedInputStream);
      WebHdfsFileSystem.jsonParse(spyConn,false);
      Assert.assertTrue(closedInputStream);
    }
 catch (    IOException ioe) {
      junit.framework.TestCase.fail();
    }
    conn.disconnect();
  }
  @Override @Test public void testMkdirsFailsForSubdirectoryOfExistingFile() throws Exception {
    Path testDir=getTestRootPath(fSys,"test/hadoop");
    Assert.assertFalse(exists(fSys,testDir));
    fSys.mkdirs(testDir);
    Assert.assertTrue(exists(fSys,testDir));
    createFile(getTestRootPath(fSys,"test/hadoop/file"));
    Path testSubDir=getTestRootPath(fSys,"test/hadoop/file/subdir");
    try {
      fSys.mkdirs(testSubDir);
      Assert.fail("Should throw IOException.");
    }
 catch (    IOException e) {
    }
    try {
      Assert.assertFalse(exists(fSys,testSubDir));
    }
 catch (    AccessControlException e) {
    }
    Path testDeepSubDir=getTestRootPath(fSys,"test/hadoop/file/deep/sub/dir");
    try {
      fSys.mkdirs(testDeepSubDir);
      Assert.fail("Should throw IOException.");
    }
 catch (    IOException e) {
    }
    try {
      Assert.assertFalse(exists(fSys,testDeepSubDir));
    }
 catch (    AccessControlException e) {
    }
  }
}
