/** 
 * Test that datanodes can correctly handle errors during block read/write.
 */
public class TestDiskError {
  private FileSystem fs;
  private MiniDFSCluster cluster;
  private Configuration conf;
  @Before public void setUp() throws Exception {
    conf=new HdfsConfiguration();
    conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,512L);
    conf.setTimeDuration(DFSConfigKeys.DFS_DATANODE_DISK_CHECK_MIN_GAP_KEY,0,TimeUnit.MILLISECONDS);
    cluster=new MiniDFSCluster.Builder(conf).numDataNodes(1).build();
    cluster.waitActive();
    fs=cluster.getFileSystem();
  }
  @After public void tearDown() throws Exception {
    if (cluster != null) {
      cluster.shutdown();
      cluster=null;
    }
  }
  /** 
 * Test to check that a DN goes down when all its volumes have failed.
 */
  @Test public void testShutdown() throws Exception {
    if (System.getProperty("os.name").startsWith("Windows")) {
      return;
    }
    cluster.startDataNodes(conf,2,true,null,null);
    cluster.waitActive();
    final int dnIndex=0;
    String bpid=cluster.getNamesystem().getBlockPoolId();
    File storageDir=cluster.getInstanceStorageDir(dnIndex,0);
    File dir1=MiniDFSCluster.getRbwDir(storageDir,bpid);
    storageDir=cluster.getInstanceStorageDir(dnIndex,1);
    File dir2=MiniDFSCluster.getRbwDir(storageDir,bpid);
    try {
      assertTrue("Couldn't chmod local vol",dir1.setReadOnly());
      assertTrue("Couldn't chmod local vol",dir2.setReadOnly());
      DataNode dn=cluster.getDataNodes().get(dnIndex);
      for (int i=0; dn.isDatanodeUp(); i++) {
        Path fileName=new Path("/test.txt" + i);
        DFSTestUtil.createFile(fs,fileName,1024,(short)2,1L);
        DFSTestUtil.waitReplication(fs,fileName,(short)2);
        fs.delete(fileName,true);
      }
    }
  finally {
      FileUtil.setWritable(dir1,true);
      FileUtil.setWritable(dir2,true);
    }
  }
  /** 
 * Test that when there is a failure replicating a block the temporary and meta files are cleaned up and subsequent replication succeeds.
 */
  @Test public void testReplicationError() throws Exception {
    final Path fileName=new Path("/test.txt");
    final int fileLen=1;
    DFSTestUtil.createFile(fs,fileName,1,(short)1,1L);
    DFSTestUtil.waitReplication(fs,fileName,(short)1);
    LocatedBlocks blocks=NameNodeAdapter.getBlockLocations(cluster.getNameNode(),fileName.toString(),0,(long)fileLen);
    assertEquals("Should only find 1 block",blocks.locatedBlockCount(),1);
    LocatedBlock block=blocks.get(0);
    cluster.startDataNodes(conf,1,true,null,null);
    cluster.waitActive();
    final int sndNode=1;
    DataNode datanode=cluster.getDataNodes().get(sndNode);
    FsDatasetTestUtils utils=cluster.getFsDatasetTestUtils(datanode);
    InetSocketAddress target=datanode.getXferAddress();
    Socket s=new Socket(target.getAddress(),target.getPort());
    DataOutputStream out=new DataOutputStream(s.getOutputStream());
    DataChecksum checksum=DataChecksum.newDataChecksum(DataChecksum.Type.CRC32,512);
    new Sender(out).writeBlock(block.getBlock(),StorageType.DEFAULT,BlockTokenSecretManager.DUMMY_TOKEN,"",new DatanodeInfo[0],new StorageType[0],null,BlockConstructionStage.PIPELINE_SETUP_CREATE,1,0L,0L,0L,checksum,CachingStrategy.newDefaultStrategy(),false,false,null,null,new String[0]);
    out.flush();
    out.close();
    String bpid=cluster.getNamesystem().getBlockPoolId();
    while (utils.getStoredReplicas(bpid).hasNext()) {
      Thread.sleep(100);
    }
    fs.setReplication(fileName,(short)2);
    DFSTestUtil.waitReplication(fs,fileName,(short)1);
    fs.delete(fileName,false);
  }
  /** 
 * Check that the permissions of the local DN directories are as expected.
 */
  @Test public void testLocalDirs() throws Exception {
    Configuration conf=new Configuration();
    final String permStr=conf.get(DFSConfigKeys.DFS_DATANODE_DATA_DIR_PERMISSION_KEY);
    FsPermission expected=new FsPermission(permStr);
    FileSystem localFS=FileSystem.getLocal(conf);
    for (    DataNode dn : cluster.getDataNodes()) {
      try (FsDatasetSpi.FsVolumeReferences volumes=dn.getFSDataset().getFsVolumeReferences()){
        for (        FsVolumeSpi vol : volumes) {
          Path dataDir=new Path(vol.getStorageLocation().getNormalizedUri());
          FsPermission actual=localFS.getFileStatus(dataDir).getPermission();
          assertEquals("Permission for dir: " + dataDir + ", is "+ actual+ ", while expected is "+ expected,expected,actual);
        }
      }
     }
  }
  /** 
 * Checks whether  {@link DataNode#checkDiskErrorAsync()} is being called or not.Before refactoring the code the above function was not getting called 
 * @throws IOException , InterruptedException
 */
  @Test(timeout=60000) public void testcheckDiskError() throws Exception {
    if (cluster.getDataNodes().size() <= 0) {
      cluster.startDataNodes(conf,1,true,null,null);
      cluster.waitActive();
    }
    DataNode dataNode=cluster.getDataNodes().get(0);
    final long lastCheckTimestamp=dataNode.getLastDiskErrorCheck();
    dataNode.checkDiskError();
    GenericTestUtils.waitFor(new Supplier<Boolean>(){
      @Override public Boolean get(){
        return dataNode.getLastDiskErrorCheck() > lastCheckTimestamp;
      }
    }
,100,60000);
  }
  @Test public void testDataTransferWhenBytesPerChecksumIsZero() throws IOException {
    DataNode dn0=cluster.getDataNodes().get(0);
    BlockScanner mockScanner=Mockito.mock(BlockScanner.class);
    Mockito.when(mockScanner.isEnabled()).thenReturn(false);
    dn0.setBlockScanner(mockScanner);
    Path filePath=new Path("test.dat");
    FSDataOutputStream out=fs.create(filePath,(short)1);
    out.write(1);
    out.hflush();
    out.close();
    ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,filePath);
    File metadataFile=cluster.getBlockMetadataFile(0,block);
    RandomAccessFile raFile=new RandomAccessFile(metadataFile,"rw");
    raFile.seek(2);
    raFile.writeByte(0);
    raFile.writeInt(0);
    raFile.close();
    String datanodeId0=dn0.getDatanodeUuid();
    LocatedBlock lb=DFSTestUtil.getAllBlocks(fs,filePath).get(0);
    String storageId=lb.getStorageIDs()[0];
    cluster.startDataNodes(conf,1,true,null,null);
    DataNode dn1=null;
    for (int i=0; i < cluster.getDataNodes().size(); i++) {
      if (!cluster.getDataNodes().get(i).equals(datanodeId0)) {
        dn1=cluster.getDataNodes().get(i);
        break;
      }
    }
    DatanodeDescriptor dnd1=NameNodeAdapter.getDatanode(cluster.getNamesystem(),dn1.getDatanodeId());
    dn0.transferBlock(block,new DatanodeInfo[]{dnd1},new StorageType[]{StorageType.DISK},new String[0]);
    try {
      Thread.sleep(1000);
    }
 catch (    InterruptedException e) {
    }
    Mockito.verify(mockScanner).markSuspectBlock(Mockito.eq(storageId),Mockito.eq(block));
  }
}
