public class TestEarlyStoppingSparkCompGraph extends BaseSparkTest {
  @Test public void testEarlyStoppingIris(){
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).updater(new Sgd()).weightInit(WeightInit.XAVIER).graphBuilder().addInputs("in").addLayer("0",new OutputLayer.Builder().nIn(4).nOut(3).lossFunction(LossFunctions.LossFunction.MCXENT).build(),"in").setOutputs("0").build();
    ComputationGraph net=new ComputationGraph(conf);
    net.setListeners(new ScoreIterationListener(1));
    JavaRDD<DataSet> irisData=getIris();
    EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
    EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(5)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(1,TimeUnit.MINUTES)).scoreCalculator(new SparkLossCalculatorComputationGraph(irisData.map(new DataSetToMultiDataSetFn()),true,sc.sc())).modelSaver(saver).build();
    TrainingMaster tm=new ParameterAveragingTrainingMaster(true,numExecutors(),1,10,1,0);
    IEarlyStoppingTrainer<ComputationGraph> trainer=new SparkEarlyStoppingGraphTrainer(getContext().sc(),tm,esConf,net,irisData.map(new DataSetToMultiDataSetFn()));
    EarlyStoppingResult<ComputationGraph> result=trainer.fit();
    System.out.println(result);
    assertEquals(5,result.getTotalEpochs());
    assertEquals(EarlyStoppingResult.TerminationReason.EpochTerminationCondition,result.getTerminationReason());
    Map<Integer,Double> scoreVsIter=result.getScoreVsEpoch();
    assertEquals(5,scoreVsIter.size());
    String expDetails=esConf.getEpochTerminationConditions().get(0).toString();
    assertEquals(expDetails,result.getTerminationDetails());
    ComputationGraph out=result.getBestModel();
    assertNotNull(out);
    ComputationGraph bestNetwork=result.getBestModel();
    double score=bestNetwork.score(new IrisDataSetIterator(150,150).next());
    double bestModelScore=result.getBestModelScore();
    assertEquals(bestModelScore,score,1e-3);
  }
  @Test public void testBadTuning(){
    Nd4j.getRandom().setSeed(12345);
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).updater(new Sgd(2.0)).weightInit(WeightInit.XAVIER).graphBuilder().addInputs("in").addLayer("0",new OutputLayer.Builder().nIn(4).nOut(3).activation(Activation.IDENTITY).lossFunction(LossFunctions.LossFunction.MSE).build(),"in").setOutputs("0").build();
    ComputationGraph net=new ComputationGraph(conf);
    net.setListeners(new ScoreIterationListener(1));
    JavaRDD<DataSet> irisData=getIris();
    EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
    EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(5000)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(1,TimeUnit.MINUTES),new MaxScoreIterationTerminationCondition(7.5)).scoreCalculator(new SparkLossCalculatorComputationGraph(irisData.map(new DataSetToMultiDataSetFn()),true,sc.sc())).modelSaver(saver).build();
    TrainingMaster tm=new ParameterAveragingTrainingMaster(true,numExecutors(),1,10,1,0);
    IEarlyStoppingTrainer<ComputationGraph> trainer=new SparkEarlyStoppingGraphTrainer(getContext().sc(),tm,esConf,net,irisData.map(new DataSetToMultiDataSetFn()));
    EarlyStoppingResult result=trainer.fit();
    assertTrue(result.getTotalEpochs() < 5);
    assertEquals(EarlyStoppingResult.TerminationReason.IterationTerminationCondition,result.getTerminationReason());
    String expDetails=new MaxScoreIterationTerminationCondition(7.5).toString();
    assertEquals(expDetails,result.getTerminationDetails());
  }
  @Test public void testTimeTermination(){
    Nd4j.getRandom().setSeed(12345);
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).updater(new Sgd(1e-6)).weightInit(WeightInit.XAVIER).graphBuilder().addInputs("in").addLayer("0",new OutputLayer.Builder().nIn(4).nOut(3).lossFunction(LossFunctions.LossFunction.MCXENT).build(),"in").setOutputs("0").build();
    ComputationGraph net=new ComputationGraph(conf);
    net.setListeners(new ScoreIterationListener(1));
    JavaRDD<DataSet> irisData=getIris();
    EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
    EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(10000)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(3,TimeUnit.SECONDS),new MaxScoreIterationTerminationCondition(7.5)).scoreCalculator(new SparkLossCalculatorComputationGraph(irisData.map(new DataSetToMultiDataSetFn()),true,sc.sc())).modelSaver(saver).build();
    TrainingMaster tm=new ParameterAveragingTrainingMaster(true,numExecutors(),1,10,1,0);
    IEarlyStoppingTrainer<ComputationGraph> trainer=new SparkEarlyStoppingGraphTrainer(getContext().sc(),tm,esConf,net,irisData.map(new DataSetToMultiDataSetFn()));
    long startTime=System.currentTimeMillis();
    EarlyStoppingResult result=trainer.fit();
    long endTime=System.currentTimeMillis();
    int durationSeconds=(int)(endTime - startTime) / 1000;
    assertTrue(durationSeconds >= 3);
    assertTrue(durationSeconds <= 9);
    assertEquals(EarlyStoppingResult.TerminationReason.IterationTerminationCondition,result.getTerminationReason());
    String expDetails=new MaxTimeIterationTerminationCondition(3,TimeUnit.SECONDS).toString();
    assertEquals(expDetails,result.getTerminationDetails());
  }
  @Test public void testNoImprovementNEpochsTermination(){
    Nd4j.getRandom().setSeed(12345);
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).updater(new Sgd(0.0)).weightInit(WeightInit.XAVIER).graphBuilder().addInputs("in").addLayer("0",new OutputLayer.Builder().nIn(4).nOut(3).lossFunction(LossFunctions.LossFunction.MCXENT).build(),"in").setOutputs("0").build();
    ComputationGraph net=new ComputationGraph(conf);
    net.setListeners(new ScoreIterationListener(1));
    JavaRDD<DataSet> irisData=getIris();
    EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
    EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(100),new ScoreImprovementEpochTerminationCondition(5)).iterationTerminationConditions(new MaxScoreIterationTerminationCondition(7.5)).scoreCalculator(new SparkLossCalculatorComputationGraph(irisData.map(new DataSetToMultiDataSetFn()),true,sc.sc())).modelSaver(saver).build();
    TrainingMaster tm=new ParameterAveragingTrainingMaster(true,numExecutors(),1,10,1,0);
    IEarlyStoppingTrainer<ComputationGraph> trainer=new SparkEarlyStoppingGraphTrainer(getContext().sc(),tm,esConf,net,irisData.map(new DataSetToMultiDataSetFn()));
    EarlyStoppingResult result=trainer.fit();
    assertTrue(result.getTotalEpochs() < 12);
    assertEquals(EarlyStoppingResult.TerminationReason.EpochTerminationCondition,result.getTerminationReason());
    String expDetails=new ScoreImprovementEpochTerminationCondition(5).toString();
    assertEquals(expDetails,result.getTerminationDetails());
  }
  @Test public void testListeners(){
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).updater(new Sgd()).weightInit(WeightInit.XAVIER).graphBuilder().addInputs("in").addLayer("0",new OutputLayer.Builder().nIn(4).nOut(3).lossFunction(LossFunctions.LossFunction.MCXENT).build(),"in").setOutputs("0").build();
    ComputationGraph net=new ComputationGraph(conf);
    net.setListeners(new ScoreIterationListener(1));
    JavaRDD<DataSet> irisData=getIris();
    EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
    EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(5)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(1,TimeUnit.MINUTES)).scoreCalculator(new SparkLossCalculatorComputationGraph(irisData.map(new DataSetToMultiDataSetFn()),true,sc.sc())).modelSaver(saver).build();
    LoggingEarlyStoppingListener listener=new LoggingEarlyStoppingListener();
    TrainingMaster tm=new ParameterAveragingTrainingMaster(true,numExecutors(),1,10,1,0);
    IEarlyStoppingTrainer<ComputationGraph> trainer=new SparkEarlyStoppingGraphTrainer(getContext().sc(),tm,esConf,net,irisData.map(new DataSetToMultiDataSetFn()));
    trainer.setListener(listener);
    trainer.fit();
    assertEquals(1,listener.onStartCallCount);
    assertEquals(5,listener.onEpochCallCount);
    assertEquals(1,listener.onCompletionCallCount);
  }
private static class LoggingEarlyStoppingListener implements EarlyStoppingListener<ComputationGraph> {
    private static Logger log=LoggerFactory.getLogger(LoggingEarlyStoppingListener.class);
    private int onStartCallCount=0;
    private int onEpochCallCount=0;
    private int onCompletionCallCount=0;
    @Override public void onStart(    EarlyStoppingConfiguration esConfig,    ComputationGraph net){
      log.info("EarlyStopping: onStart called");
      onStartCallCount++;
    }
    @Override public void onEpoch(    int epochNum,    double score,    EarlyStoppingConfiguration esConfig,    ComputationGraph net){
      log.info("EarlyStopping: onEpoch called (epochNum={}, score={}}",epochNum,score);
      onEpochCallCount++;
    }
    @Override public void onCompletion(    EarlyStoppingResult esResult){
      log.info("EorlyStopping: onCompletion called (result: {})",esResult);
      onCompletionCallCount++;
    }
  }
  private JavaRDD<DataSet> getIris(){
    JavaSparkContext sc=getContext();
    IrisDataSetIterator iter=new IrisDataSetIterator(1,150);
    List<DataSet> list=new ArrayList<>(150);
    while (iter.hasNext())     list.add(iter.next());
    return sc.parallelize(list);
  }
}
