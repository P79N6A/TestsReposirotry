/** 
 * Handle dividing things up by mini batch
 */
public class MiniBatchTests extends BaseSparkTest {
  private static final Logger log=LoggerFactory.getLogger(MiniBatchTests.class);
  @Test public void testMiniBatches() throws Exception {
    log.info("Setting up Spark Context...");
    JavaRDD<String> lines=sc.textFile(new ClassPathResource("svmLight/iris_svmLight_0.txt").getTempFileFromArchive().toURI().toString()).cache();
    long count=lines.count();
    assertEquals(300,count);
    RecordReader rr=new SVMLightRecordReader();
    Configuration c=new Configuration();
    c.set(SVMLightRecordReader.NUM_FEATURES,"5");
    rr.setConf(c);
    JavaRDD<DataSet> points=lines.map(new RecordReaderFunction(rr,4,3)).cache();
    count=points.count();
    assertEquals(300,count);
    points=points.repartition(1);
    JavaRDD<DataSet> miniBatches=new RDDMiniBatches(10,points).miniBatchesJava();
    count=miniBatches.count();
    List<DataSet> list=miniBatches.collect();
    assertEquals(30,count);
    lines.unpersist();
    points.unpersist();
    miniBatches.map(new DataSetAssertionFunction());
  }
public static class DataSetAssertionFunction implements Function<DataSet,Object> {
    @Override public Object call(    DataSet dataSet) throws Exception {
      assertTrue(dataSet.getFeatures().columns() == 150);
      assertTrue(dataSet.numExamples() == 30);
      return null;
    }
  }
}
