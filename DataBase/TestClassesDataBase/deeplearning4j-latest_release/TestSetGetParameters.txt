public class TestSetGetParameters extends BaseDL4JTest {
  @Test public void testSetParameters(){
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().list().layer(0,new DenseLayer.Builder().nIn(9).nOut(10).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).build()).layer(1,new DenseLayer.Builder().nIn(10).nOut(11).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).build()).layer(2,new AutoEncoder.Builder().corruptionLevel(0.5).nIn(11).nOut(12).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).build()).layer(3,new OutputLayer.Builder(LossFunction.MSE).nIn(12).nOut(12).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).build()).build();
    MultiLayerNetwork net=new MultiLayerNetwork(conf);
    net.init();
    INDArray initParams=net.params().dup();
    Map<String,INDArray> initParams2=net.paramTable();
    net.setParams(net.params());
    INDArray initParamsAfter=net.params();
    Map<String,INDArray> initParams2After=net.paramTable();
    for (    String s : initParams2.keySet()) {
      assertTrue("Params differ: " + s,initParams2.get(s).equals(initParams2After.get(s)));
    }
    assertEquals(initParams,initParamsAfter);
    INDArray randomParams=Nd4j.rand(initParams.shape());
    net.setParams(randomParams.dup());
    assertEquals(net.params(),randomParams);
  }
  @Test public void testSetParametersRNN(){
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().list().layer(0,new GravesLSTM.Builder().nIn(9).nOut(10).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).build()).layer(1,new GravesLSTM.Builder().nIn(10).nOut(11).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).build()).layer(2,new RnnOutputLayer.Builder(LossFunction.MSE).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).nIn(11).nOut(12).build()).build();
    MultiLayerNetwork net=new MultiLayerNetwork(conf);
    net.init();
    INDArray initParams=net.params().dup();
    Map<String,INDArray> initParams2=net.paramTable();
    net.setParams(net.params());
    INDArray initParamsAfter=net.params();
    Map<String,INDArray> initParams2After=net.paramTable();
    for (    String s : initParams2.keySet()) {
      assertTrue("Params differ: " + s,initParams2.get(s).equals(initParams2After.get(s)));
    }
    assertEquals(initParams,initParamsAfter);
    INDArray randomParams=Nd4j.rand(initParams.shape());
    net.setParams(randomParams.dup());
    assertEquals(net.params(),randomParams);
  }
  @Test public void testInitWithParams(){
    Nd4j.getRandom().setSeed(12345);
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).list().layer(0,new ConvolutionLayer.Builder().nIn(10).nOut(10).kernelSize(2,2).stride(2,2).padding(2,2).build()).layer(1,new DenseLayer.Builder().nIn(10).nOut(10).build()).layer(2,new GravesLSTM.Builder().nIn(10).nOut(10).build()).layer(3,new GravesBidirectionalLSTM.Builder().nIn(10).nOut(10).build()).layer(4,new OutputLayer.Builder(LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(10).nOut(10).build()).build();
    MultiLayerNetwork net=new MultiLayerNetwork(conf);
    net.init();
    INDArray params=net.params();
    MultiLayerNetwork net2=new MultiLayerNetwork(conf);
    net2.init(params,true);
    MultiLayerNetwork net3=new MultiLayerNetwork(conf);
    net3.init(params,false);
    assertEquals(params,net2.params());
    assertEquals(params,net3.params());
    assertFalse(params == net2.params());
    assertTrue(params == net3.params());
    Map<String,INDArray> paramsMap=net.paramTable();
    Map<String,INDArray> paramsMap2=net2.paramTable();
    Map<String,INDArray> paramsMap3=net3.paramTable();
    for (    String s : paramsMap.keySet()) {
      assertEquals(paramsMap.get(s),paramsMap2.get(s));
      assertEquals(paramsMap.get(s),paramsMap3.get(s));
    }
  }
}
