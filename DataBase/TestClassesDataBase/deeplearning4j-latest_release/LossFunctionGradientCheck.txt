/** 
 * Created by Alex on 12/09/2016.
 */
@Slf4j public class LossFunctionGradientCheck extends BaseDL4JTest {
static {
    Nd4j.setDataType(DataBuffer.Type.DOUBLE);
  }
  private static final boolean PRINT_RESULTS=true;
  private static final boolean RETURN_ON_FIRST_FAILURE=false;
  private static final double DEFAULT_EPS=1e-6;
  private static final double DEFAULT_MAX_REL_ERROR=1e-5;
  private static final double DEFAULT_MIN_ABS_ERROR=1e-8;
  @Test public void lossFunctionGradientCheck(){
    ILossFunction[] lossFunctions=new ILossFunction[]{new LossBinaryXENT(),new LossBinaryXENT(),new LossCosineProximity(),new LossHinge(),new LossKLD(),new LossKLD(),new LossL1(),new LossL1(),new LossL1(),new LossL2(),new LossL2(),new LossMAE(),new LossMAE(),new LossMAPE(),new LossMAPE(),new LossMCXENT(),new LossMSE(),new LossMSE(),new LossMSLE(),new LossMSLE(),new LossNegativeLogLikelihood(),new LossNegativeLogLikelihood(),new LossPoisson(),new LossSquaredHinge(),new LossFMeasure(),new LossFMeasure(2.0),new LossFMeasure(),new LossFMeasure(2.0),LossMixtureDensity.builder().gaussians(2).labelWidth(3).build(),LossMixtureDensity.builder().gaussians(2).labelWidth(3).build(),new LossMultiLabel()};
    Activation[] outputActivationFn=new Activation[]{Activation.SIGMOID,Activation.SIGMOID,Activation.TANH,Activation.TANH,Activation.SIGMOID,Activation.SOFTMAX,Activation.TANH,Activation.RATIONALTANH,Activation.SOFTMAX,Activation.TANH,Activation.SOFTMAX,Activation.IDENTITY,Activation.SOFTMAX,Activation.IDENTITY,Activation.SOFTMAX,Activation.SOFTMAX,Activation.IDENTITY,Activation.SOFTMAX,Activation.SIGMOID,Activation.SOFTMAX,Activation.SIGMOID,Activation.SOFTMAX,Activation.SIGMOID,Activation.TANH,Activation.SIGMOID,Activation.SIGMOID,Activation.SOFTMAX,Activation.SOFTMAX,Activation.IDENTITY,Activation.TANH,Activation.TANH};
    int[] nOut=new int[]{1,3,5,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,1,1,2,2,10,10,10};
    int[] minibatchSizes=new int[]{1,3};
    List<String> passed=new ArrayList<>();
    List<String> failed=new ArrayList<>();
    for (int i=0; i < lossFunctions.length; i++) {
      for (int j=0; j < minibatchSizes.length; j++) {
        String testName=lossFunctions[i] + " - " + outputActivationFn[i]+ " - minibatchSize = "+ minibatchSizes[j];
        Nd4j.getRandom().setSeed(12345);
        MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).seed(12345).updater(new NoOp()).weightInit(WeightInit.DISTRIBUTION).dist(new UniformDistribution(-2,2)).list().layer(0,new DenseLayer.Builder().nIn(4).nOut(4).activation(Activation.TANH).build()).layer(1,new OutputLayer.Builder().lossFunction(lossFunctions[i]).activation(outputActivationFn[i]).nIn(4).nOut(nOut[i]).build()).validateOutputLayerConfig(false).build();
        MultiLayerNetwork net=new MultiLayerNetwork(conf);
        net.init();
        INDArray[] inOut=getFeaturesAndLabels(lossFunctions[i],minibatchSizes[j],4,nOut[i],12345);
        INDArray input=inOut[0];
        INDArray labels=inOut[1];
        log.info(" ***** Starting test: {} *****",testName);
        boolean gradOK;
        try {
          gradOK=GradientCheckUtil.checkGradients(net,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
        }
 catch (        Exception e) {
          e.printStackTrace();
          failed.add(testName + "\t" + "EXCEPTION");
          continue;
        }
        if (gradOK) {
          passed.add(testName);
        }
 else {
          failed.add(testName);
        }
        System.out.println("\n\n");
        TestUtils.testModelSerialization(net);
      }
    }
    System.out.println("---- Passed ----");
    for (    String s : passed) {
      System.out.println(s);
    }
    System.out.println("---- Failed ----");
    for (    String s : failed) {
      System.out.println(s);
    }
    assertEquals("Tests failed",0,failed.size());
  }
  @Test public void lossFunctionGradientCheckLossLayer(){
    ILossFunction[] lossFunctions=new ILossFunction[]{new LossBinaryXENT(),new LossBinaryXENT(),new LossCosineProximity(),new LossHinge(),new LossKLD(),new LossKLD(),new LossL1(),new LossL1(),new LossL2(),new LossL2(),new LossMAE(),new LossMAE(),new LossMAPE(),new LossMAPE(),new LossMCXENT(),new LossMSE(),new LossMSE(),new LossMSLE(),new LossMSLE(),new LossNegativeLogLikelihood(),new LossNegativeLogLikelihood(),new LossPoisson(),new LossSquaredHinge(),new LossFMeasure(),new LossFMeasure(2.0),new LossFMeasure(),new LossFMeasure(2.0),LossMixtureDensity.builder().gaussians(2).labelWidth(3).build(),LossMixtureDensity.builder().gaussians(2).labelWidth(3).build(),new LossMultiLabel()};
    Activation[] outputActivationFn=new Activation[]{Activation.SIGMOID,Activation.SIGMOID,Activation.TANH,Activation.TANH,Activation.SIGMOID,Activation.SOFTMAX,Activation.TANH,Activation.SOFTMAX,Activation.TANH,Activation.SOFTMAX,Activation.IDENTITY,Activation.SOFTMAX,Activation.IDENTITY,Activation.SOFTMAX,Activation.SOFTMAX,Activation.IDENTITY,Activation.SOFTMAX,Activation.SIGMOID,Activation.SOFTMAX,Activation.SIGMOID,Activation.SOFTMAX,Activation.SIGMOID,Activation.TANH,Activation.SIGMOID,Activation.SIGMOID,Activation.SOFTMAX,Activation.SOFTMAX,Activation.IDENTITY,Activation.TANH,Activation.TANH};
    int[] nOut=new int[]{1,3,5,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,1,1,2,2,10,10,10};
    int[] minibatchSizes=new int[]{1,3};
    List<String> passed=new ArrayList<>();
    List<String> failed=new ArrayList<>();
    for (int i=0; i < lossFunctions.length; i++) {
      for (int j=0; j < minibatchSizes.length; j++) {
        String testName=lossFunctions[i] + " - " + outputActivationFn[i]+ " - minibatchSize = "+ minibatchSizes[j];
        try {
          ObjectMapper m=NeuralNetConfiguration.mapper();
          String s=m.writeValueAsString(lossFunctions[i]);
          ILossFunction lf2=m.readValue(s,lossFunctions[i].getClass());
          lossFunctions[i]=lf2;
        }
 catch (        IOException ex) {
          ex.printStackTrace();
          assertEquals("Tests failed: serialization of " + lossFunctions[i],0,1);
        }
        Nd4j.getRandom().setSeed(12345);
        MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).seed(12345).updater(new NoOp()).weightInit(WeightInit.DISTRIBUTION).dist(new UniformDistribution(-2,2)).list().layer(0,new DenseLayer.Builder().nIn(4).nOut(nOut[i]).activation(Activation.TANH).build()).layer(1,new LossLayer.Builder().lossFunction(lossFunctions[i]).activation(outputActivationFn[i]).build()).validateOutputLayerConfig(false).build();
        MultiLayerNetwork net=new MultiLayerNetwork(conf);
        net.init();
        assertTrue(((LossLayer)net.getLayer(1).conf().getLayer()).getLossFn().getClass() == lossFunctions[i].getClass());
        INDArray[] inOut=getFeaturesAndLabels(lossFunctions[i],minibatchSizes[j],4,nOut[i],12345);
        INDArray input=inOut[0];
        INDArray labels=inOut[1];
        log.info(" ***** Starting test: {} *****",testName);
        boolean gradOK;
        try {
          gradOK=GradientCheckUtil.checkGradients(net,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
        }
 catch (        Exception e) {
          e.printStackTrace();
          failed.add(testName + "\t" + "EXCEPTION");
          continue;
        }
        if (gradOK) {
          passed.add(testName);
        }
 else {
          failed.add(testName);
        }
        System.out.println("\n\n");
        TestUtils.testModelSerialization(net);
      }
    }
    System.out.println("---- Passed ----");
    for (    String s : passed) {
      System.out.println(s);
    }
    System.out.println("---- Failed ----");
    for (    String s : failed) {
      System.out.println(s);
    }
    assertEquals("Tests failed",0,failed.size());
  }
  public static INDArray[] getFeaturesAndLabels(  ILossFunction l,  long minibatch,  long nIn,  long nOut,  long seed){
    return getFeaturesAndLabels(l,new long[]{minibatch,nIn},new long[]{minibatch,nOut},seed);
  }
  public static INDArray[] getFeaturesAndLabels(  ILossFunction l,  int[] featuresShape,  int[] labelsShape,  long seed){
    return getFeaturesAndLabels(l,ArrayUtil.toLongArray(featuresShape),ArrayUtil.toLongArray(labelsShape),seed);
  }
  public static INDArray[] getFeaturesAndLabels(  ILossFunction l,  long[] featuresShape,  long[] labelsShape,  long seed){
    Nd4j.getRandom().setSeed(seed);
    Random r=new Random(seed);
    INDArray[] ret=new INDArray[2];
    ret[0]=Nd4j.rand(featuresShape);
switch (l.getClass().getSimpleName()) {
case "LossBinaryXENT":
      ret[1]=Nd4j.rand(labelsShape);
    BooleanIndexing.replaceWhere(ret[1],0,Conditions.lessThanOrEqual(0.5));
  BooleanIndexing.replaceWhere(ret[1],1,Conditions.greaterThanOrEqual(0.5));
break;
case "LossCosineProximity":
ret[1]=Nd4j.rand(labelsShape).subi(0.5);
break;
case "LossKLD":
ret[1]=Nd4j.rand(labelsShape);
if (labelsShape.length == 2) {
Nd4j.getExecutioner().exec(new OldSoftMax(ret[1]),1);
}
 else if (labelsShape.length == 3) {
for (int i=0; i < labelsShape[2]; i++) {
Nd4j.getExecutioner().exec(new OldSoftMax(ret[1].get(all(),all(),point(i))),1);
}
}
 else {
throw new RuntimeException();
}
break;
case "LossMCXENT":
case "LossNegativeLogLikelihood":
ret[1]=Nd4j.zeros(labelsShape);
if (labelsShape.length == 2) {
for (int i=0; i < labelsShape[0]; i++) {
ret[1].putScalar(i,r.nextInt((int)labelsShape[1]),1.0);
}
}
 else if (labelsShape.length == 3) {
for (int i=0; i < labelsShape[0]; i++) {
for (int j=0; j < labelsShape[2]; j++) {
ret[1].putScalar(i,r.nextInt((int)labelsShape[1]),j,1.0);
}
}
}
 else {
throw new UnsupportedOperationException();
}
break;
case "LossHinge":
case "LossSquaredHinge":
ret[1]=Nd4j.ones(labelsShape);
if (labelsShape.length == 2) {
for (int i=0; i < labelsShape[0]; i++) {
ret[1].putScalar(i,r.nextInt((int)labelsShape[1]),-1.0);
}
}
 else if (labelsShape.length == 3) {
for (int i=0; i < labelsShape[0]; i++) {
for (int j=0; j < labelsShape[2]; j++) {
ret[1].putScalar(i,r.nextInt((int)labelsShape[1]),j,-1.0);
}
}
}
 else {
throw new UnsupportedOperationException();
}
break;
case "LossMAPE":
ret[1]=Nd4j.rand(labelsShape).addi(1.0);
break;
case "LossMAE":
case "LossMSE":
case "LossL1":
case "LossL2":
ret[1]=Nd4j.rand(labelsShape).muli(2).subi(1);
break;
case "LossMSLE":
ret[1]=Nd4j.rand(labelsShape);
break;
case "LossPoisson":
ret[1]=Nd4j.rand(labelsShape);
BooleanIndexing.replaceWhere(ret[1],0,Conditions.lessThanOrEqual(0.5));
BooleanIndexing.replaceWhere(ret[1],1,Conditions.greaterThanOrEqual(0.5));
break;
case "LossFMeasure":
if (labelsShape[1] == 1) {
ret[1]=Nd4j.getExecutioner().exec(new BernoulliDistribution(Nd4j.createUninitialized(labelsShape),0.5));
if (labelsShape[0] >= 2) {
int count=ret[1].sumNumber().intValue();
if (count == 0) {
ret[1].putScalar(0,0,1.0);
}
 else if (count == ret[1].size(0)) {
ret[1].putScalar(0,0,0.0);
}
}
}
 else {
ret[1]=Nd4j.create(labelsShape);
for (int i=0; i < labelsShape[0]; i++) {
ret[1].putScalar(i,i % labelsShape[1],1.0);
}
}
break;
case "LossMixtureDensity":
LossMixtureDensity lmd=(LossMixtureDensity)l;
int labelWidth=lmd.getLabelWidth();
ret[1]=Nd4j.rand(new long[]{labelsShape[0],labelWidth});
break;
case "LossMultiLabel":
ret[1]=Nd4j.rand(labelsShape).lti(0.3);
final INDArray sum=ret[1].sum(0);
for (int i=0; i < labelsShape[0]; i++) {
final int rowSum=sum.getInt(i);
if (rowSum == 0) {
ret[1].putScalar(i,0,1);
}
 else if (rowSum == labelsShape[1]) {
ret[1].putScalar(i,0,0);
}
}
break;
default :
throw new IllegalArgumentException("Unknown class: " + l.getClass().getSimpleName());
}
return ret;
}
@Test public void lossFunctionWeightedGradientCheck(){
Nd4j.getRandom().setSeed(12345);
INDArray[] weights=new INDArray[]{Nd4j.create(new double[]{0.2,0.3,0.5}),Nd4j.create(new double[]{1.0,0.5,2.0})};
List<String> passed=new ArrayList<>();
List<String> failed=new ArrayList<>();
for (INDArray w : weights) {
ILossFunction[] lossFunctions=new ILossFunction[]{new LossBinaryXENT(w),new LossL1(w),new LossL1(w),new LossL2(w),new LossL2(w),new LossMAE(w),new LossMAE(w),new LossMAPE(w),new LossMAPE(w),new LossMCXENT(w),new LossMSE(w),new LossMSE(w),new LossMSLE(w),new LossMSLE(w),new LossNegativeLogLikelihood(w),new LossNegativeLogLikelihood(w)};
Activation[] outputActivationFn=new Activation[]{Activation.SIGMOID,Activation.TANH,Activation.SOFTMAX,Activation.TANH,Activation.SOFTMAX,Activation.IDENTITY,Activation.SOFTMAX,Activation.IDENTITY,Activation.SOFTMAX,Activation.SOFTMAX,Activation.IDENTITY,Activation.SOFTMAX,Activation.SIGMOID,Activation.SOFTMAX,Activation.SIGMOID,Activation.SOFTMAX};
int[] minibatchSizes=new int[]{1,3};
for (int i=0; i < lossFunctions.length; i++) {
for (int j=0; j < minibatchSizes.length; j++) {
String testName=lossFunctions[i] + " - " + outputActivationFn[i]+ " - minibatchSize = "+ minibatchSizes[j]+ "; weights = "+ w;
Nd4j.getRandom().setSeed(12345);
MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).seed(12345).updater(new NoOp()).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).list().layer(0,new DenseLayer.Builder().nIn(4).nOut(4).activation(Activation.TANH).build()).layer(1,new OutputLayer.Builder().lossFunction(lossFunctions[i]).activation(outputActivationFn[i]).nIn(4).nOut(3).build()).validateOutputLayerConfig(false).build();
MultiLayerNetwork net=new MultiLayerNetwork(conf);
net.init();
INDArray params=net.params();
for (int x=0; x < params.length(); x++) {
while (Math.abs(params.getDouble(x)) < 0.01 || Math.abs(params.getDouble(x)) > 1.5) {
double d=Nd4j.getRandom().nextDouble();
params.putScalar(x,-1.5 + d * 3);
}
}
INDArray[] inOut=getFeaturesAndLabels(lossFunctions[i],minibatchSizes[j],4,3,12345);
INDArray input=inOut[0];
INDArray labels=inOut[1];
log.info(" ***** Starting test: {} *****",testName);
boolean gradOK;
try {
gradOK=GradientCheckUtil.checkGradients(net,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
}
 catch (Exception e) {
e.printStackTrace();
failed.add(testName + "\t" + "EXCEPTION");
continue;
}
if (gradOK) {
passed.add(testName);
}
 else {
failed.add(testName);
}
System.out.println("\n\n");
}
}
}
System.out.println("---- Passed ----");
for (String s : passed) {
System.out.println(s);
}
System.out.println("---- Failed ----");
for (String s : failed) {
System.out.println(s);
}
assertEquals("Tests failed",0,failed.size());
}
}
