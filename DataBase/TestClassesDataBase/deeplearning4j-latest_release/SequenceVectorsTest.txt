/** 
 * @author raver119@gmail.com
 */
@Ignore public class SequenceVectorsTest {
  protected static final Logger logger=LoggerFactory.getLogger(SequenceVectorsTest.class);
  @Before public void setUp() throws Exception {
  }
  @Test public void testAbstractW2VModel() throws Exception {
    ClassPathResource resource=new ClassPathResource("big/raw_sentences.txt");
    File file=resource.getFile();
    logger.info("dtype: {}",Nd4j.dataType());
    AbstractCache<VocabWord> vocabCache=new AbstractCache.Builder<VocabWord>().build();
    BasicLineIterator underlyingIterator=new BasicLineIterator(file);
    TokenizerFactory t=new DefaultTokenizerFactory();
    t.setTokenPreProcessor(new CommonPreprocessor());
    SentenceTransformer transformer=new SentenceTransformer.Builder().iterator(underlyingIterator).tokenizerFactory(t).build();
    AbstractSequenceIterator<VocabWord> sequenceIterator=new AbstractSequenceIterator.Builder<>(transformer).build();
    VocabConstructor<VocabWord> constructor=new VocabConstructor.Builder<VocabWord>().addSource(sequenceIterator,5).setTargetVocabCache(vocabCache).build();
    constructor.buildJointVocabulary(false,true);
    assertEquals(242,vocabCache.numWords());
    assertEquals(634303,vocabCache.totalWordOccurrences());
    VocabWord wordz=vocabCache.wordFor("day");
    logger.info("Wordz: " + wordz);
    WeightLookupTable<VocabWord> lookupTable=new InMemoryLookupTable.Builder<VocabWord>().lr(0.025).vectorLength(150).useAdaGrad(false).cache(vocabCache).build();
    lookupTable.resetWeights(true);
    SequenceVectors<VocabWord> vectors=new SequenceVectors.Builder<VocabWord>(new VectorsConfiguration()).minWordFrequency(5).lookupTable(lookupTable).iterate(sequenceIterator).vocabCache(vocabCache).batchSize(250).iterations(1).epochs(1).resetModel(false).trainElementsRepresentation(true).trainSequencesRepresentation(false).build();
    logger.info("Starting training...");
    vectors.fit();
    logger.info("Model saved...");
    double sim=vectors.similarity("day","night");
    logger.info("Day/night similarity: " + sim);
    assertTrue(sim > 0.6d);
    Collection<String> labels=vectors.wordsNearest("day",10);
    logger.info("Nearest labels to 'day': " + labels);
    SequenceElementFactory<VocabWord> factory=new AbstractElementFactory<VocabWord>(VocabWord.class);
    WordVectorSerializer.writeSequenceVectors(vectors,factory,"seqvec.mod");
    SequenceVectors<VocabWord> model=WordVectorSerializer.readSequenceVectors(factory,new File("seqvec.mod"));
    sim=model.similarity("day","night");
    logger.info("day/night similarity: " + sim);
  }
  @Test public void testInternalVocabConstruction() throws Exception {
    ClassPathResource resource=new ClassPathResource("big/raw_sentences.txt");
    File file=resource.getFile();
    BasicLineIterator underlyingIterator=new BasicLineIterator(file);
    TokenizerFactory t=new DefaultTokenizerFactory();
    t.setTokenPreProcessor(new CommonPreprocessor());
    SentenceTransformer transformer=new SentenceTransformer.Builder().iterator(underlyingIterator).tokenizerFactory(t).build();
    AbstractSequenceIterator<VocabWord> sequenceIterator=new AbstractSequenceIterator.Builder<>(transformer).build();
    SequenceVectors<VocabWord> vectors=new SequenceVectors.Builder<VocabWord>(new VectorsConfiguration()).minWordFrequency(5).iterate(sequenceIterator).batchSize(250).iterations(1).epochs(1).resetModel(false).trainElementsRepresentation(true).build();
    logger.info("Fitting model...");
    vectors.fit();
    logger.info("Model ready...");
    double sim=vectors.similarity("day","night");
    logger.info("Day/night similarity: " + sim);
    assertTrue(sim > 0.6d);
    Collection<String> labels=vectors.wordsNearest("day",10);
    logger.info("Nearest labels to 'day': " + labels);
  }
  @Test public void testElementsLearningAlgo1() throws Exception {
    SequenceVectors<VocabWord> vectors=new SequenceVectors.Builder<VocabWord>(new VectorsConfiguration()).minWordFrequency(5).batchSize(250).iterations(1).elementsLearningAlgorithm("org.deeplearning4j.models.embeddings.learning.impl.elements.SkipGram").epochs(1).resetModel(false).trainElementsRepresentation(true).build();
  }
  @Test public void testSequenceLearningAlgo1() throws Exception {
    SequenceVectors<VocabWord> vectors=new SequenceVectors.Builder<VocabWord>(new VectorsConfiguration()).minWordFrequency(5).batchSize(250).iterations(1).sequenceLearningAlgorithm("org.deeplearning4j.models.embeddings.learning.impl.sequence.DBOW").epochs(1).resetModel(false).trainElementsRepresentation(false).build();
  }
  @Ignore @Test public void testGlove1() throws Exception {
    logger.info("Max available memory: " + Runtime.getRuntime().maxMemory());
    ClassPathResource resource=new ClassPathResource("big/raw_sentences.txt");
    File file=resource.getFile();
    BasicLineIterator underlyingIterator=new BasicLineIterator(file);
    TokenizerFactory t=new DefaultTokenizerFactory();
    t.setTokenPreProcessor(new CommonPreprocessor());
    SentenceTransformer transformer=new SentenceTransformer.Builder().iterator(underlyingIterator).tokenizerFactory(t).build();
    AbstractSequenceIterator<VocabWord> sequenceIterator=new AbstractSequenceIterator.Builder<>(transformer).build();
    VectorsConfiguration configuration=new VectorsConfiguration();
    configuration.setWindow(5);
    configuration.setLearningRate(0.06);
    configuration.setLayersSize(100);
    SequenceVectors<VocabWord> vectors=new SequenceVectors.Builder<VocabWord>(configuration).iterate(sequenceIterator).iterations(1).epochs(45).elementsLearningAlgorithm(new GloVe.Builder<VocabWord>().shuffle(true).symmetric(true).learningRate(0.05).alpha(0.75).xMax(100.0).build()).resetModel(true).trainElementsRepresentation(true).trainSequencesRepresentation(false).build();
    vectors.fit();
    double sim=vectors.similarity("day","night");
    logger.info("Day/night similarity: " + sim);
    sim=vectors.similarity("day","another");
    logger.info("Day/another similarity: " + sim);
    sim=vectors.similarity("night","year");
    logger.info("Night/year similarity: " + sim);
    sim=vectors.similarity("night","me");
    logger.info("Night/me similarity: " + sim);
    sim=vectors.similarity("day","know");
    logger.info("Day/know similarity: " + sim);
    sim=vectors.similarity("best","police");
    logger.info("Best/police similarity: " + sim);
    Collection<String> labels=vectors.wordsNearest("day",10);
    logger.info("Nearest labels to 'day': " + labels);
    sim=vectors.similarity("day","night");
    assertTrue(sim > 0.6d);
  }
  @Test @Ignore public void testDeepWalk() throws Exception {
    Heartbeat.getInstance().disableHeartbeat();
    AbstractCache<Blogger> vocabCache=new AbstractCache.Builder<Blogger>().build();
    Graph<Blogger,Double> graph=buildGraph();
    GraphWalker<Blogger> walker=new PopularityWalker.Builder<>(graph).setNoEdgeHandling(NoEdgeHandling.RESTART_ON_DISCONNECTED).setWalkLength(40).setWalkDirection(WalkDirection.FORWARD_UNIQUE).setRestartProbability(0.05).setPopularitySpread(10).setPopularityMode(PopularityMode.MAXIMUM).setSpreadSpectrum(SpreadSpectrum.PROPORTIONAL).build();
    GraphTransformer<Blogger> graphTransformer=new GraphTransformer.Builder<>(graph).setGraphWalker(walker).shuffleOnReset(true).setVocabCache(vocabCache).build();
    Blogger blogger=graph.getVertex(0).getValue();
    assertEquals(119,blogger.getElementFrequency(),0.001);
    logger.info("Blogger: " + blogger);
    AbstractSequenceIterator<Blogger> sequenceIterator=new AbstractSequenceIterator.Builder<>(graphTransformer).build();
    WeightLookupTable<Blogger> lookupTable=new InMemoryLookupTable.Builder<Blogger>().lr(0.025).vectorLength(150).useAdaGrad(false).cache(vocabCache).seed(42).build();
    lookupTable.resetWeights(true);
    SequenceVectors<Blogger> vectors=new SequenceVectors.Builder<Blogger>(new VectorsConfiguration()).lookupTable(lookupTable).iterate(sequenceIterator).vocabCache(vocabCache).batchSize(1000).iterations(1).epochs(10).resetModel(false).trainElementsRepresentation(true).trainSequencesRepresentation(false).elementsLearningAlgorithm(new SkipGram<Blogger>()).learningRate(0.025).layerSize(150).sampling(0).negativeSample(0).windowSize(4).workers(6).seed(42).build();
    vectors.fit();
    vectors.setModelUtils(new FlatModelUtils());
    double sim=vectors.similarity("12","72");
    Collection<String> list=vectors.wordsNearest("12",20);
    logger.info("12->72: " + sim);
    printWords("12",list,vectors);
    assertTrue(sim > 0.10);
    assertFalse(Double.isNaN(sim));
  }
  private List<Blogger> getBloggersFromGraph(  Graph<Blogger,Double> graph){
    List<Blogger> result=new ArrayList<>();
    List<Vertex<Blogger>> bloggers=graph.getVertices(0,graph.numVertices() - 1);
    for (    Vertex<Blogger> vertex : bloggers) {
      result.add(vertex.getValue());
    }
    return result;
  }
  private static Graph<Blogger,Double> buildGraph() throws IOException, InterruptedException {
    File nodes=new File("/ext/Temp/BlogCatalog/nodes.csv");
    CSVRecordReader reader=new CSVRecordReader(0,',');
    reader.initialize(new FileSplit(nodes));
    List<Blogger> bloggers=new ArrayList<>();
    int cnt=0;
    while (reader.hasNext()) {
      List<Writable> lines=new ArrayList<>(reader.next());
      Blogger blogger=new Blogger(lines.get(0).toInt());
      bloggers.add(blogger);
      cnt++;
    }
    reader.close();
    Graph<Blogger,Double> graph=new Graph<>(bloggers,true);
    File edges=new File("/ext/Temp/BlogCatalog/edges.csv");
    reader=new CSVRecordReader(0,',');
    reader.initialize(new FileSplit(edges));
    while (reader.hasNext()) {
      List<Writable> lines=new ArrayList<>(reader.next());
      int from=lines.get(0).toInt();
      int to=lines.get(1).toInt();
      graph.addEdge(from - 1,to - 1,1.0,false);
    }
    logger.info("Connected on 0: [" + graph.getConnectedVertices(0).size() + "]");
    logger.info("Connected on 1: [" + graph.getConnectedVertices(1).size() + "]");
    logger.info("Connected on 3: [" + graph.getConnectedVertices(3).size() + "]");
    assertEquals(119,graph.getConnectedVertices(0).size());
    assertEquals(9,graph.getConnectedVertices(1).size());
    assertEquals(6,graph.getConnectedVertices(3).size());
    return graph;
  }
@Data private static class Blogger extends SequenceElement {
    @Getter @Setter private int id;
    public Blogger(){
      super();
    }
    public Blogger(    int id){
      super();
      this.id=id;
    }
    /** 
 * This method should return string representation of this SequenceElement, so it can be used for
 * @return
 */
    @Override public String getLabel(){
      return String.valueOf(id);
    }
    /** 
 * @return
 */
    @Override public String toJSON(){
      return null;
    }
    @Override public String toString(){
      return "VocabWord{" + "wordFrequency=" + this.elementFrequency + ", index="+ index+ ", codes="+ codes+ ", word='"+ String.valueOf(id)+ '\''+ ", points="+ points+ ", codeLength="+ codeLength+ '}';
    }
  }
  private static void printWords(  String target,  Collection<String> list,  SequenceVectors vec){
    System.out.println("Words close to [" + target + "]: ");
    for (    String word : list) {
      double sim=vec.similarity(target,word);
      System.out.print("'" + word + "': ["+ sim+ "], ");
    }
    System.out.print("\n");
  }
}
