@Slf4j public class TestEarlyStoppingCompGraph extends BaseDL4JTest {
  @Test public void testEarlyStoppingIris(){
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).updater(new Sgd(0.001)).weightInit(WeightInit.XAVIER).graphBuilder().addInputs("in").addLayer("0",new OutputLayer.Builder().nIn(4).nOut(3).activation(Activation.SOFTMAX).lossFunction(LossFunctions.LossFunction.MCXENT).build(),"in").setOutputs("0").build();
    ComputationGraph net=new ComputationGraph(conf);
    net.setListeners(new ScoreIterationListener(1));
    DataSetIterator irisIter=new IrisDataSetIterator(150,150);
    EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
    EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(5)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(1,TimeUnit.MINUTES)).scoreCalculator(new DataSetLossCalculatorCG(irisIter,true)).modelSaver(saver).build();
    IEarlyStoppingTrainer<ComputationGraph> trainer=new EarlyStoppingGraphTrainer(esConf,net,irisIter);
    EarlyStoppingResult<ComputationGraph> result=trainer.fit();
    System.out.println(result);
    assertEquals(5,result.getTotalEpochs());
    assertEquals(EarlyStoppingResult.TerminationReason.EpochTerminationCondition,result.getTerminationReason());
    Map<Integer,Double> scoreVsIter=result.getScoreVsEpoch();
    assertEquals(5,scoreVsIter.size());
    String expDetails=esConf.getEpochTerminationConditions().get(0).toString();
    assertEquals(expDetails,result.getTerminationDetails());
    ComputationGraph out=result.getBestModel();
    assertNotNull(out);
    ComputationGraph bestNetwork=result.getBestModel();
    irisIter.reset();
    double score=bestNetwork.score(irisIter.next());
    assertEquals(result.getBestModelScore(),score,1e-2);
  }
  @Test public void testBadTuning(){
    Nd4j.getRandom().setSeed(12345);
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).updater(new Sgd(5.0)).weightInit(WeightInit.XAVIER).graphBuilder().addInputs("in").addLayer("0",new OutputLayer.Builder().nIn(4).nOut(3).activation(Activation.SOFTMAX).lossFunction(LossFunctions.LossFunction.MCXENT).build(),"in").setOutputs("0").build();
    ComputationGraph net=new ComputationGraph(conf);
    net.setListeners(new ScoreIterationListener(1));
    DataSetIterator irisIter=new IrisDataSetIterator(150,150);
    EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
    EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(5000)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(1,TimeUnit.MINUTES),new MaxScoreIterationTerminationCondition(10)).scoreCalculator(new DataSetLossCalculatorCG(irisIter,true)).modelSaver(saver).build();
    IEarlyStoppingTrainer trainer=new EarlyStoppingGraphTrainer(esConf,net,irisIter);
    EarlyStoppingResult result=trainer.fit();
    assertTrue(result.getTotalEpochs() < 5);
    assertEquals(EarlyStoppingResult.TerminationReason.IterationTerminationCondition,result.getTerminationReason());
    String expDetails=new MaxScoreIterationTerminationCondition(10).toString();
    assertEquals(expDetails,result.getTerminationDetails());
    assertEquals(0,result.getBestModelEpoch());
    assertNotNull(result.getBestModel());
  }
  @Test public void testTimeTermination(){
    Nd4j.getRandom().setSeed(12345);
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).updater(new Sgd(1e-6)).weightInit(WeightInit.XAVIER).graphBuilder().addInputs("in").addLayer("0",new OutputLayer.Builder().nIn(4).nOut(3).activation(Activation.SOFTMAX).lossFunction(LossFunctions.LossFunction.MCXENT).build(),"in").setOutputs("0").build();
    ComputationGraph net=new ComputationGraph(conf);
    net.setListeners(new ScoreIterationListener(1));
    DataSetIterator irisIter=new IrisDataSetIterator(150,150);
    EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
    EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(10000)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(3,TimeUnit.SECONDS),new MaxScoreIterationTerminationCondition(50)).scoreCalculator(new DataSetLossCalculator(irisIter,true)).modelSaver(saver).build();
    IEarlyStoppingTrainer trainer=new EarlyStoppingGraphTrainer(esConf,net,irisIter);
    long startTime=System.currentTimeMillis();
    EarlyStoppingResult result=trainer.fit();
    long endTime=System.currentTimeMillis();
    int durationSeconds=(int)(endTime - startTime) / 1000;
    assertTrue(durationSeconds >= 3);
    assertTrue(durationSeconds <= 9);
    assertEquals(EarlyStoppingResult.TerminationReason.IterationTerminationCondition,result.getTerminationReason());
    String expDetails=new MaxTimeIterationTerminationCondition(3,TimeUnit.SECONDS).toString();
    assertEquals(expDetails,result.getTerminationDetails());
  }
  @Test public void testNoImprovementNEpochsTermination(){
    Nd4j.getRandom().setSeed(12345);
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).updater(new Sgd(0.0)).weightInit(WeightInit.XAVIER).graphBuilder().addInputs("in").addLayer("0",new OutputLayer.Builder().nIn(4).nOut(3).activation(Activation.SOFTMAX).lossFunction(LossFunctions.LossFunction.MCXENT).build(),"in").setOutputs("0").build();
    ComputationGraph net=new ComputationGraph(conf);
    net.setListeners(new ScoreIterationListener(1));
    DataSetIterator irisIter=new IrisDataSetIterator(150,150);
    EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
    EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(100),new ScoreImprovementEpochTerminationCondition(5)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(3,TimeUnit.SECONDS),new MaxScoreIterationTerminationCondition(50)).scoreCalculator(new DataSetLossCalculatorCG(irisIter,true)).modelSaver(saver).build();
    IEarlyStoppingTrainer trainer=new EarlyStoppingGraphTrainer(esConf,net,irisIter);
    EarlyStoppingResult result=trainer.fit();
    assertEquals(6,result.getTotalEpochs());
    assertEquals(0,result.getBestModelEpoch());
    assertEquals(EarlyStoppingResult.TerminationReason.EpochTerminationCondition,result.getTerminationReason());
    String expDetails=new ScoreImprovementEpochTerminationCondition(5).toString();
    assertEquals(expDetails,result.getTerminationDetails());
  }
  @Test public void testListeners(){
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).updater(new Sgd(0.001)).weightInit(WeightInit.XAVIER).graphBuilder().addInputs("in").addLayer("0",new OutputLayer.Builder().nIn(4).nOut(3).activation(Activation.SOFTMAX).lossFunction(LossFunctions.LossFunction.MCXENT).build(),"in").setOutputs("0").build();
    ComputationGraph net=new ComputationGraph(conf);
    net.setListeners(new ScoreIterationListener(1));
    DataSetIterator irisIter=new IrisDataSetIterator(150,150);
    EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
    EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(5)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(1,TimeUnit.MINUTES)).scoreCalculator(new DataSetLossCalculatorCG(irisIter,true)).modelSaver(saver).build();
    LoggingEarlyStoppingListener listener=new LoggingEarlyStoppingListener();
    IEarlyStoppingTrainer trainer=new EarlyStoppingGraphTrainer(esConf,net,irisIter,listener);
    trainer.fit();
    assertEquals(1,listener.onStartCallCount);
    assertEquals(5,listener.onEpochCallCount);
    assertEquals(1,listener.onCompletionCallCount);
  }
private static class LoggingEarlyStoppingListener implements EarlyStoppingListener<ComputationGraph> {
    private static Logger log=LoggerFactory.getLogger(LoggingEarlyStoppingListener.class);
    private int onStartCallCount=0;
    private int onEpochCallCount=0;
    private int onCompletionCallCount=0;
    @Override public void onStart(    EarlyStoppingConfiguration esConfig,    ComputationGraph net){
      log.info("EarlyStopping: onStart called");
      onStartCallCount++;
    }
    @Override public void onEpoch(    int epochNum,    double score,    EarlyStoppingConfiguration esConfig,    ComputationGraph net){
      log.info("EarlyStopping: onEpoch called (epochNum={}, score={}}",epochNum,score);
      onEpochCallCount++;
    }
    @Override public void onCompletion(    EarlyStoppingResult esResult){
      log.info("EarlyStopping: onCompletion called (result: {})",esResult);
      onCompletionCallCount++;
    }
  }
  @Test public void testRegressionScoreFunctionSimple() throws Exception {
    for (    RegressionEvaluation.Metric metric : new RegressionEvaluation.Metric[]{RegressionEvaluation.Metric.MSE,RegressionEvaluation.Metric.MAE}) {
      log.info("Metric: " + metric);
      ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().graphBuilder().addInputs("in").layer("0",new DenseLayer.Builder().nIn(784).nOut(32).build(),"in").layer("1",new OutputLayer.Builder().nIn(32).nOut(784).activation(Activation.SIGMOID).lossFunction(LossFunctions.LossFunction.MSE).build(),"0").setOutputs("1").build();
      ComputationGraph net=new ComputationGraph(conf);
      net.init();
      DataSetIterator iter=new MnistDataSetIterator(32,false,12345);
      List<DataSet> l=new ArrayList<>();
      for (int i=0; i < 10; i++) {
        DataSet ds=iter.next();
        l.add(new DataSet(ds.getFeatures(),ds.getFeatures()));
      }
      iter=new ExistingDataSetIterator(l);
      EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
      EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(5)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(1,TimeUnit.MINUTES)).scoreCalculator(new RegressionScoreCalculator(metric,iter)).modelSaver(saver).build();
      EarlyStoppingGraphTrainer trainer=new EarlyStoppingGraphTrainer(esConf,net,iter);
      EarlyStoppingResult<ComputationGraph> result=trainer.fit();
      assertNotNull(result.getBestModel());
      assertTrue(result.getBestModelScore() > 0.0);
    }
  }
  @Test public void testAEScoreFunctionSimple() throws Exception {
    for (    RegressionEvaluation.Metric metric : new RegressionEvaluation.Metric[]{RegressionEvaluation.Metric.MSE,RegressionEvaluation.Metric.MAE}) {
      log.info("Metric: " + metric);
      ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().graphBuilder().addInputs("in").layer("0",new AutoEncoder.Builder().nIn(784).nOut(32).build(),"in").setOutputs("0").pretrain(true).backprop(false).build();
      ComputationGraph net=new ComputationGraph(conf);
      net.init();
      DataSetIterator iter=new MnistDataSetIterator(32,false,12345);
      List<DataSet> l=new ArrayList<>();
      for (int i=0; i < 10; i++) {
        DataSet ds=iter.next();
        l.add(new DataSet(ds.getFeatures(),ds.getFeatures()));
      }
      iter=new ExistingDataSetIterator(l);
      EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
      EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(5)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(1,TimeUnit.MINUTES)).scoreCalculator(new AutoencoderScoreCalculator(metric,iter)).modelSaver(saver).build();
      EarlyStoppingGraphTrainer trainer=new EarlyStoppingGraphTrainer(esConf,net,iter);
      EarlyStoppingResult<ComputationGraph> result=trainer.fit();
      assertNotNull(result.getBestModel());
      assertTrue(result.getBestModelScore() > 0.0);
    }
  }
  @Test public void testVAEScoreFunctionSimple() throws Exception {
    for (    RegressionEvaluation.Metric metric : new RegressionEvaluation.Metric[]{RegressionEvaluation.Metric.MSE,RegressionEvaluation.Metric.MAE}) {
      log.info("Metric: " + metric);
      ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().graphBuilder().addInputs("in").layer("0",new VariationalAutoencoder.Builder().nIn(784).nOut(32).encoderLayerSizes(64).decoderLayerSizes(64).build(),"in").setOutputs("0").pretrain(true).backprop(false).build();
      ComputationGraph net=new ComputationGraph(conf);
      net.init();
      DataSetIterator iter=new MnistDataSetIterator(32,false,12345);
      List<DataSet> l=new ArrayList<>();
      for (int i=0; i < 10; i++) {
        DataSet ds=iter.next();
        l.add(new DataSet(ds.getFeatures(),ds.getFeatures()));
      }
      iter=new ExistingDataSetIterator(l);
      EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
      EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(5)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(1,TimeUnit.MINUTES)).scoreCalculator(new VAEReconErrorScoreCalculator(metric,iter)).modelSaver(saver).build();
      EarlyStoppingGraphTrainer trainer=new EarlyStoppingGraphTrainer(esConf,net,iter);
      EarlyStoppingResult<ComputationGraph> result=trainer.fit();
      assertNotNull(result.getBestModel());
      assertTrue(result.getBestModelScore() > 0.0);
    }
  }
  @Test public void testVAEScoreFunctionReconstructionProbSimple() throws Exception {
    for (    boolean logProb : new boolean[]{false,true}) {
      ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().graphBuilder().addInputs("in").layer("0",new VariationalAutoencoder.Builder().nIn(784).nOut(32).encoderLayerSizes(64).decoderLayerSizes(64).reconstructionDistribution(new BernoulliReconstructionDistribution(Activation.SIGMOID)).build(),"in").setOutputs("0").pretrain(true).backprop(false).build();
      ComputationGraph net=new ComputationGraph(conf);
      net.init();
      DataSetIterator iter=new MnistDataSetIterator(32,false,12345);
      List<DataSet> l=new ArrayList<>();
      for (int i=0; i < 10; i++) {
        DataSet ds=iter.next();
        l.add(new DataSet(ds.getFeatures(),ds.getFeatures()));
      }
      iter=new ExistingDataSetIterator(l);
      EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
      EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(5)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(1,TimeUnit.MINUTES)).scoreCalculator(new VAEReconProbScoreCalculator(iter,20,logProb)).modelSaver(saver).build();
      EarlyStoppingGraphTrainer trainer=new EarlyStoppingGraphTrainer(esConf,net,iter);
      EarlyStoppingResult<ComputationGraph> result=trainer.fit();
      assertNotNull(result.getBestModel());
      assertTrue(result.getBestModelScore() > 0.0);
    }
  }
  @Test public void testClassificationScoreFunctionSimple() throws Exception {
    for (    Evaluation.Metric metric : Evaluation.Metric.values()) {
      log.info("Metric: " + metric);
      ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().graphBuilder().addInputs("in").layer("0",new DenseLayer.Builder().nIn(784).nOut(32).build(),"in").layer("1",new OutputLayer.Builder().nIn(32).nOut(10).activation(Activation.SOFTMAX).build(),"0").setOutputs("1").build();
      ComputationGraph net=new ComputationGraph(conf);
      net.init();
      DataSetIterator iter=new MnistDataSetIterator(32,false,12345);
      List<DataSet> l=new ArrayList<>();
      for (int i=0; i < 10; i++) {
        DataSet ds=iter.next();
        l.add(ds);
      }
      iter=new ExistingDataSetIterator(l);
      EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
      EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(5)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(1,TimeUnit.MINUTES)).scoreCalculator(new ClassificationScoreCalculator(metric,iter)).modelSaver(saver).build();
      EarlyStoppingGraphTrainer trainer=new EarlyStoppingGraphTrainer(esConf,net,iter);
      EarlyStoppingResult<ComputationGraph> result=trainer.fit();
      assertNotNull(result.getBestModel());
    }
  }
  @Test public void testEarlyStoppingListenersCG(){
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().updater(new Sgd(0.001)).weightInit(WeightInit.XAVIER).graphBuilder().addInputs("in").layer("0",new OutputLayer.Builder().nIn(4).nOut(3).activation(Activation.SOFTMAX).lossFunction(LossFunctions.LossFunction.MCXENT).build(),"in").setOutputs("0").build();
    ComputationGraph net=new ComputationGraph(conf);
    TestEarlyStopping.TestListener tl=new TestEarlyStopping.TestListener();
    net.setListeners(tl);
    DataSetIterator irisIter=new IrisDataSetIterator(50,150);
    EarlyStoppingModelSaver<ComputationGraph> saver=new InMemoryModelSaver<>();
    EarlyStoppingConfiguration<ComputationGraph> esConf=new EarlyStoppingConfiguration.Builder<ComputationGraph>().epochTerminationConditions(new MaxEpochsTerminationCondition(5)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(1,TimeUnit.MINUTES)).scoreCalculator(new DataSetLossCalculator(irisIter,true)).modelSaver(saver).build();
    IEarlyStoppingTrainer<ComputationGraph> trainer=new EarlyStoppingGraphTrainer(esConf,net,irisIter);
    trainer.fit();
    assertEquals(5,tl.getCountEpochStart());
    assertEquals(5,tl.getCountEpochEnd());
    assertEquals(5 * 150 / 50,tl.getIterCount());
    assertEquals(4,tl.getMaxEpochStart());
    assertEquals(4,tl.getMaxEpochEnd());
  }
}
