/** 
 * Created by Alex on 03/07/2016.
 */
public class TestPreProcessedData extends BaseSparkTest {
  @Test public void testPreprocessedData(){
    int dataSetObjSize=5;
    int batchSizePerExecutor=10;
    String path=FilenameUtils.concat(System.getProperty("java.io.tmpdir"),"dl4j_testpreprocdata");
    File f=new File(path);
    if (f.exists())     f.delete();
    f.mkdir();
    DataSetIterator iter=new IrisDataSetIterator(5,150);
    int i=0;
    while (iter.hasNext()) {
      File f2=new File(FilenameUtils.concat(path,"data" + (i++) + ".bin"));
      iter.next().save(f2);
    }
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(Updater.RMSPROP).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).list().layer(0,new org.deeplearning4j.nn.conf.layers.DenseLayer.Builder().nIn(4).nOut(3).activation(Activation.TANH).build()).layer(1,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(3).nOut(3).activation(Activation.SOFTMAX).build()).build();
    SparkDl4jMultiLayer sparkNet=new SparkDl4jMultiLayer(sc,conf,new ParameterAveragingTrainingMaster.Builder(numExecutors(),dataSetObjSize).batchSizePerWorker(batchSizePerExecutor).averagingFrequency(1).repartionData(Repartition.Always).build());
    sparkNet.setCollectTrainingStats(true);
    sparkNet.fit("file:///" + path.replaceAll("\\\\","/"));
    SparkTrainingStats sts=sparkNet.getSparkTrainingStats();
    int expNumFits=12;
    assertTrue(Math.abs(expNumFits - sts.getValue("ParameterAveragingWorkerFitTimesMs").size()) < 3);
    assertEquals(3,sts.getValue("ParameterAveragingMasterMapPartitionsTimesMs").size());
  }
  @Test public void testPreprocessedDataCompGraphDataSet(){
    int dataSetObjSize=5;
    int batchSizePerExecutor=10;
    String path=FilenameUtils.concat(System.getProperty("java.io.tmpdir"),"dl4j_testpreprocdata2");
    File f=new File(path);
    if (f.exists())     f.delete();
    f.mkdir();
    DataSetIterator iter=new IrisDataSetIterator(5,150);
    int i=0;
    while (iter.hasNext()) {
      File f2=new File(FilenameUtils.concat(path,"data" + (i++) + ".bin"));
      iter.next().save(f2);
    }
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().updater(Updater.RMSPROP).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).graphBuilder().addInputs("in").addLayer("0",new org.deeplearning4j.nn.conf.layers.DenseLayer.Builder().nIn(4).nOut(3).activation(Activation.TANH).build(),"in").addLayer("1",new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(3).nOut(3).activation(Activation.SOFTMAX).build(),"0").setOutputs("1").build();
    SparkComputationGraph sparkNet=new SparkComputationGraph(sc,conf,new ParameterAveragingTrainingMaster.Builder(numExecutors(),dataSetObjSize).batchSizePerWorker(batchSizePerExecutor).averagingFrequency(1).repartionData(Repartition.Always).build());
    sparkNet.setCollectTrainingStats(true);
    sparkNet.fit("file:///" + path.replaceAll("\\\\","/"));
    SparkTrainingStats sts=sparkNet.getSparkTrainingStats();
    int expNumFits=12;
    assertTrue(Math.abs(expNumFits - sts.getValue("ParameterAveragingWorkerFitTimesMs").size()) < 3);
    assertEquals(3,sts.getValue("ParameterAveragingMasterMapPartitionsTimesMs").size());
  }
  @Test public void testPreprocessedDataCompGraphMultiDataSet() throws IOException {
    int dataSetObjSize=5;
    int batchSizePerExecutor=10;
    String path=FilenameUtils.concat(System.getProperty("java.io.tmpdir"),"dl4j_testpreprocdata3");
    File f=new File(path);
    if (f.exists())     f.delete();
    f.mkdir();
    DataSetIterator iter=new IrisDataSetIterator(5,150);
    int i=0;
    while (iter.hasNext()) {
      File f2=new File(FilenameUtils.concat(path,"data" + (i++) + ".bin"));
      DataSet ds=iter.next();
      MultiDataSet mds=new MultiDataSet(ds.getFeatures(),ds.getLabels());
      mds.save(f2);
    }
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().updater(Updater.RMSPROP).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).graphBuilder().addInputs("in").addLayer("0",new org.deeplearning4j.nn.conf.layers.DenseLayer.Builder().nIn(4).nOut(3).activation(Activation.TANH).build(),"in").addLayer("1",new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(3).nOut(3).activation(Activation.SOFTMAX).build(),"0").setOutputs("1").build();
    SparkComputationGraph sparkNet=new SparkComputationGraph(sc,conf,new ParameterAveragingTrainingMaster.Builder(numExecutors(),dataSetObjSize).batchSizePerWorker(batchSizePerExecutor).averagingFrequency(1).repartionData(Repartition.Always).build());
    sparkNet.setCollectTrainingStats(true);
    sparkNet.fitMultiDataSet("file:///" + path.replaceAll("\\\\","/"));
    SparkTrainingStats sts=sparkNet.getSparkTrainingStats();
    int expNumFits=12;
    assertTrue(Math.abs(expNumFits - sts.getValue("ParameterAveragingWorkerFitTimesMs").size()) < 3);
    assertEquals(3,sts.getValue("ParameterAveragingMasterMapPartitionsTimesMs").size());
  }
  @Test public void testCsvPreprocessedDataGeneration() throws Exception {
    List<String> list=new ArrayList<>();
    DataSetIterator iter=new IrisDataSetIterator(1,150);
    while (iter.hasNext()) {
      DataSet ds=iter.next();
      list.add(toString(ds.getFeatures(),Nd4j.argMax(ds.getLabels(),1).getInt(0)));
    }
    JavaRDD<String> rdd=sc.parallelize(list);
    int partitions=rdd.partitions().size();
    URI tempDir=new File(System.getProperty("java.io.tmpdir")).toURI();
    URI outputDir=new URI(tempDir.getPath() + "/dl4j_testPreprocessedData2");
    File temp=new File(outputDir.getPath());
    if (temp.exists())     FileUtils.deleteDirectory(temp);
    int numBinFiles=0;
    try {
      int batchSize=5;
      int labelIdx=4;
      int numPossibleLabels=3;
      rdd.foreachPartition(new StringToDataSetExportFunction(outputDir,new CSVRecordReader(0),batchSize,false,labelIdx,numPossibleLabels));
      File[] fileList=new File(outputDir.getPath()).listFiles();
      int totalExamples=0;
      for (      File f2 : fileList) {
        if (!f2.getPath().endsWith(".bin"))         continue;
        numBinFiles++;
        DataSet ds=new DataSet();
        ds.load(f2);
        assertEquals(4,ds.numInputs());
        assertEquals(3,ds.numOutcomes());
        totalExamples+=ds.numExamples();
      }
      assertEquals(150,totalExamples);
      assertTrue(Math.abs(150 / batchSize - numBinFiles) <= partitions);
      JavaPairRDD<String,PortableDataStream> pds=sc.binaryFiles(outputDir.getPath());
      List<PortableDataStream> pdsList=pds.values().collect();
      DataSetIterator pdsIter=new PortableDataStreamDataSetIterator(pdsList);
      int pdsCount=0;
      int totalExamples2=0;
      while (pdsIter.hasNext()) {
        DataSet ds=pdsIter.next();
        pdsCount++;
        totalExamples2+=ds.numExamples();
        assertEquals(4,ds.numInputs());
        assertEquals(3,ds.numOutcomes());
      }
      assertEquals(150,totalExamples2);
      assertEquals(numBinFiles,pdsCount);
    }
  finally {
      FileUtils.deleteDirectory(temp);
    }
  }
  private static String toString(  INDArray rowVector,  int labelIdx){
    StringBuilder sb=new StringBuilder();
    long length=rowVector.length();
    for (int i=0; i < length; i++) {
      sb.append(rowVector.getDouble(i));
      sb.append(",");
    }
    sb.append(labelIdx);
    return sb.toString();
  }
  @Test public void testCsvPreprocessedDataGenerationNoLabel() throws Exception {
    List<String> list=new ArrayList<>();
    DataSetIterator iter=new IrisDataSetIterator(1,150);
    while (iter.hasNext()) {
      DataSet ds=iter.next();
      list.add(toString(ds.getFeatures(),Nd4j.argMax(ds.getLabels(),1).getInt(0)));
    }
    JavaRDD<String> rdd=sc.parallelize(list);
    int partitions=rdd.partitions().size();
    URI tempDir=new File(System.getProperty("java.io.tmpdir")).toURI();
    URI outputDir=new URI(tempDir.getPath() + "/dl4j_testPreprocessedData3");
    File temp=new File(outputDir.getPath());
    if (temp.exists())     FileUtils.deleteDirectory(temp);
    int numBinFiles=0;
    try {
      int batchSize=5;
      int labelIdx=-1;
      int numPossibleLabels=-1;
      rdd.foreachPartition(new StringToDataSetExportFunction(outputDir,new CSVRecordReader(0),batchSize,false,labelIdx,numPossibleLabels));
      File[] fileList=new File(outputDir.getPath()).listFiles();
      int totalExamples=0;
      for (      File f2 : fileList) {
        if (!f2.getPath().endsWith(".bin"))         continue;
        numBinFiles++;
        DataSet ds=new DataSet();
        ds.load(f2);
        assertEquals(5,ds.numInputs());
        assertEquals(5,ds.numOutcomes());
        totalExamples+=ds.numExamples();
      }
      assertEquals(150,totalExamples);
      assertTrue(Math.abs(150 / batchSize - numBinFiles) <= partitions);
      JavaPairRDD<String,PortableDataStream> pds=sc.binaryFiles(outputDir.getPath());
      List<PortableDataStream> pdsList=pds.values().collect();
      DataSetIterator pdsIter=new PortableDataStreamDataSetIterator(pdsList);
      int pdsCount=0;
      int totalExamples2=0;
      while (pdsIter.hasNext()) {
        DataSet ds=pdsIter.next();
        pdsCount++;
        totalExamples2+=ds.numExamples();
        assertEquals(5,ds.numInputs());
        assertEquals(5,ds.numOutcomes());
      }
      assertEquals(150,totalExamples2);
      assertEquals(numBinFiles,pdsCount);
    }
  finally {
      FileUtils.deleteDirectory(temp);
    }
  }
}
