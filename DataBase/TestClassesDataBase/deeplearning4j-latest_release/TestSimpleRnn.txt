public class TestSimpleRnn extends BaseDL4JTest {
  @Test public void testSimpleRnn(){
    Nd4j.getRandom().setSeed(12345);
    int m=3;
    int nIn=5;
    int layerSize=6;
    int tsLength=7;
    INDArray in=Nd4j.rand(new int[]{m,nIn,tsLength});
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new NoOp()).weightInit(WeightInit.XAVIER).activation(Activation.TANH).list().layer(new SimpleRnn.Builder().nIn(nIn).nOut(layerSize).build()).build();
    MultiLayerNetwork net=new MultiLayerNetwork(conf);
    net.init();
    INDArray out=net.output(in);
    INDArray w=net.getParam("0_W");
    INDArray rw=net.getParam("0_RW");
    INDArray b=net.getParam("0_b");
    INDArray outLast=null;
    for (int i=0; i < tsLength; i++) {
      INDArray inCurrent=in.get(all(),all(),point(i));
      INDArray outExpCurrent=inCurrent.mmul(w);
      if (outLast != null) {
        outExpCurrent.addi(outLast.mmul(rw));
      }
      outExpCurrent.addiRowVector(b);
      Transforms.tanh(outExpCurrent,false);
      INDArray outActCurrent=out.get(all(),all(),point(i));
      assertEquals(String.valueOf(i),outExpCurrent,outActCurrent);
      outLast=outExpCurrent;
    }
    TestUtils.testModelSerialization(net);
  }
  @Test public void testBiasInit(){
    Nd4j.getRandom().setSeed(12345);
    int nIn=5;
    int layerSize=6;
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new NoOp()).weightInit(WeightInit.XAVIER).activation(Activation.TANH).list().layer(new SimpleRnn.Builder().nIn(nIn).nOut(layerSize).biasInit(100).build()).build();
    MultiLayerNetwork net=new MultiLayerNetwork(conf);
    net.init();
    INDArray bArr=net.getParam("0_b");
    assertEquals(Nd4j.valueArrayOf(new long[]{1,layerSize},100),bArr);
  }
}
