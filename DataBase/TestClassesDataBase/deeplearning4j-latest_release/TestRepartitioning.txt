/** 
 * Created by Alex on 03/07/2016.
 */
public class TestRepartitioning extends BaseSparkTest {
  @Test public void testRepartitioning(){
    List<String> list=new ArrayList<>();
    for (int i=0; i < 1000; i++) {
      list.add(String.valueOf(i));
    }
    JavaRDD<String> rdd=sc.parallelize(list);
    rdd=rdd.repartition(200);
    JavaRDD<String> rdd2=SparkUtils.repartitionBalanceIfRequired(rdd,Repartition.Always,100,10);
    assertFalse(rdd == rdd2);
    assertEquals(10,rdd2.partitions().size());
    for (int i=0; i < 10; i++) {
      List<String> partition=rdd2.collectPartitions(new int[]{i})[0];
      System.out.println("Partition " + i + " size: "+ partition.size());
      assertEquals(100,partition.size());
    }
  }
  @Test public void testRepartitioning2() throws Exception {
    int[] ns={320,321,25600,25601,25615};
    for (    int n : ns) {
      List<String> list=new ArrayList<>();
      for (int i=0; i < n; i++) {
        list.add(String.valueOf(i));
      }
      JavaRDD<String> rdd=sc.parallelize(list);
      rdd.repartition(65);
      int totalDataSetObjectCount=n;
      int dataSetObjectsPerSplit=8 * 4 * 10;
      int valuesPerPartition=10;
      int nPartitions=32;
      JavaRDD<String>[] splits=org.deeplearning4j.spark.util.SparkUtils.balancedRandomSplit(totalDataSetObjectCount,dataSetObjectsPerSplit,rdd,new Random().nextLong());
      List<Integer> counts=new ArrayList<>();
      List<List<Tuple2<Integer,Integer>>> partitionCountList=new ArrayList<>();
      for (      JavaRDD<String> split : splits) {
        JavaRDD<String> repartitioned=SparkUtils.repartition(split,Repartition.Always,RepartitionStrategy.Balanced,valuesPerPartition,nPartitions);
        List<Tuple2<Integer,Integer>> partitionCounts=repartitioned.mapPartitionsWithIndex(new CountPartitionsFunction<String>(),true).collect();
        partitionCountList.add(partitionCounts);
        counts.add((int)split.count());
      }
      int expNumPartitionsWithMore=totalDataSetObjectCount % nPartitions;
      int actNumPartitionsWithMore=0;
      for (      List<Tuple2<Integer,Integer>> l : partitionCountList) {
        assertEquals(nPartitions,l.size());
        for (        Tuple2<Integer,Integer> t2 : l) {
          int partitionSize=t2._2();
          if (partitionSize > valuesPerPartition)           actNumPartitionsWithMore++;
        }
      }
      assertEquals(expNumPartitionsWithMore,actNumPartitionsWithMore);
    }
  }
  @Test public void testRepartitioning3(){
    List<Integer> ints=new ArrayList<>();
    for (int i=0; i < 224; i++) {
      ints.add(i);
    }
    JavaRDD<Integer> rdd=sc.parallelize(ints);
    JavaPairRDD<Integer,Integer> pRdd=SparkUtils.indexedRDD(rdd);
    JavaPairRDD<Integer,Integer> initial=pRdd.partitionBy(new Partitioner(){
      @Override public int getPartition(      Object key){
        int i=(Integer)key;
        if (i < 29) {
          return 0;
        }
 else         if (i < 29 + 29) {
          return 1;
        }
 else         if (i < 29 + 29 + 29) {
          return 2;
        }
 else         if (i < 29 + 29 + 29+ 34) {
          return 3;
        }
 else         if (i < 29 + 29 + 29+ 34+ 34) {
          return 4;
        }
 else         if (i < 29 + 29 + 29+ 34+ 34+ 35) {
          return 5;
        }
 else {
          return 6;
        }
      }
      @Override public int numPartitions(){
        return 7;
      }
    }
);
    List<Tuple2<Integer,Integer>> partitionCounts=initial.values().mapPartitionsWithIndex(new CountPartitionsFunction<Integer>(),true).collect();
    System.out.println(partitionCounts);
    List<Tuple2<Integer,Integer>> initialExpected=Arrays.asList(new Tuple2<>(0,29),new Tuple2<>(1,29),new Tuple2<>(2,29),new Tuple2<>(3,34),new Tuple2<>(4,34),new Tuple2<>(5,35),new Tuple2<>(6,34));
    Assert.assertEquals(initialExpected,partitionCounts);
    JavaRDD<Integer> afterRepartition=SparkUtils.repartitionBalanceIfRequired(initial.values(),Repartition.Always,2,112);
    List<Tuple2<Integer,Integer>> partitionCountsAfter=afterRepartition.mapPartitionsWithIndex(new CountPartitionsFunction<Integer>(),true).collect();
    System.out.println(partitionCountsAfter);
    for (    Tuple2<Integer,Integer> t2 : partitionCountsAfter) {
      assertEquals(2,(int)t2._2());
    }
  }
  @Test public void testRepartitioning4(){
    List<Integer> ints=new ArrayList<>();
    for (int i=0; i < 7040; i++) {
      ints.add(i);
    }
    JavaRDD<Integer> rdd=sc.parallelize(ints);
    JavaRDD<Integer> afterRepartition=new DefaultRepartitioner().repartition(rdd,1,32);
    List<Tuple2<Integer,Integer>> partitionCountsAfter=afterRepartition.mapPartitionsWithIndex(new CountPartitionsFunction<Integer>(),true).collect();
    int min=Integer.MAX_VALUE;
    int max=Integer.MIN_VALUE;
    int minIdx=0;
    int maxIdx=0;
    for (    Tuple2<Integer,Integer> t2 : partitionCountsAfter) {
      min=Math.min(min,t2._2());
      max=Math.max(max,t2._2());
      if (min == t2._2()) {
        minIdx=t2._1();
      }
      if (max == t2._2()) {
        maxIdx=t2._1();
      }
    }
    System.out.println("min: " + min + "\t@\t"+ minIdx);
    System.out.println("max: " + max + "\t@\t"+ maxIdx);
    assertEquals(1,min);
    assertEquals(2,max);
  }
  @Test public void testRepartitioningApprox(){
    List<String> list=new ArrayList<>();
    for (int i=0; i < 1000; i++) {
      list.add(String.valueOf(i));
    }
    JavaRDD<String> rdd=sc.parallelize(list);
    rdd=rdd.repartition(200);
    JavaRDD<String> rdd2=SparkUtils.repartitionApproximateBalance(rdd,Repartition.Always,10);
    assertFalse(rdd == rdd2);
    assertEquals(10,rdd2.partitions().size());
    for (int i=0; i < 10; i++) {
      List<String> partition=rdd2.collectPartitions(new int[]{i})[0];
      System.out.println("Partition " + i + " size: "+ partition.size());
      assertTrue(partition.size() >= 90 && partition.size() <= 110);
    }
  }
  @Test public void testRepartitioningApproxReverse(){
    List<String> list=new ArrayList<>();
    for (int i=0; i < 1000; i++) {
      list.add(String.valueOf(i));
    }
    JavaRDD<String> rdd=sc.parallelize(list);
    JavaRDD<String> rdd2=SparkUtils.repartitionApproximateBalance(rdd,Repartition.Always,100);
    assertFalse(rdd == rdd2);
    assertEquals(100,rdd2.partitions().size());
  }
}
