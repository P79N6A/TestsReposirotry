/** 
 * Test CenterLossOutputLayer.
 * @author Justin Long (@crockpotveggies)
 */
public class CenterLossOutputLayerTest extends BaseDL4JTest {
  private ComputationGraph getGraph(  int numLabels,  double lambda){
    Nd4j.getRandom().setSeed(12345);
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).updater(new NoOp()).graphBuilder().addInputs("input1").addLayer("l1",new DenseLayer.Builder().nIn(4).nOut(5).activation(Activation.RELU).build(),"input1").addLayer("lossLayer",new CenterLossOutputLayer.Builder().lossFunction(LossFunctions.LossFunction.MCXENT).nIn(5).nOut(numLabels).lambda(lambda).activation(Activation.SOFTMAX).build(),"l1").setOutputs("lossLayer").build();
    ComputationGraph graph=new ComputationGraph(conf);
    graph.init();
    return graph;
  }
  public ComputationGraph getCNNMnistConfig(){
    int nChannels=1;
    int outputNum=10;
    ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).l2(0.0005).weightInit(WeightInit.XAVIER).updater(new Nesterovs(0.01,0.9)).graphBuilder().addInputs("input").setInputTypes(InputType.convolutionalFlat(28,28,1)).addLayer("0",new ConvolutionLayer.Builder(5,5).nIn(nChannels).stride(1,1).nOut(20).activation(Activation.IDENTITY).build(),"input").addLayer("1",new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX).kernelSize(2,2).stride(2,2).build(),"0").addLayer("2",new ConvolutionLayer.Builder(5,5).stride(1,1).nOut(50).activation(Activation.IDENTITY).build(),"1").addLayer("3",new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX).kernelSize(2,2).stride(2,2).build(),"2").addLayer("4",new DenseLayer.Builder().activation(Activation.RELU).nOut(500).build(),"3").addLayer("output",new org.deeplearning4j.nn.conf.layers.CenterLossOutputLayer.Builder(LossFunction.MCXENT).nOut(outputNum).activation(Activation.SOFTMAX).build(),"4").setOutputs("output").build();
    ComputationGraph graph=new ComputationGraph(conf);
    graph.init();
    return graph;
  }
  @Test public void testLambdaConf(){
    double[] lambdas=new double[]{0.1,0.01};
    double[] results=new double[2];
    int numClasses=2;
    INDArray input=Nd4j.rand(150,4);
    INDArray labels=Nd4j.zeros(150,numClasses);
    Random r=new Random(12345);
    for (int i=0; i < 150; i++) {
      labels.putScalar(i,r.nextInt(numClasses),1.0);
    }
    ComputationGraph graph;
    for (int i=0; i < lambdas.length; i++) {
      graph=getGraph(numClasses,lambdas[i]);
      graph.setInput(0,input);
      graph.setLabel(0,labels);
      graph.computeGradientAndScore();
      results[i]=graph.score();
    }
    assertNotEquals(results[0],results[1]);
  }
  @Test @Ignore public void testMNISTConfig() throws Exception {
    int batchSize=64;
    DataSetIterator mnistTrain=new MnistDataSetIterator(batchSize,true,12345);
    ComputationGraph net=getCNNMnistConfig();
    net.init();
    net.setListeners(new ScoreIterationListener(1));
    for (int i=0; i < 50; i++) {
      net.fit(mnistTrain.next());
      Thread.sleep(1000);
    }
    Thread.sleep(100000);
  }
}
