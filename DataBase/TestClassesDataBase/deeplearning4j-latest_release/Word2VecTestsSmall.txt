@Slf4j public class Word2VecTestsSmall {
  WordVectors word2vec;
  @Before public void setUp() throws Exception {
    word2vec=WordVectorSerializer.readWord2VecModel(new ClassPathResource("vec.bin").getFile());
  }
  @Test public void testWordsNearest2VecTxt(){
    String word="Adam";
    String expectedNeighbour="is";
    int neighbours=1;
    Collection<String> nearestWords=word2vec.wordsNearest(word,neighbours);
    System.out.println(nearestWords);
    assertEquals(expectedNeighbour,nearestWords.iterator().next());
  }
  @Test public void testWordsNearest2NNeighbours(){
    String word="Adam";
    int neighbours=2;
    Collection<String> nearestWords=word2vec.wordsNearest(word,neighbours);
    System.out.println(nearestWords);
    assertEquals(neighbours,nearestWords.size());
  }
  @Test public void testUnkSerialization_1() throws Exception {
    val inputFile=new ClassPathResource("/big/raw_sentences.txt").getFile();
    val iter=new BasicLineIterator(inputFile);
    val t=new DefaultTokenizerFactory();
    t.setTokenPreProcessor(new CommonPreprocessor());
    val vec=new Word2Vec.Builder().minWordFrequency(1).epochs(1).layerSize(300).limitVocabularySize(1).windowSize(5).allowParallelTokenization(true).batchSize(512).learningRate(0.025).minLearningRate(0.0001).negativeSample(0.0).sampling(0.0).useAdaGrad(false).useHierarchicSoftmax(true).iterations(1).useUnknown(true).seed(42).iterate(iter).workers(4).tokenizerFactory(t).build();
    vec.fit();
    val tmpFile=File.createTempFile("temp","temp");
    tmpFile.deleteOnExit();
    WordVectorSerializer.writeWord2VecModel(vec,tmpFile);
  }
  @Test public void testLabelAwareIterator_1() throws Exception {
    val resource=new ClassPathResource("/labeled");
    val file=resource.getFile();
    val iter=(LabelAwareIterator)new FileLabelAwareIterator.Builder().addSourceFolder(file).build();
    val t=new DefaultTokenizerFactory();
    val w2v=new Word2Vec.Builder().iterate(iter).tokenizerFactory(t).build();
  }
  @Test public void testPlot(){
  }
}
