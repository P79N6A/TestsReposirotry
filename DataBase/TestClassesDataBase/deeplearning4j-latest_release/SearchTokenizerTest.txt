public class SearchTokenizerTest {
  private static Tokenizer tokenizer;
  @BeforeClass public static void beforeClass() throws Exception {
    tokenizer=new Tokenizer.Builder().mode(Mode.SEARCH).build();
  }
  @Test public void testCompoundSplitting() throws IOException {
    assertSegmentation("deeplearning4j-nlp-japanese/search-segmentation-tests.txt");
  }
  public void assertSegmentation(  String testFilename) throws IOException {
    LineNumberReader reader=new LineNumberReader(new InputStreamReader(new ClassPathResource(testFilename).getInputStream(),StandardCharsets.UTF_8));
    String line;
    while ((line=reader.readLine()) != null) {
      line=line.replaceAll("#.*$","");
      if (line.trim().isEmpty()) {
        continue;
      }
      String[] fields=line.split("\t",2);
      String text=fields[0];
      List<String> expectedSurfaces=Arrays.asList(fields[1].split("\\s+"));
      assertSegmentation(text,expectedSurfaces);
    }
  }
  public void assertSegmentation(  String text,  List<String> expectedSurfaces){
    List<Token> tokens=tokenizer.tokenize(text);
    assertEquals("Input: " + text,expectedSurfaces.size(),tokens.size());
    for (int i=0; i < tokens.size(); i++) {
      assertEquals(expectedSurfaces.get(i),tokens.get(i).getSurface());
    }
  }
  private InputStream getResourceAsStream(  String resource){
    return this.getClass().getResourceAsStream(resource);
  }
}
