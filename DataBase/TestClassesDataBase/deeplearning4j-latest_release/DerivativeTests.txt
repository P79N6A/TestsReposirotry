@RunWith(Parameterized.class) public class DerivativeTests extends BaseNd4jTest {
  public static final double REL_ERROR_TOLERANCE=1e-3;
  DataBuffer.Type initialType;
  public DerivativeTests(  Nd4jBackend backend){
    super(backend);
    this.initialType=Nd4j.dataType();
  }
  @Before public void before(){
    Nd4j.setDataType(DataBuffer.Type.DOUBLE);
  }
  @After public void after(){
    Nd4j.setDataType(this.initialType);
  }
  @Test public void testHardTanhDerivative(){
    INDArray z=Nd4j.zeros(100);
    double[] expOut=new double[100];
    for (int i=0; i < 100; i++) {
      double x=0.1 * (i - 50);
      z.putScalar(i,x);
      expOut[i]=(Math.abs(x) <= 1.0 ? 1 : 0);
    }
    INDArray zPrime=Nd4j.getExecutioner().execAndReturn(new HardTanhDerivative(z));
    for (int i=0; i < 100; i++) {
      assertEquals(expOut[i],zPrime.getDouble(i),1e-1);
    }
  }
  @Test public void testRectifiedLinearDerivative(){
    DataTypeUtil.setDTypeForContext(DataBuffer.Type.DOUBLE);
    INDArray z=Nd4j.zeros(100);
    double[] expOut=new double[100];
    for (int i=0; i < 100; i++) {
      double x=0.1 * (i - 50);
      z.putScalar(i,x);
      expOut[i]=(x > 0 ? 1 : 0);
    }
    INDArray zPrime=Nd4j.getExecutioner().execAndReturn(new Step(z));
    for (int i=0; i < 100; i++) {
      assertTrue(expOut[i] == zPrime.getDouble(i));
    }
  }
  @Test public void testSigmoidDerivative(){
    INDArray z=Nd4j.zeros(100);
    double[] expOut=new double[100];
    for (int i=0; i < 100; i++) {
      double x=0.1 * (i - 50);
      z.putScalar(i,x);
      double sigmoid=1.0 / (FastMath.exp(-x) + 1);
      expOut[i]=sigmoid * (1 - sigmoid);
    }
    INDArray zPrime=Nd4j.getExecutioner().execAndReturn(new SigmoidDerivative(z));
    for (int i=0; i < 100; i++) {
      double relError=Math.abs(expOut[i] - zPrime.getDouble(i)) / (Math.abs(expOut[i]) + Math.abs(zPrime.getDouble(i)));
      assertTrue(relError < REL_ERROR_TOLERANCE);
    }
  }
  @Test public void testHardSigmoidDerivative(){
    double[] expHSOut=new double[300];
    double[] expDerivOut=new double[300];
    INDArray xArr=Nd4j.linspace(-3,3,300);
    for (int i=0; i < xArr.length(); i++) {
      double x=xArr.getDouble(i);
      double hs=0.2 * x + 0.5;
      if (hs < 0)       hs=0;
      if (hs > 1)       hs=1;
      expHSOut[i]=hs;
      double hsDeriv;
      if (x < -2.5 || x > 2.5)       hsDeriv=0;
 else       hsDeriv=0.2;
      expDerivOut[i]=hsDeriv;
    }
    INDArray z=Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform("hard_sigmoid",xArr.dup()));
    INDArray zPrime=Nd4j.getExecutioner().execAndReturn(new HardSigmoidDerivative(xArr.dup()));
    System.out.println(xArr);
    System.out.println(z);
    System.out.println(zPrime);
    for (int i=0; i < expHSOut.length; i++) {
      double relErrorHS=Math.abs(expHSOut[i] - z.getDouble(i)) / (Math.abs(expHSOut[i]) + Math.abs(z.getDouble(i)));
      if (!(expHSOut[i] == 0 && z.getDouble(i) == 0)) {
        assertTrue(relErrorHS < REL_ERROR_TOLERANCE);
      }
      double relErrorDeriv=Math.abs(expDerivOut[i] - zPrime.getDouble(i)) / (Math.abs(expDerivOut[i]) + Math.abs(zPrime.getDouble(i)));
      if (!(expDerivOut[i] == 0 && zPrime.getDouble(i) == 0)) {
        assertTrue(relErrorDeriv < REL_ERROR_TOLERANCE);
      }
    }
  }
  @Test public void testSoftMaxDerivative(){
    Random r=new Random(12345L);
    int[] mb=new int[]{10,2,1};
    for (    int minibatch : mb) {
      System.out.println("Minibatch size: " + minibatch);
      INDArray z=Nd4j.zeros(minibatch,5);
      double[][] in=new double[minibatch][5];
      double[][] softmax=new double[minibatch][5];
      double[][] expOut=new double[minibatch][5];
      for (int i=0; i < minibatch; i++) {
        double rowSumExp=0.0;
        for (int j=0; j < 5; j++) {
          in[i][j]=10 * r.nextDouble();
          z.putScalar(new int[]{i,j},in[i][j]);
          rowSumExp+=FastMath.exp(in[i][j]);
        }
        for (int j=0; j < 5; j++) {
          softmax[i][j]=FastMath.exp(in[i][j]) / rowSumExp;
          expOut[i][j]=softmax[i][j] * (1.0 - softmax[i][j]);
        }
      }
      INDArray sm=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(z.dup()));
      INDArray zPrime=Nd4j.getExecutioner().execAndReturn(new SoftMaxDerivative(z));
      System.out.println(Arrays.toString(sm.data().asDouble()));
      System.out.println(Arrays.toString(zPrime.data().asDouble()));
      assertNotEquals(sm,zPrime);
      for (int i=0; i < minibatch; i++) {
        for (int j=0; j < 5; j++) {
          double relError=Math.abs(expOut[i][j] - zPrime.getDouble(i,j)) / (Math.abs(expOut[i][j]) + Math.abs(zPrime.getDouble(i,j)));
          assertTrue(relError < REL_ERROR_TOLERANCE);
        }
      }
    }
  }
  @Test public void testSoftPlusDerivative(){
    INDArray z=Nd4j.zeros(100);
    double[] expOut=new double[100];
    for (int i=0; i < 100; i++) {
      double x=0.1 * (i - 50);
      z.putScalar(i,x);
      expOut[i]=1.0 / (1.0 + FastMath.exp(-x));
    }
    INDArray zPrime=Nd4j.getExecutioner().execAndReturn(new Sigmoid(z));
    for (int i=0; i < 100; i++) {
      double relError=Math.abs(expOut[i] - zPrime.getDouble(i)) / (Math.abs(expOut[i]) + Math.abs(zPrime.getDouble(i)));
      assertTrue(relError < REL_ERROR_TOLERANCE);
    }
  }
  @Test public void testTanhDerivative(){
    INDArray z=Nd4j.zeros(100);
    double[] expOut=new double[100];
    for (int i=0; i < 100; i++) {
      double x=0.1 * (i - 50);
      z.putScalar(i,x);
      double tanh=FastMath.tanh(x);
      expOut[i]=1.0 - tanh * tanh;
    }
    INDArray zPrime=Nd4j.getExecutioner().execAndReturn(new TanhDerivative(z));
    for (int i=0; i < 100; i++) {
      double relError=Math.abs(expOut[i] - zPrime.getDouble(i)) / (Math.abs(expOut[i]) + Math.abs(zPrime.getDouble(i)));
      assertTrue(relError < REL_ERROR_TOLERANCE);
    }
  }
  @Test public void testCubeDerivative(){
    INDArray z=Nd4j.zeros(100);
    double[] expOut=new double[100];
    for (int i=0; i < 100; i++) {
      double x=0.1 * (i - 50);
      z.putScalar(i,x);
      expOut[i]=3 * x * x;
    }
    INDArray zPrime=Nd4j.getExecutioner().execAndReturn(new CubeDerivative(z));
    for (int i=0; i < 100; i++) {
      double d1=expOut[i];
      double d2=zPrime.getDouble(i);
      double relError=Math.abs(d1 - d1) / (Math.abs(d1) + Math.abs(d2));
      if (d1 == 0.0 && d2 == 0.0)       relError=0.0;
      String str="exp=" + expOut[i] + ", act="+ zPrime.getDouble(i)+ "; relError = "+ relError;
      assertTrue(str,relError < REL_ERROR_TOLERANCE);
    }
  }
  @Test public void testLeakyReLUDerivative(){
    INDArray z=Nd4j.zeros(100);
    double[] expOut=new double[100];
    for (int i=0; i < 100; i++) {
      double x=0.1 * (i - 50);
      z.putScalar(i,x);
      expOut[i]=(x >= 0 ? 1 : 0.25);
    }
    INDArray zPrime=Nd4j.getExecutioner().execAndReturn(new LeakyReLUDerivative(z,0.25));
    for (int i=0; i < 100; i++) {
      double relError=Math.abs(expOut[i] - zPrime.getDouble(i)) / (Math.abs(expOut[i]) + Math.abs(zPrime.getDouble(i)));
      assertTrue(relError < REL_ERROR_TOLERANCE);
    }
  }
  @Test public void testSoftSignDerivative(){
    INDArray z=Nd4j.zeros(100);
    double[] expOut=new double[100];
    for (int i=0; i < 100; i++) {
      double x=0.1 * (i - 50);
      z.putScalar(i,x);
      double temp=1 + Math.abs(x);
      expOut[i]=1.0 / (temp * temp);
    }
    INDArray zPrime=Nd4j.getExecutioner().execAndReturn(new SoftSignDerivative(z));
    for (int i=0; i < 100; i++) {
      double relError=Math.abs(expOut[i] - zPrime.getDouble(i)) / (Math.abs(expOut[i]) + Math.abs(zPrime.getDouble(i)));
      assertTrue(relError < REL_ERROR_TOLERANCE);
    }
  }
  @Test public void testELUDerivative(){
    INDArray z=Nd4j.zeros(100);
    double[] out=new double[100];
    double[] outDeriv=new double[100];
    for (int i=0; i < 100; i++) {
      double x=0.1 * (i - 50);
      z.putScalar(i,x);
      if (x >= 0) {
        out[i]=x;
        outDeriv[i]=1.0;
      }
 else {
        out[i]=FastMath.exp(x) - 1.0;
        outDeriv[i]=FastMath.exp(x);
      }
    }
    INDArray act=Nd4j.getExecutioner().execAndReturn(Nd4j.getOpFactory().createTransform("elu",z.dup()));
    INDArray actDeriv=Nd4j.getExecutioner().execAndReturn(new ELUDerivative(z.dup()));
    System.out.println(act);
    for (int i=0; i < 100; i++) {
      double relError1=Math.abs(out[i] - act.getDouble(i)) / (Math.abs(out[i]) + Math.abs(act.getDouble(i)));
      if (out[i] == 0.0 && act.getDouble(i) == 0.0)       relError1=0.0;
      double relError2=Math.abs(outDeriv[i] - actDeriv.getDouble(i)) / (Math.abs(outDeriv[i]) + Math.abs(actDeriv.getDouble(i)));
      if (outDeriv[i] == 0.0 && actDeriv.getDouble(i) == 0.0)       relError2=0.0;
      assertTrue(relError1 < REL_ERROR_TOLERANCE);
      assertTrue(relError2 < REL_ERROR_TOLERANCE);
    }
  }
  @Test public void softmaxsimpleLossTest(){
    INDArray X=Nd4j.rand(1,2);
    INDArray YHat=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(X.dup()));
    INDArray Y=Nd4j.create(new double[][]{{0.123,1 - 0.123}});
    double lossHere=Transforms.pow(Y.sub(YHat),2).sumNumber().doubleValue();
    INDArray softmaxDer=Nd4j.getExecutioner().execAndReturn(new SoftMaxDerivative(X.dup()));
    INDArray dLdY=Y.sub(YHat).mul(-2);
    INDArray currentGradient=dLdY.mul(softmaxDer);
    double y0y1=softmaxDer.getDouble(0,0);
    INDArray mysoftmaxDer=correctSoftmax(X);
    INDArray myGradient=mysoftmaxDer.mulRowVector(dLdY).sum(1);
    double epsilon=0.0001;
    INDArray Xiplus, Ximinus;
    INDArray YHatplus, YHatminus;
    double lossplus, lossminus;
    INDArray numGradient=Nd4j.zeros(1,2);
    for (int i=0; i < 2; i++) {
      double x=X.getDouble(0,i);
      Xiplus=X.dup();
      Xiplus.put(0,i,x + epsilon);
      YHatplus=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(Xiplus.dup()));
      lossplus=Transforms.pow(Y.sub(YHatplus),2).sumNumber().doubleValue();
      Ximinus=X.dup();
      Ximinus.put(0,i,x - epsilon);
      YHatminus=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(Ximinus.dup()));
      lossminus=Transforms.pow(Y.sub(YHatminus),2).sumNumber().doubleValue();
      double gradienti=(lossplus - lossminus) / (2 * epsilon);
      numGradient.put(0,i,gradienti);
    }
    System.out.println("=========================");
    System.out.println("NUMERICAL:");
    System.out.println(numGradient);
    System.out.println("\nCURRENTLY:");
    System.out.println(currentGradient);
    System.out.println("\nMY GRADIENT:");
    System.out.println(myGradient + "\n");
    System.out.println("Because of the nature of the derivative of the softmax for length = 2, our current method will make it off by a factor of 2");
    System.out.println("=========================");
  }
  @Test public void softmaxsimplelongerlengthLossTest(){
    int someLength=7;
    INDArray X=Nd4j.rand(1,someLength);
    INDArray YHat=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(X.dup()));
    INDArray temp=Nd4j.rand(1,someLength);
    INDArray Y=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(temp));
    double lossHere=Transforms.pow(Y.sub(YHat),2).sumNumber().doubleValue();
    INDArray softmaxDer=Nd4j.getExecutioner().execAndReturn(new SoftMaxDerivative(X.dup()));
    INDArray dLdY=Y.sub(YHat).mul(-2);
    INDArray currentGradient=dLdY.mul(softmaxDer);
    INDArray mysoftmaxDer=correctSoftmax(X);
    INDArray myGradient=mysoftmaxDer.mulRowVector(dLdY).sum(1);
    double epsilon=0.0001;
    INDArray Xiplus, Ximinus;
    INDArray YHatplus, YHatminus;
    double lossplus, lossminus;
    INDArray numGradient=Nd4j.zeros(1,someLength);
    for (int i=0; i < someLength; i++) {
      double x=X.getDouble(0,i);
      Xiplus=X.dup();
      Xiplus.put(0,i,x + epsilon);
      YHatplus=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(Xiplus.dup()));
      lossplus=Transforms.pow(Y.sub(YHatplus),2).sumNumber().doubleValue();
      Ximinus=X.dup();
      Ximinus.put(0,i,x - epsilon);
      YHatminus=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(Ximinus.dup()));
      lossminus=Transforms.pow(Y.sub(YHatminus),2).sumNumber().doubleValue();
      double gradienti=(lossplus - lossminus) / (2 * epsilon);
      numGradient.put(0,i,gradienti);
    }
    System.out.println("=========================");
    System.out.println("NUMERICAL GRADIENT:");
    System.out.println(new NDArrayStrings(6).format(numGradient).toString());
    System.out.println("\nANALYTIC USING EXISTING SOFTMAX DER:");
    System.out.println(new NDArrayStrings(6).format(currentGradient).toString());
    System.out.println("\nGRADIENT USING MY VERSION OF SOFTMAX DER:");
    System.out.println(new NDArrayStrings(6).format(myGradient).toString());
    System.out.println("=========================");
  }
  public static INDArray correctSoftmax(  INDArray X){
    INDArray p=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(X.dup()));
    INDArray pCol=p.dup().transpose();
    INDArray pipj=pCol.mmul(p);
    pipj.muli(-1);
    INDArray diagp=Nd4j.getExecutioner().execAndReturn(new SoftMaxDerivative(X.dup()));
    for (int i=0; i < X.length(); i++) {
      pipj.put(i,i,diagp.getDouble(0,i));
    }
    return pipj;
  }
  @Override public char ordering(){
    return 'f';
  }
}
