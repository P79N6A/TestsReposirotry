/** 
 * @author Alex Black 14 Aug 2015
 */
@Slf4j public class GradientCheckTests extends BaseDL4JTest {
  private static final boolean PRINT_RESULTS=true;
  private static final boolean RETURN_ON_FIRST_FAILURE=false;
  private static final double DEFAULT_EPS=1e-6;
  private static final double DEFAULT_MAX_REL_ERROR=1e-3;
  private static final double DEFAULT_MIN_ABS_ERROR=1e-8;
static {
    Nd4j.setDataType(DataBuffer.Type.DOUBLE);
  }
  @Test public void testMinibatchApplication(){
    IrisDataSetIterator iter=new IrisDataSetIterator(30,150);
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().miniBatch(false).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).updater(new NoOp()).list().layer(0,new DenseLayer.Builder().nIn(4).nOut(3).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).activation(Activation.TANH).build()).layer(1,new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(3).nOut(3).build()).build();
    MultiLayerNetwork mln=new MultiLayerNetwork(conf);
    mln.init();
    assertEquals(1,mln.getInputMiniBatchSize());
    DataNormalization scaler=new NormalizerMinMaxScaler();
    scaler.fit(iter);
    iter.setPreProcessor(scaler);
    DataSet ds=iter.next();
    boolean doLearningFirst=true;
    String outputActivation="tanh";
    String afn=outputActivation;
    String lf="negativeloglikelihood";
    if (doLearningFirst) {
      mln.setInput(ds.getFeatures());
      mln.setLabels(ds.getLabels());
      mln.computeGradientAndScore();
      double scoreBefore=mln.score();
      for (int j=0; j < 10; j++)       mln.fit(ds);
      mln.computeGradientAndScore();
      double scoreAfter=mln.score();
      String msg="testMinibatchApplication() - score did not (sufficiently) decrease during learning - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst+ " (before="+ scoreBefore+ ", scoreAfter="+ scoreAfter+ ")";
    }
    if (PRINT_RESULTS) {
      System.out.println("testMinibatchApplication() - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst);
      for (int j=0; j < mln.getnLayers(); j++)       System.out.println("Layer " + j + " # params: "+ mln.getLayer(j).numParams());
    }
    boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,ds.getFeatures(),ds.getLabels());
    String msg="testMinibatchApplication() - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst;
    assertTrue(msg,gradOK);
    TestUtils.testModelSerialization(mln);
  }
  @Test public void testGradientMLP2LayerIrisSimple(){
    Activation[] activFns={Activation.SIGMOID,Activation.TANH};
    boolean[] characteristic={false,true};
    LossFunction[] lossFunctions={LossFunction.MCXENT,LossFunction.MSE};
    Activation[] outputActivations={Activation.SOFTMAX,Activation.TANH};
    DataNormalization scaler=new NormalizerMinMaxScaler();
    DataSetIterator iter=new IrisDataSetIterator(150,150);
    scaler.fit(iter);
    iter.setPreProcessor(scaler);
    DataSet ds=iter.next();
    INDArray input=ds.getFeatures();
    INDArray labels=ds.getLabels();
    for (    Activation afn : activFns) {
      for (      boolean doLearningFirst : characteristic) {
        for (int i=0; i < lossFunctions.length; i++) {
          LossFunction lf=lossFunctions[i];
          Activation outputActivation=outputActivations[i];
          MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT).updater(new NoOp()).seed(12345L).list().layer(0,new DenseLayer.Builder().nIn(4).nOut(3).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).activation(afn).build()).layer(1,new OutputLayer.Builder(lf).activation(outputActivation).nIn(3).nOut(3).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).build()).build();
          MultiLayerNetwork mln=new MultiLayerNetwork(conf);
          mln.init();
          if (doLearningFirst) {
            mln.setInput(ds.getFeatures());
            mln.setLabels(ds.getLabels());
            mln.computeGradientAndScore();
            double scoreBefore=mln.score();
            for (int j=0; j < 10; j++)             mln.fit(ds);
            mln.computeGradientAndScore();
            double scoreAfter=mln.score();
            String msg="testGradMLP2LayerIrisSimple() - score did not (sufficiently) decrease during learning - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst+ " (before="+ scoreBefore+ ", scoreAfter="+ scoreAfter+ ")";
          }
          if (PRINT_RESULTS) {
            System.out.println("testGradientMLP2LayerIrisSimpleRandom() - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst);
            for (int j=0; j < mln.getnLayers(); j++)             System.out.println("Layer " + j + " # params: "+ mln.getLayer(j).numParams());
          }
          boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
          String msg="testGradMLP2LayerIrisSimple() - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst;
          assertTrue(msg,gradOK);
          TestUtils.testModelSerialization(mln);
        }
      }
    }
  }
  @Test public void testGradientMLP2LayerIrisL1L2Simple(){
    Activation[] activFns={Activation.SIGMOID,Activation.TANH,Activation.THRESHOLDEDRELU};
    boolean[] characteristic={false,true};
    LossFunction[] lossFunctions={LossFunction.MCXENT,LossFunction.MSE};
    Activation[] outputActivations={Activation.SOFTMAX,Activation.TANH};
    DataNormalization scaler=new NormalizerMinMaxScaler();
    DataSetIterator iter=new IrisDataSetIterator(150,150);
    scaler.fit(iter);
    iter.setPreProcessor(scaler);
    DataSet ds=iter.next();
    INDArray input=ds.getFeatures();
    INDArray labels=ds.getLabels();
    double[] l2vals={0.4,0.0,0.4,0.4};
    double[] l1vals={0.0,0.0,0.5,0.0};
    double[] biasL2={0.0,0.0,0.0,0.2};
    double[] biasL1={0.0,0.0,0.6,0.0};
    for (    Activation afn : activFns) {
      for (      boolean doLearningFirst : characteristic) {
        for (int i=0; i < lossFunctions.length; i++) {
          for (int k=0; k < l2vals.length; k++) {
            LossFunction lf=lossFunctions[i];
            Activation outputActivation=outputActivations[i];
            double l2=l2vals[k];
            double l1=l1vals[k];
            MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().l2(l2).l1(l1).l2Bias(biasL2[k]).l1Bias(biasL1[k]).optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT).seed(12345L).list().layer(0,new DenseLayer.Builder().nIn(4).nOut(3).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).updater(new NoOp()).activation(afn).build()).layer(1,new OutputLayer.Builder(lf).nIn(3).nOut(3).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).updater(new NoOp()).activation(outputActivation).build()).build();
            MultiLayerNetwork mln=new MultiLayerNetwork(conf);
            mln.init();
            doLearningFirst=false;
            if (doLearningFirst) {
              mln.setInput(ds.getFeatures());
              mln.setLabels(ds.getLabels());
              mln.computeGradientAndScore();
              double scoreBefore=mln.score();
              for (int j=0; j < 10; j++)               mln.fit(ds);
              mln.computeGradientAndScore();
              double scoreAfter=mln.score();
              String msg="testGradMLP2LayerIrisSimple() - score did not (sufficiently) decrease during learning - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst+ ", l2="+ l2+ ", l1="+ l1+ " (before="+ scoreBefore+ ", scoreAfter="+ scoreAfter+ ")";
              assertTrue(msg,scoreAfter < 0.8 * scoreBefore);
            }
            if (PRINT_RESULTS) {
              System.out.println("testGradientMLP2LayerIrisSimpleRandom() - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst+ ", l2="+ l2+ ", l1="+ l1);
              for (int j=0; j < mln.getnLayers(); j++)               System.out.println("Layer " + j + " # params: "+ mln.getLayer(j).numParams());
            }
            boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
            String msg="testGradMLP2LayerIrisSimple() - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst+ ", l2="+ l2+ ", l1="+ l1;
            assertTrue(msg,gradOK);
            TestUtils.testModelSerialization(mln);
          }
        }
      }
    }
  }
  @Test public void testEmbeddingLayerPreluSimple(){
    Random r=new Random(12345);
    int nExamples=5;
    INDArray input=Nd4j.zeros(nExamples,1);
    INDArray labels=Nd4j.zeros(nExamples,3);
    for (int i=0; i < nExamples; i++) {
      input.putScalar(i,r.nextInt(4));
      labels.putScalar(new int[]{i,r.nextInt(3)},1.0);
    }
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().l2(0.2).l1(0.1).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).seed(12345L).list().layer(new EmbeddingLayer.Builder().nIn(4).nOut(3).weightInit(WeightInit.XAVIER).dist(new NormalDistribution(0,1)).updater(new NoOp()).build()).layer(new PReLULayer.Builder().inputShape(3).sharedAxes(1).updater(new NoOp()).build()).layer(new OutputLayer.Builder(LossFunction.MCXENT).nIn(3).nOut(3).weightInit(WeightInit.XAVIER).dist(new NormalDistribution(0,1)).updater(new NoOp()).activation(Activation.SOFTMAX).build()).build();
    MultiLayerNetwork mln=new MultiLayerNetwork(conf);
    mln.init();
    if (PRINT_RESULTS) {
      System.out.println("testEmbeddingLayerSimple");
      for (int j=0; j < mln.getnLayers(); j++)       System.out.println("Layer " + j + " # params: "+ mln.getLayer(j).numParams());
    }
    boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
    String msg="testEmbeddingLayerSimple";
    assertTrue(msg,gradOK);
  }
  @Test public void testEmbeddingLayerSimple(){
    Random r=new Random(12345);
    int nExamples=5;
    INDArray input=Nd4j.zeros(nExamples,1);
    INDArray labels=Nd4j.zeros(nExamples,3);
    for (int i=0; i < nExamples; i++) {
      input.putScalar(i,r.nextInt(4));
      labels.putScalar(new int[]{i,r.nextInt(3)},1.0);
    }
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().l2(0.2).l1(0.1).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).seed(12345L).list().layer(0,new EmbeddingLayer.Builder().nIn(4).nOut(3).weightInit(WeightInit.XAVIER).dist(new NormalDistribution(0,1)).updater(new NoOp()).activation(Activation.TANH).build()).layer(1,new OutputLayer.Builder(LossFunction.MCXENT).nIn(3).nOut(3).weightInit(WeightInit.XAVIER).dist(new NormalDistribution(0,1)).updater(new NoOp()).activation(Activation.SOFTMAX).build()).build();
    MultiLayerNetwork mln=new MultiLayerNetwork(conf);
    mln.init();
    if (PRINT_RESULTS) {
      System.out.println("testEmbeddingLayerSimple");
      for (int j=0; j < mln.getnLayers(); j++)       System.out.println("Layer " + j + " # params: "+ mln.getLayer(j).numParams());
    }
    boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
    String msg="testEmbeddingLayerSimple";
    assertTrue(msg,gradOK);
    TestUtils.testModelSerialization(mln);
  }
  @Test public void testAutoEncoder(){
    Activation[] activFns={Activation.SIGMOID,Activation.TANH};
    boolean[] characteristic={false,true};
    LossFunction[] lossFunctions={LossFunction.MCXENT,LossFunction.MSE};
    Activation[] outputActivations={Activation.SOFTMAX,Activation.TANH};
    DataNormalization scaler=new NormalizerMinMaxScaler();
    DataSetIterator iter=new IrisDataSetIterator(150,150);
    scaler.fit(iter);
    iter.setPreProcessor(scaler);
    DataSet ds=iter.next();
    INDArray input=ds.getFeatures();
    INDArray labels=ds.getLabels();
    NormalizerStandardize norm=new NormalizerStandardize();
    norm.fit(ds);
    norm.transform(ds);
    double[] l2vals={0.2,0.0,0.2};
    double[] l1vals={0.0,0.3,0.3};
    for (    Activation afn : activFns) {
      for (      boolean doLearningFirst : characteristic) {
        for (int i=0; i < lossFunctions.length; i++) {
          for (int k=0; k < l2vals.length; k++) {
            LossFunction lf=lossFunctions[i];
            Activation outputActivation=outputActivations[i];
            double l2=l2vals[k];
            double l1=l1vals[k];
            Nd4j.getRandom().setSeed(12345);
            MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new NoOp()).l2(l2).l1(l1).optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT).seed(12345L).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).list().layer(0,new AutoEncoder.Builder().nIn(4).nOut(3).activation(afn).build()).layer(1,new OutputLayer.Builder(lf).nIn(3).nOut(3).activation(outputActivation).build()).pretrain(true).build();
            MultiLayerNetwork mln=new MultiLayerNetwork(conf);
            mln.init();
            String msg;
            if (doLearningFirst) {
              mln.setInput(ds.getFeatures());
              mln.setLabels(ds.getLabels());
              mln.computeGradientAndScore();
              double scoreBefore=mln.score();
              for (int j=0; j < 10; j++)               mln.fit(ds);
              mln.computeGradientAndScore();
              double scoreAfter=mln.score();
              msg="testGradMLP2LayerIrisSimple() - score did not (sufficiently) decrease during learning - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst+ ", l2="+ l2+ ", l1="+ l1+ " (before="+ scoreBefore+ ", scoreAfter="+ scoreAfter+ ")";
              assertTrue(msg,scoreAfter < scoreBefore);
            }
            msg="testGradMLP2LayerIrisSimple() - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst+ ", l2="+ l2+ ", l1="+ l1;
            if (PRINT_RESULTS) {
              System.out.println(msg);
              for (int j=0; j < mln.getnLayers(); j++)               System.out.println("Layer " + j + " # params: "+ mln.getLayer(j).numParams());
            }
            boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
            assertTrue(msg,gradOK);
            TestUtils.testModelSerialization(mln);
          }
        }
      }
    }
  }
  @Test public void elementWiseMultiplicationLayerTest(){
    for (    Activation a : new Activation[]{Activation.IDENTITY,Activation.TANH}) {
      ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT).updater(new NoOp()).seed(12345L).weightInit(new UniformDistribution(0,1)).graphBuilder().addInputs("features").addLayer("dense",new DenseLayer.Builder().nIn(4).nOut(4).activation(Activation.TANH).build(),"features").addLayer("elementWiseMul",new ElementWiseMultiplicationLayer.Builder().nIn(4).nOut(4).activation(a).build(),"dense").addLayer("loss",new LossLayer.Builder(LossFunctions.LossFunction.COSINE_PROXIMITY).activation(Activation.IDENTITY).build(),"elementWiseMul").setOutputs("loss").build();
      ComputationGraph netGraph=new ComputationGraph(conf);
      netGraph.init();
      log.info("params before learning: " + netGraph.getLayer(1).paramTable());
      INDArray features=Nd4j.create(new double[][]{{1,2,3,4},{1,2,3,1},{1,2,3,0}});
      INDArray labels=Nd4j.create(new double[][]{{1,1,1,8},{1,1,1,2},{1,1,1,1}});
      netGraph.setInputs(features);
      netGraph.setLabels(labels);
      netGraph.computeGradientAndScore();
      double scoreBefore=netGraph.score();
      String msg;
      for (int epoch=0; epoch < 5; epoch++)       netGraph.fit(new INDArray[]{features},new INDArray[]{labels});
      netGraph.computeGradientAndScore();
      double scoreAfter=netGraph.score();
      msg="elementWiseMultiplicationLayerTest() - score did not (sufficiently) decrease during learning - activationFn=" + "Id" + ", lossFn="+ "Cos-sim"+ ", outputActivation="+ "Id"+ ", doLearningFirst="+ "true"+ " (before=" + scoreBefore + ", scoreAfter="+ scoreAfter+ ")";
      assertTrue(msg,scoreAfter < 0.8 * scoreBefore);
      log.info("params after learning: " + netGraph.getLayer(1).paramTable());
      boolean gradOK=checkGradients(netGraph,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,new INDArray[]{features},new INDArray[]{labels});
      msg="elementWiseMultiplicationLayerTest() - activationFn=" + "ID" + ", lossFn="+ "Cos-sim"+ ", outputActivation="+ "Id"+ ", doLearningFirst="+ "true";
      assertTrue(msg,gradOK);
      TestUtils.testModelSerialization(netGraph);
    }
  }
  @Test public void testEmbeddingSequenceLayer(){
    Nd4j.getRandom().setSeed(12345);
    for (    boolean maskArray : new boolean[]{false,true}) {
      for (      int inputRank : new int[]{2,3}) {
        MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).updater(new NoOp()).weightInit(new NormalDistribution(0,1)).list().layer(new EmbeddingSequenceLayer.Builder().nIn(8).nOut(4).build()).layer(new RnnOutputLayer.Builder().nIn(4).nOut(3).activation(Activation.TANH).lossFunction(LossFunction.MSE).build()).build();
        MultiLayerNetwork net=new MultiLayerNetwork(conf);
        net.init();
        INDArray in=Transforms.floor(Nd4j.rand(3,6).muli(8));
        INDArray label=Nd4j.rand(new int[]{3,3,6});
        if (inputRank == 3) {
          in=in.reshape('c',3,1,6);
        }
        INDArray fMask=null;
        if (maskArray) {
          fMask=Nd4j.create(new double[][]{{1,1,1,1,1,1},{1,1,0,0,0,0},{1,0,0,0,0,0}});
        }
        String msg="mask=" + maskArray + ", inputRank="+ inputRank;
        boolean gradOK=GradientCheckUtil.checkGradients(net,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,in,label,fMask,null);
        assertTrue(msg,gradOK);
        TestUtils.testModelSerialization(net);
        if (maskArray) {
          DataSet ds=new DataSet(in,label,fMask,null);
          double score=net.score(ds);
          if (inputRank == 2) {
            in.putScalar(1,2,0);
            in.putScalar(2,1,0);
            in.putScalar(2,2,0);
          }
 else {
            in.putScalar(1,0,2,0);
            in.putScalar(2,0,1,0);
            in.putScalar(2,0,2,0);
          }
          double score2=net.score(ds);
          assertEquals(score,score2,1e-6);
          if (inputRank == 2) {
            in.putScalar(1,2,1);
            in.putScalar(2,1,1);
            in.putScalar(2,2,1);
          }
 else {
            in.putScalar(1,0,2,1);
            in.putScalar(2,0,1,1);
            in.putScalar(2,0,2,1);
          }
          double score3=net.score(ds);
          assertEquals(score,score3,1e-6);
        }
      }
    }
  }
  @Test public void testGradientLegacyL2(){
    Activation[] activFns={Activation.SIGMOID,Activation.TANH,Activation.THRESHOLDEDRELU};
    boolean[] characteristic={false,true};
    LossFunction[] lossFunctions={LossFunction.MCXENT,LossFunction.MSE};
    Activation[] outputActivations={Activation.SOFTMAX,Activation.TANH};
    DataNormalization scaler=new NormalizerMinMaxScaler();
    DataSetIterator iter=new IrisDataSetIterator(150,150);
    scaler.fit(iter);
    iter.setPreProcessor(scaler);
    DataSet ds=iter.next();
    INDArray input=ds.getFeatures();
    INDArray labels=ds.getLabels();
    double[] l2vals={0.4,0.0,0.4,0.4};
    double[] l1vals={0.0,0.0,0.5,0.0};
    double[] biasL2={0.0,0.0,0.0,0.2};
    double[] biasL1={0.0,0.0,0.6,0.0};
    for (    Activation afn : activFns) {
      for (int i=0; i < lossFunctions.length; i++) {
        for (int k=0; k < l2vals.length; k++) {
          LossFunction lf=lossFunctions[i];
          Activation outputActivation=outputActivations[i];
          double l2=l2vals[k];
          double l1=l1vals[k];
          MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().l2(l2).l1(l1).l2Bias(biasL2[k]).l1Bias(biasL1[k]).optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT).seed(12345L).legacyBatchScaledL2(false).list().layer(0,new DenseLayer.Builder().nIn(4).nOut(3).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).updater(new NoOp()).activation(afn).build()).layer(1,new OutputLayer.Builder(lf).nIn(3).nOut(3).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).updater(new NoOp()).activation(outputActivation).build()).build();
          MultiLayerConfiguration conf2=new NeuralNetConfiguration.Builder().l2(l2).l1(l1).l2Bias(biasL2[k]).l1Bias(biasL1[k]).optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT).seed(12345L).legacyBatchScaledL2(true).list().layer(0,new DenseLayer.Builder().nIn(4).nOut(3).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).updater(new NoOp()).activation(afn).build()).layer(1,new OutputLayer.Builder(lf).nIn(3).nOut(3).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).updater(new NoOp()).activation(outputActivation).build()).build();
          MultiLayerNetwork mln=new MultiLayerNetwork(conf);
          mln.init();
          MultiLayerNetwork mlnLegacy=new MultiLayerNetwork(conf2);
          mlnLegacy.init();
          mlnLegacy.params().assign(mln.params());
          boolean gradOK1=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
          boolean gradOKLegacy=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
          String msg="testGradientLegacyL2() - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", l2="+ l2+ ", l1="+ l1;
          assertTrue(msg,gradOK1);
          assertTrue(msg,gradOKLegacy);
          TestUtils.testModelSerialization(mln);
          TestUtils.testModelSerialization(mlnLegacy);
          double score=mln.score(new DataSet(input,labels));
          double scoreLegacy=mlnLegacy.score(new DataSet(input,labels));
          if (l1 > 0 || l2 > 0) {
            assertNotEquals(score,scoreLegacy,1e-6);
            assertTrue(score > scoreLegacy);
          }
 else {
            assertEquals(score,scoreLegacy,1e-6);
          }
        }
      }
    }
  }
}
