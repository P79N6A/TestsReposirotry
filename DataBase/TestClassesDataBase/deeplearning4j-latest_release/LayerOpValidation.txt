@Slf4j public class LayerOpValidation extends BaseOpValidation {
  public LayerOpValidation(  Nd4jBackend backend){
    super(backend);
  }
  @Test public void testXwPlusB(){
    Nd4j.getRandom().setSeed(12345);
    SameDiff sameDiff=SameDiff.create();
    INDArray input=Nd4j.rand(new long[]{2,3});
    INDArray weights=Nd4j.rand(new long[]{3,4});
    INDArray b=Nd4j.rand(new long[]{4});
    SDVariable sdInput=sameDiff.var("input",input);
    SDVariable sdWeights=sameDiff.var("weights",weights);
    SDVariable sdBias=sameDiff.var("bias",b);
    SDVariable res=sameDiff.linear(sdInput,sdWeights,sdBias);
    SDVariable loss=sameDiff.standardDeviation(res,true);
    INDArray exp=input.mmul(weights).addiRowVector(b);
    TestCase tc=new TestCase(sameDiff).gradientCheck(true).expectedOutput(res.getVarName(),exp);
    System.out.println(sameDiff.summary());
    System.out.println("============================");
    sameDiff.createGradFunction();
    System.out.println(sameDiff.getFunction("grad").summary());
    String err=OpValidation.validate(tc);
    assertNull(err);
  }
  @Test public void testReluLayer(){
    Nd4j.getRandom().setSeed(12345);
    SameDiff sameDiff=SameDiff.create();
    INDArray input=Nd4j.rand(new long[]{2,3});
    INDArray weights=Nd4j.rand(new long[]{3,4});
    INDArray b=Nd4j.rand(new long[]{4});
    SDVariable sdInput=sameDiff.var("input",input);
    SDVariable sdWeights=sameDiff.var("weights",weights);
    SDVariable sdBias=sameDiff.var("bias",b);
    SDVariable res=sameDiff.reluLayer(sdInput,sdWeights,sdBias);
    SDVariable loss=sameDiff.standardDeviation(res,true);
    INDArray exp=input.mmul(weights).addiRowVector(b);
    Transforms.relu(exp,false);
    TestCase tc=new TestCase(sameDiff).gradientCheck(true).expectedOutput(res.getVarName(),exp);
    String err=OpValidation.validate(tc);
    assertNull(err);
  }
  @Test public void testBiasAdd(){
    Nd4j.getRandom().setSeed(12345);
    for (    boolean rank1Bias : new boolean[]{false,true}) {
      SameDiff sameDiff=SameDiff.create();
      INDArray input=Nd4j.linspace(1,8,8).reshape(new long[]{2,4});
      INDArray b=Nd4j.linspace(1,4,4).reshape(rank1Bias ? new long[]{4} : new long[]{1,4}).divi(4);
      SDVariable sdInput=sameDiff.var("input",input);
      SDVariable sdBias=sameDiff.var("bias",b);
      SDVariable res=sameDiff.biasAdd(sdInput,sdBias);
      SDVariable loss=sameDiff.standardDeviation(res,true);
      INDArray exp=input.addRowVector(b);
      TestCase tc=new TestCase(sameDiff).gradientCheck(true).expectedOutput(res.getVarName(),exp);
      String err=OpValidation.validate(tc);
      assertNull(err);
    }
  }
  @Test public void testConv2d(){
    Nd4j.getRandom().setSeed(12345);
    int[][] inputSizes=new int[][]{{1,3,8,8}};
    List<String> failed=new ArrayList<>();
    for (int i=0; i < 8; i++) {
      for (      int[] inSizeNCHW : inputSizes) {
        SameDiff sd=SameDiff.create();
        SDVariable in=null;
        int[] inSize;
        SDVariable out;
        String msg;
switch (i) {
case 0:
          msg="0 - conv2d+bias, nchw - input " + Arrays.toString(inSizeNCHW);
        inSize=inSizeNCHW;
      in=sd.var("in",inSize);
    SDVariable w0=sd.var("w0",Nd4j.rand(new int[]{3,3,inSizeNCHW[1],3}).muli(10));
  SDVariable b0=sd.var("b0",Nd4j.rand(new long[]{3}).muli(10));
out=sd.conv2d(in,w0,b0,Conv2DConfig.builder().dataFormat(Conv2DConfig.NCHW).isSameMode(true).kH(3).kW(3).sH(1).sW(1).build());
break;
case 1:
msg="1 - conv2d+bias, nhwc - input " + Arrays.toString(inSizeNCHW);
inSize=nchwToNhwc(inSizeNCHW);
in=sd.var("in",inSize);
SDVariable w1=sd.var("w1",Nd4j.rand(new int[]{2,4,inSizeNCHW[1],3}).muli(10));
SDVariable b1=sd.var("b1",Nd4j.rand(new long[]{3}).muli(10));
out=sd.conv2d(in,w1,b1,Conv2DConfig.builder().dataFormat(Conv2DConfig.NHWC).isSameMode(false).kH(2).kW(4).sH(2).sW(2).build());
break;
case 2:
msg="2 - conv2d, no bias, nchw - input " + Arrays.toString(inSizeNCHW);
inSize=inSizeNCHW;
in=sd.var("in",inSize);
SDVariable w2=sd.var("w0",Nd4j.rand(new int[]{1,3,inSizeNCHW[1],3}).muli(10));
out=sd.conv2d(in,w2,Conv2DConfig.builder().dataFormat(Conv2DConfig.NCHW).isSameMode(true).kH(1).kW(3).sH(1).sW(2).build());
break;
case 3:
msg="3 - avg pool, NCHW, same - input " + Arrays.toString(inSizeNCHW);
inSize=inSizeNCHW;
in=sd.var("in",inSize);
out=sd.avgPooling2d(in,Pooling2DConfig.builder().isNHWC(true).isSameMode(true).kH(2).kW(2).sH(1).sW(1).build());
break;
case 4:
msg="3 - avg pool, NHWC, not same - input " + Arrays.toString(inSizeNCHW);
inSize=nchwToNhwc(inSizeNCHW);
in=sd.var("in",inSize);
out=sd.avgPooling2d(in,Pooling2DConfig.builder().isNHWC(true).isSameMode(false).kH(3).kW(2).sH(2).sW(2).build());
break;
case 5:
msg="5 - avg pool, NCHW, same - input " + Arrays.toString(inSizeNCHW);
inSize=inSizeNCHW;
in=sd.var("in",inSize);
out=sd.maxPooling2d(in,Pooling2DConfig.builder().isNHWC(false).isSameMode(true).kH(2).kW(2).sH(1).sW(1).build());
break;
case 6:
msg="6 - avg pool, NHWC, not same - input " + Arrays.toString(inSizeNCHW);
inSize=inSizeNCHW;
in=sd.var("in",inSize);
out=sd.maxPooling2d(in,Pooling2DConfig.builder().isNHWC(true).isSameMode(false).kH(3).kW(2).sH(2).sW(2).build());
break;
case 7:
msg="7 - upsampling2d, NCHW, 2x2 - " + Arrays.toString(inSizeNCHW);
inSize=inSizeNCHW;
in=sd.var("in",inSize);
out=sd.upsampling2d(in,true,2,2);
break;
default :
throw new RuntimeException();
}
INDArray inArr=Nd4j.rand(inSize).muli(10);
in.setArray(inArr);
SDVariable loss=sd.standardDeviation("loss",out,true);
log.info("Starting test: " + msg);
TestCase tc=new TestCase(sd);
String error=OpValidation.validate(tc);
if (error != null) {
failed.add(msg);
}
}
}
assertEquals(failed.toString(),0,failed.size());
}
@Test public void testLrn2d(){
OpValidationSuite.ignoreFailing();
Nd4j.getRandom().setSeed(12345);
int[][] inputSizes=new int[][]{{1,3,8,8},{3,6,12,12}};
List<String> failed=new ArrayList<>();
for (int[] inSizeNCHW : inputSizes) {
SameDiff sd=SameDiff.create();
SDVariable in=null;
int[] inSize;
String msg="7 - LRN with NCHW - input" + Arrays.toString(inSizeNCHW);
inSize=inSizeNCHW;
in=sd.var("in",inSize);
SDVariable out=sd.localResponseNormalization(in,LocalResponseNormalizationConfig.builder().depth(3).bias(1).alpha(1).beta(0.5).build());
INDArray inArr=Nd4j.rand(inSize).muli(10);
in.setArray(inArr);
SDVariable loss=sd.standardDeviation("loss",out,true);
log.info("Starting test: " + msg);
TestCase tc=new TestCase(sd);
String error=OpValidation.validate(tc);
if (error != null) {
failed.add(msg);
}
}
assertEquals(failed.toString(),0,failed.size());
}
@Test public void testIm2Col(){
Nd4j.getRandom().setSeed(12345);
int[][] inputSizes=new int[][]{{1,3,8,8},{3,6,12,12}};
List<String> failed=new ArrayList<>();
for (int[] inSizeNCHW : inputSizes) {
SameDiff sd=SameDiff.create();
SDVariable var=sd.var("in",Nd4j.rand(inSizeNCHW));
SDVariable im2col=sd.im2Col(var,Conv2DConfig.builder().kH(2).kW(2).sH(1).sW(1).isSameMode(true).build());
SDVariable loss=sd.standardDeviation("loss",im2col,true);
String msg=Arrays.toString(inSizeNCHW);
TestCase tc=new TestCase(sd).testName(msg);
String error=OpValidation.validate(tc);
if (error != null) {
failed.add(msg);
}
}
assertEquals(failed.toString(),0,failed.size());
}
private static int[] nchwToNhwc(int[] in){
return new int[]{in[0],in[2],in[3],in[1]};
}
@Test public void testOutputShape(){
long[] inSize={1,8,8,3};
SameDiff sd=SameDiff.create();
SDVariable in=sd.var("in",inSize);
Pooling2DConfig conf=Pooling2DConfig.builder().isNHWC(true).isSameMode(false).kH(3).kW(2).sH(2).sW(2).build();
INDArray input=Nd4j.create(inSize);
AvgPooling2D avgPooling2D=AvgPooling2D.builder().arrayInput(input).config(conf).build();
List<long[]> outSizes=Nd4j.getExecutioner().calculateOutputShape(avgPooling2D);
assertEquals(1,outSizes.size());
int outH=(8 - 3) / 2 + 1;
int outW=(8 - 2) / 2 + 1;
long[] exp=new long[]{1,outH,outW,3};
assertEquals(1,outSizes.size());
assertArrayEquals(exp,outSizes.get(0));
INDArray grad=Nd4j.create(exp);
Pooling2DDerivative avg2dDeriv=Pooling2DDerivative.derivativeBuilder().arrayInputs(new INDArray[]{input,grad}).config(conf).build();
List<long[]> outSizesBP=Nd4j.getExecutioner().calculateOutputShape(avg2dDeriv);
assertEquals(1,outSizesBP.size());
assertArrayEquals(inSize,outSizesBP.get(0));
}
@Test public void testAvgPool(){
long[] inSize={1,8,8,3};
Pooling2DConfig conf=Pooling2DConfig.builder().isNHWC(true).isSameMode(false).kH(3).kW(2).sH(2).sW(2).type(Pooling2D.Pooling2DType.AVG).build();
INDArray input=Nd4j.create(inSize);
AvgPooling2D avgPooling2D=AvgPooling2D.builder().arrayInput(input).config(conf).build();
List<long[]> outSizes=Nd4j.getExecutioner().calculateOutputShape(avgPooling2D);
assertEquals(1,outSizes.size());
int outH=(8 - 3) / 2 + 1;
int outW=(8 - 2) / 2 + 1;
long[] exp=new long[]{1,outH,outW,3};
assertEquals(1,outSizes.size());
assertArrayEquals(exp,outSizes.get(0));
INDArray grad=Nd4j.create(exp);
Pooling2DDerivative avg2dDeriv=Pooling2DDerivative.derivativeBuilder().arrayInputs(new INDArray[]{input,grad}).arrayOutputs(new INDArray[]{Nd4j.create(inSize)}).config(conf).build();
List<long[]> outSizesBP=Nd4j.getExecutioner().calculateOutputShape(avg2dDeriv);
assertEquals(1,outSizesBP.size());
assertArrayEquals(inSize,outSizesBP.get(0));
Nd4j.getExecutioner().exec(avg2dDeriv);
}
private static int[] ncdhwToNdhwc(int[] in){
return new int[]{in[0],in[2],in[3],in[4],in[1]};
}
@Test public void testConv3d(){
Nd4j.getRandom().setSeed(12345);
int[][] inputSizes=new int[][]{{2,3,4,5,5}};
List<String> failed=new ArrayList<>();
for (int[] inSizeNCDHW : inputSizes) {
for (boolean ncdhw : new boolean[]{true,false}) {
int nIn=inSizeNCDHW[1];
int[] shape=(ncdhw ? inSizeNCDHW : ncdhwToNdhwc(inSizeNCDHW));
for (int i=0; i < 4; i++) {
SameDiff sd=SameDiff.create();
SDVariable in=sd.var("in",shape);
SDVariable out;
String msg;
switch (i) {
case 0:
msg="0 - conv3d+bias+same, ncdhw=" + ncdhw + " - input "+ Arrays.toString(shape);
SDVariable w0=sd.var("w0",Nd4j.rand(new int[]{2,2,2,nIn,3}).muli(10));
SDVariable b0=sd.var("b0",Nd4j.rand(new long[]{3}).muli(10));
out=sd.conv3d(in,w0,b0,Conv3DConfig.builder().dataFormat(ncdhw ? Conv3DConfig.NCDHW : Conv3DConfig.NDHWC).isSameMode(true).kH(2).kW(2).kD(2).sD(1).sH(1).sW(1).build());
break;
case 1:
msg="1 - conv3d+no bias+no same, ncdhw=" + ncdhw + " - input "+ Arrays.toString(shape);
SDVariable w1=sd.var("w1",Nd4j.rand(new int[]{2,2,2,nIn,3}).muli(10));
out=sd.conv3d(in,w1,Conv3DConfig.builder().dataFormat(ncdhw ? Conv3DConfig.NCDHW : Conv3DConfig.NDHWC).isSameMode(false).kH(2).kW(2).kD(2).sD(1).sH(1).sW(1).build());
break;
case 2:
msg="2 - pooling 3d, average, same";
out=sd.avgPooling3d(in,Pooling3DConfig.builder().kH(2).kW(2).kD(2).sH(1).sW(1).sD(1).isSameMode(false).build());
break;
case 3:
msg="3 - pooling 3d, max, no same";
out=sd.avgPooling3d(in,Pooling3DConfig.builder().kH(2).kW(2).kD(2).sH(1).sW(1).sD(1).isSameMode(true).build());
break;
case 4:
throw new RuntimeException("Batch norm test not yet implemented");
default :
throw new RuntimeException();
}
INDArray inArr=Nd4j.rand(shape).muli(10);
in.setArray(inArr);
SDVariable loss=sd.standardDeviation("loss",out,true);
log.info("Starting test: " + msg);
TestCase tc=new TestCase(sd);
tc.testName(msg);
String error=OpValidation.validate(tc);
if (error != null) {
failed.add(name);
}
}
}
}
assertEquals(failed.toString(),0,failed.size());
}
@Test public void testDepthWiseConv2dBasic(){
int nIn=3;
int depthWise=4;
int kH=2;
int kW=2;
int mb=3;
int imgH=28;
int imgW=28;
SameDiff sd=SameDiff.create();
INDArray depthWeightArr=Nd4j.create(kH,kW,nIn,depthWise);
INDArray bArr=Nd4j.create(1,depthWise * nIn);
INDArray inArr=Nd4j.create(mb,nIn,imgH,imgW);
SDVariable in=sd.var("in",inArr);
SDVariable dW=sd.var("dW",depthWeightArr);
SDVariable b=sd.var("b",bArr);
SDVariable[] vars=new SDVariable[]{in,dW,b};
Conv2DConfig c=Conv2DConfig.builder().kH(kH).kW(kW).pH(0).pW(0).sH(1).sW(1).dH(1).dW(1).isSameMode(false).build();
SDVariable out=sd.sconv2d(vars,c);
out=sd.tanh("out",out);
INDArray outArr=sd.execAndEndResult();
val outShape=outArr.shape();
assertArrayEquals(new long[]{mb,depthWise * nIn,27,27},outShape);
}
@Test public void testSeparableConv2dBasic(){
Nd4j.getRandom().setSeed(12345);
int nIn=2;
int nOut=3;
int kH=2;
int kW=2;
int mb=2;
int imgH=8;
int imgW=8;
int depthWise=3;
SameDiff sd=SameDiff.create();
INDArray depthWeightArr=Nd4j.rand(new int[]{kH,kW,nIn,depthWise});
INDArray pointWeightArr=Nd4j.rand(new int[]{1,1,nIn * depthWise,nOut});
INDArray bArr=Nd4j.rand(new int[]{nOut});
INDArray inArr=Nd4j.rand(new int[]{mb,nIn,imgH,imgW});
SDVariable in=sd.var("in",inArr);
SDVariable dW=sd.var("dW",depthWeightArr);
SDVariable pW=sd.var("pW",pointWeightArr);
SDVariable b=sd.var("b",bArr);
SDVariable[] vars=new SDVariable[]{in,dW,pW,b};
Conv2DConfig c=Conv2DConfig.builder().kH(kH).kW(kW).pH(0).pW(0).sH(1).sW(1).dH(1).dW(1).isSameMode(false).dataFormat(Conv2DConfig.NCHW).build();
SDVariable out=sd.sconv2d(vars,c);
out=sd.tanh("out",out);
INDArray outArr=sd.execAndEndResult();
val outShape=outArr.shape();
assertArrayEquals(new long[]{mb,nOut,7,7},outShape);
SDVariable loss=out.std(true);
TestCase tc=new TestCase(sd);
String err=OpValidation.validate(tc);
assertNull(err);
}
@Test public void testDeconv2dBasic(){
int nIn=2;
int nOut=3;
int kH=2;
int kW=2;
int mb=2;
int imgH=8;
int imgW=8;
SameDiff sd=SameDiff.create();
INDArray wArr=Nd4j.rand(new int[]{kH,kW,nOut,nIn});
INDArray bArr=Nd4j.rand(new long[]{nOut});
INDArray inArr=Nd4j.rand(new long[]{mb,nIn,imgH,imgW});
SDVariable in=sd.var("in",inArr);
SDVariable w=sd.var("W",wArr);
SDVariable b=sd.var("b",bArr);
SDVariable[] vars=new SDVariable[]{in,w,b};
DeConv2DConfig deconv=DeConv2DConfig.builder().kH(kH).kW(kW).pH(0).pW(0).sH(1).sW(1).dH(1).dW(1).isSameMode(false).build();
SDVariable out=sd.deconv2d(vars,deconv);
out=sd.tanh("out",out);
INDArray outArr=sd.execAndEndResult();
val outShape=outArr.shape();
assertArrayEquals(new long[]{mb,nOut,9,9},outShape);
SDVariable loss=out.std(true);
TestCase tc=new TestCase(sd);
String err=OpValidation.validate(tc);
assertNull(err);
}
@Test public void testConv2dBasic(){
int nIn=3;
int nOut=4;
int kH=2;
int kW=2;
int mb=3;
int imgH=28;
int imgW=28;
SameDiff sd=SameDiff.create();
INDArray wArr=Nd4j.create(kH,kW,nIn,nOut);
INDArray bArr=Nd4j.create(1,nOut);
INDArray inArr=Nd4j.create(mb,nIn,imgH,imgW);
SDVariable in=sd.var("in",inArr);
SDVariable w=sd.var("W",wArr);
SDVariable b=sd.var("b",bArr);
SDVariable[] vars=new SDVariable[]{in,w,b};
Conv2DConfig c=Conv2DConfig.builder().kH(kH).kW(kW).pH(0).pW(0).sH(1).sW(1).dH(1).dW(1).isSameMode(false).build();
SDVariable out=sd.conv2d(vars,c);
out=sd.tanh("out",out);
INDArray outArr=sd.execAndEndResult();
val outShape=outArr.shape();
assertArrayEquals(new long[]{mb,nOut,27,27},outShape);
}
@Test public void testMaxPooling2dBasic(){
Nd4j.getRandom().setSeed(12345);
int nIn=3;
int kH=2;
int kW=2;
int mb=3;
int imgH=8;
int imgW=8;
SameDiff sd=SameDiff.create();
INDArray inArr=Nd4j.rand(new int[]{mb,nIn,imgH,imgW});
SDVariable in=sd.var("in",inArr);
Pooling2DConfig pooling2DConfig=Pooling2DConfig.builder().kH(kH).kW(kW).pH(0).pW(0).sH(1).sW(1).dH(1).dW(1).isSameMode(false).build();
SDVariable outPool=sd.maxPooling2d(in,pooling2DConfig);
SDVariable out=sd.tanh("out",outPool);
INDArray outArr=sd.execAndEndResult();
val outShape=outArr.shape();
assertArrayEquals(new long[]{mb,nIn,7,7},outShape);
SDVariable loss=out.std(true);
INDArray exp=Nd4j.create(mb,nIn,7,7);
NdIndexIterator iter=new NdIndexIterator(mb,nIn,7,7);
while (iter.hasNext()) {
long[] next=iter.next();
double max=max(inArr.getDouble(next),inArr.getDouble(next[0],next[1],next[2] + 1,next[3]),inArr.getDouble(next[0],next[1],next[2],next[3] + 1),inArr.getDouble(next[0],next[1],next[2] + 1,next[3] + 1));
exp.putScalar(next,max);
}
assertNull(OpValidation.validate(new TestCase(sd).expected(outPool,exp)));
}
private double max(double... in){
double max=-Double.MAX_VALUE;
for (double d : in) {
if (d > max) max=d;
}
return max;
}
@Test public void testAvgPooling2dBasic(){
Nd4j.getRandom().setSeed(12345);
int nIn=3;
int kH=2;
int kW=2;
int mb=3;
int imgH=8;
int imgW=8;
SameDiff sd=SameDiff.create();
INDArray inArr=Nd4j.rand(new int[]{mb,nIn,imgH,imgW});
SDVariable in=sd.var("in",inArr);
Pooling2DConfig pooling2DConfig=Pooling2DConfig.builder().kH(kH).kW(kW).pH(0).pW(0).sH(1).sW(1).dH(1).dW(1).isSameMode(false).build();
SDVariable outPool=sd.avgPooling2d(in,pooling2DConfig);
SDVariable out=sd.tanh("out",outPool);
INDArray outArr=sd.execAndEndResult();
val outShape=outArr.shape();
assertArrayEquals(new long[]{mb,nIn,7,7},outShape);
SDVariable loss=out.std(true);
INDArray exp=Nd4j.create(mb,nIn,7,7);
NdIndexIterator iter=new NdIndexIterator(mb,nIn,7,7);
while (iter.hasNext()) {
long[] next=iter.next();
double avg=(inArr.getDouble(next) + inArr.getDouble(next[0],next[1],next[2] + 1,next[3]) + inArr.getDouble(next[0],next[1],next[2],next[3] + 1)+ inArr.getDouble(next[0],next[1],next[2] + 1,next[3] + 1)) / 4.0;
exp.putScalar(next,avg);
}
assertNull(OpValidation.validate(new TestCase(sd).expected(outPool,exp)));
}
@Test public void testAvgPooling3dBasic(){
int nIn=3;
int kH=2;
int kW=2;
int kD=2;
int mb=3;
int imgH=8;
int imgW=8;
int imgD=8;
SameDiff sd=SameDiff.create();
INDArray inArr=Nd4j.rand(new long[]{mb,nIn,imgD,imgH,imgW});
SDVariable in=sd.var("in",inArr);
Pooling3DConfig pooling3DConfig=Pooling3DConfig.builder().kH(kH).kW(kW).kD(kD).pH(0).pW(0).pD(0).sH(1).sW(1).sD(1).dH(1).dW(1).dD(1).isSameMode(false).isNCDHW(true).build();
SDVariable out=sd.avgPooling3d(in,pooling3DConfig);
out=sd.tanh("out",out);
INDArray outArr=sd.execAndEndResult();
val outShape=outArr.shape();
assertArrayEquals(new long[]{mb,nIn,7,7,7},outShape);
SDVariable loss=out.std(true);
TestCase tc=new TestCase(sd);
String err=OpValidation.validate(tc);
assertNull(err);
}
@Test public void testMaxPooling3dBasic(){
int nIn=3;
int kH=2;
int kW=2;
int kD=2;
int mb=3;
int imgH=28;
int imgW=28;
int imgD=28;
SameDiff sd=SameDiff.create();
INDArray inArr=Nd4j.create(mb,nIn,imgD,imgH,imgW);
SDVariable in=sd.var("in",inArr);
Pooling3DConfig pooling3DConfig=Pooling3DConfig.builder().kH(kH).kW(kW).kD(kD).pH(0).pW(0).pD(0).sH(1).sW(1).sD(1).dH(1).dW(1).dD(1).isSameMode(false).build();
SDVariable out=sd.maxPooling3d(in,pooling3DConfig);
out=sd.tanh("out",out);
INDArray outArr=sd.execAndEndResult();
val outShape=outArr.shape();
assertArrayEquals(new long[]{mb,nIn,27,27,27},outShape);
}
@Test public void testConv1dBasic(){
int nIn=3;
int nOut=4;
int k=2;
int mb=3;
int img=28;
SameDiff sd=SameDiff.create();
INDArray wArr=Nd4j.create(k,nIn,nOut);
INDArray inArr=Nd4j.create(mb,nIn,img);
SDVariable in=sd.var("in",inArr);
SDVariable w=sd.var("W",wArr);
SDVariable[] vars=new SDVariable[]{in,w};
Conv1DConfig conv1DConfig=Conv1DConfig.builder().k(k).p(0).s(1).isSameMode(false).build();
SDVariable out=sd.conv1d(in,w,conv1DConfig);
out=sd.tanh("out",out);
INDArray outArr=sd.execAndEndResult();
INDArray iOut=out.getArr();
val outShape=outArr.shape();
assertArrayEquals(new long[]{mb,nOut,27},outShape);
}
@Test public void testConv3dBasic(){
int nIn=3;
int nOut=4;
int kH=2;
int kW=2;
int kD=2;
int mb=3;
int imgH=8;
int imgW=8;
int imgT=8;
SameDiff sd=SameDiff.create();
INDArray wArr=Nd4j.rand(new int[]{kD,kH,kW,nIn,nOut});
INDArray bArr=Nd4j.rand(1,nOut);
INDArray inArr=Nd4j.rand(new int[]{mb,nIn,imgT,imgH,imgW});
SDVariable in=sd.var("in",inArr);
SDVariable w=sd.var("W",wArr);
SDVariable b=sd.var("b",bArr);
Conv3DConfig conv3DConfig=Conv3DConfig.builder().kH(kH).kW(kW).kD(kD).sD(1).sH(1).sW(1).dH(1).dW(1).dD(1).isSameMode(true).biasUsed(false).dataFormat(Conv3DConfig.NCDHW).build();
SDVariable out=sd.conv3d(in,w,b,conv3DConfig);
out=sd.tanh("out",out);
INDArray outArr=sd.execAndEndResult();
val outShape=outArr.shape();
assertArrayEquals(new long[]{mb,nOut,8,8,8},outShape);
SDVariable loss=out.std(true);
TestCase tc=new TestCase(sd);
String err=OpValidation.validate(tc);
assertNull(err);
}
}
