public class LeaderChangeStateCleanupTest extends TestLogger {
  private static Logger LOG=LoggerFactory.getLogger(LeaderChangeStateCleanupTest.class);
  private static FiniteDuration timeout=TestingUtils.TESTING_DURATION();
  private int numJMs=2;
  private int numTMs=2;
  private int numSlotsPerTM=2;
  private int parallelism=numTMs * numSlotsPerTM;
  private JobID jobId;
  private Configuration configuration;
  private TestingManualHighAvailabilityServices highAvailabilityServices;
  private TestingCluster cluster=null;
  private JobGraph job=createBlockingJob(parallelism);
  @Before public void before() throws Exception {
    jobId=HighAvailabilityServices.DEFAULT_JOB_ID;
    Tasks.BlockingOnceReceiver$.MODULE$.blocking_$eq(true);
    configuration=new Configuration();
    configuration.setInteger(ConfigConstants.LOCAL_NUMBER_JOB_MANAGER,numJMs);
    configuration.setInteger(ConfigConstants.LOCAL_NUMBER_TASK_MANAGER,numTMs);
    configuration.setInteger(TaskManagerOptions.NUM_TASK_SLOTS,numSlotsPerTM);
    highAvailabilityServices=new TestingManualHighAvailabilityServices();
    cluster=new TestingCluster(configuration,highAvailabilityServices,true,false);
    cluster.start(false);
    cluster.waitForActorsToBeAlive();
  }
  @After public void after(){
    if (cluster != null) {
      cluster.stop();
    }
  }
  /** 
 * Tests that a job is properly canceled in the case of a leader change. In such an event all TaskManagers have to disconnect from the previous leader and connect to the newly elected leader.
 */
  @Test public void testStateCleanupAfterNewLeaderElectionAndListenerNotification() throws Exception {
    UUID leaderSessionID1=UUID.randomUUID();
    UUID leaderSessionID2=UUID.randomUUID();
    highAvailabilityServices.grantLeadership(jobId,0,leaderSessionID1);
    highAvailabilityServices.notifyRetrievers(jobId,0,leaderSessionID1);
    cluster.waitForTaskManagersToBeRegistered(timeout);
    cluster.submitJobDetached(job);
    ActorGateway jm=cluster.getLeaderGateway(timeout);
    Future<Object> wait=jm.ask(new WaitForAllVerticesToBeRunningOrFinished(job.getJobID()),timeout);
    Await.ready(wait,timeout);
    Future<Object> jobRemoval=jm.ask(new NotifyWhenJobRemoved(job.getJobID()),timeout);
    highAvailabilityServices.grantLeadership(jobId,1,leaderSessionID2);
    highAvailabilityServices.notifyRetrievers(jobId,1,leaderSessionID2);
    Await.ready(jobRemoval,timeout);
    cluster.waitForTaskManagersToBeRegistered(timeout);
    ActorGateway jm2=cluster.getLeaderGateway(timeout);
    Future<Object> futureNumberSlots=jm2.ask(JobManagerMessages.getRequestTotalNumberOfSlots(),timeout);
    int numberSlots=(Integer)Await.result(futureNumberSlots,timeout);
    assertEquals(parallelism,numberSlots);
    Tasks.BlockingOnceReceiver$.MODULE$.blocking_$eq(false);
    cluster.submitJobAndWait(job,false,timeout);
  }
  /** 
 * Tests that a job is properly canceled in the case of a leader change. However, this time only the JMs are notified about the leader change and the TMs still believe the old leader to have leadership.
 */
  @Test public void testStateCleanupAfterNewLeaderElection() throws Exception {
    UUID leaderSessionID=UUID.randomUUID();
    UUID newLeaderSessionID=UUID.randomUUID();
    highAvailabilityServices.grantLeadership(jobId,0,leaderSessionID);
    highAvailabilityServices.notifyRetrievers(jobId,0,leaderSessionID);
    cluster.waitForTaskManagersToBeRegistered(timeout);
    cluster.submitJobDetached(job);
    ActorGateway jm=cluster.getLeaderGateway(timeout);
    Future<Object> wait=jm.ask(new WaitForAllVerticesToBeRunningOrFinished(job.getJobID()),timeout);
    Await.ready(wait,timeout);
    Future<Object> jobRemoval=jm.ask(new NotifyWhenJobRemoved(job.getJobID()),timeout);
    highAvailabilityServices.grantLeadership(jobId,1,newLeaderSessionID);
    Await.ready(jobRemoval,timeout);
  }
  /** 
 * Tests that a job is properly canceled in the event of a leader change. However, this time only the TMs are notified about the changing leader. This should be enough to cancel the currently running job, though.
 */
  @Test public void testStateCleanupAfterListenerNotification() throws Exception {
    UUID leaderSessionID=UUID.randomUUID();
    UUID newLeaderSessionID=UUID.randomUUID();
    highAvailabilityServices.grantLeadership(jobId,0,leaderSessionID);
    highAvailabilityServices.notifyRetrievers(jobId,0,leaderSessionID);
    cluster.waitForTaskManagersToBeRegistered(timeout);
    cluster.submitJobDetached(job);
    ActorGateway jm=cluster.getLeaderGateway(timeout);
    Future<Object> wait=jm.ask(new WaitForAllVerticesToBeRunningOrFinished(job.getJobID()),timeout);
    Await.ready(wait,timeout);
    Future<Object> jobRemoval=jm.ask(new NotifyWhenJobRemoved(job.getJobID()),timeout);
    highAvailabilityServices.notifyRetrievers(jobId,1,newLeaderSessionID);
    Await.ready(jobRemoval,timeout);
  }
  /** 
 * Tests that the same JobManager can be reelected as the leader. Even though, the same JM is elected as the next leader, all currently running jobs should be canceled properly and all TMs should disconnect from the leader and then reconnect to it.
 */
  @Test public void testReelectionOfSameJobManager() throws Exception {
    UUID leaderSessionID=UUID.randomUUID();
    UUID newLeaderSessionID=UUID.randomUUID();
    FiniteDuration shortTimeout=new FiniteDuration(10,TimeUnit.SECONDS);
    highAvailabilityServices.grantLeadership(jobId,0,leaderSessionID);
    highAvailabilityServices.notifyRetrievers(jobId,0,leaderSessionID);
    cluster.waitForTaskManagersToBeRegistered(timeout);
    cluster.submitJobDetached(job);
    ActorGateway jm=cluster.getLeaderGateway(timeout);
    Future<Object> wait=jm.ask(new WaitForAllVerticesToBeRunningOrFinished(job.getJobID()),timeout);
    Await.ready(wait,timeout);
    Future<Object> jobRemoval=jm.ask(new NotifyWhenJobRemoved(job.getJobID()),timeout);
    LOG.info("Make JM(0) again the leader. This should first revoke the leadership.");
    highAvailabilityServices.grantLeadership(jobId,0,newLeaderSessionID);
    Await.ready(jobRemoval,timeout);
    LOG.info("Job removed.");
    try {
      cluster.waitForTaskManagersToBeRegistered(shortTimeout);
      fail("TaskManager should not be able to register at JobManager.");
    }
 catch (    TimeoutException e) {
    }
    LOG.info("Notify TMs about the new (old) leader.");
    highAvailabilityServices.notifyRetrievers(jobId,0,newLeaderSessionID);
    cluster.waitForTaskManagersToBeRegistered(timeout);
    ActorGateway leaderGateway=cluster.getLeaderGateway(timeout);
    Tasks.BlockingOnceReceiver$.MODULE$.blocking_$eq(false);
    cluster.submitJobAndWait(job,false,timeout);
  }
  public JobGraph createBlockingJob(  int parallelism){
    Tasks.BlockingOnceReceiver$.MODULE$.blocking_$eq(true);
    JobVertex sender=new JobVertex("sender");
    JobVertex receiver=new JobVertex("receiver");
    sender.setInvokableClass(Tasks.Sender.class);
    receiver.setInvokableClass(Tasks.BlockingOnceReceiver.class);
    sender.setParallelism(parallelism);
    receiver.setParallelism(parallelism);
    receiver.connectNewDataSetAsInput(sender,DistributionPattern.POINTWISE,ResultPartitionType.PIPELINED);
    SlotSharingGroup slotSharingGroup=new SlotSharingGroup();
    sender.setSlotSharingGroup(slotSharingGroup);
    receiver.setSlotSharingGroup(slotSharingGroup);
    return new JobGraph("Blocking test job",sender,receiver);
  }
}
