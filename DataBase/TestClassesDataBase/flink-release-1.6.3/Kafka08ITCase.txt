/** 
 * IT cases for Kafka 0.8 .
 */
public class Kafka08ITCase extends KafkaConsumerTestBase {
  @BeforeClass public static void prepare() throws ClassNotFoundException {
    prepare(false);
  }
  @Test(timeout=60000) public void testFailOnNoBroker() throws Exception {
    runFailOnNoBrokerTest();
  }
  @Test(timeout=60000) public void testConcurrentProducerConsumerTopology() throws Exception {
    runSimpleConcurrentProducerConsumerTopology();
  }
  @Test(timeout=60000) public void testKeyValueSupport() throws Exception {
    runKeyValueTest();
  }
  @Test(timeout=60000) public void testCancelingEmptyTopic() throws Exception {
    runCancelingOnEmptyInputTest();
  }
  @Test(timeout=60000) public void testCancelingFullTopic() throws Exception {
    runCancelingOnFullInputTest();
  }
  @Test(timeout=60000) public void testInvalidOffset() throws Exception {
    final int parallelism=1;
    final String topic=writeSequence("invalidOffsetTopic",20,parallelism,1);
    CuratorFramework curatorClient=((KafkaTestEnvironmentImpl)kafkaServer).createCuratorClient();
    ZookeeperOffsetHandler.setOffsetInZooKeeper(curatorClient,standardProps.getProperty("group.id"),topic,0,1234);
    curatorClient.close();
    final int valuesCount=20;
    final int startFrom=0;
    final StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment();
    env.getConfig().disableSysoutLogging();
    readSequence(env,StartupMode.GROUP_OFFSETS,null,null,standardProps,parallelism,topic,valuesCount,startFrom);
    deleteTestTopic(topic);
  }
  @Test(timeout=60000) public void testOneToOneSources() throws Exception {
    runOneToOneExactlyOnceTest();
  }
  @Test(timeout=60000) public void testOneSourceMultiplePartitions() throws Exception {
    runOneSourceMultiplePartitionsExactlyOnceTest();
  }
  @Test(timeout=60000) public void testMultipleSourcesOnePartition() throws Exception {
    runMultipleSourcesOnePartitionExactlyOnceTest();
  }
  @Test(timeout=60000) public void testBrokerFailure() throws Exception {
    runBrokerFailureTest();
  }
  @Test(timeout=60000) public void testStartFromEarliestOffsets() throws Exception {
    runStartFromEarliestOffsets();
  }
  @Test(timeout=60000) public void testStartFromLatestOffsets() throws Exception {
    runStartFromLatestOffsets();
  }
  @Test(timeout=60000) public void testStartFromGroupOffsets() throws Exception {
    runStartFromGroupOffsets();
  }
  @Test(timeout=60000) public void testStartFromSpecificOffsets() throws Exception {
    runStartFromSpecificOffsets();
  }
  @Test(timeout=60000) public void testCommitOffsetsToZookeeper() throws Exception {
    runCommitOffsetsToKafka();
  }
  @Test(timeout=60000) public void testAutoOffsetRetrievalAndCommitToZookeeper() throws Exception {
    runAutoOffsetRetrievalAndCommitToKafka();
  }
  @Test public void runOffsetManipulationInZooKeeperTest(){
    try {
      final String topicName="ZookeeperOffsetHandlerTest-Topic";
      final String groupId="ZookeeperOffsetHandlerTest-Group";
      final Long offset=(long)(Math.random() * Long.MAX_VALUE);
      CuratorFramework curatorFramework=((KafkaTestEnvironmentImpl)kafkaServer).createCuratorClient();
      kafkaServer.createTestTopic(topicName,3,2);
      ZookeeperOffsetHandler.setOffsetInZooKeeper(curatorFramework,groupId,topicName,0,offset);
      Long fetchedOffset=ZookeeperOffsetHandler.getOffsetFromZooKeeper(curatorFramework,groupId,topicName,0);
      curatorFramework.close();
      assertEquals(offset,fetchedOffset);
    }
 catch (    Exception e) {
      e.printStackTrace();
      fail(e.getMessage());
    }
  }
  @Test(timeout=60000) public void testOffsetAutocommitTest() throws Exception {
    final int parallelism=3;
    final String topicName=writeSequence("testOffsetAutocommit",100,parallelism,1);
    StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment();
    env.getConfig().disableSysoutLogging();
    env.getConfig().setRestartStrategy(RestartStrategies.noRestart());
    env.setParallelism(parallelism);
    Properties readProps=new Properties();
    readProps.putAll(standardProps);
    readProps.setProperty("auto.commit.enable","true");
    readProps.setProperty("auto.commit.interval.ms","500");
    readSequence(env,StartupMode.GROUP_OFFSETS,null,null,readProps,parallelism,topicName,100,0);
    CuratorFramework curatorFramework=((KafkaTestEnvironmentImpl)kafkaServer).createCuratorClient();
    Long o1=ZookeeperOffsetHandler.getOffsetFromZooKeeper(curatorFramework,standardProps.getProperty("group.id"),topicName,0);
    Long o2=ZookeeperOffsetHandler.getOffsetFromZooKeeper(curatorFramework,standardProps.getProperty("group.id"),topicName,1);
    Long o3=ZookeeperOffsetHandler.getOffsetFromZooKeeper(curatorFramework,standardProps.getProperty("group.id"),topicName,2);
    curatorFramework.close();
    LOG.info("Got final offsets from zookeeper o1={}, o2={}, o3={}",o1,o2,o3);
    boolean atLeastOneOffsetSet=(o1 != null && o1 > 0 && o1 <= 100) || (o2 != null && o2 > 0 && o2 <= 100) || (o3 != null && o3 > 0 && o3 <= 100);
    assertTrue("Expecting at least one offset to be set o1=" + o1 + " o2="+ o2+ " o3="+ o3,atLeastOneOffsetSet);
    deleteTestTopic(topicName);
  }
  @Test(timeout=60000) public void testBigRecordJob() throws Exception {
    runBigRecordTestTopology();
  }
  @Test(timeout=60000) public void testMultipleTopics() throws Exception {
    runProduceConsumeMultipleTopics();
  }
  @Test(timeout=60000) public void testAllDeletes() throws Exception {
    runAllDeletesTest();
  }
  @Test(timeout=60000) public void testEndOfStream() throws Exception {
    runEndOfStreamTest();
  }
  @Test(timeout=60000) public void testMetrics() throws Throwable {
    runMetricsTest();
  }
}
