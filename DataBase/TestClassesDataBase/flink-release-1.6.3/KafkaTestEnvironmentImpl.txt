/** 
 * An implementation of the KafkaServerProvider for Kafka 0.10 .
 */
public class KafkaTestEnvironmentImpl extends KafkaTestEnvironment {
  protected static final Logger LOG=LoggerFactory.getLogger(KafkaTestEnvironmentImpl.class);
  private File tmpZkDir;
  private File tmpKafkaParent;
  private List<File> tmpKafkaDirs;
  private List<KafkaServer> brokers;
  private TestingServer zookeeper;
  private String zookeeperConnectionString;
  private String brokerConnectionString="";
  private Properties standardProps;
  private Config config;
  private int zkTimeout=30000;
  public String getBrokerConnectionString(){
    return brokerConnectionString;
  }
  @Override public Properties getStandardProperties(){
    return standardProps;
  }
  @Override public Properties getSecureProperties(){
    Properties prop=new Properties();
    if (config.isSecureMode()) {
      prop.put("security.inter.broker.protocol","SASL_PLAINTEXT");
      prop.put("security.protocol","SASL_PLAINTEXT");
      prop.put("sasl.kerberos.service.name","kafka");
      prop.setProperty("zookeeper.session.timeout.ms",String.valueOf(zkTimeout));
      prop.setProperty("zookeeper.connection.timeout.ms",String.valueOf(zkTimeout));
      prop.setProperty("metadata.fetch.timeout.ms","120000");
    }
    return prop;
  }
  @Override public String getVersion(){
    return "0.10";
  }
  @Override public List<KafkaServer> getBrokers(){
    return brokers;
  }
  @Override public <T>FlinkKafkaConsumerBase<T> getConsumer(  List<String> topics,  KeyedDeserializationSchema<T> readSchema,  Properties props){
    return new FlinkKafkaConsumer010<>(topics,readSchema,props);
  }
  @Override public <K,V>Collection<ConsumerRecord<K,V>> getAllRecordsFromTopic(  Properties properties,  String topic,  int partition,  long timeout){
    List<ConsumerRecord<K,V>> result=new ArrayList<>();
    try (KafkaConsumer<K,V> consumer=new KafkaConsumer<>(properties)){
      consumer.assign(Arrays.asList(new TopicPartition(topic,partition)));
      while (true) {
        boolean processedAtLeastOneRecord=false;
        Iterator<ConsumerRecord<K,V>> iterator=consumer.poll(timeout).iterator();
        while (iterator.hasNext()) {
          ConsumerRecord<K,V> record=iterator.next();
          result.add(record);
          processedAtLeastOneRecord=true;
        }
        if (!processedAtLeastOneRecord) {
          break;
        }
      }
      consumer.commitSync();
    }
     return UnmodifiableList.decorate(result);
  }
  @Override public <T>StreamSink<T> getProducerSink(  String topic,  KeyedSerializationSchema<T> serSchema,  Properties props,  FlinkKafkaPartitioner<T> partitioner){
    FlinkKafkaProducer010<T> prod=new FlinkKafkaProducer010<>(topic,serSchema,props,partitioner);
    prod.setFlushOnCheckpoint(true);
    return new StreamSink<>(prod);
  }
  @Override public <T>DataStreamSink<T> produceIntoKafka(  DataStream<T> stream,  String topic,  KeyedSerializationSchema<T> serSchema,  Properties props,  FlinkKafkaPartitioner<T> partitioner){
    FlinkKafkaProducer010<T> prod=new FlinkKafkaProducer010<>(topic,serSchema,props,partitioner);
    prod.setFlushOnCheckpoint(true);
    return stream.addSink(prod);
  }
  @Override public <T>DataStreamSink<T> writeToKafkaWithTimestamps(  DataStream<T> stream,  String topic,  KeyedSerializationSchema<T> serSchema,  Properties props){
    FlinkKafkaProducer010<T> prod=new FlinkKafkaProducer010<>(topic,serSchema,props);
    prod.setFlushOnCheckpoint(true);
    prod.setWriteTimestampToKafka(true);
    return stream.addSink(prod);
  }
  @Override public KafkaOffsetHandler createOffsetHandler(){
    return new KafkaOffsetHandlerImpl();
  }
  @Override public void restartBroker(  int leaderId) throws Exception {
    brokers.set(leaderId,getKafkaServer(leaderId,tmpKafkaDirs.get(leaderId)));
  }
  @Override public int getLeaderToShutDown(  String topic) throws Exception {
    ZkUtils zkUtils=getZkUtils();
    try {
      MetadataResponse.PartitionMetadata firstPart=null;
      do {
        if (firstPart != null) {
          LOG.info("Unable to find leader. error code {}",firstPart.error().code());
          Thread.sleep(150);
        }
        List<MetadataResponse.PartitionMetadata> partitionMetadata=AdminUtils.fetchTopicMetadataFromZk(topic,zkUtils).partitionMetadata();
        firstPart=partitionMetadata.get(0);
      }
 while (firstPart.error().code() != 0);
      return firstPart.leader().id();
    }
  finally {
      zkUtils.close();
    }
  }
  @Override public int getBrokerId(  KafkaServer server){
    return server.config().brokerId();
  }
  @Override public boolean isSecureRunSupported(){
    return true;
  }
  @Override public void prepare(  Config config){
    if (config.isSecureMode()) {
      config.setKafkaServersNumber(1);
      zkTimeout=zkTimeout * 15;
    }
    this.config=config;
    File tempDir=new File(System.getProperty("java.io.tmpdir"));
    tmpZkDir=new File(tempDir,"kafkaITcase-zk-dir-" + (UUID.randomUUID().toString()));
    assertTrue("cannot create zookeeper temp dir",tmpZkDir.mkdirs());
    tmpKafkaParent=new File(tempDir,"kafkaITcase-kafka-dir-" + (UUID.randomUUID().toString()));
    assertTrue("cannot create kafka temp dir",tmpKafkaParent.mkdirs());
    tmpKafkaDirs=new ArrayList<>(config.getKafkaServersNumber());
    for (int i=0; i < config.getKafkaServersNumber(); i++) {
      File tmpDir=new File(tmpKafkaParent,"server-" + i);
      assertTrue("cannot create kafka temp dir",tmpDir.mkdir());
      tmpKafkaDirs.add(tmpDir);
    }
    zookeeper=null;
    brokers=null;
    try {
      zookeeper=new TestingServer(-1,tmpZkDir);
      zookeeperConnectionString=zookeeper.getConnectString();
      LOG.info("Starting Zookeeper with zookeeperConnectionString: {}",zookeeperConnectionString);
      LOG.info("Starting KafkaServer");
      brokers=new ArrayList<>(config.getKafkaServersNumber());
      ListenerName listenerName=ListenerName.forSecurityProtocol(config.isSecureMode() ? SecurityProtocol.SASL_PLAINTEXT : SecurityProtocol.PLAINTEXT);
      for (int i=0; i < config.getKafkaServersNumber(); i++) {
        KafkaServer kafkaServer=getKafkaServer(i,tmpKafkaDirs.get(i));
        brokers.add(kafkaServer);
        brokerConnectionString+=hostAndPortToUrlString(KAFKA_HOST,kafkaServer.socketServer().boundPort(listenerName));
        brokerConnectionString+=",";
      }
      LOG.info("ZK and KafkaServer started.");
    }
 catch (    Throwable t) {
      t.printStackTrace();
      fail("Test setup failed: " + t.getMessage());
    }
    standardProps=new Properties();
    standardProps.setProperty("zookeeper.connect",zookeeperConnectionString);
    standardProps.setProperty("bootstrap.servers",brokerConnectionString);
    standardProps.setProperty("group.id","flink-tests");
    standardProps.setProperty("enable.auto.commit","false");
    standardProps.setProperty("zookeeper.session.timeout.ms",String.valueOf(zkTimeout));
    standardProps.setProperty("zookeeper.connection.timeout.ms",String.valueOf(zkTimeout));
    standardProps.setProperty("auto.offset.reset","earliest");
    standardProps.setProperty("max.partition.fetch.bytes","256");
  }
  @Override public void shutdown() throws Exception {
    for (    KafkaServer broker : brokers) {
      if (broker != null) {
        broker.shutdown();
      }
    }
    brokers.clear();
    if (zookeeper != null) {
      try {
        zookeeper.stop();
      }
 catch (      Exception e) {
        LOG.warn("ZK.stop() failed",e);
      }
      zookeeper=null;
    }
    if (tmpKafkaParent != null && tmpKafkaParent.exists()) {
      try {
        FileUtils.deleteDirectory(tmpKafkaParent);
      }
 catch (      Exception e) {
      }
    }
    if (tmpZkDir != null && tmpZkDir.exists()) {
      try {
        FileUtils.deleteDirectory(tmpZkDir);
      }
 catch (      Exception e) {
      }
    }
    super.shutdown();
  }
  public ZkUtils getZkUtils(){
    ZkClient creator=new ZkClient(zookeeperConnectionString,Integer.valueOf(standardProps.getProperty("zookeeper.session.timeout.ms")),Integer.valueOf(standardProps.getProperty("zookeeper.connection.timeout.ms")),new ZooKeeperStringSerializer());
    return ZkUtils.apply(creator,false);
  }
  @Override public void createTestTopic(  String topic,  int numberOfPartitions,  int replicationFactor,  Properties topicConfig){
    LOG.info("Creating topic {}",topic);
    ZkUtils zkUtils=getZkUtils();
    try {
      AdminUtils.createTopic(zkUtils,topic,numberOfPartitions,replicationFactor,topicConfig,kafka.admin.RackAwareMode.Enforced$.MODULE$);
    }
  finally {
      zkUtils.close();
    }
    final long deadline=System.nanoTime() + 30_000_000_000L;
    do {
      try {
        if (config.isSecureMode()) {
          int wait=zkTimeout / 100;
          LOG.info("waiting for {} msecs before the topic {} can be checked",wait,topic);
          Thread.sleep(wait);
        }
 else {
          Thread.sleep(100);
        }
      }
 catch (      InterruptedException e) {
      }
      ZkUtils checkZKConn=getZkUtils();
      if (AdminUtils.topicExists(checkZKConn,topic)) {
        checkZKConn.close();
        return;
      }
      checkZKConn.close();
    }
 while (System.nanoTime() < deadline);
    fail("Test topic could not be created");
  }
  @Override public void deleteTestTopic(  String topic){
    ZkUtils zkUtils=getZkUtils();
    try {
      LOG.info("Deleting topic {}",topic);
      ZkClient zk=new ZkClient(zookeeperConnectionString,Integer.valueOf(standardProps.getProperty("zookeeper.session.timeout.ms")),Integer.valueOf(standardProps.getProperty("zookeeper.connection.timeout.ms")),new ZooKeeperStringSerializer());
      AdminUtils.deleteTopic(zkUtils,topic);
      zk.close();
    }
  finally {
      zkUtils.close();
    }
  }
  /** 
 * Copied from com.github.sakserv.minicluster.KafkaLocalBrokerIntegrationTest (ASL licensed).
 */
  protected KafkaServer getKafkaServer(  int brokerId,  File tmpFolder) throws Exception {
    Properties kafkaProperties=new Properties();
    kafkaProperties.put("advertised.host.name",KAFKA_HOST);
    kafkaProperties.put("broker.id",Integer.toString(brokerId));
    kafkaProperties.put("log.dir",tmpFolder.toString());
    kafkaProperties.put("zookeeper.connect",zookeeperConnectionString);
    kafkaProperties.put("message.max.bytes",String.valueOf(50 * 1024 * 1024));
    kafkaProperties.put("replica.fetch.max.bytes",String.valueOf(50 * 1024 * 1024));
    kafkaProperties.put("zookeeper.session.timeout.ms",zkTimeout);
    kafkaProperties.put("zookeeper.connection.timeout.ms",zkTimeout);
    if (config.getKafkaServerProperties() != null) {
      kafkaProperties.putAll(config.getKafkaServerProperties());
    }
    final int numTries=5;
    for (int i=1; i <= numTries; i++) {
      int kafkaPort=NetUtils.getAvailablePort();
      kafkaProperties.put("port",Integer.toString(kafkaPort));
      if (config.isHideKafkaBehindProxy()) {
        NetworkFailuresProxy proxy=createProxy(KAFKA_HOST,kafkaPort);
        kafkaProperties.put("advertised.port",proxy.getLocalPort());
      }
      if (config.isSecureMode()) {
        LOG.info("Adding Kafka secure configurations");
        kafkaProperties.put("listeners","SASL_PLAINTEXT://" + KAFKA_HOST + ":"+ kafkaPort);
        kafkaProperties.put("advertised.listeners","SASL_PLAINTEXT://" + KAFKA_HOST + ":"+ kafkaPort);
        kafkaProperties.putAll(getSecureProperties());
      }
      KafkaConfig kafkaConfig=new KafkaConfig(kafkaProperties);
      try {
        scala.Option<String> stringNone=scala.Option.apply(null);
        KafkaServer server=new KafkaServer(kafkaConfig,Time.SYSTEM,stringNone,new ArraySeq<KafkaMetricsReporter>(0));
        server.startup();
        return server;
      }
 catch (      KafkaException e) {
        if (e.getCause() instanceof BindException) {
          LOG.info("Port conflict when starting Kafka Broker. Retrying...");
        }
 else {
          throw e;
        }
      }
    }
    throw new Exception("Could not start Kafka after " + numTries + " retries due to port conflicts.");
  }
private class KafkaOffsetHandlerImpl implements KafkaOffsetHandler {
    private final KafkaConsumer<byte[],byte[]> offsetClient;
    public KafkaOffsetHandlerImpl(){
      Properties props=new Properties();
      props.putAll(standardProps);
      props.setProperty("key.deserializer","org.apache.kafka.common.serialization.ByteArrayDeserializer");
      props.setProperty("value.deserializer","org.apache.kafka.common.serialization.ByteArrayDeserializer");
      offsetClient=new KafkaConsumer<>(props);
    }
    @Override public Long getCommittedOffset(    String topicName,    int partition){
      OffsetAndMetadata committed=offsetClient.committed(new TopicPartition(topicName,partition));
      return (committed != null) ? committed.offset() : null;
    }
    @Override public void setCommittedOffset(    String topicName,    int partition,    long offset){
      Map<TopicPartition,OffsetAndMetadata> partitionAndOffset=new HashMap<>();
      partitionAndOffset.put(new TopicPartition(topicName,partition),new OffsetAndMetadata(offset));
      offsetClient.commitSync(partitionAndOffset);
    }
    @Override public void close(){
      offsetClient.close();
    }
  }
}
