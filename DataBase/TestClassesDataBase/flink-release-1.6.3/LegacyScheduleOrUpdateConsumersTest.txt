public class LegacyScheduleOrUpdateConsumersTest extends TestLogger {
  private static final int NUMBER_OF_TMS=2;
  private static final int NUMBER_OF_SLOTS_PER_TM=2;
  private static final int PARALLELISM=NUMBER_OF_TMS * NUMBER_OF_SLOTS_PER_TM;
  private static TestingCluster flink;
  @BeforeClass public static void setUp() throws Exception {
    flink=TestingUtils.startTestingCluster(NUMBER_OF_SLOTS_PER_TM,NUMBER_OF_TMS,TestingUtils.DEFAULT_AKKA_ASK_TIMEOUT());
  }
  @AfterClass public static void tearDown() throws Exception {
    flink.stop();
  }
  /** 
 * Tests notifications of multiple receivers when a task produces both a pipelined and blocking result. <pre> +----------+ +-- pipelined -> | Receiver | +--------+ |                +----------+ | Sender |-| +--------+ |                +----------+ +-- blocking --> | Receiver | +----------+ </pre> The pipelined receiver gets deployed after the first buffer is available and the blocking one after all subtasks are finished.
 */
  @Test public void testMixedPipelinedAndBlockingResults() throws Exception {
    final JobVertex sender=new JobVertex("Sender");
    sender.setInvokableClass(BinaryRoundRobinSubtaskIndexSender.class);
    sender.getConfiguration().setInteger(BinaryRoundRobinSubtaskIndexSender.CONFIG_KEY,PARALLELISM);
    sender.setParallelism(PARALLELISM);
    final JobVertex pipelinedReceiver=new JobVertex("Pipelined Receiver");
    pipelinedReceiver.setInvokableClass(SlotCountExceedingParallelismTest.SubtaskIndexReceiver.class);
    pipelinedReceiver.getConfiguration().setInteger(CONFIG_KEY,PARALLELISM);
    pipelinedReceiver.setParallelism(PARALLELISM);
    pipelinedReceiver.connectNewDataSetAsInput(sender,DistributionPattern.ALL_TO_ALL,ResultPartitionType.PIPELINED);
    final JobVertex blockingReceiver=new JobVertex("Blocking Receiver");
    blockingReceiver.setInvokableClass(SlotCountExceedingParallelismTest.SubtaskIndexReceiver.class);
    blockingReceiver.getConfiguration().setInteger(CONFIG_KEY,PARALLELISM);
    blockingReceiver.setParallelism(PARALLELISM);
    blockingReceiver.connectNewDataSetAsInput(sender,DistributionPattern.ALL_TO_ALL,ResultPartitionType.BLOCKING);
    SlotSharingGroup slotSharingGroup=new SlotSharingGroup(sender.getID(),pipelinedReceiver.getID(),blockingReceiver.getID());
    sender.setSlotSharingGroup(slotSharingGroup);
    pipelinedReceiver.setSlotSharingGroup(slotSharingGroup);
    blockingReceiver.setSlotSharingGroup(slotSharingGroup);
    final JobGraph jobGraph=new JobGraph("Mixed pipelined and blocking result",sender,pipelinedReceiver,blockingReceiver);
    flink.submitJobAndWait(jobGraph,false,TestingUtils.TESTING_DURATION());
  }
public static class BinaryRoundRobinSubtaskIndexSender extends AbstractInvokable {
    public static final String CONFIG_KEY="number-of-times-to-send";
    public BinaryRoundRobinSubtaskIndexSender(    Environment environment){
      super(environment);
    }
    @Override public void invoke() throws Exception {
      List<RecordWriter<IntValue>> writers=Lists.newArrayListWithCapacity(2);
      final RecordWriter<IntValue> pipelinedWriter=new RecordWriter<>(getEnvironment().getWriter(0));
      final RecordWriter<IntValue> blockingWriter=new RecordWriter<>(getEnvironment().getWriter(1));
      writers.add(pipelinedWriter);
      writers.add(blockingWriter);
      final int numberOfTimesToSend=getTaskConfiguration().getInteger(CONFIG_KEY,0);
      final IntValue subtaskIndex=new IntValue(getEnvironment().getTaskInfo().getIndexOfThisSubtask());
      for (      RecordWriter<IntValue> writer : writers) {
        try {
          for (int i=0; i < numberOfTimesToSend; i++) {
            writer.emit(subtaskIndex);
          }
          writer.flushAll();
        }
  finally {
          writer.clearBuffers();
        }
      }
    }
  }
}
