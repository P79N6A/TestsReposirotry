/** 
 * Abstract test base for  {@link KafkaTableSourceSinkFactoryBase}.
 */
public abstract class KafkaTableSourceSinkFactoryTestBase extends TestLogger {
  private static final String TOPIC="myTopic";
  private static final int PARTITION_0=0;
  private static final long OFFSET_0=100L;
  private static final int PARTITION_1=1;
  private static final long OFFSET_1=123L;
  private static final String FRUIT_NAME="fruit-name";
  private static final String NAME="name";
  private static final String COUNT="count";
  private static final String TIME="time";
  private static final String EVENT_TIME="event-time";
  private static final String PROC_TIME="proc-time";
  private static final Properties KAFKA_PROPERTIES=new Properties();
static {
    KAFKA_PROPERTIES.setProperty("zookeeper.connect","dummy");
    KAFKA_PROPERTIES.setProperty("group.id","dummy");
    KAFKA_PROPERTIES.setProperty("bootstrap.servers","dummy");
  }
  private static final Map<Integer,Long> OFFSETS=new HashMap<>();
static {
    OFFSETS.put(PARTITION_0,OFFSET_0);
    OFFSETS.put(PARTITION_1,OFFSET_1);
  }
  @Test @SuppressWarnings("unchecked") public void testTableSource(){
    final TableSchema schema=TableSchema.builder().field(FRUIT_NAME,Types.STRING()).field(COUNT,Types.DECIMAL()).field(EVENT_TIME,Types.SQL_TIMESTAMP()).field(PROC_TIME,Types.SQL_TIMESTAMP()).build();
    final List<RowtimeAttributeDescriptor> rowtimeAttributeDescriptors=Collections.singletonList(new RowtimeAttributeDescriptor(EVENT_TIME,new ExistingField(TIME),new AscendingTimestamps()));
    final Map<String,String> fieldMapping=new HashMap<>();
    fieldMapping.put(FRUIT_NAME,NAME);
    fieldMapping.put(NAME,NAME);
    fieldMapping.put(COUNT,COUNT);
    fieldMapping.put(TIME,TIME);
    final Map<KafkaTopicPartition,Long> specificOffsets=new HashMap<>();
    specificOffsets.put(new KafkaTopicPartition(TOPIC,PARTITION_0),OFFSET_0);
    specificOffsets.put(new KafkaTopicPartition(TOPIC,PARTITION_1),OFFSET_1);
    final TestDeserializationSchema deserializationSchema=new TestDeserializationSchema(TableSchema.builder().field(NAME,Types.STRING()).field(COUNT,Types.DECIMAL()).field(TIME,Types.SQL_TIMESTAMP()).build().toRowType());
    final KafkaTableSource expected=getExpectedKafkaTableSource(schema,Optional.of(PROC_TIME),rowtimeAttributeDescriptors,fieldMapping,TOPIC,KAFKA_PROPERTIES,deserializationSchema,StartupMode.SPECIFIC_OFFSETS,specificOffsets);
    TableSourceUtil.validateTableSource(expected);
    final TestTableDescriptor testDesc=new TestTableDescriptor(new Kafka().version(getKafkaVersion()).topic(TOPIC).properties(KAFKA_PROPERTIES).sinkPartitionerRoundRobin().startFromSpecificOffsets(OFFSETS)).withFormat(new TestTableFormat()).withSchema(new Schema().field(FRUIT_NAME,Types.STRING()).from(NAME).field(COUNT,Types.DECIMAL()).field(EVENT_TIME,Types.SQL_TIMESTAMP()).rowtime(new Rowtime().timestampsFromField(TIME).watermarksPeriodicAscending()).field(PROC_TIME,Types.SQL_TIMESTAMP()).proctime()).inAppendMode();
    final Map<String,String> propertiesMap=DescriptorProperties.toJavaMap(testDesc);
    final TableSource<?> actualSource=TableFactoryService.find(StreamTableSourceFactory.class,propertiesMap).createStreamTableSource(propertiesMap);
    assertEquals(expected,actualSource);
    final KafkaTableSource actualKafkaSource=(KafkaTableSource)actualSource;
    final StreamExecutionEnvironmentMock mock=new StreamExecutionEnvironmentMock();
    actualKafkaSource.getDataStream(mock);
    assertTrue(getExpectedFlinkKafkaConsumer().isAssignableFrom(mock.sourceFunction.getClass()));
  }
  /** 
 * This test can be unified with the corresponding source test once we have fixed FLINK-9870.
 */
  @Test public void testTableSink(){
    final TableSchema schema=TableSchema.builder().field(FRUIT_NAME,Types.STRING()).field(COUNT,Types.DECIMAL()).field(EVENT_TIME,Types.SQL_TIMESTAMP()).build();
    final KafkaTableSink expected=getExpectedKafkaTableSink(schema,TOPIC,KAFKA_PROPERTIES,Optional.of(new FlinkFixedPartitioner<>()),new TestSerializationSchema(schema.toRowType()));
    final TestTableDescriptor testDesc=new TestTableDescriptor(new Kafka().version(getKafkaVersion()).topic(TOPIC).properties(KAFKA_PROPERTIES).sinkPartitionerFixed().startFromSpecificOffsets(OFFSETS)).withFormat(new TestTableFormat()).withSchema(new Schema().field(FRUIT_NAME,Types.STRING()).field(COUNT,Types.DECIMAL()).field(EVENT_TIME,Types.SQL_TIMESTAMP())).inAppendMode();
    final Map<String,String> propertiesMap=DescriptorProperties.toJavaMap(testDesc);
    final TableSink<?> actualSink=TableFactoryService.find(StreamTableSinkFactory.class,propertiesMap).createStreamTableSink(propertiesMap);
    assertEquals(expected,actualSink);
    final KafkaTableSink actualKafkaSink=(KafkaTableSink)actualSink;
    final DataStreamMock streamMock=new DataStreamMock(new StreamExecutionEnvironmentMock(),schema.toRowType());
    actualKafkaSink.emitDataStream(streamMock);
    assertTrue(getExpectedFlinkKafkaProducer().isAssignableFrom(streamMock.sinkFunction.getClass()));
  }
private static class StreamExecutionEnvironmentMock extends StreamExecutionEnvironment {
    public SourceFunction<?> sourceFunction;
    @Override public <OUT>DataStreamSource<OUT> addSource(    SourceFunction<OUT> sourceFunction){
      this.sourceFunction=sourceFunction;
      return super.addSource(sourceFunction);
    }
    @Override public JobExecutionResult execute(    String jobName){
      throw new UnsupportedOperationException();
    }
  }
private static class DataStreamMock extends DataStream<Row> {
    public SinkFunction<?> sinkFunction;
    public DataStreamMock(    StreamExecutionEnvironment environment,    TypeInformation<Row> outType){
      super(environment,new StreamTransformationMock("name",outType,1));
    }
    @Override public DataStreamSink<Row> addSink(    SinkFunction<Row> sinkFunction){
      this.sinkFunction=sinkFunction;
      return super.addSink(sinkFunction);
    }
  }
private static class StreamTransformationMock extends StreamTransformation<Row> {
    public StreamTransformationMock(    String name,    TypeInformation<Row> outputType,    int parallelism){
      super(name,outputType,parallelism);
    }
    @Override public void setChainingStrategy(    ChainingStrategy strategy){
    }
    @Override public Collection<StreamTransformation<?>> getTransitivePredecessors(){
      return null;
    }
  }
  protected abstract String getKafkaVersion();
  protected abstract Class<FlinkKafkaConsumerBase<Row>> getExpectedFlinkKafkaConsumer();
  protected abstract Class<?> getExpectedFlinkKafkaProducer();
  protected abstract KafkaTableSource getExpectedKafkaTableSource(  TableSchema schema,  Optional<String> proctimeAttribute,  List<RowtimeAttributeDescriptor> rowtimeAttributeDescriptors,  Map<String,String> fieldMapping,  String topic,  Properties properties,  DeserializationSchema<Row> deserializationSchema,  StartupMode startupMode,  Map<KafkaTopicPartition,Long> specificStartupOffsets);
  protected abstract KafkaTableSink getExpectedKafkaTableSink(  TableSchema schema,  String topic,  Properties properties,  Optional<FlinkKafkaPartitioner<Row>> partitioner,  SerializationSchema<Row> serializationSchema);
}
