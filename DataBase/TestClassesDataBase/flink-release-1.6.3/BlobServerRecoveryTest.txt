/** 
 * Tests for the recovery of files of a  {@link BlobServer} from a HA store.
 */
public class BlobServerRecoveryTest extends TestLogger {
  @Rule public TemporaryFolder temporaryFolder=new TemporaryFolder();
  /** 
 * Tests that with  {@link HighAvailabilityMode#ZOOKEEPER} distributed JARs are recoverable from anyparticipating BlobServer.
 */
  @Test public void testBlobServerRecovery() throws Exception {
    Configuration config=new Configuration();
    config.setString(HighAvailabilityOptions.HA_MODE,"ZOOKEEPER");
    config.setString(BlobServerOptions.STORAGE_DIRECTORY,temporaryFolder.newFolder().getAbsolutePath());
    config.setString(HighAvailabilityOptions.HA_STORAGE_PATH,temporaryFolder.newFolder().getPath());
    BlobStoreService blobStoreService=null;
    try {
      blobStoreService=BlobUtils.createBlobStoreFromConfig(config);
      testBlobServerRecovery(config,blobStoreService);
    }
  finally {
      if (blobStoreService != null) {
        blobStoreService.closeAndCleanupAllData();
      }
    }
  }
  /** 
 * Helper to test that the  {@link BlobServer} recovery from its HA store works.<p>Uploads two BLOBs to one  {@link BlobServer} and expects a second one to be able to retrievethem via a shared HA store upon request of a  {@link BlobCacheService}.
 * @param config blob server configuration (including HA settings like  {@link HighAvailabilityOptions#HA_STORAGE_PATH}and  {@link HighAvailabilityOptions#HA_CLUSTER_ID}) used to set up <tt>blobStore</tt>
 * @param blobStore shared HA blob store to use
 * @throws IOException in case of failures
 */
  public static void testBlobServerRecovery(  final Configuration config,  final BlobStore blobStore) throws IOException {
    final String clusterId=config.getString(HighAvailabilityOptions.HA_CLUSTER_ID);
    String storagePath=config.getString(HighAvailabilityOptions.HA_STORAGE_PATH) + "/" + clusterId;
    Random rand=new Random();
    try (BlobServer server0=new BlobServer(config,blobStore);BlobServer server1=new BlobServer(config,blobStore);BlobCacheService cache1=new BlobCacheService(config,new VoidBlobStore(),new InetSocketAddress("localhost",server1.getPort()))){
      server0.start();
      server1.start();
      byte[] expected=new byte[1024];
      rand.nextBytes(expected);
      byte[] expected2=Arrays.copyOfRange(expected,32,288);
      BlobKey[] keys=new BlobKey[2];
      BlobKey nonHAKey;
      JobID[] jobId=new JobID[]{new JobID(),new JobID()};
      keys[0]=put(server0,jobId[0],expected,PERMANENT_BLOB);
      keys[1]=put(server0,jobId[1],expected2,PERMANENT_BLOB);
      nonHAKey=put(server0,jobId[0],expected2,TRANSIENT_BLOB);
      verifyKeyDifferentHashEquals(keys[1],nonHAKey);
      final Path blobServerPath=new Path(storagePath,"blob");
      FileSystem fs=blobServerPath.getFileSystem();
      assertTrue("Unknown storage dir: " + blobServerPath,fs.exists(blobServerPath));
      verifyContents(cache1,jobId[0],keys[0],expected);
      verifyContents(cache1,jobId[1],keys[1],expected2);
      verifyDeleted(cache1,jobId[0],nonHAKey);
      server1.cleanupJob(jobId[0],true);
      server1.cleanupJob(jobId[1],true);
      assertTrue("HA storage directory does not exist",fs.exists(new Path(storagePath)));
      if (fs.exists(blobServerPath)) {
        final org.apache.flink.core.fs.FileStatus[] recoveryFiles=fs.listStatus(blobServerPath);
        ArrayList<String> filenames=new ArrayList<>(recoveryFiles.length);
        for (        org.apache.flink.core.fs.FileStatus file : recoveryFiles) {
          filenames.add(file.toString());
        }
        fail("Unclean state backend: " + filenames);
      }
    }
   }
}
