/** 
 * Test that runs an iterative job after a failure in another iterative job. This test validates that task slots in co-location constraints are properly freed in the presence of failures.
 */
public class SuccessAfterNetworkBuffersFailureITCase extends TestLogger {
  private static final int PARALLELISM=16;
  @ClassRule public static final MiniClusterResource MINI_CLUSTER_RESOURCE=new MiniClusterResource(new MiniClusterResourceConfiguration.Builder().setConfiguration(getConfiguration()).setNumberTaskManagers(2).setNumberSlotsPerTaskManager(8).build());
  private static Configuration getConfiguration(){
    Configuration config=new Configuration();
    config.setString(TaskManagerOptions.MANAGED_MEMORY_SIZE,"80m");
    config.setInteger(TaskManagerOptions.NETWORK_NUM_BUFFERS,800);
    return config;
  }
  @Test public void testSuccessfulProgramAfterFailure() throws Exception {
    ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
    runConnectedComponents(env);
    try {
      runKMeans(env);
      fail("This program execution should have failed.");
    }
 catch (    JobExecutionException e) {
      assertTrue(e.getCause().getMessage().contains("Insufficient number of network buffers"));
    }
    runConnectedComponents(env);
  }
  private static void runConnectedComponents(  ExecutionEnvironment env) throws Exception {
    env.setParallelism(PARALLELISM);
    env.getConfig().disableSysoutLogging();
    DataSet<Long> vertices=ConnectedComponentsData.getDefaultVertexDataSet(env).rebalance();
    DataSet<Tuple2<Long,Long>> edges=ConnectedComponentsData.getDefaultEdgeDataSet(env).rebalance().flatMap(new ConnectedComponents.UndirectEdge());
    DataSet<Tuple2<Long,Long>> verticesWithInitialId=vertices.map(new ConnectedComponents.DuplicateValue<Long>());
    DeltaIteration<Tuple2<Long,Long>,Tuple2<Long,Long>> iteration=verticesWithInitialId.iterateDelta(verticesWithInitialId,100,0);
    DataSet<Tuple2<Long,Long>> changes=iteration.getWorkset().join(edges).where(0).equalTo(0).with(new ConnectedComponents.NeighborWithComponentIDJoin()).groupBy(0).aggregate(Aggregations.MIN,1).join(iteration.getSolutionSet()).where(0).equalTo(0).with(new ConnectedComponents.ComponentIdFilter());
    DataSet<Tuple2<Long,Long>> result=iteration.closeWith(changes,changes);
    result.output(new DiscardingOutputFormat<Tuple2<Long,Long>>());
    env.execute();
  }
  private static void runKMeans(  ExecutionEnvironment env) throws Exception {
    env.setParallelism(PARALLELISM);
    env.getConfig().disableSysoutLogging();
    DataSet<KMeans.Point> points=KMeansData.getDefaultPointDataSet(env).rebalance();
    DataSet<KMeans.Centroid> centroids=KMeansData.getDefaultCentroidDataSet(env).rebalance();
    IterativeDataSet<KMeans.Centroid> loop=centroids.iterate(20);
    DataSet<KMeans.Centroid> newCentroids=points.map(new KMeans.SelectNearestCenter()).withBroadcastSet(loop,"centroids").rebalance().map(new KMeans.CountAppender()).groupBy(0).reduce(new KMeans.CentroidAccumulator()).rebalance().map(new KMeans.CentroidAverager());
    DataSet<KMeans.Centroid> finalCentroids=loop.closeWith(newCentroids);
    DataSet<Tuple2<Integer,KMeans.Point>> clusteredPoints=points.map(new KMeans.SelectNearestCenter()).withBroadcastSet(finalCentroids,"centroids");
    clusteredPoints.output(new DiscardingOutputFormat<Tuple2<Integer,KMeans.Point>>());
    env.execute("KMeans Example");
  }
}
