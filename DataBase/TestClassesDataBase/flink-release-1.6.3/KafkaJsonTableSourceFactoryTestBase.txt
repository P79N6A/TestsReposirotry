/** 
 * Tests for legacy KafkaJsonTableSourceFactory.
 * @deprecated Ensures backwards compatibility with Flink 1.5. Can be removed once wedrop support for format-specific table sources.
 */
@Deprecated public abstract class KafkaJsonTableSourceFactoryTestBase {
  private static final String JSON_SCHEMA="{" + "  'title': 'Fruit'," + "  'type': 'object',"+ "  'properties': {"+ "    'name': {"+ "      'type': 'string'"+ "    },"+ "    'count': {"+ "      'type': 'integer'"+ "    },"+ "    'time': {"+ "      'description': 'row time',"+ "      'type': 'string',"+ "      'format': 'date-time'"+ "    }"+ "  },"+ "  'required': ['name', 'count', 'time']"+ "}";
  private static final String TOPIC="test-topic";
  protected abstract String version();
  protected abstract KafkaJsonTableSource.Builder builder();
  @Test public void testTableSourceFromJsonSchema(){
    testTableSource(new Json().jsonSchema(JSON_SCHEMA).failOnMissingField(true));
  }
  @Test public void testTableSourceDerivedSchema(){
    testTableSource(new Json().deriveSchema().failOnMissingField(true));
  }
  private void testTableSource(  FormatDescriptor format){
    final Map<String,String> tableJsonMapping=new HashMap<>();
    tableJsonMapping.put("fruit-name","name");
    tableJsonMapping.put("name","name");
    tableJsonMapping.put("count","count");
    tableJsonMapping.put("time","time");
    final Properties props=new Properties();
    props.put("group.id","test-group");
    props.put("bootstrap.servers","localhost:1234");
    final Map<KafkaTopicPartition,Long> specificOffsets=new HashMap<>();
    specificOffsets.put(new KafkaTopicPartition(TOPIC,0),100L);
    specificOffsets.put(new KafkaTopicPartition(TOPIC,1),123L);
    final KafkaTableSource builderSource=builder().forJsonSchema(TableSchema.fromTypeInfo(JsonRowSchemaConverter.convert(JSON_SCHEMA))).failOnMissingField(true).withTableToJsonMapping(tableJsonMapping).withKafkaProperties(props).forTopic(TOPIC).fromSpecificOffsets(specificOffsets).withSchema(TableSchema.builder().field("fruit-name",Types.STRING).field("count",Types.BIG_DEC).field("event-time",Types.SQL_TIMESTAMP).field("proc-time",Types.SQL_TIMESTAMP).build()).withProctimeAttribute("proc-time").withRowtimeAttribute("event-time",new ExistingField("time"),new AscendingTimestamps()).build();
    TableSourceUtil.validateTableSource(builderSource);
    final Map<Integer,Long> offsets=new HashMap<>();
    offsets.put(0,100L);
    offsets.put(1,123L);
    final TestTableDescriptor testDesc=new TestTableDescriptor(new Kafka().version(version()).topic(TOPIC).properties(props).startFromSpecificOffsets(offsets)).withFormat(format).withSchema(new Schema().field("fruit-name",Types.STRING).from("name").field("count",Types.BIG_DEC).field("event-time",Types.SQL_TIMESTAMP).rowtime(new Rowtime().timestampsFromField("time").watermarksPeriodicAscending()).field("proc-time",Types.SQL_TIMESTAMP).proctime()).inAppendMode();
    DescriptorProperties properties=new DescriptorProperties(true);
    testDesc.addProperties(properties);
    final TableSource<?> factorySource=TableFactoryService.find(StreamTableSourceFactory.class,testDesc).createStreamTableSource(properties.asMap());
    assertEquals(builderSource,factorySource);
  }
}
