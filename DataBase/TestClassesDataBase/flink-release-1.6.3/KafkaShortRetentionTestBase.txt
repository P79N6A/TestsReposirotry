/** 
 * A class containing a special Kafka broker which has a log retention of only 250 ms. This way, we can make sure our consumer is properly handling cases where we run into out of offset errors
 */
@SuppressWarnings("serial") public class KafkaShortRetentionTestBase implements Serializable {
  protected static final Logger LOG=LoggerFactory.getLogger(KafkaShortRetentionTestBase.class);
  protected static final int NUM_TMS=1;
  protected static final int TM_SLOTS=8;
  protected static final int PARALLELISM=NUM_TMS * TM_SLOTS;
  private static KafkaTestEnvironment kafkaServer;
  private static Properties standardProps;
  @ClassRule public static MiniClusterResource flink=new MiniClusterResource(new MiniClusterResourceConfiguration.Builder().setConfiguration(getConfiguration()).setNumberTaskManagers(NUM_TMS).setNumberSlotsPerTaskManager(TM_SLOTS).build());
  @ClassRule public static TemporaryFolder tempFolder=new TemporaryFolder();
  protected static Properties secureProps=new Properties();
  private static Configuration getConfiguration(){
    Configuration flinkConfig=new Configuration();
    flinkConfig.setString(TaskManagerOptions.MANAGED_MEMORY_SIZE,"16m");
    flinkConfig.setString(ConfigConstants.RESTART_STRATEGY_FIXED_DELAY_DELAY,"0 s");
    return flinkConfig;
  }
  @BeforeClass public static void prepare() throws ClassNotFoundException {
    LOG.info("-------------------------------------------------------------------------");
    LOG.info("    Starting KafkaShortRetentionTestBase ");
    LOG.info("-------------------------------------------------------------------------");
    Class<?> clazz=Class.forName("org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl");
    kafkaServer=(KafkaTestEnvironment)InstantiationUtil.instantiate(clazz);
    LOG.info("Starting KafkaTestBase.prepare() for Kafka " + kafkaServer.getVersion());
    if (kafkaServer.isSecureRunSupported()) {
      secureProps=kafkaServer.getSecureProperties();
    }
    Properties specificProperties=new Properties();
    specificProperties.setProperty("log.retention.hours","0");
    specificProperties.setProperty("log.retention.minutes","0");
    specificProperties.setProperty("log.retention.ms","250");
    specificProperties.setProperty("log.retention.check.interval.ms","100");
    kafkaServer.prepare(kafkaServer.createConfig().setKafkaServerProperties(specificProperties));
    standardProps=kafkaServer.getStandardProperties();
  }
  @AfterClass public static void shutDownServices() throws Exception {
    kafkaServer.shutdown();
    secureProps.clear();
  }
  /** 
 * This test is concurrently reading and writing from a kafka topic. The job will run for a while In a special deserializationSchema, we make sure that the offsets from the topic are non-continuous (because the data is expiring faster than its consumed --> with auto.offset.reset = 'earliest', some offsets will not show up)
 */
  private static boolean stopProducer=false;
  public void runAutoOffsetResetTest() throws Exception {
    final String topic="auto-offset-reset-test";
    final int parallelism=1;
    final int elementsPerPartition=50000;
    Properties tprops=new Properties();
    tprops.setProperty("retention.ms","250");
    kafkaServer.createTestTopic(topic,parallelism,1,tprops);
    final StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment();
    env.setParallelism(parallelism);
    env.setRestartStrategy(RestartStrategies.noRestart());
    env.getConfig().disableSysoutLogging();
    DataStream<String> stream=env.addSource(new RichParallelSourceFunction<String>(){
      private boolean running=true;
      @Override public void run(      SourceContext<String> ctx) throws InterruptedException {
        int cnt=getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition;
        int limit=cnt + elementsPerPartition;
        while (running && !stopProducer && cnt < limit) {
          ctx.collect("element-" + cnt);
          cnt++;
          Thread.sleep(10);
        }
        LOG.info("Stopping producer");
      }
      @Override public void cancel(){
        running=false;
      }
    }
);
    Properties props=new Properties();
    props.putAll(standardProps);
    props.putAll(secureProps);
    kafkaServer.produceIntoKafka(stream,topic,new KeyedSerializationSchemaWrapper<>(new SimpleStringSchema()),props,null);
    NonContinousOffsetsDeserializationSchema deserSchema=new NonContinousOffsetsDeserializationSchema();
    FlinkKafkaConsumerBase<String> source=kafkaServer.getConsumer(topic,deserSchema,props);
    DataStreamSource<String> consuming=env.addSource(source);
    consuming.addSink(new DiscardingSink<String>());
    tryExecute(env,"run auto offset reset test");
    kafkaServer.deleteTestTopic(topic);
  }
private class NonContinousOffsetsDeserializationSchema implements KeyedDeserializationSchema<String> {
    private int numJumps;
    long nextExpected=0;
    @Override public String deserialize(    byte[] messageKey,    byte[] message,    String topic,    int partition,    long offset) throws IOException {
      if (offset != nextExpected) {
        numJumps++;
        nextExpected=offset;
        LOG.info("Registered now jump at offset {}",offset);
      }
      nextExpected++;
      try {
        Thread.sleep(10);
      }
 catch (      InterruptedException e) {
        throw new RuntimeException("Stopping it");
      }
      return "";
    }
    @Override public boolean isEndOfStream(    String nextElement){
      if (numJumps >= 5) {
        stopProducer=true;
        return true;
      }
      return false;
    }
    @Override public TypeInformation<String> getProducedType(){
      return Types.STRING;
    }
  }
  /** 
 * Ensure that the consumer is properly failing if "auto.offset.reset" is set to "none".
 */
  public void runFailOnAutoOffsetResetNone() throws Exception {
    final String topic="auto-offset-reset-none-test";
    final int parallelism=1;
    kafkaServer.createTestTopic(topic,parallelism,1);
    final StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment();
    env.setParallelism(parallelism);
    env.setRestartStrategy(RestartStrategies.noRestart());
    env.getConfig().disableSysoutLogging();
    Properties customProps=new Properties();
    customProps.putAll(standardProps);
    customProps.putAll(secureProps);
    customProps.setProperty("auto.offset.reset","none");
    FlinkKafkaConsumerBase<String> source=kafkaServer.getConsumer(topic,new SimpleStringSchema(),customProps);
    DataStreamSource<String> consuming=env.addSource(source);
    consuming.addSink(new DiscardingSink<String>());
    try {
      env.execute("Test auto offset reset none");
    }
 catch (    Throwable e) {
      if (!e.getCause().getCause().getMessage().contains("Unable to find previous offset") && !e.getCause().getCause().getMessage().contains("Undefined offset with no reset policy for partition")) {
        throw e;
      }
    }
    kafkaServer.deleteTestTopic(topic);
  }
  public void runFailOnAutoOffsetResetNoneEager() throws Exception {
    final String topic="auto-offset-reset-none-test";
    final int parallelism=1;
    kafkaServer.createTestTopic(topic,parallelism,1);
    Properties customProps=new Properties();
    customProps.putAll(standardProps);
    customProps.putAll(secureProps);
    customProps.setProperty("auto.offset.reset","none");
    try {
      kafkaServer.getConsumer(topic,new SimpleStringSchema(),customProps);
      fail("should fail with an exception");
    }
 catch (    IllegalArgumentException e) {
      assertTrue(e.getMessage().contains("none"));
    }
    kafkaServer.deleteTestTopic(topic);
  }
}
