/** 
 * Tests for the  {@link FlinkKafkaProducerBase}.
 */
public class FlinkKafkaProducerBaseTest {
  /** 
 * Tests that the constructor eagerly checks bootstrap servers are set in config.
 */
  @Test(expected=IllegalArgumentException.class) public void testInstantiationFailsWhenBootstrapServersMissing() throws Exception {
    Properties props=new Properties();
    new DummyFlinkKafkaProducer<>(props,new KeyedSerializationSchemaWrapper<>(new SimpleStringSchema()),null);
  }
  /** 
 * Tests that constructor defaults to key value serializers in config to byte array deserializers if not set.
 */
  @Test public void testKeyValueDeserializersSetIfMissing() throws Exception {
    Properties props=new Properties();
    props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:12345");
    new DummyFlinkKafkaProducer<>(props,new KeyedSerializationSchemaWrapper<>(new SimpleStringSchema()),null);
    assertTrue(props.containsKey(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG));
    assertTrue(props.containsKey(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG));
    assertTrue(props.getProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).equals(ByteArraySerializer.class.getName()));
    assertTrue(props.getProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).equals(ByteArraySerializer.class.getName()));
  }
  /** 
 * Tests that partitions list is determinate and correctly provided to custom partitioner.
 */
  @SuppressWarnings("unchecked") @Test public void testPartitionerInvokedWithDeterminatePartitionList() throws Exception {
    FlinkKafkaPartitioner<String> mockPartitioner=mock(FlinkKafkaPartitioner.class);
    RuntimeContext mockRuntimeContext=mock(StreamingRuntimeContext.class);
    when(mockRuntimeContext.getIndexOfThisSubtask()).thenReturn(0);
    when(mockRuntimeContext.getNumberOfParallelSubtasks()).thenReturn(1);
    List<PartitionInfo> mockPartitionsList=new ArrayList<>(4);
    mockPartitionsList.add(new PartitionInfo(DummyFlinkKafkaProducer.DUMMY_TOPIC,3,null,null,null));
    mockPartitionsList.add(new PartitionInfo(DummyFlinkKafkaProducer.DUMMY_TOPIC,1,null,null,null));
    mockPartitionsList.add(new PartitionInfo(DummyFlinkKafkaProducer.DUMMY_TOPIC,0,null,null,null));
    mockPartitionsList.add(new PartitionInfo(DummyFlinkKafkaProducer.DUMMY_TOPIC,2,null,null,null));
    final DummyFlinkKafkaProducer<String> producer=new DummyFlinkKafkaProducer<>(FakeStandardProducerConfig.get(),new KeyedSerializationSchemaWrapper<>(new SimpleStringSchema()),mockPartitioner);
    producer.setRuntimeContext(mockRuntimeContext);
    final KafkaProducer mockProducer=producer.getMockKafkaProducer();
    when(mockProducer.partitionsFor(anyString())).thenReturn(mockPartitionsList);
    when(mockProducer.metrics()).thenReturn(null);
    producer.open(new Configuration());
    verify(mockPartitioner,times(1)).open(0,1);
    producer.invoke("foobar",SinkContextUtil.forTimestamp(0));
    verify(mockPartitioner,times(1)).partition("foobar",null,"foobar".getBytes(),DummyFlinkKafkaProducer.DUMMY_TOPIC,new int[]{0,1,2,3});
  }
  /** 
 * Test ensuring that if an invoke call happens right after an async exception is caught, it should be rethrown.
 */
  @Test public void testAsyncErrorRethrownOnInvoke() throws Throwable {
    final DummyFlinkKafkaProducer<String> producer=new DummyFlinkKafkaProducer<>(FakeStandardProducerConfig.get(),new KeyedSerializationSchemaWrapper<>(new SimpleStringSchema()),null);
    OneInputStreamOperatorTestHarness<String,Object> testHarness=new OneInputStreamOperatorTestHarness<>(new StreamSink<>(producer));
    testHarness.open();
    testHarness.processElement(new StreamRecord<>("msg-1"));
    producer.getPendingCallbacks().get(0).onCompletion(null,new Exception("artificial async exception"));
    try {
      testHarness.processElement(new StreamRecord<>("msg-2"));
    }
 catch (    Exception e) {
      Assert.assertTrue(e.getCause().getMessage().contains("artificial async exception"));
      return;
    }
    Assert.fail();
  }
  /** 
 * Test ensuring that if a snapshot call happens right after an async exception is caught, it should be rethrown.
 */
  @Test public void testAsyncErrorRethrownOnCheckpoint() throws Throwable {
    final DummyFlinkKafkaProducer<String> producer=new DummyFlinkKafkaProducer<>(FakeStandardProducerConfig.get(),new KeyedSerializationSchemaWrapper<>(new SimpleStringSchema()),null);
    OneInputStreamOperatorTestHarness<String,Object> testHarness=new OneInputStreamOperatorTestHarness<>(new StreamSink<>(producer));
    testHarness.open();
    testHarness.processElement(new StreamRecord<>("msg-1"));
    producer.getPendingCallbacks().get(0).onCompletion(null,new Exception("artificial async exception"));
    try {
      testHarness.snapshot(123L,123L);
    }
 catch (    Exception e) {
      Assert.assertTrue(e.getCause().getMessage().contains("artificial async exception"));
      return;
    }
    Assert.fail();
  }
  /** 
 * Test ensuring that if an async exception is caught for one of the flushed requests on checkpoint, it should be rethrown; we set a timeout because the test will not finish if the logic is broken. <p>Note that this test does not test the snapshot method is blocked correctly when there are pending records. The test for that is covered in testAtLeastOnceProducer.
 */
  @SuppressWarnings("unchecked") @Test(timeout=5000) public void testAsyncErrorRethrownOnCheckpointAfterFlush() throws Throwable {
    final DummyFlinkKafkaProducer<String> producer=new DummyFlinkKafkaProducer<>(FakeStandardProducerConfig.get(),new KeyedSerializationSchemaWrapper<>(new SimpleStringSchema()),null);
    producer.setFlushOnCheckpoint(true);
    final KafkaProducer<?,?> mockProducer=producer.getMockKafkaProducer();
    final OneInputStreamOperatorTestHarness<String,Object> testHarness=new OneInputStreamOperatorTestHarness<>(new StreamSink<>(producer));
    testHarness.open();
    testHarness.processElement(new StreamRecord<>("msg-1"));
    testHarness.processElement(new StreamRecord<>("msg-2"));
    testHarness.processElement(new StreamRecord<>("msg-3"));
    verify(mockProducer,times(3)).send(any(ProducerRecord.class),any(Callback.class));
    producer.getPendingCallbacks().get(0).onCompletion(null,null);
    CheckedThread snapshotThread=new CheckedThread(){
      @Override public void go() throws Exception {
        testHarness.snapshot(123L,123L);
      }
    }
;
    snapshotThread.start();
    producer.getPendingCallbacks().get(1).onCompletion(null,new Exception("artificial async failure for 2nd message"));
    producer.getPendingCallbacks().get(2).onCompletion(null,null);
    try {
      snapshotThread.sync();
    }
 catch (    Exception e) {
      Assert.assertTrue(e.getCause().getMessage().contains("artificial async failure for 2nd message"));
      return;
    }
    Assert.fail();
  }
  /** 
 * Test ensuring that the producer is not dropping buffered records; we set a timeout because the test will not finish if the logic is broken.
 */
  @SuppressWarnings("unchecked") @Test(timeout=10000) public void testAtLeastOnceProducer() throws Throwable {
    final DummyFlinkKafkaProducer<String> producer=new DummyFlinkKafkaProducer<>(FakeStandardProducerConfig.get(),new KeyedSerializationSchemaWrapper<>(new SimpleStringSchema()),null);
    producer.setFlushOnCheckpoint(true);
    final KafkaProducer<?,?> mockProducer=producer.getMockKafkaProducer();
    final OneInputStreamOperatorTestHarness<String,Object> testHarness=new OneInputStreamOperatorTestHarness<>(new StreamSink<>(producer));
    testHarness.open();
    testHarness.processElement(new StreamRecord<>("msg-1"));
    testHarness.processElement(new StreamRecord<>("msg-2"));
    testHarness.processElement(new StreamRecord<>("msg-3"));
    verify(mockProducer,times(3)).send(any(ProducerRecord.class),any(Callback.class));
    Assert.assertEquals(3,producer.getPendingSize());
    CheckedThread snapshotThread=new CheckedThread(){
      @Override public void go() throws Exception {
        testHarness.snapshot(123L,123L);
      }
    }
;
    snapshotThread.start();
    producer.waitUntilFlushStarted();
    Assert.assertTrue("Snapshot returned before all records were flushed",snapshotThread.isAlive());
    producer.getPendingCallbacks().get(0).onCompletion(null,null);
    Assert.assertTrue("Snapshot returned before all records were flushed",snapshotThread.isAlive());
    Assert.assertEquals(2,producer.getPendingSize());
    producer.getPendingCallbacks().get(1).onCompletion(null,null);
    Assert.assertTrue("Snapshot returned before all records were flushed",snapshotThread.isAlive());
    Assert.assertEquals(1,producer.getPendingSize());
    producer.getPendingCallbacks().get(2).onCompletion(null,null);
    Assert.assertEquals(0,producer.getPendingSize());
    snapshotThread.sync();
    testHarness.close();
  }
  /** 
 * This test is meant to assure that testAtLeastOnceProducer is valid by testing that if flushing is disabled, the snapshot method does indeed finishes without waiting for pending records; we set a timeout because the test will not finish if the logic is broken.
 */
  @SuppressWarnings("unchecked") @Test(timeout=5000) public void testDoesNotWaitForPendingRecordsIfFlushingDisabled() throws Throwable {
    final DummyFlinkKafkaProducer<String> producer=new DummyFlinkKafkaProducer<>(FakeStandardProducerConfig.get(),new KeyedSerializationSchemaWrapper<>(new SimpleStringSchema()),null);
    producer.setFlushOnCheckpoint(false);
    final KafkaProducer<?,?> mockProducer=producer.getMockKafkaProducer();
    final OneInputStreamOperatorTestHarness<String,Object> testHarness=new OneInputStreamOperatorTestHarness<>(new StreamSink<>(producer));
    testHarness.open();
    testHarness.processElement(new StreamRecord<>("msg"));
    verify(mockProducer,times(1)).send(any(ProducerRecord.class),any(Callback.class));
    testHarness.snapshot(123L,123L);
    testHarness.close();
  }
private static class DummyFlinkKafkaProducer<T> extends FlinkKafkaProducerBase<T> {
    private static final long serialVersionUID=1L;
    private static final String DUMMY_TOPIC="dummy-topic";
    private transient KafkaProducer<?,?> mockProducer;
    private transient List<Callback> pendingCallbacks;
    private transient MultiShotLatch flushLatch;
    private boolean isFlushed;
    @SuppressWarnings("unchecked") DummyFlinkKafkaProducer(    Properties producerConfig,    KeyedSerializationSchema<T> schema,    FlinkKafkaPartitioner partitioner){
      super(DUMMY_TOPIC,schema,producerConfig,partitioner);
      this.mockProducer=mock(KafkaProducer.class);
      when(mockProducer.send(any(ProducerRecord.class),any(Callback.class))).thenAnswer(new Answer<Object>(){
        @Override public Object answer(        InvocationOnMock invocationOnMock) throws Throwable {
          pendingCallbacks.add(invocationOnMock.getArgumentAt(1,Callback.class));
          return null;
        }
      }
);
      this.pendingCallbacks=new ArrayList<>();
      this.flushLatch=new MultiShotLatch();
    }
    long getPendingSize(){
      if (flushOnCheckpoint) {
        return numPendingRecords();
      }
 else {
        throw new UnsupportedOperationException("getPendingSize not supported when flushing is disabled");
      }
    }
    List<Callback> getPendingCallbacks(){
      return pendingCallbacks;
    }
    KafkaProducer<?,?> getMockKafkaProducer(){
      return mockProducer;
    }
    @Override public void snapshotState(    FunctionSnapshotContext ctx) throws Exception {
      isFlushed=false;
      super.snapshotState(ctx);
      if (flushOnCheckpoint && !isFlushed) {
        throw new RuntimeException("Flushing is enabled; snapshots should be blocked until all pending records are flushed");
      }
    }
    public void waitUntilFlushStarted() throws Exception {
      flushLatch.await();
    }
    @SuppressWarnings("unchecked") @Override protected <K,V>KafkaProducer<K,V> getKafkaProducer(    Properties props){
      return (KafkaProducer<K,V>)mockProducer;
    }
    @Override protected void flush(){
      flushLatch.trigger();
      while (numPendingRecords() > 0) {
        try {
          Thread.sleep(10);
        }
 catch (        InterruptedException e) {
          throw new RuntimeException("Unable to flush producer, task was interrupted");
        }
      }
      isFlushed=true;
    }
  }
}
