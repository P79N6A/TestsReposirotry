/** 
 * Simple integration test case for writing bulk encoded files with the {@link StreamingFileSink} with Parquet.
 */
@SuppressWarnings("serial") public class ParquetStreamingFileSinkITCase extends AbstractTestBase {
  @Test public void testWriteParquetAvroSpecific() throws Exception {
    final File folder=TEMPORARY_FOLDER.newFolder();
    final List<Address> data=Arrays.asList(new Address(1,"a","b","c","12345"),new Address(2,"p","q","r","12345"),new Address(3,"x","y","z","12345"));
    final StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment();
    env.setParallelism(1);
    env.enableCheckpointing(100);
    DataStream<Address> stream=env.addSource(new FiniteTestSource<>(data),TypeInformation.of(Address.class));
    stream.addSink(StreamingFileSink.forBulkFormat(Path.fromLocalFile(folder),ParquetAvroWriters.forSpecificRecord(Address.class)).build());
    env.execute();
    validateResults(folder,SpecificData.get(),data);
  }
  @Test public void testWriteParquetAvroGeneric() throws Exception {
    final File folder=TEMPORARY_FOLDER.newFolder();
    final Schema schema=Address.getClassSchema();
    final Collection<GenericRecord> data=new GenericTestDataCollection();
    final StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment();
    env.setParallelism(1);
    env.enableCheckpointing(100);
    DataStream<GenericRecord> stream=env.addSource(new FiniteTestSource<>(data),new GenericRecordAvroTypeInfo(schema));
    stream.addSink(StreamingFileSink.forBulkFormat(Path.fromLocalFile(folder),ParquetAvroWriters.forGenericRecord(schema)).build());
    env.execute();
    List<Address> expected=Arrays.asList(new Address(1,"a","b","c","12345"),new Address(2,"x","y","z","98765"));
    validateResults(folder,SpecificData.get(),expected);
  }
  @Test public void testWriteParquetAvroReflect() throws Exception {
    final File folder=TEMPORARY_FOLDER.newFolder();
    final List<Datum> data=Arrays.asList(new Datum("a",1),new Datum("b",2),new Datum("c",3));
    final StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment();
    env.setParallelism(1);
    env.enableCheckpointing(100);
    DataStream<Datum> stream=env.addSource(new FiniteTestSource<>(data),TypeInformation.of(Datum.class));
    stream.addSink(StreamingFileSink.forBulkFormat(Path.fromLocalFile(folder),ParquetAvroWriters.forReflectRecord(Datum.class)).build());
    env.execute();
    validateResults(folder,ReflectData.get(),data);
  }
  private static <T>void validateResults(  File folder,  GenericData dataModel,  List<T> expected) throws Exception {
    File[] buckets=folder.listFiles();
    assertNotNull(buckets);
    assertEquals(1,buckets.length);
    File[] partFiles=buckets[0].listFiles();
    assertNotNull(partFiles);
    assertEquals(1,partFiles.length);
    assertTrue(partFiles[0].length() > 0);
    List<Address> results=readParquetFile(partFiles[0],dataModel);
    assertEquals(expected,results);
  }
  private static <T>List<T> readParquetFile(  File file,  GenericData dataModel) throws IOException {
    InputFile inFile=HadoopInputFile.fromPath(new org.apache.hadoop.fs.Path(file.toURI()),new Configuration());
    ParquetReader<T> reader=AvroParquetReader.<T>builder(inFile).withDataModel(dataModel).build();
    ArrayList<T> results=new ArrayList<>();
    T next;
    while ((next=reader.read()) != null) {
      results.add(next);
    }
    return results;
  }
private static class GenericTestDataCollection extends AbstractCollection<GenericRecord> implements Serializable {
    @Override public Iterator<GenericRecord> iterator(){
      final GenericRecord rec1=new GenericData.Record(Address.getClassSchema());
      rec1.put(0,1);
      rec1.put(1,"a");
      rec1.put(2,"b");
      rec1.put(3,"c");
      rec1.put(4,"12345");
      final GenericRecord rec2=new GenericData.Record(Address.getClassSchema());
      rec2.put(0,2);
      rec2.put(1,"x");
      rec2.put(2,"y");
      rec2.put(3,"z");
      rec2.put(4,"98765");
      return Arrays.asList(rec1,rec2).iterator();
    }
    @Override public int size(){
      return 2;
    }
  }
  /** 
 * Test datum. 
 */
public static class Datum implements Serializable {
    public String a;
    public int b;
    public Datum(){
    }
    public Datum(    String a,    int b){
      this.a=a;
      this.b=b;
    }
    @Override public boolean equals(    Object o){
      if (this == o) {
        return true;
      }
      if (o == null || getClass() != o.getClass()) {
        return false;
      }
      Datum datum=(Datum)o;
      return b == datum.b && (a != null ? a.equals(datum.a) : datum.a == null);
    }
    @Override public int hashCode(){
      int result=a != null ? a.hashCode() : 0;
      result=31 * result + b;
      return result;
    }
  }
}
