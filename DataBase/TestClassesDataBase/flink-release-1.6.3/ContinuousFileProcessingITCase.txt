/** 
 * IT cases for the  {@link ContinuousFileMonitoringFunction} and {@link ContinuousFileReaderOperator}.
 */
public class ContinuousFileProcessingITCase extends AbstractTestBase {
  private static final int NO_OF_FILES=5;
  private static final int LINES_PER_FILE=100;
  private static final int PARALLELISM=4;
  private static final long INTERVAL=100;
  private File baseDir;
  private org.apache.hadoop.fs.FileSystem hdfs;
  private String hdfsURI;
  private MiniDFSCluster hdfsCluster;
  private static Map<Integer,String> expectedContents=new HashMap<>();
  @Before public void createHDFS(){
    try {
      baseDir=new File("./target/hdfs/hdfsTesting").getAbsoluteFile();
      FileUtil.fullyDelete(baseDir);
      org.apache.hadoop.conf.Configuration hdConf=new org.apache.hadoop.conf.Configuration();
      hdConf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR,baseDir.getAbsolutePath());
      hdConf.set("dfs.block.size",String.valueOf(1048576));
      MiniDFSCluster.Builder builder=new MiniDFSCluster.Builder(hdConf);
      hdfsCluster=builder.build();
      hdfsURI="hdfs://" + hdfsCluster.getURI().getHost() + ":"+ hdfsCluster.getNameNodePort()+ "/";
      hdfs=new org.apache.hadoop.fs.Path(hdfsURI).getFileSystem(hdConf);
    }
 catch (    Throwable e) {
      e.printStackTrace();
      Assert.fail("Test failed " + e.getMessage());
    }
  }
  @After public void destroyHDFS(){
    try {
      FileUtil.fullyDelete(baseDir);
      hdfsCluster.shutdown();
    }
 catch (    Throwable t) {
      throw new RuntimeException(t);
    }
  }
  @Test public void testProgram() throws Exception {
    TextInputFormat format=new TextInputFormat(new Path(hdfsURI));
    format.setFilePath(hdfsURI);
    format.setFilesFilter(FilePathFilter.createDefaultFilter());
    final StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment();
    env.setParallelism(PARALLELISM);
    ContinuousFileMonitoringFunction<String> monitoringFunction=new ContinuousFileMonitoringFunction<>(format,FileProcessingMode.PROCESS_CONTINUOUSLY,env.getParallelism(),INTERVAL);
    DataStream<TimestampedFileInputSplit> splits=env.addSource(monitoringFunction);
    Assert.assertEquals(1,splits.getParallelism());
    ContinuousFileReaderOperator<String> reader=new ContinuousFileReaderOperator<>(format);
    TypeInformation<String> typeInfo=TypeExtractor.getInputFormatTypes(format);
    DataStream<String> content=splits.transform("FileSplitReader",typeInfo,reader);
    Assert.assertEquals(PARALLELISM,content.getParallelism());
    TestingSinkFunction sink=new TestingSinkFunction();
    content.addSink(sink).setParallelism(1);
    Thread job=new Thread(){
      @Override public void run(){
        try {
          env.execute("ContinuousFileProcessingITCase Job.");
        }
 catch (        Exception e) {
          Throwable th=e;
          for (int depth=0; depth < 20; depth++) {
            if (th instanceof SuccessException) {
              return;
            }
 else             if (th.getCause() != null) {
              th=th.getCause();
            }
 else {
              break;
            }
          }
          e.printStackTrace();
          Assert.fail(e.getMessage());
        }
      }
    }
;
    job.start();
    long lastCreatedModTime=Long.MIN_VALUE;
    for (int i=0; i < NO_OF_FILES; i++) {
      Tuple2<org.apache.hadoop.fs.Path,String> tmpFile;
      long modTime;
      do {
        Thread.sleep(50);
        tmpFile=fillWithData(hdfsURI,"file",i,"This is test line.");
        modTime=hdfs.getFileStatus(tmpFile.f0).getModificationTime();
        if (modTime <= lastCreatedModTime) {
          hdfs.delete(tmpFile.f0,false);
        }
      }
 while (modTime <= lastCreatedModTime);
      lastCreatedModTime=modTime;
      expectedContents.put(i,tmpFile.f1);
      org.apache.hadoop.fs.Path file=new org.apache.hadoop.fs.Path(hdfsURI + "/file" + i);
      hdfs.rename(tmpFile.f0,file);
      Assert.assertTrue(hdfs.exists(file));
    }
    job.join();
  }
private static class TestingSinkFunction extends RichSinkFunction<String> {
    private int elementCounter=0;
    private Map<Integer,Set<String>> actualContent=new HashMap<>();
    private transient Comparator<String> comparator;
    @Override public void open(    Configuration parameters) throws Exception {
      assertEquals(1,getRuntimeContext().getNumberOfParallelSubtasks());
      comparator=new Comparator<String>(){
        @Override public int compare(        String o1,        String o2){
          return getLineNo(o1) - getLineNo(o2);
        }
      }
;
    }
    @Override public void invoke(    String value) throws Exception {
      int fileIdx=getFileIdx(value);
      Set<String> content=actualContent.get(fileIdx);
      if (content == null) {
        content=new HashSet<>();
        actualContent.put(fileIdx,content);
      }
      if (!content.add(value + "\n")) {
        Assert.fail("Duplicate line: " + value);
        System.exit(0);
      }
      elementCounter++;
      if (elementCounter == NO_OF_FILES * LINES_PER_FILE) {
        throw new SuccessException();
      }
    }
    @Override public void close(){
      Assert.assertEquals(expectedContents.size(),actualContent.size());
      for (      Integer fileIdx : expectedContents.keySet()) {
        Assert.assertTrue(actualContent.keySet().contains(fileIdx));
        List<String> cntnt=new ArrayList<>(actualContent.get(fileIdx));
        Collections.sort(cntnt,comparator);
        StringBuilder cntntStr=new StringBuilder();
        for (        String line : cntnt) {
          cntntStr.append(line);
        }
        Assert.assertEquals(expectedContents.get(fileIdx),cntntStr.toString());
      }
      expectedContents.clear();
    }
    private int getLineNo(    String line){
      String[] tkns=line.split("\\s");
      return Integer.parseInt(tkns[tkns.length - 1]);
    }
    private int getFileIdx(    String line){
      String[] tkns=line.split(":");
      return Integer.parseInt(tkns[0]);
    }
  }
  /** 
 * Create a file and fill it with content. 
 */
  private Tuple2<org.apache.hadoop.fs.Path,String> fillWithData(  String base,  String fileName,  int fileIdx,  String sampleLine) throws IOException, InterruptedException {
    assert (hdfs != null);
    org.apache.hadoop.fs.Path tmp=new org.apache.hadoop.fs.Path(base + "/." + fileName+ fileIdx);
    FSDataOutputStream stream=hdfs.create(tmp);
    StringBuilder str=new StringBuilder();
    for (int i=0; i < LINES_PER_FILE; i++) {
      String line=fileIdx + ": " + sampleLine+ " "+ i+ "\n";
      str.append(line);
      stream.write(line.getBytes(ConfigConstants.DEFAULT_CHARSET));
    }
    stream.close();
    return new Tuple2<>(tmp,str.toString());
  }
private static class SuccessException extends Exception {
    private static final long serialVersionUID=-7011865671593955887L;
  }
}
