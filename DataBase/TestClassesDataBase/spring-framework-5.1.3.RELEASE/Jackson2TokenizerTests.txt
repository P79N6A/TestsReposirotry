/** 
 * @author Arjen Poutsma
 * @author Rossen Stoyanchev
 */
public class Jackson2TokenizerTests extends AbstractLeakCheckingTestCase {
  private ObjectMapper objectMapper;
  private JsonFactory jsonFactory;
  @Before public void createParser(){
    this.jsonFactory=new JsonFactory();
    this.objectMapper=new ObjectMapper(this.jsonFactory);
  }
  @Test public void doNotTokenizeArrayElements(){
    testTokenize(singletonList("{\"foo\": \"foofoo\", \"bar\": \"barbar\"}"),singletonList("{\"foo\": \"foofoo\", \"bar\": \"barbar\"}"),false);
    testTokenize(asList("{\"foo\": \"foofoo\"",", \"bar\": \"barbar\"}"),singletonList("{\"foo\":\"foofoo\",\"bar\":\"barbar\"}"),false);
    testTokenize(singletonList("[" + "{\"foo\": \"foofoo\", \"bar\": \"barbar\"}," + "{\"foo\": \"foofoofoo\", \"bar\": \"barbarbar\"}]"),singletonList("[" + "{\"foo\": \"foofoo\", \"bar\": \"barbar\"}," + "{\"foo\": \"foofoofoo\", \"bar\": \"barbarbar\"}]"),false);
    testTokenize(singletonList("[{\"foo\": \"bar\"},{\"foo\": \"baz\"}]"),singletonList("[{\"foo\": \"bar\"},{\"foo\": \"baz\"}]"),false);
    testTokenize(asList("[" + "{\"foo\": \"foofoo\", \"bar\"",": \"barbar\"}," + "{\"foo\": \"foofoofoo\", \"bar\": \"barbarbar\"}]"),singletonList("[" + "{\"foo\": \"foofoo\", \"bar\": \"barbar\"}," + "{\"foo\": \"foofoofoo\", \"bar\": \"barbarbar\"}]"),false);
    testTokenize(asList("[","{\"id\":1,\"name\":\"Robert\"}",",","{\"id\":2,\"name\":\"Raide\"}",",","{\"id\":3,\"name\":\"Ford\"}","]"),singletonList("[" + "{\"id\":1,\"name\":\"Robert\"}," + "{\"id\":2,\"name\":\"Raide\"},"+ "{\"id\":3,\"name\":\"Ford\"}]"),false);
    testTokenize(asList("\"foo","bar\""),singletonList("\"foobar\""),false);
    testTokenize(asList("12","34"),singletonList("1234"),false);
    testTokenize(asList("12.","34"),singletonList("12.34"),false);
  }
  @Test public void tokenizeArrayElements(){
    testTokenize(singletonList("{\"foo\": \"foofoo\", \"bar\": \"barbar\"}"),singletonList("{\"foo\": \"foofoo\", \"bar\": \"barbar\"}"),true);
    testTokenize(asList("{\"foo\": \"foofoo\"",", \"bar\": \"barbar\"}"),singletonList("{\"foo\":\"foofoo\",\"bar\":\"barbar\"}"),true);
    testTokenize(singletonList("[" + "{\"foo\": \"foofoo\", \"bar\": \"barbar\"}," + "{\"foo\": \"foofoofoo\", \"bar\": \"barbarbar\"}]"),asList("{\"foo\": \"foofoo\", \"bar\": \"barbar\"}","{\"foo\": \"foofoofoo\", \"bar\": \"barbarbar\"}"),true);
    testTokenize(singletonList("[{\"foo\": \"bar\"},{\"foo\": \"baz\"}]"),asList("{\"foo\": \"bar\"}","{\"foo\": \"baz\"}"),true);
    testTokenize(singletonList("[" + "{\"id\":\"0\",\"start\":[-999999999,1,1],\"end\":[999999999,12,31]}," + "{\"id\":\"1\",\"start\":[-999999999,1,1],\"end\":[999999999,12,31]},"+ "{\"id\":\"2\",\"start\":[-999999999,1,1],\"end\":[999999999,12,31]}"+ "]"),asList("{\"id\":\"0\",\"start\":[-999999999,1,1],\"end\":[999999999,12,31]}","{\"id\":\"1\",\"start\":[-999999999,1,1],\"end\":[999999999,12,31]}","{\"id\":\"2\",\"start\":[-999999999,1,1],\"end\":[999999999,12,31]}"),true);
    testTokenize(singletonList("{\"speakerIds\":[\"tastapod\"],\"language\":\"ENGLISH\"}"),singletonList("{\"speakerIds\":[\"tastapod\"],\"language\":\"ENGLISH\"}"),true);
    testTokenize(asList("[" + "{\"foo\": \"foofoo\", \"bar\"",": \"barbar\"}," + "{\"foo\": \"foofoofoo\", \"bar\": \"barbarbar\"}]"),asList("{\"foo\": \"foofoo\", \"bar\": \"barbar\"}","{\"foo\": \"foofoofoo\", \"bar\": \"barbarbar\"}"),true);
    testTokenize(asList("[","{\"id\":1,\"name\":\"Robert\"}",",","{\"id\":2,\"name\":\"Raide\"}",",","{\"id\":3,\"name\":\"Ford\"}","]"),asList("{\"id\":1,\"name\":\"Robert\"}","{\"id\":2,\"name\":\"Raide\"}","{\"id\":3,\"name\":\"Ford\"}"),true);
    testTokenize(asList("\"foo","bar\""),singletonList("\"foobar\""),true);
    testTokenize(asList("12","34"),singletonList("1234"),true);
    testTokenize(asList("12.","34"),singletonList("12.34"),true);
    testTokenize(asList("[1",",2,","3]"),asList("1","2","3"),true);
  }
  @Test public void errorInStream(){
    DataBuffer buffer=stringBuffer("{\"id\":1,\"name\":");
    Flux<DataBuffer> source=Flux.just(buffer).concatWith(Flux.error(new RuntimeException()));
    Flux<TokenBuffer> result=Jackson2Tokenizer.tokenize(source,this.jsonFactory,true);
    StepVerifier.create(result).expectError(RuntimeException.class).verify();
  }
  @Test public void jsonEOFExceptionIsWrappedAsDecodingError(){
    Flux<DataBuffer> source=Flux.just(stringBuffer("{\"status\": \"noClosingQuote}"));
    Flux<TokenBuffer> tokens=Jackson2Tokenizer.tokenize(source,this.jsonFactory,false);
    StepVerifier.create(tokens).expectError(DecodingException.class).verify();
  }
  private void testTokenize(  List<String> source,  List<String> expected,  boolean tokenizeArrayElements){
    Flux<TokenBuffer> tokenBufferFlux=Jackson2Tokenizer.tokenize(Flux.fromIterable(source).map(this::stringBuffer),this.jsonFactory,tokenizeArrayElements);
    Flux<String> result=tokenBufferFlux.map(tokenBuffer -> {
      try {
        TreeNode root=this.objectMapper.readTree(tokenBuffer.asParser());
        return this.objectMapper.writeValueAsString(root);
      }
 catch (      IOException ex) {
        throw new UncheckedIOException(ex);
      }
    }
);
    StepVerifier.FirstStep<String> builder=StepVerifier.create(result);
    expected.forEach(s -> builder.assertNext(new JSONAssertConsumer(s)));
    builder.verifyComplete();
  }
  private DataBuffer stringBuffer(  String value){
    byte[] bytes=value.getBytes(StandardCharsets.UTF_8);
    DataBuffer buffer=this.bufferFactory.allocateBuffer(bytes.length);
    buffer.write(bytes);
    return buffer;
  }
private static class JSONAssertConsumer implements Consumer<String> {
    private final String expected;
    JSONAssertConsumer(    String expected){
      this.expected=expected;
    }
    @Override public void accept(    String s){
      try {
        JSONAssert.assertEquals(this.expected,s,true);
      }
 catch (      JSONException ex) {
        throw new RuntimeException(ex);
      }
    }
  }
}
