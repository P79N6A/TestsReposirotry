@ThreadLeakLingering(linger=1000) @Test public void testTrain(){
  Random gen=RandomUtils.getRandom();
  Exponential exp=new Exponential(0.5,gen);
  Vector beta=new DenseVector(200);
  for (  Vector.Element element : beta.all()) {
    int sign=1;
    if (gen.nextDouble() < 0.5) {
      sign=-1;
    }
    element.set(sign * exp.nextDouble());
  }
  AdaptiveLogisticRegression.Wrapper cl=new AdaptiveLogisticRegression.Wrapper(2,200,new L1());
  cl.update(new double[]{1.0e-5,1});
  for (int i=0; i < 10000; i++) {
    AdaptiveLogisticRegression.TrainingExample r=getExample(i,gen,beta);
    cl.train(r);
    if (i % 1000 == 0) {
      System.out.printf("%10d %10.3f\n",i,cl.getLearner().auc());
    }
  }
  assertEquals(1,cl.getLearner().auc(),0.1);
  AdaptiveLogisticRegression adaptiveLogisticRegression=new AdaptiveLogisticRegression(2,200,new L1());
  adaptiveLogisticRegression.setInterval(1000);
  for (int i=0; i < 20000; i++) {
    AdaptiveLogisticRegression.TrainingExample r=getExample(i,gen,beta);
    adaptiveLogisticRegression.train(r.getKey(),r.getActual(),r.getInstance());
    if (i % 1000 == 0 && adaptiveLogisticRegression.getBest() != null) {
      System.out.printf("%10d %10.4f %10.8f %.3f\n",i,adaptiveLogisticRegression.auc(),Math.log10(adaptiveLogisticRegression.getBest().getMappedParams()[0]),adaptiveLogisticRegression.getBest().getMappedParams()[1]);
    }
  }
  assertEquals(1,adaptiveLogisticRegression.auc(),0.1);
  adaptiveLogisticRegression.close();
}
