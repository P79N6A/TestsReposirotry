/** 
 * Tests that interesting properties can be pushed out of the bulk iteration. This requires that a NoOp node is appended to the step function which re-establishes the properties of the initial input. If this does not work, then Flink won't find a plan, because the optimizer will not consider plans where the partitioning is done after the partial solution node in this case (because of pruning).
 * @throws Exception
 */
@Test public void testBulkIterationWithPartialSolutionProperties() throws Exception {
  ExecutionEnvironment env=ExecutionEnvironment.getExecutionEnvironment();
  DataSet<Tuple1<Long>> input1=env.generateSequence(1,10).map(new MapFunction<Long,Tuple1<Long>>(){
    @Override public Tuple1<Long> map(    Long value) throws Exception {
      return new Tuple1<>(value);
    }
  }
);
  DataSet<Tuple1<Long>> input2=env.generateSequence(1,10).map(new MapFunction<Long,Tuple1<Long>>(){
    @Override public Tuple1<Long> map(    Long value) throws Exception {
      return new Tuple1<>(value);
    }
  }
);
  DataSet<Tuple1<Long>> distinctInput=input1.distinct();
  IterativeDataSet<Tuple1<Long>> iteration=distinctInput.iterate(10);
  DataSet<Tuple1<Long>> iterationStep=iteration.coGroup(input2).where(0).equalTo(0).with(new CoGroupFunction<Tuple1<Long>,Tuple1<Long>,Tuple1<Long>>(){
    @Override public void coGroup(    Iterable<Tuple1<Long>> first,    Iterable<Tuple1<Long>> second,    Collector<Tuple1<Long>> out) throws Exception {
      Iterator<Tuple1<Long>> it=first.iterator();
      if (it.hasNext()) {
        out.collect(it.next());
      }
    }
  }
);
  DataSet<Tuple1<Long>> iterationResult=iteration.closeWith(iterationStep);
  iterationResult.output(new DiscardingOutputFormat<Tuple1<Long>>());
  Plan p=env.createProgramPlan();
  OptimizedPlan op=compileNoStats(p);
  new JobGraphGenerator().compileJobGraph(op);
}
