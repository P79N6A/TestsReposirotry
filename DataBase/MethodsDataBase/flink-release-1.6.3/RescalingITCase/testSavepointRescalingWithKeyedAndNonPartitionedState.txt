/** 
 * Tests that a job with non partitioned state can be restarted from a savepoint with a different parallelism if the operator with non-partitioned state are not rescaled.
 * @throws Exception
 */
@Test public void testSavepointRescalingWithKeyedAndNonPartitionedState() throws Exception {
  int numberKeys=42;
  int numberElements=1000;
  int numberElements2=500;
  int parallelism=numSlots / 2;
  int parallelism2=numSlots;
  int maxParallelism=13;
  Duration timeout=Duration.ofMinutes(3);
  Deadline deadline=Deadline.now().plus(timeout);
  ClusterClient<?> client=cluster.getClusterClient();
  try {
    JobGraph jobGraph=createJobGraphWithKeyedAndNonPartitionedOperatorState(parallelism,maxParallelism,parallelism,numberKeys,numberElements,false,100);
    final JobID jobID=jobGraph.getJobID();
    client.setDetached(true);
    client.submitJob(jobGraph,RescalingITCase.class.getClassLoader());
    SubtaskIndexFlatMapper.workCompletedLatch.await(deadline.timeLeft().toMillis(),TimeUnit.MILLISECONDS);
    Set<Tuple2<Integer,Integer>> actualResult=CollectionSink.getElementsSet();
    Set<Tuple2<Integer,Integer>> expectedResult=new HashSet<>();
    for (int key=0; key < numberKeys; key++) {
      int keyGroupIndex=KeyGroupRangeAssignment.assignToKeyGroup(key,maxParallelism);
      expectedResult.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism,parallelism,keyGroupIndex),numberElements * key));
    }
    assertEquals(expectedResult,actualResult);
    CollectionSink.clearElementsSet();
    CompletableFuture<String> savepointPathFuture=client.triggerSavepoint(jobID,null);
    final String savepointPath=savepointPathFuture.get(deadline.timeLeft().toMillis(),TimeUnit.MILLISECONDS);
    client.cancel(jobID);
    while (!getRunningJobs(client).isEmpty()) {
      Thread.sleep(50);
    }
    JobGraph scaledJobGraph=createJobGraphWithKeyedAndNonPartitionedOperatorState(parallelism2,maxParallelism,parallelism,numberKeys,numberElements + numberElements2,true,100);
    scaledJobGraph.setSavepointRestoreSettings(SavepointRestoreSettings.forPath(savepointPath));
    client.setDetached(false);
    client.submitJob(scaledJobGraph,RescalingITCase.class.getClassLoader());
    Set<Tuple2<Integer,Integer>> actualResult2=CollectionSink.getElementsSet();
    Set<Tuple2<Integer,Integer>> expectedResult2=new HashSet<>();
    for (int key=0; key < numberKeys; key++) {
      int keyGroupIndex=KeyGroupRangeAssignment.assignToKeyGroup(key,maxParallelism);
      expectedResult2.add(Tuple2.of(KeyGroupRangeAssignment.computeOperatorIndexForKeyGroup(maxParallelism,parallelism2,keyGroupIndex),key * (numberElements + numberElements2)));
    }
    assertEquals(expectedResult2,actualResult2);
  }
  finally {
    CollectionSink.clearElementsSet();
  }
}
