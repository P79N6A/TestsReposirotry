/** 
 * Test restoring from an empty state taken using a previous Flink version, when some partitions could be found for topics.
 */
@Test public void testRestoreFromEmptyStateWithPartitions() throws Exception {
  final List<KafkaTopicPartition> partitions=new ArrayList<>(PARTITION_STATE.keySet());
  final DummyFlinkKafkaConsumer<String> consumerFunction=new DummyFlinkKafkaConsumer<>(partitions,FlinkKafkaConsumerBase.PARTITION_DISCOVERY_DISABLED);
  StreamSource<String,DummyFlinkKafkaConsumer<String>> consumerOperator=new StreamSource<>(consumerFunction);
  final AbstractStreamOperatorTestHarness<String> testHarness=new AbstractStreamOperatorTestHarness<>(consumerOperator,1,1,0);
  testHarness.setTimeCharacteristic(TimeCharacteristic.ProcessingTime);
  testHarness.setup();
  MigrationTestUtil.restoreFromSnapshot(testHarness,OperatorSnapshotUtil.getResourceFilename("kafka-consumer-migration-test-flink" + testMigrateVersion + "-empty-state-snapshot"),testMigrateVersion);
  testHarness.open();
  final HashMap<KafkaTopicPartition,Long> expectedSubscribedPartitionsWithStartOffsets=new HashMap<>();
  for (  KafkaTopicPartition partition : PARTITION_STATE.keySet()) {
    expectedSubscribedPartitionsWithStartOffsets.put(partition,KafkaTopicPartitionStateSentinel.EARLIEST_OFFSET);
  }
  assertTrue(consumerFunction.getSubscribedPartitionsToStartOffsets() != null);
  assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty());
  assertEquals(expectedSubscribedPartitionsWithStartOffsets,consumerFunction.getSubscribedPartitionsToStartOffsets());
  assertTrue(consumerFunction.getRestoredState() != null);
  assertTrue(!consumerFunction.getSubscribedPartitionsToStartOffsets().isEmpty());
  for (  Map.Entry<KafkaTopicPartition,Long> expectedEntry : expectedSubscribedPartitionsWithStartOffsets.entrySet()) {
    assertEquals(expectedEntry.getValue(),consumerFunction.getRestoredState().get(expectedEntry.getKey()));
  }
  consumerOperator.close();
  consumerOperator.cancel();
}
