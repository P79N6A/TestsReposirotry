/** 
 * Tests that with  {@link HighAvailabilityMode#ZOOKEEPER} distributed JARs are recoverable from anyparticipating BlobLibraryCacheManager.
 */
@Test public void testRecoveryRegisterAndDownload() throws Exception {
  Random rand=new Random();
  BlobServer[] server=new BlobServer[2];
  InetSocketAddress[] serverAddress=new InetSocketAddress[2];
  BlobLibraryCacheManager[] libServer=new BlobLibraryCacheManager[2];
  PermanentBlobCache cache=null;
  BlobStoreService blobStoreService=null;
  Configuration config=new Configuration();
  config.setString(HighAvailabilityOptions.HA_MODE,"ZOOKEEPER");
  config.setString(BlobServerOptions.STORAGE_DIRECTORY,temporaryFolder.newFolder().getAbsolutePath());
  config.setString(HighAvailabilityOptions.HA_STORAGE_PATH,temporaryFolder.newFolder().getAbsolutePath());
  config.setLong(BlobServerOptions.CLEANUP_INTERVAL,3_600L);
  try {
    blobStoreService=BlobUtils.createBlobStoreFromConfig(config);
    for (int i=0; i < server.length; i++) {
      server[i]=new BlobServer(config,blobStoreService);
      server[i].start();
      serverAddress[i]=new InetSocketAddress("localhost",server[i].getPort());
      libServer[i]=new BlobLibraryCacheManager(server[i],FlinkUserCodeClassLoaders.ResolveOrder.CHILD_FIRST,new String[0]);
    }
    byte[] expected=new byte[1024];
    rand.nextBytes(expected);
    ArrayList<PermanentBlobKey> keys=new ArrayList<>(2);
    JobID jobId=new JobID();
    keys.add(server[0].putPermanent(jobId,expected));
    byte[] expected2=Arrays.copyOfRange(expected,32,288);
    keys.add(server[0].putPermanent(jobId,expected2));
    cache=new PermanentBlobCache(config,blobStoreService,serverAddress[0]);
    ExecutionAttemptID executionId=new ExecutionAttemptID();
    libServer[0].registerTask(jobId,executionId,keys,Collections.<URL>emptyList());
    File f=cache.getFile(jobId,keys.get(0));
    assertEquals(expected.length,f.length());
    try (FileInputStream fis=new FileInputStream(f)){
      for (int i=0; i < expected.length && fis.available() > 0; i++) {
        assertEquals(expected[i],(byte)fis.read());
      }
      assertEquals(0,fis.available());
    }
     cache.close();
    cache=new PermanentBlobCache(config,blobStoreService,serverAddress[1]);
    f=cache.getFile(jobId,keys.get(0));
    assertEquals(expected.length,f.length());
    try (FileInputStream fis=new FileInputStream(f)){
      for (int i=0; i < expected.length && fis.available() > 0; i++) {
        assertEquals(expected[i],(byte)fis.read());
      }
      assertEquals(0,fis.available());
    }
     f=cache.getFile(jobId,keys.get(1));
    assertEquals(expected2.length,f.length());
    try (FileInputStream fis=new FileInputStream(f)){
      for (int i=0; i < 256 && fis.available() > 0; i++) {
        assertEquals(expected2[i],(byte)fis.read());
      }
      assertEquals(0,fis.available());
    }
     server[1].cleanupJob(jobId,true);
    final String clusterId=config.getString(HighAvailabilityOptions.HA_CLUSTER_ID);
    String haBlobStorePath=config.getString(HighAvailabilityOptions.HA_STORAGE_PATH);
    File haBlobStoreDir=new File(haBlobStorePath,clusterId);
    File[] recoveryFiles=haBlobStoreDir.listFiles();
    assertNotNull("HA storage directory does not exist",recoveryFiles);
    assertEquals("Unclean state backend: " + Arrays.toString(recoveryFiles),0,recoveryFiles.length);
  }
  finally {
    for (    BlobLibraryCacheManager s : libServer) {
      if (s != null) {
        s.shutdown();
      }
    }
    for (    BlobServer s : server) {
      if (s != null) {
        s.close();
      }
    }
    if (cache != null) {
      cache.close();
    }
    if (blobStoreService != null) {
      blobStoreService.closeAndCleanupAllData();
    }
  }
}
