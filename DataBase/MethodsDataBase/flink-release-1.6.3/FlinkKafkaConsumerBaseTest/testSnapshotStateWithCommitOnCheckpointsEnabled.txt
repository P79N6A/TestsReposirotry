@Test @SuppressWarnings("unchecked") public void testSnapshotStateWithCommitOnCheckpointsEnabled() throws Exception {
  final HashMap<KafkaTopicPartition,Long> state1=new HashMap<>();
  state1.put(new KafkaTopicPartition("abc",13),16768L);
  state1.put(new KafkaTopicPartition("def",7),987654321L);
  final HashMap<KafkaTopicPartition,Long> state2=new HashMap<>();
  state2.put(new KafkaTopicPartition("abc",13),16770L);
  state2.put(new KafkaTopicPartition("def",7),987654329L);
  final HashMap<KafkaTopicPartition,Long> state3=new HashMap<>();
  state3.put(new KafkaTopicPartition("abc",13),16780L);
  state3.put(new KafkaTopicPartition("def",7),987654377L);
  final MockFetcher<String> fetcher=new MockFetcher<>(state1,state2,state3);
  final FlinkKafkaConsumerBase<String> consumer=new DummyFlinkKafkaConsumer<>(fetcher,mock(AbstractPartitionDiscoverer.class),false);
  final TestingListState<Serializable> listState=new TestingListState<>();
  setupConsumer(consumer,false,listState,true,0,1);
  final CheckedThread runThread=new CheckedThread(){
    @Override public void go() throws Exception {
      consumer.run(new TestSourceContext<>());
    }
  }
;
  runThread.start();
  fetcher.waitUntilRun();
  assertEquals(0,consumer.getPendingOffsetsToCommit().size());
  consumer.snapshotState(new StateSnapshotContextSynchronousImpl(138,138));
  HashMap<KafkaTopicPartition,Long> snapshot1=new HashMap<>();
  for (  Serializable serializable : listState.get()) {
    Tuple2<KafkaTopicPartition,Long> kafkaTopicPartitionLongTuple2=(Tuple2<KafkaTopicPartition,Long>)serializable;
    snapshot1.put(kafkaTopicPartitionLongTuple2.f0,kafkaTopicPartitionLongTuple2.f1);
  }
  assertEquals(state1,snapshot1);
  assertEquals(1,consumer.getPendingOffsetsToCommit().size());
  assertEquals(state1,consumer.getPendingOffsetsToCommit().get(138L));
  consumer.snapshotState(new StateSnapshotContextSynchronousImpl(140,140));
  HashMap<KafkaTopicPartition,Long> snapshot2=new HashMap<>();
  for (  Serializable serializable : listState.get()) {
    Tuple2<KafkaTopicPartition,Long> kafkaTopicPartitionLongTuple2=(Tuple2<KafkaTopicPartition,Long>)serializable;
    snapshot2.put(kafkaTopicPartitionLongTuple2.f0,kafkaTopicPartitionLongTuple2.f1);
  }
  assertEquals(state2,snapshot2);
  assertEquals(2,consumer.getPendingOffsetsToCommit().size());
  assertEquals(state2,consumer.getPendingOffsetsToCommit().get(140L));
  consumer.notifyCheckpointComplete(138L);
  assertEquals(1,consumer.getPendingOffsetsToCommit().size());
  assertTrue(consumer.getPendingOffsetsToCommit().containsKey(140L));
  assertEquals(state1,fetcher.getAndClearLastCommittedOffsets());
  assertEquals(1,fetcher.getCommitCount());
  consumer.snapshotState(new StateSnapshotContextSynchronousImpl(141,141));
  HashMap<KafkaTopicPartition,Long> snapshot3=new HashMap<>();
  for (  Serializable serializable : listState.get()) {
    Tuple2<KafkaTopicPartition,Long> kafkaTopicPartitionLongTuple2=(Tuple2<KafkaTopicPartition,Long>)serializable;
    snapshot3.put(kafkaTopicPartitionLongTuple2.f0,kafkaTopicPartitionLongTuple2.f1);
  }
  assertEquals(state3,snapshot3);
  assertEquals(2,consumer.getPendingOffsetsToCommit().size());
  assertEquals(state3,consumer.getPendingOffsetsToCommit().get(141L));
  consumer.notifyCheckpointComplete(141L);
  assertEquals(0,consumer.getPendingOffsetsToCommit().size());
  assertEquals(state3,fetcher.getAndClearLastCommittedOffsets());
  assertEquals(2,fetcher.getCommitCount());
  consumer.notifyCheckpointComplete(666);
  assertEquals(0,consumer.getPendingOffsetsToCommit().size());
  assertNull(fetcher.getAndClearLastCommittedOffsets());
  assertEquals(2,fetcher.getCommitCount());
  consumer.cancel();
  runThread.sync();
}
