/** 
 * This tests verifies that custom partitioning works correctly, with a default topic and dynamic topic. The number of partitions for each topic is deliberately different. <p>Test topology: <pre> +------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+ /                  |                             |          |        | |                   |                             |          |  ------+--> (sink) +------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+ /                  | |                   | (source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+ |                   |                             |          |        | \                  |                             |          |        | +------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink) |                   |                             |          |        | \                  |                             |          |        | +------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+ </pre> <p>Each topic has an independent mapper that validates the values come consistently from the correct Kafka partition of the topic is is responsible of. <p>Each topic also has a final sink that validates that there are no duplicates and that all partitions are present.
 */
@Test public void testCustomPartitioning(){
  try {
    LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()");
    final String defaultTopic="defaultTopic";
    final int defaultTopicPartitions=2;
    final String dynamicTopic="dynamicTopic";
    final int dynamicTopicPartitions=3;
    createTestTopic(defaultTopic,defaultTopicPartitions,1);
    createTestTopic(dynamicTopic,dynamicTopicPartitions,1);
    Map<String,Integer> expectedTopicsToNumPartitions=new HashMap<>(2);
    expectedTopicsToNumPartitions.put(defaultTopic,defaultTopicPartitions);
    expectedTopicsToNumPartitions.put(dynamicTopic,dynamicTopicPartitions);
    TypeInformation<Tuple2<Long,String>> longStringInfo=TypeInformation.of(new TypeHint<Tuple2<Long,String>>(){
    }
);
    StreamExecutionEnvironment env=StreamExecutionEnvironment.getExecutionEnvironment();
    env.setRestartStrategy(RestartStrategies.noRestart());
    env.getConfig().disableSysoutLogging();
    TypeInformationSerializationSchema<Tuple2<Long,String>> serSchema=new TypeInformationSerializationSchema<>(longStringInfo,env.getConfig());
    TypeInformationSerializationSchema<Tuple2<Long,String>> deserSchema=new TypeInformationSerializationSchema<>(longStringInfo,env.getConfig());
    DataStream<Tuple2<Long,String>> stream=env.addSource(new SourceFunction<Tuple2<Long,String>>(){
      private boolean running=true;
      @Override public void run(      SourceContext<Tuple2<Long,String>> ctx) throws Exception {
        long cnt=0;
        while (running) {
          ctx.collect(new Tuple2<Long,String>(cnt,"kafka-" + cnt));
          cnt++;
          if (cnt % 100 == 0) {
            Thread.sleep(1);
          }
        }
      }
      @Override public void cancel(){
        running=false;
      }
    }
).setParallelism(1);
    Properties props=new Properties();
    props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings));
    props.putAll(secureProps);
    kafkaServer.produceIntoKafka(stream,defaultTopic,new CustomKeyedSerializationSchemaWrapper(serSchema,defaultTopic,dynamicTopic),props,new CustomPartitioner(expectedTopicsToNumPartitions)).setParallelism(Math.max(defaultTopicPartitions,dynamicTopicPartitions));
    Properties consumerProps=new Properties();
    consumerProps.putAll(standardProps);
    consumerProps.putAll(secureProps);
    FlinkKafkaConsumerBase<Tuple2<Long,String>> defaultTopicSource=kafkaServer.getConsumer(defaultTopic,deserSchema,consumerProps);
    FlinkKafkaConsumerBase<Tuple2<Long,String>> dynamicTopicSource=kafkaServer.getConsumer(dynamicTopic,deserSchema,consumerProps);
    env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions).map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions).addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1);
    env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions).map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions).addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1);
    tryExecute(env,"custom partitioning test");
    deleteTestTopic(defaultTopic);
    deleteTestTopic(dynamicTopic);
    LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()");
  }
 catch (  Exception e) {
    e.printStackTrace();
    fail(e.getMessage());
  }
}
