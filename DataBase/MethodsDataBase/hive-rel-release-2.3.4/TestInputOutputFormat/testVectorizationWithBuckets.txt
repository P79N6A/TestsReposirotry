/** 
 * Test vectorization, non-acid, non-combine.
 * @throws Exception
 */
@Test public void testVectorizationWithBuckets() throws Exception {
  StructObjectInspector inspector;
synchronized (TestOrcFile.class) {
    inspector=(StructObjectInspector)ObjectInspectorFactory.getReflectionObjectInspector(MyRow.class,ObjectInspectorFactory.ObjectInspectorOptions.JAVA);
  }
  JobConf conf=createMockExecutionEnvironment(workDir,new Path("mock:///"),"vectorBuckets",inspector,true,1);
  Path path=new Path(conf.get("mapred.input.dir") + "/0_0");
  Writer writer=OrcFile.createWriter(path,OrcFile.writerOptions(conf).blockPadding(false).bufferSize(1024).inspector(inspector));
  for (int i=0; i < 10; ++i) {
    writer.addRow(new MyRow(i,2 * i));
  }
  writer.close();
  setBlocks(path,conf,new MockBlock("host0","host1"));
  conf.setInt(hive_metastoreConstants.BUCKET_COUNT,3);
  HiveInputFormat<?,?> inputFormat=new HiveInputFormat<WritableComparable,Writable>();
  InputSplit[] splits=inputFormat.getSplits(conf,10);
  assertEquals(1,splits.length);
  org.apache.hadoop.mapred.RecordReader<NullWritable,VectorizedRowBatch> reader=inputFormat.getRecordReader(splits[0],conf,Reporter.NULL);
  NullWritable key=reader.createKey();
  VectorizedRowBatch value=reader.createValue();
  assertEquals(true,reader.next(key,value));
  assertEquals(10,value.count());
  LongColumnVector col0=(LongColumnVector)value.cols[0];
  for (int i=0; i < 10; i++) {
    assertEquals("checking " + i,i,col0.vector[i]);
  }
  assertEquals(false,reader.next(key,value));
}
