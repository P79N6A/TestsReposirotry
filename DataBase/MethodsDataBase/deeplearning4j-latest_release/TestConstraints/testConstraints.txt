@Test public void testConstraints(){
  double learningRate=0.001;
  int nIn=10;
  int lstmLayerSize=32;
  ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).weightInit(WeightInit.RELU_UNIFORM).updater(new RmsProp(learningRate)).graphBuilder().addInputs("input_lstm","input_cpc").addLayer("first_lstm_layer",new LSTM.Builder().nIn(nIn).nOut(lstmLayerSize).activation(Activation.RELU).constrainWeights(new NonNegativeConstraint()).build(),"input_lstm").addVertex("lastTimeStep",new LastTimeStepVertex("input_lstm"),"first_lstm_layer").addVertex("merge",new MergeVertex(),"lastTimeStep","input_cpc").addLayer("dense",new DenseLayer.Builder().constrainWeights(new NonNegativeConstraint()).nIn(lstmLayerSize + 1).nOut(lstmLayerSize / 2).activation(Activation.RELU).build(),"merge").addLayer("second_dense",new DenseLayer.Builder().constrainWeights(new NonNegativeConstraint()).nIn(lstmLayerSize / 2).nOut(lstmLayerSize / 8).activation(Activation.RELU).build(),"dense").addLayer("output_layer",new OutputLayer.Builder(LossFunctions.LossFunction.MSE).constrainWeights(new NonNegativeConstraint()).nIn(lstmLayerSize / 8).nOut(1).activation(Activation.IDENTITY).build(),"second_dense").setOutputs("output_layer").backpropType(BackpropType.Standard).build();
  ComputationGraph g=new ComputationGraph(conf);
  g.init();
  for (int i=0; i < 100; i++) {
    INDArray in1=Nd4j.rand(new int[]{1,nIn,5});
    INDArray in2=Nd4j.rand(new int[]{1,1});
    INDArray label=Nd4j.rand(new int[]{1,1});
    g.fit(new INDArray[]{in1,in2},new INDArray[]{label});
    for (    Map.Entry<String,INDArray> e : g.paramTable().entrySet()) {
      if (!e.getKey().contains("W")) {
        continue;
      }
      double min=e.getValue().minNumber().doubleValue();
      assertTrue(min >= 0.0);
    }
  }
}
