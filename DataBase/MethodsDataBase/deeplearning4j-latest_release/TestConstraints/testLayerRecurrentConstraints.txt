@Test public void testLayerRecurrentConstraints() throws Exception {
  LayerConstraint[] constraints=new LayerConstraint[]{new MaxNormConstraint(0.5,1),new MinMaxNormConstraint(0.3,0.4,1.0,1),new NonNegativeConstraint(),new UnitNormConstraint(1)};
  for (  LayerConstraint lc : constraints) {
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new Sgd(0.0)).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,5)).list().layer(new LSTM.Builder().nIn(12).nOut(10).constrainRecurrent(lc).build()).layer(new OutputLayer.Builder().lossFunction(LossFunctions.LossFunction.MSE).nIn(10).nOut(8).build()).build();
    MultiLayerNetwork net=new MultiLayerNetwork(conf);
    net.init();
    LayerConstraint exp=lc.clone();
    assertEquals(exp.toString(),net.getLayer(0).conf().getLayer().getConstraints().get(0).toString());
    INDArray input=Nd4j.rand(3,12);
    INDArray labels=Nd4j.rand(3,8);
    net.fit(input.reshape(3,12,1),labels);
    INDArray RW0=net.getParam("0_RW");
    if (lc instanceof MaxNormConstraint) {
      assertTrue(RW0.norm2(1).maxNumber().doubleValue() <= 0.5);
    }
 else     if (lc instanceof MinMaxNormConstraint) {
      assertTrue(RW0.norm2(1).minNumber().doubleValue() >= 0.3);
      assertTrue(RW0.norm2(1).maxNumber().doubleValue() <= 0.4);
    }
 else     if (lc instanceof NonNegativeConstraint) {
      assertTrue(RW0.minNumber().doubleValue() >= 0.0);
    }
 else     if (lc instanceof UnitNormConstraint) {
      assertEquals(RW0.norm2(1).minNumber().doubleValue(),1.0,1e-6);
      assertEquals(RW0.norm2(1).maxNumber().doubleValue(),1.0,1e-6);
    }
    TestUtils.testModelSerialization(net);
  }
}
