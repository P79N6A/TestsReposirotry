@Test public void testBatchNorm(){
  Nd4j.getRandom().setSeed(123);
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.LINE_GRADIENT_DESCENT).seed(123).list().layer(0,new DenseLayer.Builder().nIn(4).nOut(3).weightInit(WeightInit.XAVIER).activation(Activation.TANH).build()).layer(1,new DenseLayer.Builder().nIn(3).nOut(2).weightInit(WeightInit.XAVIER).activation(Activation.TANH).build()).layer(2,new BatchNormalization.Builder().nOut(2).build()).layer(3,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).weightInit(WeightInit.XAVIER).activation(Activation.SOFTMAX).nIn(2).nOut(3).build()).build();
  MultiLayerNetwork network=new MultiLayerNetwork(conf);
  network.init();
  network.setListeners(new ScoreIterationListener(1));
  DataSetIterator iter=new IrisDataSetIterator(150,150);
  DataSet next=iter.next();
  next.normalizeZeroMeanZeroUnitVariance();
  SplitTestAndTrain trainTest=next.splitTestAndTrain(110);
  network.setLabels(trainTest.getTrain().getLabels());
  network.init();
  for (int i=0; i < 5; i++) {
    network.fit(trainTest.getTrain());
  }
}
