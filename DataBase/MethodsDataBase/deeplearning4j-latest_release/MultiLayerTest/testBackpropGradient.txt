@Test public void testBackpropGradient(){
  int nIn=10;
  int nOut=40;
  int miniBatch=5;
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new Sgd(0.1)).list().layer(0,new DenseLayer.Builder().nIn(nIn).nOut(20).activation(Activation.RELU).weightInit(WeightInit.XAVIER).build()).layer(1,new DenseLayer.Builder().nIn(20).nOut(30).activation(Activation.RELU).weightInit(WeightInit.XAVIER).build()).layer(2,new DenseLayer.Builder().nIn(30).nOut(nOut).activation(Activation.RELU).weightInit(WeightInit.XAVIER).build()).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  Nd4j.getRandom().setSeed(12345);
  INDArray eps=Nd4j.rand(miniBatch,nOut);
  INDArray input=Nd4j.rand(miniBatch,nIn);
  net.setInput(input);
  net.feedForward(true,false);
  Pair<Gradient,INDArray> pair=net.backpropGradient(eps,LayerWorkspaceMgr.noWorkspaces());
  INDArray epsOut=pair.getSecond();
  assertNotNull(epsOut);
  assertArrayEquals(new long[]{miniBatch,nIn},epsOut.shape());
  Gradient g=pair.getFirst();
  Map<String,INDArray> gradMap=g.gradientForVariable();
  assertEquals(6,gradMap.size());
  String[] expKeys={"0_" + DefaultParamInitializer.WEIGHT_KEY,"0_" + DefaultParamInitializer.BIAS_KEY,"1_" + DefaultParamInitializer.WEIGHT_KEY,"2_" + DefaultParamInitializer.BIAS_KEY,"2_" + DefaultParamInitializer.WEIGHT_KEY,"2_" + DefaultParamInitializer.BIAS_KEY};
  Set<String> keys=gradMap.keySet();
  for (  String s : expKeys) {
    assertTrue(keys.contains(s));
  }
}
