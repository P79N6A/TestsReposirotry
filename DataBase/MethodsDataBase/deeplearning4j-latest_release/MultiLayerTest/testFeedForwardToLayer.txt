@Test public void testFeedForwardToLayer(){
  int nIn=30;
  int nOut=25;
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT).updater(new Sgd(1e-3)).list().layer(0,new DenseLayer.Builder().nIn(nIn).nOut(600).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1e-5)).build()).layer(1,new DenseLayer.Builder().nIn(600).nOut(250).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1e-5)).build()).layer(2,new DenseLayer.Builder().nIn(250).nOut(100).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1e-5)).build()).layer(3,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(100).nOut(25).activation(Activation.SOFTMAX).weightInit(new NormalDistribution(0,1e-5)).build()).build();
  MultiLayerNetwork network=new MultiLayerNetwork(conf);
  network.init();
  INDArray input=Nd4j.rand(5,nIn);
  List<INDArray> activations=network.feedForward(input);
  assertEquals(5,activations.size());
  List<INDArray> activationsAll=network.feedForwardToLayer(3,input);
  assertEquals(activations,activationsAll);
  for (int i=3; i >= 0; i--) {
    List<INDArray> activationsPartial=network.feedForwardToLayer(i,input);
    assertEquals(i + 2,activationsPartial.size());
    for (int j=0; j <= i; j++) {
      INDArray exp=activationsAll.get(j);
      INDArray act=activationsPartial.get(j);
      assertEquals(exp,act);
    }
  }
}
