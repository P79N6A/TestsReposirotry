@Test public void testExternalErrors2(){
  Nd4j.getExecutioner().setProfilingMode(OpExecutioner.ProfilingMode.SCOPE_PANIC);
  int nIn=4;
  int nOut=3;
  for (  WorkspaceMode ws : WorkspaceMode.values()) {
    System.out.println("***** WORKSPACE: " + ws);
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new Adam(0.01)).trainingWorkspaceMode(ws).inferenceWorkspaceMode(ws).list().layer(new DenseLayer.Builder().nIn(nIn).nOut(nOut).activation(Activation.RELU).build()).layer(new ActivationLayer.Builder().activation(Activation.IDENTITY).build()).inputPreProcessor(0,new RnnToFeedForwardPreProcessor()).inputPreProcessor(1,new FeedForwardToRnnPreProcessor()).build();
    MultiLayerNetwork graph=new MultiLayerNetwork(conf);
    graph.init();
    final int minibatch=5;
    final int seqLen=6;
    INDArray param=Nd4j.create(new double[]{0.54,0.31,0.98,-0.30,-0.66,-0.19,-0.29,-0.62,0.13,-0.32,0.01,-0.03,0.00,0.00,0.00});
    graph.setParams(param);
    INDArray input=Nd4j.rand(new int[]{minibatch,nIn,seqLen},12);
    INDArray expected=Nd4j.ones(minibatch,nOut,seqLen);
    graph.setInput(input);
    INDArray output=graph.feedForward(false,false).get(2);
    INDArray error=output.sub(expected);
    for (    org.deeplearning4j.nn.api.Layer l : graph.getLayers()) {
      assertNotNull(l.input());
      assertFalse(l.input().isAttached());
    }
    Pair<Gradient,INDArray> gradient=graph.backpropGradient(error,LayerWorkspaceMgr.noWorkspaces());
    graph.getUpdater().update(graph,gradient.getFirst(),0,0,minibatch,LayerWorkspaceMgr.noWorkspaces());
    Nd4j.getWorkspaceManager().destroyAllWorkspacesForCurrentThread();
  }
  Nd4j.getExecutioner().setProfilingMode(OpExecutioner.ProfilingMode.DISABLED);
}
