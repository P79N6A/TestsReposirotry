@Test public void testSameDiffDenseBackward(){
  int nIn=3;
  int nOut=4;
  for (  boolean workspaces : new boolean[]{false,true}) {
    for (    int minibatch : new int[]{5,1}) {
      Activation[] afns=new Activation[]{Activation.TANH,Activation.SIGMOID,Activation.ELU,Activation.IDENTITY,Activation.SOFTPLUS,Activation.SOFTSIGN,Activation.HARDTANH,Activation.CUBE,Activation.RELU};
      for (      Activation a : afns) {
        log.info("Starting test - " + a + " - minibatch "+ minibatch+ ", workspaces: "+ workspaces);
        MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().trainingWorkspaceMode(workspaces ? WorkspaceMode.ENABLED : WorkspaceMode.NONE).inferenceWorkspaceMode(workspaces ? WorkspaceMode.ENABLED : WorkspaceMode.NONE).list().layer(new SameDiffDense.Builder().nIn(nIn).nOut(nOut).activation(a).build()).layer(new OutputLayer.Builder().nIn(nOut).nOut(nOut).activation(Activation.SOFTMAX).lossFunction(LossFunctions.LossFunction.MCXENT).build()).build();
        MultiLayerNetwork netSD=new MultiLayerNetwork(conf);
        netSD.init();
        MultiLayerConfiguration conf2=new NeuralNetConfiguration.Builder().list().layer(new DenseLayer.Builder().activation(a).nIn(nIn).nOut(nOut).build()).layer(new OutputLayer.Builder().nIn(nOut).nOut(nOut).activation(Activation.SOFTMAX).lossFunction(LossFunctions.LossFunction.MCXENT).build()).build();
        MultiLayerNetwork netStandard=new MultiLayerNetwork(conf2);
        netStandard.init();
        netSD.params().assign(netStandard.params());
        assertEquals(netStandard.params(),netSD.params());
        assertEquals(netStandard.paramTable(),netSD.paramTable());
        INDArray in=Nd4j.rand(minibatch,nIn);
        INDArray l=TestUtils.randomOneHot(minibatch,nOut,12345);
        netSD.setInput(in);
        netStandard.setInput(in);
        netSD.setLabels(l);
        netStandard.setLabels(l);
        netSD.computeGradientAndScore();
        netStandard.computeGradientAndScore();
        Gradient gSD=netSD.gradient();
        Gradient gStd=netStandard.gradient();
        Map<String,INDArray> m1=gSD.gradientForVariable();
        Map<String,INDArray> m2=gStd.gradientForVariable();
        assertEquals(m2.keySet(),m1.keySet());
        for (        String s : m1.keySet()) {
          INDArray i1=m1.get(s);
          INDArray i2=m2.get(s);
          assertEquals(s,i2,i1);
        }
        assertEquals(gStd.gradient(),gSD.gradient());
        in=Nd4j.rand(2 * minibatch,nIn);
        l=TestUtils.randomOneHot(2 * minibatch,nOut,12345);
        netSD.setInput(in);
        netStandard.setInput(in);
        netSD.setLabels(l);
        netStandard.setLabels(l);
        netSD.computeGradientAndScore();
      }
    }
  }
}
