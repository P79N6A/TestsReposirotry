@Test public void testGradientGravesBidirectionalLSTMFull(){
  Activation[] activFns={Activation.TANH,Activation.SOFTSIGN};
  LossFunction[] lossFunctions={LossFunction.MCXENT,LossFunction.MSE};
  Activation[] outputActivations={Activation.SOFTMAX,Activation.TANH};
  int timeSeriesLength=4;
  int nIn=2;
  int layerSize=2;
  int nOut=2;
  int miniBatchSize=3;
  Random r=new Random(12345L);
  INDArray input=Nd4j.zeros(miniBatchSize,nIn,timeSeriesLength);
  for (int i=0; i < miniBatchSize; i++) {
    for (int j=0; j < nIn; j++) {
      for (int k=0; k < timeSeriesLength; k++) {
        input.putScalar(new int[]{i,j,k},r.nextDouble() - 0.5);
      }
    }
  }
  INDArray labels=Nd4j.zeros(miniBatchSize,nOut,timeSeriesLength);
  for (int i=0; i < miniBatchSize; i++) {
    for (int j=0; j < timeSeriesLength; j++) {
      int idx=r.nextInt(nOut);
      labels.putScalar(new int[]{i,idx,j},1.0f);
    }
  }
  double[] l2vals={0.4,0.0,0.4,0.4};
  double[] l1vals={0.0,0.0,0.5,0.0};
  double[] biasL2={0.0,0.0,0.0,0.2};
  double[] biasL1={0.0,0.0,0.6,0.0};
  for (  Activation afn : activFns) {
    for (int i=0; i < lossFunctions.length; i++) {
      for (int k=0; k < l2vals.length; k++) {
        LossFunction lf=lossFunctions[i];
        Activation outputActivation=outputActivations[i];
        double l2=l2vals[k];
        double l1=l1vals[k];
        NeuralNetConfiguration.Builder conf=new NeuralNetConfiguration.Builder();
        if (l1 > 0.0)         conf.l1(l1);
        if (l2 > 0.0)         conf.l2(l2);
        if (biasL2[k] > 0)         conf.l2Bias(biasL2[k]);
        if (biasL1[k] > 0)         conf.l1Bias(biasL1[k]);
        MultiLayerConfiguration mlc=conf.seed(12345L).list().layer(0,new GravesBidirectionalLSTM.Builder().nIn(nIn).nOut(layerSize).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).activation(afn).updater(Updater.NONE).build()).layer(1,new RnnOutputLayer.Builder(lf).activation(outputActivation).nIn(layerSize).nOut(nOut).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).updater(new NoOp()).build()).build();
        MultiLayerNetwork mln=new MultiLayerNetwork(mlc);
        mln.init();
        if (PRINT_RESULTS) {
          System.out.println("testGradientGravesBidirectionalLSTMFull() - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", l2="+ l2+ ", l1="+ l1);
          for (int j=0; j < mln.getnLayers(); j++)           System.out.println("Layer " + j + " # params: "+ mln.getLayer(j).numParams());
        }
        boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
        String msg="testGradientGravesLSTMFull() - activationFn=" + afn + ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", l2="+ l2+ ", l1="+ l1;
        assertTrue(msg,gradOK);
        TestUtils.testModelSerialization(mln);
      }
    }
  }
}
