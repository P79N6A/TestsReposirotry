@Test public void testComprehensive(){
  BaseActivationFunction a1=new ActivationTanH();
  BaseActivationFunction a2=new ActivationSigmoid();
  INDArray input=Nd4j.create(new double[][]{{0.2,0.3,0.5},{0.7,1.1,1.3},{1.7,1.9,2.3},{2.9,3.1,3.7}});
  double sf=4.1;
  INDArray target=Nd4j.create(new double[][]{{0.05,0.10,0.15,0.20,0.25},{0.30,0.35,0.40,0.45,0.50},{0.55,0.60,0.65,0.70,0.75},{0.80,0.85,0.90,0.95,0.99}});
  ComputationGraphConfiguration cgc=new NeuralNetConfiguration.Builder().weightInit(WeightInit.XAVIER).updater(new Sgd(0.01)).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).graphBuilder().addInputs("input").addLayer("denselayer",new DenseLayer.Builder().nIn(input.columns()).nOut(input.columns()).activation(a1).build(),"input").addVertex("shiftvertex",new ShiftVertex(sf),"denselayer").addLayer("output",new OutputLayer.Builder().nIn(input.columns()).nOut(target.columns()).activation(a2).lossFunction(LossFunction.MSE).build(),"shiftvertex").setOutputs("output").build();
  ComputationGraph cg=new ComputationGraph(cgc);
  cg.init();
  cg.setInput(0,input);
  cg.setLabel(0,target);
  cg.computeGradientAndScore();
  double score_dl4j=cg.score();
  Map<String,INDArray> weights=cg.paramTable();
  Gradient g=cg.gradient();
  Map<String,INDArray> gradients=g.gradientForVariable();
  Map<String,INDArray> manual_gradients=new TreeMap<String,INDArray>();
  INDArray W=nullsafe(weights.get("denselayer_W"));
  INDArray b=nullsafe(weights.get("denselayer_b"));
  INDArray V=nullsafe(weights.get("output_W"));
  INDArray c=nullsafe(weights.get("output_b"));
  Map<String,INDArray> manual_weights=new TreeMap<String,INDArray>();
  manual_weights.put("denselayer_W",W);
  manual_weights.put("denselayer_b",b);
  manual_weights.put("output_W",V);
  manual_weights.put("output_b",c);
  int batchsz=(int)input.shape()[0];
  INDArray z=input.mmul(W).add(b.repmat(batchsz,1));
  INDArray a=a1.getActivation(z.dup(),true).add(sf);
  INDArray q=a.mmul(V).add(c.repmat(batchsz,1));
  INDArray o=nullsafe(a2.getActivation(q.dup(),true));
  double score_manual=sum_errors(o,target) / (o.columns() * o.rows());
  INDArray dEdo=Nd4j.zeros(target.shape());
  dEdo.addi(o).subi(target).muli(2);
  dEdo.divi(target.shape()[1]);
  Pair<INDArray,INDArray> derivs2=a2.backprop(q,dEdo);
  INDArray dEdq=derivs2.getFirst();
  INDArray dqdc=Nd4j.ones(1,batchsz);
  INDArray dEdc=dqdc.mmul(dEdq);
  INDArray dEdV=a.transpose().mmul(dEdq);
  INDArray dEda=dEdq.mmul(V.transpose());
  Pair<INDArray,INDArray> derivs1=a1.backprop(z,dEda);
  INDArray dEdz=derivs1.getFirst();
  INDArray dzdb=Nd4j.ones(1,batchsz);
  INDArray dEdb=dzdb.mmul(dEdz);
  INDArray dEdW=input.transpose().mmul(dEdz);
  manual_gradients.put("output_b",dEdc);
  manual_gradients.put("output_W",dEdV);
  manual_gradients.put("denselayer_b",dEdb);
  manual_gradients.put("denselayer_W",dEdW);
  double summse=Math.pow((score_manual - score_dl4j),2);
  int denominator=1;
  for (  Map.Entry<String,INDArray> mesi : gradients.entrySet()) {
    String name=mesi.getKey();
    INDArray dl4j_gradient=nullsafe(mesi.getValue());
    INDArray manual_gradient=nullsafe(manual_gradients.get(name));
    double se=sum_errors(dl4j_gradient,manual_gradient);
    summse+=se;
    denominator+=dl4j_gradient.columns() * dl4j_gradient.rows();
  }
  Assert.assertEquals(0.0,summse / denominator,this.epsilon);
}
