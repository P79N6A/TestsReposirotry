@Test public void testModelSerializerFrozenLayersCompGraph() throws Exception {
  FineTuneConfiguration finetune=new FineTuneConfiguration.Builder().updater(new Sgd(0.1)).build();
  int nIn=6;
  int nOut=3;
  ComputationGraphConfiguration origConf=new NeuralNetConfiguration.Builder().activation(Activation.TANH).graphBuilder().addInputs("in").addLayer("0",new DenseLayer.Builder().nIn(nIn).nOut(5).build(),"in").addLayer("1",new DenseLayer.Builder().nIn(5).nOut(4).build(),"0").addLayer("2",new DenseLayer.Builder().nIn(4).nOut(3).build(),"1").addLayer("3",new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(3).nOut(nOut).build(),"2").setOutputs("3").build();
  ComputationGraph origModel=new ComputationGraph(origConf);
  origModel.init();
  ComputationGraph withFrozen=new TransferLearning.GraphBuilder(origModel).fineTuneConfiguration(finetune).setFeatureExtractor("1").build();
  assertTrue(withFrozen.getLayer(0) instanceof FrozenLayer);
  assertTrue(withFrozen.getLayer(1) instanceof FrozenLayer);
  Map<String,GraphVertex> m=withFrozen.getConfiguration().getVertices();
  Layer l0=((LayerVertex)m.get("0")).getLayerConf().getLayer();
  Layer l1=((LayerVertex)m.get("1")).getLayerConf().getLayer();
  assertTrue(l0 instanceof org.deeplearning4j.nn.conf.layers.misc.FrozenLayer);
  assertTrue(l1 instanceof org.deeplearning4j.nn.conf.layers.misc.FrozenLayer);
  ComputationGraph restored=TestUtils.testModelSerialization(withFrozen);
  assertTrue(restored.getLayer(0) instanceof FrozenLayer);
  assertTrue(restored.getLayer(1) instanceof FrozenLayer);
  assertFalse(restored.getLayer(2) instanceof FrozenLayer);
  assertFalse(restored.getLayer(3) instanceof FrozenLayer);
  INDArray in=Nd4j.rand(3,nIn);
  INDArray out=withFrozen.outputSingle(in);
  INDArray out2=restored.outputSingle(in);
  assertEquals(out,out2);
  out=withFrozen.outputSingle(true,in);
  out2=restored.outputSingle(true,in);
}
