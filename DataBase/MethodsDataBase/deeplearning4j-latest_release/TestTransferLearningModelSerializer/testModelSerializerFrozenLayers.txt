@Test public void testModelSerializerFrozenLayers() throws Exception {
  FineTuneConfiguration finetune=new FineTuneConfiguration.Builder().updater(new Sgd(0.1)).build();
  int nIn=6;
  int nOut=3;
  MultiLayerConfiguration origConf=new NeuralNetConfiguration.Builder().updater(new Sgd(0.1)).activation(Activation.TANH).dropOut(0.5).list().layer(0,new DenseLayer.Builder().nIn(nIn).nOut(5).build()).layer(1,new DenseLayer.Builder().nIn(5).nOut(4).build()).layer(2,new DenseLayer.Builder().nIn(4).nOut(3).build()).layer(3,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(3).nOut(nOut).build()).build();
  MultiLayerNetwork origModel=new MultiLayerNetwork(origConf);
  origModel.init();
  MultiLayerNetwork withFrozen=new TransferLearning.Builder(origModel).fineTuneConfiguration(finetune).setFeatureExtractor(1).build();
  assertTrue(withFrozen.getLayer(0) instanceof FrozenLayer);
  assertTrue(withFrozen.getLayer(1) instanceof FrozenLayer);
  assertTrue(withFrozen.getLayerWiseConfigurations().getConf(0).getLayer() instanceof org.deeplearning4j.nn.conf.layers.misc.FrozenLayer);
  assertTrue(withFrozen.getLayerWiseConfigurations().getConf(1).getLayer() instanceof org.deeplearning4j.nn.conf.layers.misc.FrozenLayer);
  MultiLayerNetwork restored=TestUtils.testModelSerialization(withFrozen);
  assertTrue(restored.getLayer(0) instanceof FrozenLayer);
  assertTrue(restored.getLayer(1) instanceof FrozenLayer);
  assertFalse(restored.getLayer(2) instanceof FrozenLayer);
  assertFalse(restored.getLayer(3) instanceof FrozenLayer);
  INDArray in=Nd4j.rand(3,nIn);
  INDArray out=withFrozen.output(in);
  INDArray out2=restored.output(in);
  assertEquals(out,out2);
  out=withFrozen.output(in,true);
  out2=restored.output(in,true);
}
