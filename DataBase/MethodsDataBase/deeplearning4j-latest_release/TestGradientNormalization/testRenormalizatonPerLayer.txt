@Test public void testRenormalizatonPerLayer(){
  Nd4j.getRandom().setSeed(12345);
  NeuralNetConfiguration conf=new NeuralNetConfiguration.Builder().layer(new DenseLayer.Builder().nIn(10).nOut(20).updater(new NoOp()).gradientNormalization(GradientNormalization.RenormalizeL2PerLayer).build()).build();
  long numParams=conf.getLayer().initializer().numParams(conf);
  INDArray params=Nd4j.create(1,numParams);
  Layer layer=conf.getLayer().instantiate(conf,null,0,params,true);
  INDArray gradArray=Nd4j.rand(1,220).muli(10).subi(5);
  layer.setBackpropGradientsViewArray(gradArray);
  INDArray weightGrad=Shape.newShapeNoCopy(gradArray.get(NDArrayIndex.point(0),NDArrayIndex.interval(0,200)),new int[]{10,20},true);
  INDArray biasGrad=gradArray.get(NDArrayIndex.point(0),NDArrayIndex.interval(200,220));
  INDArray weightGradCopy=weightGrad.dup();
  INDArray biasGradCopy=biasGrad.dup();
  Gradient gradient=new DefaultGradient(gradArray);
  gradient.setGradientFor(DefaultParamInitializer.WEIGHT_KEY,weightGrad);
  gradient.setGradientFor(DefaultParamInitializer.BIAS_KEY,biasGrad);
  Updater updater=UpdaterCreator.getUpdater(layer);
  updater.update(layer,gradient,0,0,1,LayerWorkspaceMgr.noWorkspaces());
  assertNotEquals(weightGradCopy,weightGrad);
  assertNotEquals(biasGradCopy,biasGrad);
  double sumSquaresWeight=weightGradCopy.mul(weightGradCopy).sumNumber().doubleValue();
  double sumSquaresBias=biasGradCopy.mul(biasGradCopy).sumNumber().doubleValue();
  double sumSquares=sumSquaresWeight + sumSquaresBias;
  double l2Layer=Math.sqrt(sumSquares);
  INDArray normWeightsExpected=weightGradCopy.div(l2Layer);
  INDArray normBiasExpected=biasGradCopy.div(l2Layer);
  double l2Weight=gradient.getGradientFor(DefaultParamInitializer.WEIGHT_KEY).norm2Number().doubleValue();
  double l2Bias=gradient.getGradientFor(DefaultParamInitializer.BIAS_KEY).norm2Number().doubleValue();
  assertTrue(!Double.isNaN(l2Weight) && l2Weight > 0.0);
  assertTrue(!Double.isNaN(l2Bias) && l2Bias > 0.0);
  assertEquals(normWeightsExpected,gradient.getGradientFor(DefaultParamInitializer.WEIGHT_KEY));
  assertEquals(normBiasExpected,gradient.getGradientFor(DefaultParamInitializer.BIAS_KEY));
}
