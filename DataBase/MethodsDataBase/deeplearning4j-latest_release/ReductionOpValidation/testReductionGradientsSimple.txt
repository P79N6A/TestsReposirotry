@Test public void testReductionGradientsSimple(){
  Nd4j.getRandom().setSeed(12345);
  List<String> failed=new ArrayList<>();
  for (int i=0; i < 21; i++) {
    SameDiff sd=SameDiff.create();
    int nOut=4;
    int minibatch=10;
    SDVariable input=sd.var("in",new int[]{-1,nOut});
    INDArray inputArr=Nd4j.randn(minibatch,nOut).muli(100);
    long length=nOut * minibatch;
    SDVariable loss;
    String name;
    TestCase tc=new TestCase(sd);
switch (i) {
case 0:
      loss=sd.mean("loss",input);
    name="mean";
  tc.expectedOutput("loss",inputArr.mean());
break;
case 1:
loss=sd.sum("loss",input);
name="sum";
tc.expectedOutput("loss",inputArr.sum());
break;
case 2:
loss=sd.standardDeviation("loss",input,true);
name="stdev";
tc.expectedOutput("loss",inputArr.std(true));
break;
case 3:
loss=sd.min("loss",input);
name="min";
tc.expectedOutput("loss",inputArr.min());
break;
case 4:
loss=sd.max("loss",input);
name="max";
tc.expectedOutput("loss",inputArr.max());
break;
case 5:
loss=sd.variance("loss",input,true);
name="variance";
tc.expectedOutput("loss",inputArr.var());
break;
case 6:
inputArr=Nd4j.rand(minibatch,nOut).addi(0.5);
loss=sd.prod("loss",input);
tc.expectedOutput("loss",inputArr.prod());
name="prod";
break;
case 7:
loss=sd.norm1("loss",input);
name="norm1";
tc.expectedOutput("loss",inputArr.norm1());
break;
case 8:
loss=sd.norm2("loss",input);
name="norm2";
tc.expectedOutput("loss",inputArr.norm2());
break;
case 9:
loss=sd.normmax("loss",input);
name="normmax";
tc.expectedOutput("loss",inputArr.normmax());
break;
case 10:
loss=sd.countNonZero("loss",input);
name="countNonZero";
tc.expectedOutput("loss",Nd4j.trueScalar(inputArr.length()));
break;
case 11:
loss=sd.countZero("loss",input);
name="countZero";
tc.expectedOutput("loss",Nd4j.trueScalar(0));
break;
case 12:
loss=sd.amax("loss",input);
name="amax";
tc.expectedOutput("loss",inputArr.amax());
break;
case 13:
loss=sd.amin("loss",input);
name="amin";
tc.expectedOutput("loss",inputArr.amin());
break;
case 14:
loss=sd.asum("loss",input);
name="asum";
tc.expectedOutput("loss",Nd4j.getExecutioner().exec(new ASum(inputArr.dup())).z());
break;
case 15:
loss=sd.amean("loss",input);
name="amean";
tc.expectedOutput("loss",Nd4j.getExecutioner().exec(new AMean(inputArr.dup())).z());
break;
case 16:
loss=sd.entropy("loss",input);
name="entropy";
inputArr=Nd4j.linspace(0.01,0.99,length).reshape('c',minibatch,nOut);
tc.expected("loss",inputArr.mul(Transforms.log(inputArr,true)).sum(Integer.MAX_VALUE).negi());
break;
case 17:
inputArr=Nd4j.rand(minibatch,nOut);
name="logsumexp";
loss=sd.logSumExp("loss",input);
INDArray expArr=Transforms.exp(inputArr);
double sum=expArr.sumNumber().doubleValue();
tc.expected("loss",Nd4j.create(new double[]{Math.log(sum)}));
break;
case 18:
inputArr=Nd4j.rand(minibatch,nOut);
name="sqnorm";
loss=sd.squaredNorm("loss",input);
double norm2=inputArr.norm2Number().doubleValue();
tc.expected("loss",Nd4j.trueScalar(norm2 * norm2));
break;
case 19:
inputArr=Nd4j.rand(minibatch,nOut);
name="logEntropy";
loss=sd.logEntropy("loss",input);
double logEntropy=inputArr.logEntropyNumber().doubleValue();
tc.expected(loss,Nd4j.trueScalar(logEntropy));
break;
case 20:
inputArr=Nd4j.rand(minibatch,nOut);
name="shannonEntropy";
loss=sd.shannonEntropy("loss",input);
double shannonEntropy=inputArr.shannonEntropyNumber().doubleValue();
tc.expected(loss,Nd4j.trueScalar(shannonEntropy));
if (OpValidationSuite.IGNORE_FAILING) {
continue;
}
break;
default :
throw new RuntimeException();
}
String msg="test: " + i + " - "+ name;
log.info("*** Starting test: " + msg);
sd.associateArrayWithVariable(inputArr,input);
tc.testName(msg);
String error=OpValidation.validate(tc,true);
if (error != null) failed.add(error);
}
assertEquals(failed.toString(),0,failed.size());
}
