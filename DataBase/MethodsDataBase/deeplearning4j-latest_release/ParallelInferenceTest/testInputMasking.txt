@Test(timeout=60000) public void testInputMasking() throws Exception {
  Nd4j.getRandom().setSeed(12345);
  int nIn=10;
  int tsLength=16;
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).list().layer(new LSTM.Builder().nIn(nIn).nOut(5).build()).layer(new GlobalPoolingLayer(PoolingType.AVG)).layer(new OutputLayer.Builder().nIn(5).nOut(5).activation(Activation.SOFTMAX).build()).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  Random r=new Random();
  for (  InferenceMode m : InferenceMode.values()) {
    log.info("Testing inference mode: [{}]",m);
    for (    int w : new int[]{1,2}) {
      for (      boolean randomTSLength : new boolean[]{false,true}) {
        final ParallelInference inf=new ParallelInference.Builder(net).inferenceMode(m).batchLimit(5).queueLimit(64).workers(w).build();
        List<INDArray> in=new ArrayList<>();
        List<INDArray> inMasks=new ArrayList<>();
        List<INDArray> exp=new ArrayList<>();
        for (int i=0; i < 100; i++) {
          int currTSLength=(randomTSLength ? 1 + r.nextInt(tsLength) : tsLength);
          int currNumEx=1 + r.nextInt(3);
          INDArray inArr=Nd4j.rand(new int[]{currNumEx,nIn,currTSLength});
          in.add(inArr);
          INDArray inMask=null;
          if (r.nextDouble() < 0.5) {
            inMask=Nd4j.ones(currNumEx,currTSLength);
            for (int mb=0; mb < currNumEx; mb++) {
              if (currTSLength > 1) {
                int firstMaskedStep=1 + r.nextInt(currTSLength);
                for (int j=firstMaskedStep; j < currTSLength; j++) {
                  inMask.putScalar(mb,j,0.0);
                }
              }
            }
          }
          inMasks.add(inMask);
          INDArray out=net.output(inArr,false,inMask,null);
          exp.add(out);
        }
        testParallelInference(inf,in,inMasks,exp);
        inf.shutdown();
      }
    }
  }
}
