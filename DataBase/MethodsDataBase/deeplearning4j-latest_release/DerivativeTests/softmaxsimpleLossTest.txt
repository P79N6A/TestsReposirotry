@Test public void softmaxsimpleLossTest(){
  INDArray X=Nd4j.rand(1,2);
  INDArray YHat=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(X.dup()));
  INDArray Y=Nd4j.create(new double[][]{{0.123,1 - 0.123}});
  double lossHere=Transforms.pow(Y.sub(YHat),2).sumNumber().doubleValue();
  INDArray softmaxDer=Nd4j.getExecutioner().execAndReturn(new SoftMaxDerivative(X.dup()));
  INDArray dLdY=Y.sub(YHat).mul(-2);
  INDArray currentGradient=dLdY.mul(softmaxDer);
  double y0y1=softmaxDer.getDouble(0,0);
  INDArray mysoftmaxDer=correctSoftmax(X);
  INDArray myGradient=mysoftmaxDer.mulRowVector(dLdY).sum(1);
  double epsilon=0.0001;
  INDArray Xiplus, Ximinus;
  INDArray YHatplus, YHatminus;
  double lossplus, lossminus;
  INDArray numGradient=Nd4j.zeros(1,2);
  for (int i=0; i < 2; i++) {
    double x=X.getDouble(0,i);
    Xiplus=X.dup();
    Xiplus.put(0,i,x + epsilon);
    YHatplus=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(Xiplus.dup()));
    lossplus=Transforms.pow(Y.sub(YHatplus),2).sumNumber().doubleValue();
    Ximinus=X.dup();
    Ximinus.put(0,i,x - epsilon);
    YHatminus=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(Ximinus.dup()));
    lossminus=Transforms.pow(Y.sub(YHatminus),2).sumNumber().doubleValue();
    double gradienti=(lossplus - lossminus) / (2 * epsilon);
    numGradient.put(0,i,gradienti);
  }
  System.out.println("=========================");
  System.out.println("NUMERICAL:");
  System.out.println(numGradient);
  System.out.println("\nCURRENTLY:");
  System.out.println(currentGradient);
  System.out.println("\nMY GRADIENT:");
  System.out.println(myGradient + "\n");
  System.out.println("Because of the nature of the derivative of the softmax for length = 2, our current method will make it off by a factor of 2");
  System.out.println("=========================");
}
