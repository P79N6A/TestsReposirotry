@Test public void softmaxsimplelongerlengthLossTest(){
  int someLength=7;
  INDArray X=Nd4j.rand(1,someLength);
  INDArray YHat=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(X.dup()));
  INDArray temp=Nd4j.rand(1,someLength);
  INDArray Y=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(temp));
  double lossHere=Transforms.pow(Y.sub(YHat),2).sumNumber().doubleValue();
  INDArray softmaxDer=Nd4j.getExecutioner().execAndReturn(new SoftMaxDerivative(X.dup()));
  INDArray dLdY=Y.sub(YHat).mul(-2);
  INDArray currentGradient=dLdY.mul(softmaxDer);
  INDArray mysoftmaxDer=correctSoftmax(X);
  INDArray myGradient=mysoftmaxDer.mulRowVector(dLdY).sum(1);
  double epsilon=0.0001;
  INDArray Xiplus, Ximinus;
  INDArray YHatplus, YHatminus;
  double lossplus, lossminus;
  INDArray numGradient=Nd4j.zeros(1,someLength);
  for (int i=0; i < someLength; i++) {
    double x=X.getDouble(0,i);
    Xiplus=X.dup();
    Xiplus.put(0,i,x + epsilon);
    YHatplus=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(Xiplus.dup()));
    lossplus=Transforms.pow(Y.sub(YHatplus),2).sumNumber().doubleValue();
    Ximinus=X.dup();
    Ximinus.put(0,i,x - epsilon);
    YHatminus=Nd4j.getExecutioner().execAndReturn(new OldSoftMax(Ximinus.dup()));
    lossminus=Transforms.pow(Y.sub(YHatminus),2).sumNumber().doubleValue();
    double gradienti=(lossplus - lossminus) / (2 * epsilon);
    numGradient.put(0,i,gradienti);
  }
  System.out.println("=========================");
  System.out.println("NUMERICAL GRADIENT:");
  System.out.println(new NDArrayStrings(6).format(numGradient).toString());
  System.out.println("\nANALYTIC USING EXISTING SOFTMAX DER:");
  System.out.println(new NDArrayStrings(6).format(currentGradient).toString());
  System.out.println("\nGRADIENT USING MY VERSION OF SOFTMAX DER:");
  System.out.println(new NDArrayStrings(6).format(myGradient).toString());
  System.out.println("=========================");
}
