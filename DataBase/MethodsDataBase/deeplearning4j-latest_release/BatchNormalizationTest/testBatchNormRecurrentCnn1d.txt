@Test public void testBatchNormRecurrentCnn1d(){
  for (  boolean rnn : new boolean[]{true,false}) {
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).weightInit(WeightInit.XAVIER).convolutionMode(ConvolutionMode.Same).list().layer(rnn ? new LSTM.Builder().nOut(3).build() : new Convolution1DLayer.Builder().kernelSize(3).stride(1).nOut(3).build()).layer(new BatchNormalization()).layer(new RnnOutputLayer.Builder().nOut(3).activation(Activation.TANH).lossFunction(LossFunctions.LossFunction.MSE).build()).setInputType(InputType.recurrent(3)).build();
    MultiLayerNetwork net=new MultiLayerNetwork(conf);
    net.init();
    INDArray in=Nd4j.rand(new int[]{1,3,5});
    INDArray label=Nd4j.rand(new int[]{1,3,5});
    INDArray out=net.output(in);
    assertArrayEquals(new long[]{1,3,5},out.shape());
    net.fit(in,label);
    log.info("OK: {}",(rnn ? "rnn" : "cnn1d"));
  }
}
