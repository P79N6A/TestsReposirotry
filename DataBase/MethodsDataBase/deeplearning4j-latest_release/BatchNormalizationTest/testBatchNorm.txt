@Test public void testBatchNorm() throws Exception {
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).updater(new Adam(1e-3)).activation(Activation.TANH).list().layer(new ConvolutionLayer.Builder().nOut(5).kernelSize(2,2).build()).layer(new BatchNormalization()).layer(new ConvolutionLayer.Builder().nOut(5).kernelSize(2,2).build()).layer(new OutputLayer.Builder().activation(Activation.SOFTMAX).lossFunction(LossFunctions.LossFunction.MCXENT).nOut(10).build()).setInputType(InputType.convolutionalFlat(28,28,1)).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  DataSetIterator iter=new EarlyTerminationDataSetIterator(new MnistDataSetIterator(32,true,12345),10);
  net.fit(iter);
  MultiLayerNetwork net2=new TransferLearning.Builder(net).fineTuneConfiguration(FineTuneConfiguration.builder().updater(new AdaDelta()).build()).removeOutputLayer().addLayer(new BatchNormalization.Builder().nOut(3380).build()).addLayer(new OutputLayer.Builder().activation(Activation.SOFTMAX).lossFunction(LossFunctions.LossFunction.MCXENT).nIn(3380).nOut(10).build()).build();
  net2.fit(iter);
}
