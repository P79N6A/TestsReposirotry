@Test public void testGradientAndUpdaters() throws Exception {
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).updater(Updater.RMSPROP).seed(12345).list().layer(0,new ConvolutionLayer.Builder().nIn(1).nOut(6).weightInit(WeightInit.XAVIER).activation(Activation.IDENTITY).build()).layer(1,new BatchNormalization.Builder().build()).layer(2,new ActivationLayer.Builder().activation(Activation.LEAKYRELU).build()).layer(3,new DenseLayer.Builder().nOut(10).activation(Activation.LEAKYRELU).build()).layer(4,new BatchNormalization.Builder().build()).layer(5,new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).weightInit(WeightInit.XAVIER).activation(Activation.SOFTMAX).nOut(10).build()).setInputType(InputType.convolutionalFlat(28,28,1)).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  DataSetIterator iter=new MnistDataSetIterator(16,true,12345);
  DataSet ds=iter.next();
  net.setInput(ds.getFeatures());
  net.setLabels(ds.getLabels());
  net.computeGradientAndScore();
  Gradient g=net.gradient();
  Map<String,INDArray> map=g.gradientForVariable();
  org.deeplearning4j.nn.api.Updater u=net.getUpdater();
  MultiLayerUpdater mlu=(MultiLayerUpdater)u;
  List<UpdaterBlock> l=mlu.getUpdaterBlocks();
  assertNotNull(l);
  assertEquals(5,l.size());
  for (  UpdaterBlock ub : l) {
    List<UpdaterBlock.ParamState> list=ub.getLayersAndVariablesInBlock();
    for (    UpdaterBlock.ParamState v : list) {
      if (BatchNormalizationParamInitializer.GLOBAL_MEAN.equals(v.getParamName()) || BatchNormalizationParamInitializer.GLOBAL_VAR.equals(v.getParamName())) {
        assertTrue(ub.getGradientUpdater() instanceof NoOpUpdater);
      }
 else {
        assertTrue(ub.getGradientUpdater() instanceof RmsPropUpdater);
      }
    }
  }
}
