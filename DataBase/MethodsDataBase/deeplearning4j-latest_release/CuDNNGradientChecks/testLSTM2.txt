@Test public void testLSTM2() throws Exception {
  Nd4j.getRandom().setSeed(12345);
  int minibatch=10;
  int inputSize=3;
  int lstmLayerSize=4;
  int timeSeriesLength=3;
  int nOut=2;
  INDArray input=Nd4j.rand(new int[]{minibatch,inputSize,timeSeriesLength});
  INDArray labels=Nd4j.zeros(minibatch,nOut,timeSeriesLength);
  Random r=new Random(12345);
  for (int i=0; i < minibatch; i++) {
    for (int j=0; j < timeSeriesLength; j++) {
      labels.putScalar(i,r.nextInt(nOut),j,1.0);
    }
  }
  MultiLayerConfiguration.Builder builder=new NeuralNetConfiguration.Builder().updater(new NoOp()).seed(12345L).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,2)).list().layer(0,new LSTM.Builder().nIn(input.size(1)).nOut(lstmLayerSize).gateActivationFunction(Activation.SIGMOID).activation(Activation.TANH).build()).layer(1,new LSTM.Builder().nIn(lstmLayerSize).nOut(lstmLayerSize).gateActivationFunction(Activation.SIGMOID).activation(Activation.TANH).build()).layer(2,new RnnOutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(lstmLayerSize).nOut(nOut).build());
  MultiLayerNetwork mln=new MultiLayerNetwork(builder.build());
  mln.init();
  Field f=org.deeplearning4j.nn.layers.recurrent.LSTM.class.getDeclaredField("helper");
  f.setAccessible(true);
  org.deeplearning4j.nn.layers.recurrent.LSTM l=(org.deeplearning4j.nn.layers.recurrent.LSTM)mln.getLayer(1);
  LSTMHelper helper=(LSTMHelper)f.get(l);
  assertTrue(helper instanceof CudnnLSTMHelper);
  if (PRINT_RESULTS) {
    for (int j=0; j < mln.getnLayers(); j++)     System.out.println("Layer " + j + " # params: "+ mln.getLayer(j).numParams());
  }
  boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
  assertTrue(gradOK);
}
