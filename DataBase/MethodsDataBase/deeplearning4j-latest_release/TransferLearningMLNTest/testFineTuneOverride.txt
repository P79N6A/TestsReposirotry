@Test public void testFineTuneOverride(){
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new Adam(1e-4)).activation(Activation.TANH).weightInit(WeightInit.RELU).l1(0.1).l2(0.2).list().layer(0,new DenseLayer.Builder().nIn(10).nOut(5).build()).layer(1,new OutputLayer.Builder().nIn(5).nOut(4).activation(Activation.HARDSIGMOID).build()).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  MultiLayerNetwork net2=new TransferLearning.Builder(net).fineTuneConfiguration(new FineTuneConfiguration.Builder().updater(new Adam(2e-2)).backpropType(BackpropType.TruncatedBPTT).build()).build();
  BaseLayer l0=(BaseLayer)net.getLayer(0).conf().getLayer();
  assertEquals(new Adam(1e-4),l0.getIUpdater());
  assertEquals(Activation.TANH.getActivationFunction(),l0.getActivationFn());
  assertEquals(WeightInit.RELU,l0.getWeightInit());
  assertEquals(0.1,l0.getL1(),1e-6);
  BaseLayer l1=(BaseLayer)net.getLayer(1).conf().getLayer();
  assertEquals(new Adam(1e-4),l1.getIUpdater());
  assertEquals(Activation.HARDSIGMOID.getActivationFunction(),l1.getActivationFn());
  assertEquals(WeightInit.RELU,l1.getWeightInit());
  assertEquals(0.2,l1.getL2(),1e-6);
  assertEquals(BackpropType.Standard,conf.getBackpropType());
  l0=(BaseLayer)net2.getLayer(0).conf().getLayer();
  assertEquals(new Adam(2e-2),l0.getIUpdater());
  assertEquals(Activation.TANH.getActivationFunction(),l0.getActivationFn());
  assertEquals(WeightInit.RELU,l0.getWeightInit());
  assertEquals(0.1,l0.getL1(),1e-6);
  l1=(BaseLayer)net2.getLayer(1).conf().getLayer();
  assertEquals(new Adam(2e-2),l1.getIUpdater());
  assertEquals(Activation.HARDSIGMOID.getActivationFunction(),l1.getActivationFn());
  assertEquals(WeightInit.RELU,l1.getWeightInit());
  assertEquals(0.2,l1.getL2(),1e-6);
  assertEquals(BackpropType.TruncatedBPTT,net2.getLayerWiseConfigurations().getBackpropType());
}
