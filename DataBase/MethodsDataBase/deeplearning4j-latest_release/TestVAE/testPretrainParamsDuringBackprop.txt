@Test public void testPretrainParamsDuringBackprop(){
  Nd4j.getRandom().setSeed(12345);
  MultiLayerConfiguration mlc=new NeuralNetConfiguration.Builder().seed(12345).list().layer(0,new org.deeplearning4j.nn.conf.layers.variational.VariationalAutoencoder.Builder().nIn(10).nOut(5).encoderLayerSizes(12,13).decoderLayerSizes(14,15).build()).layer(1,new OutputLayer.Builder().lossFunction(LossFunctions.LossFunction.MSE).nIn(5).nOut(6).activation(new ActivationTanH()).build()).pretrain(true).build();
  NeuralNetConfiguration c=mlc.getConf(0);
  org.deeplearning4j.nn.conf.layers.variational.VariationalAutoencoder vae=(org.deeplearning4j.nn.conf.layers.variational.VariationalAutoencoder)c.getLayer();
  MultiLayerNetwork net=new MultiLayerNetwork(mlc);
  net.init();
  net.initGradientsView();
  org.deeplearning4j.nn.layers.variational.VariationalAutoencoder layer=(org.deeplearning4j.nn.layers.variational.VariationalAutoencoder)net.getLayer(0);
  INDArray input=Nd4j.rand(3,10);
  net.pretrainLayer(0,input);
  Map<String,INDArray> layerParams=layer.paramTable();
  Map<String,INDArray> pretrainParamsBefore=new HashMap<>();
  for (  String s : layerParams.keySet()) {
    if (layer.isPretrainParam(s)) {
      pretrainParamsBefore.put(s,layerParams.get(s).dup());
    }
  }
  INDArray features=Nd4j.rand(3,10);
  INDArray labels=Nd4j.rand(3,6);
  net.getLayerWiseConfigurations().setPretrain(false);
  for (int i=0; i < 3; i++) {
    net.fit(features,labels);
  }
  Map<String,INDArray> layerParamsAfter=layer.paramTable();
  for (  String s : pretrainParamsBefore.keySet()) {
    INDArray before=pretrainParamsBefore.get(s);
    INDArray after=layerParamsAfter.get(s);
    assertEquals(before,after);
  }
}
