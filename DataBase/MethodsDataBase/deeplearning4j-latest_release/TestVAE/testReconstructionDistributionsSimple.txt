@Test public void testReconstructionDistributionsSimple(){
  int inOutSize=6;
  ReconstructionDistribution[] reconstructionDistributions=new ReconstructionDistribution[]{new GaussianReconstructionDistribution(Activation.IDENTITY),new GaussianReconstructionDistribution(Activation.TANH),new BernoulliReconstructionDistribution(Activation.SIGMOID),new CompositeReconstructionDistribution.Builder().addDistribution(2,new GaussianReconstructionDistribution(Activation.IDENTITY)).addDistribution(2,new BernoulliReconstructionDistribution()).addDistribution(2,new GaussianReconstructionDistribution(Activation.TANH)).build()};
  Nd4j.getRandom().setSeed(12345);
  for (  int minibatch : new int[]{1,5}) {
    for (int i=0; i < reconstructionDistributions.length; i++) {
      INDArray data;
switch (i) {
case 0:
case 1:
        data=Nd4j.rand(minibatch,inOutSize);
      break;
case 2:
    data=Nd4j.create(minibatch,inOutSize);
  Nd4j.getExecutioner().exec(new BernoulliDistribution(data,0.5),Nd4j.getRandom());
break;
case 3:
data=Nd4j.create(minibatch,inOutSize);
data.get(NDArrayIndex.all(),NDArrayIndex.interval(0,2)).assign(Nd4j.rand(minibatch,2));
Nd4j.getExecutioner().exec(new BernoulliDistribution(data.get(NDArrayIndex.all(),NDArrayIndex.interval(2,4)),0.5),Nd4j.getRandom());
data.get(NDArrayIndex.all(),NDArrayIndex.interval(4,6)).assign(Nd4j.rand(minibatch,2));
break;
default :
throw new RuntimeException();
}
MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().l2(0.2).l1(0.3).updater(new Sgd(1.0)).seed(12345L).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).list().layer(0,new VariationalAutoencoder.Builder().nIn(inOutSize).nOut(3).encoderLayerSizes(5).decoderLayerSizes(6).pzxActivationFunction(Activation.TANH).reconstructionDistribution(reconstructionDistributions[i]).activation(new ActivationTanH()).build()).pretrain(true).backprop(false).build();
MultiLayerNetwork mln=new MultiLayerNetwork(conf);
mln.init();
mln.initGradientsView();
mln.pretrainLayer(0,data);
org.deeplearning4j.nn.layers.variational.VariationalAutoencoder layer=(org.deeplearning4j.nn.layers.variational.VariationalAutoencoder)mln.getLayer(0);
assertFalse(layer.hasLossFunction());
Nd4j.getRandom().setSeed(12345);
INDArray reconstructionProb=layer.reconstructionProbability(data,50);
assertArrayEquals(new long[]{minibatch,1},reconstructionProb.shape());
Nd4j.getRandom().setSeed(12345);
INDArray reconstructionLogProb=layer.reconstructionLogProbability(data,50);
assertArrayEquals(new long[]{minibatch,1},reconstructionLogProb.shape());
for (int j=0; j < minibatch; j++) {
double p=reconstructionProb.getDouble(j);
double logp=reconstructionLogProb.getDouble(j);
assertTrue(p >= 0.0 && p <= 1.0);
assertTrue(logp <= 0.0);
double pFromLogP=Math.exp(logp);
assertEquals(p,pFromLogP,1e-6);
}
}
}
}
