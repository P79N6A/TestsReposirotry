@Test public void testReconstructionErrorSimple(){
  int inOutSize=6;
  ReconstructionDistribution[] reconstructionDistributions=new ReconstructionDistribution[]{new LossFunctionWrapper(Activation.TANH,new LossMSE()),new LossFunctionWrapper(Activation.IDENTITY,new LossMAE()),new CompositeReconstructionDistribution.Builder().addDistribution(3,new LossFunctionWrapper(Activation.TANH,new LossMSE())).addDistribution(3,new LossFunctionWrapper(Activation.IDENTITY,new LossMAE())).build()};
  Nd4j.getRandom().setSeed(12345);
  for (  int minibatch : new int[]{1,5}) {
    for (int i=0; i < reconstructionDistributions.length; i++) {
      INDArray data=Nd4j.rand(minibatch,inOutSize).muli(2).subi(1);
      MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().l2(0.2).l1(0.3).updater(new Sgd(1.0)).seed(12345L).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).list().layer(0,new VariationalAutoencoder.Builder().nIn(inOutSize).nOut(3).encoderLayerSizes(5).decoderLayerSizes(6).pzxActivationFunction(Activation.TANH).reconstructionDistribution(reconstructionDistributions[i]).activation(new ActivationTanH()).build()).pretrain(true).backprop(false).build();
      MultiLayerNetwork mln=new MultiLayerNetwork(conf);
      mln.init();
      mln.initGradientsView();
      mln.pretrainLayer(0,data);
      org.deeplearning4j.nn.layers.variational.VariationalAutoencoder layer=(org.deeplearning4j.nn.layers.variational.VariationalAutoencoder)mln.getLayer(0);
      assertTrue(layer.hasLossFunction());
      Nd4j.getRandom().setSeed(12345);
      INDArray reconstructionError=layer.reconstructionError(data);
      assertArrayEquals(new long[]{minibatch,1},reconstructionError.shape());
      for (int j=0; j < minibatch; j++) {
        double re=reconstructionError.getDouble(j);
        assertTrue(re >= 0.0);
      }
    }
  }
}
