@Test public void testOutputLayerMasking(){
  Nd4j.getRandom().setSeed(12345);
  int mb=10;
  int tsLength=5;
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().weightInit(new NormalDistribution(0,2)).updater(new NoOp()).list().layer(new LSTM.Builder().nIn(10).nOut(10).build()).layer(new GlobalPoolingLayer.Builder().poolingType(PoolingType.AVG).build()).layer(new OutputLayer.Builder().nIn(10).nOut(10).activation(Activation.SOFTMAX).build()).setInputType(InputType.recurrent(10)).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  INDArray f=Nd4j.rand(new int[]{mb,10,tsLength});
  INDArray l=TestUtils.randomOneHot(mb,10);
  INDArray lm=TestUtils.randomBernoulli(mb,1);
  assertTrue(lm.sumNumber().intValue() > 0);
  boolean gradOK=GradientCheckUtil.checkGradients(net,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,f,l,null,lm);
  assertTrue(gradOK);
  double score=net.score(new DataSet(f,l,null,lm));
  for (int i=0; i < mb; i++) {
    if (lm.getDouble(i) != 0.0) {
      continue;
    }
    INDArray fView=f.get(point(i),all(),all());
    fView.assign(Nd4j.rand(fView.shape()));
    INDArray lView=l.get(point(i),all());
    lView.assign(TestUtils.randomOneHot(1,lView.size(1)));
    double score2=net.score(new DataSet(f,l,null,lm));
    assertEquals(String.valueOf(i),score,score2,1e-8);
  }
}
