@Test public void testBidirectionalLSTMMasking(){
  Nd4j.getRandom().setSeed(12345L);
  int timeSeriesLength=5;
  int nIn=5;
  int layerSize=4;
  int nOut=3;
  int miniBatchSize=3;
  INDArray[] masks=new INDArray[]{null,Nd4j.create(new double[][]{{1,1,1,1,1},{1,1,1,1,1},{1,1,1,1,1}}),Nd4j.create(new double[][]{{1,1,1,1,1},{1,1,1,1,0},{1,1,1,0,0}}),Nd4j.create(new double[][]{{1,1,1,1,1},{0,1,1,1,1},{0,0,1,1,1}})};
  int testNum=0;
  for (  INDArray mask : masks) {
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new NoOp()).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1.0)).seed(12345L).list().layer(0,new GravesBidirectionalLSTM.Builder().nIn(nIn).nOut(layerSize).activation(Activation.TANH).build()).layer(1,new GravesBidirectionalLSTM.Builder().nIn(layerSize).nOut(layerSize).activation(Activation.TANH).build()).layer(2,new RnnOutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(layerSize).nOut(nOut).build()).build();
    MultiLayerNetwork mln=new MultiLayerNetwork(conf);
    mln.init();
    Random r=new Random(12345L);
    INDArray input=Nd4j.rand(new int[]{miniBatchSize,nIn,timeSeriesLength},'f').subi(0.5);
    INDArray labels=Nd4j.zeros(miniBatchSize,nOut,timeSeriesLength);
    for (int i=0; i < miniBatchSize; i++) {
      for (int j=0; j < nIn; j++) {
        labels.putScalar(i,r.nextInt(nOut),j,1.0);
      }
    }
    if (PRINT_RESULTS) {
      System.out.println("testBidirectionalLSTMMasking() - testNum = " + testNum++);
      for (int j=0; j < mln.getnLayers(); j++)       System.out.println("Layer " + j + " # params: "+ mln.getLayer(j).numParams());
    }
    boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels,mask,mask);
    assertTrue(gradOK);
    TestUtils.testModelSerialization(mln);
  }
}
