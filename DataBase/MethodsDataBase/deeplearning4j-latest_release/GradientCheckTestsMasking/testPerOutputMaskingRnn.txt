@Test public void testPerOutputMaskingRnn(){
  int nIn=4;
  int layerSize=4;
  int nOut=4;
  INDArray mask1=Nd4j.create(new double[]{1,0,0,1,0,1,0,1,1,1,1,0},new int[]{1,nOut,3},'f');
  INDArray mask2=Nd4j.create(new double[]{1,1,0,1},new int[]{1,nOut,1},'f');
  INDArray mask3=Nd4j.create(new double[]{1,0,1,0,1,1,1,1,1,0,1,0,0,1,1,1,1,0,1,0,1,0,1,1,1,1,1,0,0,1,0,1,0,1,0,0},new int[]{3,nOut,3},'f');
  INDArray[] labelMasks=new INDArray[]{mask1,mask2,mask3};
  ILossFunction[] lossFunctions=new ILossFunction[]{new LossBinaryXENT(),new LossHinge(),new LossKLD(),new LossKLD(),new LossL1(),new LossL2(),new LossMAE(),new LossMAE(),new LossMAPE(),new LossMAPE(),new LossMCXENT(),new LossMSE(),new LossMSE(),new LossMSLE(),new LossMSLE(),new LossNegativeLogLikelihood(),new LossPoisson(),new LossSquaredHinge()};
  Activation[] act=new Activation[]{Activation.SIGMOID,Activation.TANH,Activation.SIGMOID,Activation.SOFTMAX,Activation.TANH,Activation.TANH,Activation.TANH,Activation.SOFTMAX,Activation.TANH,Activation.SOFTMAX,Activation.SIGMOID,Activation.TANH,Activation.SOFTMAX,Activation.SIGMOID,Activation.SOFTMAX,Activation.SIGMOID,Activation.SIGMOID,Activation.TANH};
  for (  INDArray labelMask : labelMasks) {
    val minibatch=labelMask.size(0);
    val tsLength=labelMask.size(2);
    for (int i=0; i < lossFunctions.length; i++) {
      ILossFunction lf=lossFunctions[i];
      Activation a=act[i];
      Nd4j.getRandom().setSeed(12345);
      MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new NoOp()).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).seed(12345).list().layer(0,new GravesLSTM.Builder().nIn(nIn).nOut(layerSize).activation(Activation.TANH).build()).layer(1,new RnnOutputLayer.Builder().nIn(layerSize).nOut(nOut).lossFunction(lf).activation(a).build()).validateOutputLayerConfig(false).build();
      MultiLayerNetwork net=new MultiLayerNetwork(conf);
      net.init();
      INDArray[] fl=LossFunctionGradientCheck.getFeaturesAndLabels(lf,new long[]{minibatch,nIn,tsLength},new long[]{minibatch,nOut,tsLength},12345);
      INDArray features=fl[0];
      INDArray labels=fl[1];
      String msg="testPerOutputMaskingRnn(): maskShape = " + Arrays.toString(labelMask.shape()) + ", loss function = "+ lf+ ", activation = "+ a;
      System.out.println(msg);
      boolean gradOK=GradientCheckUtil.checkGradients(net,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,features,labels,null,labelMask);
      assertTrue(msg,gradOK);
      Nd4j.getRandom().setSeed(12345);
      ComputationGraphConfiguration cg=new NeuralNetConfiguration.Builder().updater(new NoOp()).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,2)).seed(12345).graphBuilder().addInputs("in").addLayer("0",new GravesLSTM.Builder().nIn(nIn).nOut(layerSize).activation(Activation.TANH).build(),"in").addLayer("1",new RnnOutputLayer.Builder().nIn(layerSize).nOut(nOut).lossFunction(lf).activation(a).build(),"0").setOutputs("1").validateOutputLayerConfig(false).build();
      ComputationGraph graph=new ComputationGraph(cg);
      graph.init();
      gradOK=GradientCheckUtil.checkGradients(graph,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,new INDArray[]{features},new INDArray[]{labels},null,new INDArray[]{labelMask},null);
      assertTrue(msg + " (compgraph)",gradOK);
      TestUtils.testModelSerialization(graph);
    }
  }
}
