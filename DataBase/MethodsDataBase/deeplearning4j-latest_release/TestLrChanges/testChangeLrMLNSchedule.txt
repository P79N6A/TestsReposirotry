@Test public void testChangeLrMLNSchedule(){
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).updater(new Adam(0.1)).list().layer(new DenseLayer.Builder().nIn(10).nOut(10).build()).layer(new DenseLayer.Builder().nIn(10).nOut(10).build()).layer(new OutputLayer.Builder().nIn(10).nOut(10).activation(Activation.SOFTMAX).build()).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  for (int i=0; i < 10; i++) {
    net.fit(Nd4j.rand(10,10),Nd4j.rand(10,10));
  }
  MultiLayerConfiguration conf2=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).updater(new Adam(new ExponentialSchedule(ScheduleType.ITERATION,0.5,0.8))).list().layer(new DenseLayer.Builder().nIn(10).nOut(10).build()).layer(new DenseLayer.Builder().nIn(10).nOut(10).build()).layer(new OutputLayer.Builder().nIn(10).nOut(10).activation(Activation.SOFTMAX).build()).build();
  MultiLayerNetwork net2=new MultiLayerNetwork(conf2);
  net2.init();
  net2.getUpdater().getStateViewArray().assign(net.getUpdater().getStateViewArray());
  conf2.setIterationCount(conf.getIterationCount());
  net2.setParams(net.params().dup());
  net.setLearningRate(new ExponentialSchedule(ScheduleType.ITERATION,0.5,0.8));
  assertEquals(conf,conf2);
  assertEquals(conf.toJson(),conf2.toJson());
  assertEquals(net.getUpdater().getStateViewArray(),net2.getUpdater().getStateViewArray());
  for (int i=0; i < 3; i++) {
    INDArray in=Nd4j.rand(10,10);
    INDArray l=Nd4j.rand(10,10);
    net.fit(in,l);
    net2.fit(in,l);
  }
  assertEquals(net.params(),net2.params());
  assertEquals(net.getUpdater().getStateViewArray(),net2.getUpdater().getStateViewArray());
}
