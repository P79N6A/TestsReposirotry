@Test public void testChangeLrCompGraphSchedule(){
  ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).updater(new Adam(0.1)).graphBuilder().addInputs("in").addLayer("0",new DenseLayer.Builder().nIn(10).nOut(10).build(),"in").addLayer("1",new DenseLayer.Builder().nIn(10).nOut(10).build(),"0").addLayer("2",new OutputLayer.Builder().nIn(10).nOut(10).activation(Activation.SOFTMAX).build(),"1").setOutputs("2").build();
  ComputationGraph net=new ComputationGraph(conf);
  net.init();
  for (int i=0; i < 10; i++) {
    net.fit(new DataSet(Nd4j.rand(10,10),Nd4j.rand(10,10)));
  }
  ComputationGraphConfiguration conf2=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).updater(new Adam(new ExponentialSchedule(ScheduleType.ITERATION,0.5,0.8))).graphBuilder().addInputs("in").addLayer("0",new DenseLayer.Builder().nIn(10).nOut(10).build(),"in").addLayer("1",new DenseLayer.Builder().nIn(10).nOut(10).build(),"0").layer("2",new OutputLayer.Builder().nIn(10).nOut(10).activation(Activation.SOFTMAX).build(),"1").setOutputs("2").build();
  ComputationGraph net2=new ComputationGraph(conf2);
  net2.init();
  net2.getUpdater().getStateViewArray().assign(net.getUpdater().getStateViewArray());
  conf2.setIterationCount(conf.getIterationCount());
  net2.setParams(net.params().dup());
  net.setLearningRate(new ExponentialSchedule(ScheduleType.ITERATION,0.5,0.8));
  assertEquals(conf,conf2);
  assertEquals(conf.toJson(),conf2.toJson());
  assertEquals(net.getUpdater().getStateViewArray(),net2.getUpdater().getStateViewArray());
  for (int i=0; i < 3; i++) {
    INDArray in=Nd4j.rand(10,10);
    INDArray l=Nd4j.rand(10,10);
    net.fit(new DataSet(in,l));
    net2.fit(new DataSet(in,l));
  }
  assertEquals(net.params(),net2.params());
  assertEquals(net.getUpdater().getStateViewArray(),net2.getUpdater().getStateViewArray());
}
