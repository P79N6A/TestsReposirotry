@Test public void testChangeLrMLN(){
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).list().layer(new DenseLayer.Builder().nIn(10).nOut(10).updater(new Adam(0.1)).build()).layer(new DenseLayer.Builder().nIn(10).nOut(10).updater(new RmsProp(0.01)).build()).layer(new OutputLayer.Builder().nIn(10).nOut(10).updater(new NoOp()).lossFunction(LossFunctions.LossFunction.MSE).build()).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  for (int i=0; i < 10; i++) {
    net.fit(Nd4j.rand(10,10),Nd4j.rand(10,10));
  }
  MultiLayerConfiguration conf2=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).list().layer(new DenseLayer.Builder().nIn(10).nOut(10).updater(new Adam(0.5)).build()).layer(new DenseLayer.Builder().nIn(10).nOut(10).updater(new RmsProp(0.01)).build()).layer(new OutputLayer.Builder().nIn(10).nOut(10).updater(new NoOp()).lossFunction(LossFunctions.LossFunction.MSE).build()).build();
  MultiLayerNetwork net2=new MultiLayerNetwork(conf2);
  net2.init();
  net2.getUpdater().getStateViewArray().assign(net.getUpdater().getStateViewArray());
  conf2.setIterationCount(conf.getIterationCount());
  net2.setParams(net.params().dup());
  assertEquals(0.1,net.getLearningRate(0).doubleValue(),0.0);
  net.setLearningRate(0,0.5);
  assertEquals(0.5,net.getLearningRate(0).doubleValue(),0.0);
  assertEquals(conf,conf2);
  assertEquals(conf.toJson(),conf2.toJson());
  assertEquals(net.getUpdater().getStateViewArray(),net2.getUpdater().getStateViewArray());
  for (int i=0; i < 3; i++) {
    INDArray in=Nd4j.rand(10,10);
    INDArray l=Nd4j.rand(10,10);
    net.fit(in,l);
    net2.fit(in,l);
  }
  assertEquals(net.params(),net2.params());
  assertEquals(net.getUpdater().getStateViewArray(),net2.getUpdater().getStateViewArray());
  INDArray in1=Nd4j.rand(10,10);
  INDArray l1=Nd4j.rand(10,10);
  net.setInput(in1);
  net.setLabels(l1);
  net.computeGradientAndScore();
  net2.setInput(in1);
  net2.setLabels(l1);
  net2.computeGradientAndScore();
  assertEquals(net.score(),net2.score(),1e-8);
  MultiLayerConfiguration conf3=new NeuralNetConfiguration.Builder().activation(Activation.TANH).seed(12345).list().layer(new DenseLayer.Builder().nIn(10).nOut(10).updater(new Adam(0.3)).build()).layer(new DenseLayer.Builder().nIn(10).nOut(10).updater(new RmsProp(0.3)).build()).layer(new OutputLayer.Builder().nIn(10).nOut(10).updater(new NoOp()).lossFunction(LossFunctions.LossFunction.MSE).build()).build();
  MultiLayerNetwork net3=new MultiLayerNetwork(conf3);
  net3.init();
  net3.getUpdater().getStateViewArray().assign(net.getUpdater().getStateViewArray());
  conf3.setIterationCount(conf.getIterationCount());
  net3.setParams(net.params().dup());
  net.setLearningRate(0.3);
  for (int i=0; i < 3; i++) {
    INDArray in=Nd4j.rand(10,10);
    INDArray l=Nd4j.rand(10,10);
    net.fit(in,l);
    net3.fit(in,l);
  }
  assertEquals(net.params(),net3.params());
  assertEquals(net.getUpdater().getStateViewArray(),net3.getUpdater().getStateViewArray());
}
