@Test public void testLayer(){
  DataSetIterator dataSetIterator=getNormalizedIterator();
  boolean doLearningFirst=true;
  MultiLayerNetwork network=getGradientCheckNetwork(2);
  DataSet ds=dataSetIterator.next();
  INDArray arr=ds.getFeatures();
  network.setInput(arr);
  if (doLearningFirst) {
    network.setInput(arr);
    network.setListeners(new ScoreIterationListener(1));
    network.computeGradientAndScore();
    double scoreBefore=network.score();
    for (int j=0; j < 10; j++)     network.fit(ds);
    network.computeGradientAndScore();
    double scoreAfter=network.score();
    String msg="testLayer() - score did not (sufficiently) decrease during learning - activationFn=" + "relu" + ", lossFn="+ "ocnn"+ ", "+ "sigmoid"+ ", doLearningFirst=" + doLearningFirst + " (before="+ scoreBefore+ ", scoreAfter="+ scoreAfter+ ")";
  }
  if (PRINT_RESULTS) {
    System.out.println("testLayer() - activationFn=" + "relu" + ", lossFn="+ "ocnn"+ "sigmoid"+ ", doLearningFirst="+ doLearningFirst);
    for (int j=0; j < network.getnLayers(); j++)     System.out.println("Layer " + j + " # params: "+ network.getLayer(j).numParams());
  }
  boolean gradOK=GradientCheckUtil.checkGradients(network,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,ds.getFeatures(),ds.getLabels());
  String msg="testLayer() - activationFn=" + "relu" + ", lossFn="+ "ocnn"+ ",="+ "sigmoid"+ ", doLearningFirst="+ doLearningFirst;
  assertTrue(msg,gradOK);
}
