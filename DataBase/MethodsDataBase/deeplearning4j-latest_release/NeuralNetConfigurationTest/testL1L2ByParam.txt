@Test public void testL1L2ByParam(){
  double l1=0.01;
  double l2=0.07;
  int[] nIns={4,3,3};
  int[] nOuts={3,3,3};
  int oldScore=1;
  int newScore=1;
  int iteration=3;
  INDArray gradientW=Nd4j.ones(nIns[0],nOuts[0]);
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().l1(l1).l2(l2).list().layer(0,new DenseLayer.Builder().nIn(nIns[0]).nOut(nOuts[0]).build()).layer(1,new BatchNormalization.Builder().nIn(nIns[1]).nOut(nOuts[1]).l2(0.5).build()).layer(2,new OutputLayer.Builder().nIn(nIns[2]).nOut(nOuts[2]).lossFunction(LossFunctions.LossFunction.MSE).build()).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  ConvexOptimizer opt=new StochasticGradientDescent(net.getDefaultConfiguration(),new NegativeDefaultStepFunction(),null,net);
  assertEquals(l1,net.getLayer(0).conf().getLayer().getL1ByParam("W"),1e-4);
  assertEquals(0.0,net.getLayer(0).conf().getLayer().getL1ByParam("b"),0.0);
  assertEquals(0.0,net.getLayer(1).conf().getLayer().getL2ByParam("beta"),0.0);
  assertEquals(0.0,net.getLayer(1).conf().getLayer().getL2ByParam("gamma"),0.0);
  assertEquals(0.0,net.getLayer(1).conf().getLayer().getL2ByParam("mean"),0.0);
  assertEquals(0.0,net.getLayer(1).conf().getLayer().getL2ByParam("var"),0.0);
  assertEquals(l2,net.getLayer(2).conf().getLayer().getL2ByParam("W"),1e-4);
  assertEquals(0.0,net.getLayer(2).conf().getLayer().getL2ByParam("b"),0.0);
}
