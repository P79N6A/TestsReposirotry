@Test public void testLearningRateByParam(){
  double lr=0.01;
  double biasLr=0.02;
  int[] nIns={4,3,3};
  int[] nOuts={3,3,3};
  int oldScore=1;
  int newScore=1;
  int iteration=3;
  INDArray gradientW=Nd4j.ones(nIns[0],nOuts[0]);
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new Sgd(0.3)).list().layer(0,new DenseLayer.Builder().nIn(nIns[0]).nOut(nOuts[0]).updater(new Sgd(lr)).biasUpdater(new Sgd(biasLr)).build()).layer(1,new BatchNormalization.Builder().nIn(nIns[1]).nOut(nOuts[1]).updater(new Sgd(0.7)).build()).layer(2,new OutputLayer.Builder().nIn(nIns[2]).nOut(nOuts[2]).lossFunction(LossFunctions.LossFunction.MSE).build()).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  ConvexOptimizer opt=new StochasticGradientDescent(net.getDefaultConfiguration(),new NegativeDefaultStepFunction(),null,net);
  assertEquals(lr,((Sgd)net.getLayer(0).conf().getLayer().getUpdaterByParam("W")).getLearningRate(),1e-4);
  assertEquals(biasLr,((Sgd)net.getLayer(0).conf().getLayer().getUpdaterByParam("b")).getLearningRate(),1e-4);
  assertEquals(0.7,((Sgd)net.getLayer(1).conf().getLayer().getUpdaterByParam("gamma")).getLearningRate(),1e-4);
  assertEquals(0.3,((Sgd)net.getLayer(2).conf().getLayer().getUpdaterByParam("W")).getLearningRate(),1e-4);
  assertEquals(0.3,((Sgd)net.getLayer(2).conf().getLayer().getUpdaterByParam("W")).getLearningRate(),1e-4);
}
