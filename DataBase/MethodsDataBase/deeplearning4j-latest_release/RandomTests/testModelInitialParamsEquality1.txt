/** 
 * In this test we check for equality of model params after initialization in different threads
 * @throws Exception
 */
@Test public void testModelInitialParamsEquality1() throws Exception {
  final List<Model> models=new CopyOnWriteArrayList<>();
  for (int i=0; i < 4; i++) {
    Thread thread=new Thread(new Runnable(){
      @Override public void run(){
        MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().seed(119).l2(0.0005).weightInit(WeightInit.XAVIER).updater(new Nesterovs(0.01,0.9)).trainingWorkspaceMode(WorkspaceMode.ENABLED).list().layer(0,new ConvolutionLayer.Builder(5,5).nIn(1).stride(1,1).nOut(20).activation(Activation.IDENTITY).build()).layer(1,new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX).kernelSize(2,2).stride(2,2).build()).layer(2,new ConvolutionLayer.Builder(5,5).stride(1,1).nOut(50).activation(Activation.IDENTITY).build()).layer(3,new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX).kernelSize(2,2).stride(2,2).build()).layer(4,new DenseLayer.Builder().activation(Activation.RELU).nOut(500).build()).layer(5,new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD).nOut(10).activation(Activation.SOFTMAX).build()).setInputType(InputType.convolutionalFlat(28,28,1)).build();
        MultiLayerNetwork network=new MultiLayerNetwork(conf);
        network.init();
        models.add(network);
      }
    }
);
    thread.start();
    thread.join();
  }
  for (int i=0; i < models.size(); i++) {
    assertEquals(models.get(0).params(),models.get(i).params());
  }
}
