@Test public void testSameDiffConvForward(){
  assumeTrue("CPU".equalsIgnoreCase(Nd4j.getExecutioner().getEnvironmentInformation().getProperty("backend")));
  int imgH=16;
  int imgW=20;
  int count=0;
  Random r=new Random(12345);
  int n=1;
  int m=30;
  for (  int minibatch : new int[]{5,1}) {
    Activation[] afns=new Activation[]{Activation.TANH,Activation.SIGMOID,Activation.ELU,Activation.IDENTITY,Activation.SOFTPLUS,Activation.SOFTSIGN,Activation.CUBE,Activation.HARDTANH,Activation.RELU};
    for (    boolean hasBias : new boolean[]{true,false}) {
      for (      int nIn : new int[]{3,4}) {
        for (        int nOut : new int[]{4,5}) {
          for (          int[] kernel : new int[][]{{2,2},{2,1},{3,2}}) {
            for (            int[] strides : new int[][]{{1,1},{2,2},{2,1}}) {
              for (              int[] dilation : new int[][]{{1,1},{2,2},{1,2}}) {
                for (                ConvolutionMode cm : new ConvolutionMode[]{ConvolutionMode.Truncate,ConvolutionMode.Same}) {
                  for (                  Activation a : afns) {
                    int i=r.nextInt(m);
                    if (i >= n) {
                      continue;
                    }
                    String msg="Test " + (count++) + " - minibatch="+ minibatch+ ", nIn="+ nIn+ ", nOut="+ nOut+ ", kernel="+ Arrays.toString(kernel)+ ", stride="+ Arrays.toString(strides)+ ", dilation="+ Arrays.toString(dilation)+ ", ConvolutionMode="+ cm+ ", ActFn="+ a+ ", hasBias="+ hasBias;
                    log.info("Starting test: " + msg);
                    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).list().layer(new SameDiffConv.Builder().weightInit(WeightInit.XAVIER).nIn(nIn).nOut(nOut).kernelSize(kernel).stride(strides).dilation(dilation).convolutionMode(cm).activation(a).hasBias(hasBias).build()).layer(new SameDiffConv.Builder().weightInit(WeightInit.XAVIER).nIn(nOut).nOut(nOut).kernelSize(kernel).stride(strides).dilation(dilation).convolutionMode(cm).activation(a).hasBias(hasBias).build()).build();
                    MultiLayerNetwork net=new MultiLayerNetwork(conf);
                    net.init();
                    assertNotNull(net.paramTable());
                    MultiLayerConfiguration conf2=new NeuralNetConfiguration.Builder().weightInit(WeightInit.XAVIER).seed(12345).list().layer(new ConvolutionLayer.Builder().nIn(nIn).nOut(nOut).kernelSize(kernel).stride(strides).dilation(dilation).convolutionMode(cm).activation(a).hasBias(hasBias).build()).layer(new ConvolutionLayer.Builder().nIn(nOut).nOut(nOut).kernelSize(kernel).stride(strides).dilation(dilation).convolutionMode(cm).activation(a).hasBias(hasBias).build()).build();
                    MultiLayerNetwork net2=new MultiLayerNetwork(conf2);
                    net2.init();
                    Map<String,INDArray> params1=net.paramTable();
                    Map<String,INDArray> params2=net2.paramTable();
                    for (                    Map.Entry<String,INDArray> e : params1.entrySet()) {
                      if (e.getKey().endsWith("_W")) {
                        INDArray p1=e.getValue();
                        INDArray p2=params2.get(e.getKey());
                        p2=p2.permute(2,3,1,0);
                        p1.assign(p2);
                      }
 else {
                        assertEquals(params2.get(e.getKey()),e.getValue());
                      }
                    }
                    INDArray in=Nd4j.rand(new int[]{minibatch,nIn,imgH,imgW});
                    INDArray out=net.output(in);
                    INDArray outExp=net2.output(in);
                    assertEquals(msg,outExp,out);
                    MultiLayerNetwork netLoaded=TestUtils.testModelSerialization(net);
                    INDArray outLoaded=netLoaded.output(in);
                    assertEquals(msg,outExp,outLoaded);
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
