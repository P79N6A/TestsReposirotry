/** 
 * This is very long test, to track memory consumption over time
 * @throws Exception
 */
@Ignore @Test public void testsParallelFit1() throws Exception {
  final File file=new ClassPathResource("/big/raw_sentences.txt").getFile();
  for (int i=0; i < 1000; i++) {
    List<Thread> threads=new ArrayList<>();
    for (int t=0; t < 3; t++) {
      threads.add(new Thread(new Runnable(){
        @Override public void run(){
          try {
            TokenizerFactory t=new DefaultTokenizerFactory();
            LabelsSource source=new LabelsSource("DOC_");
            SentenceIteratorConverter sic=new SentenceIteratorConverter(new BasicLineIterator(file),source);
            ParagraphVectors vec=new ParagraphVectors.Builder().seed(42).minWordFrequency(1).iterations(1).epochs(5).layerSize(100).learningRate(0.05).windowSize(5).trainWordVectors(true).allowParallelTokenization(false).tokenizerFactory(t).workers(1).iterate(sic).build();
            vec.fit();
          }
 catch (          Exception e) {
            throw new RuntimeException(e);
          }
        }
      }
));
    }
    for (    Thread t : threads) {
      t.start();
    }
    for (    Thread t : threads) {
      t.join();
    }
  }
}
