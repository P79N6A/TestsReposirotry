@Test public void testPerOutputMaskingMLN(){
  int nIn=6;
  int layerSize=4;
  INDArray mask1=Nd4j.create(new double[]{1,0,0,1,0});
  INDArray mask3=Nd4j.create(new double[][]{{1,1,1,1,1},{0,1,0,1,0},{1,0,0,1,1}});
  INDArray[] labelMasks=new INDArray[]{mask1,mask3};
  ILossFunction[] lossFunctions=new ILossFunction[]{new LossBinaryXENT(),new LossHinge(),new LossKLD(),new LossKLD(),new LossL1(),new LossL2(),new LossMAE(),new LossMAE(),new LossMAPE(),new LossMAPE(),new LossMCXENT(),new LossMSE(),new LossMSE(),new LossMSLE(),new LossMSLE(),new LossNegativeLogLikelihood(),new LossPoisson(),new LossSquaredHinge()};
  Activation[] act=new Activation[]{Activation.SIGMOID,Activation.TANH,Activation.SIGMOID,Activation.SOFTMAX,Activation.TANH,Activation.TANH,Activation.TANH,Activation.SOFTMAX,Activation.TANH,Activation.SOFTMAX,Activation.SIGMOID,Activation.TANH,Activation.SOFTMAX,Activation.SIGMOID,Activation.SOFTMAX,Activation.SIGMOID,Activation.SIGMOID,Activation.TANH};
  for (  INDArray labelMask : labelMasks) {
    val minibatch=labelMask.size(0);
    val nOut=labelMask.size(1);
    for (int i=0; i < lossFunctions.length; i++) {
      ILossFunction lf=lossFunctions[i];
      Activation a=act[i];
      MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new NoOp()).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).seed(12345).list().layer(0,new DenseLayer.Builder().nIn(nIn).nOut(layerSize).activation(Activation.TANH).build()).layer(1,new OutputLayer.Builder().nIn(layerSize).nOut(nOut).lossFunction(lf).activation(a).build()).validateOutputLayerConfig(false).build();
      MultiLayerNetwork net=new MultiLayerNetwork(conf);
      net.init();
      net.setLayerMaskArrays(null,labelMask);
      INDArray[] fl=LossFunctionGradientCheck.getFeaturesAndLabels(lf,minibatch,nIn,nOut,12345);
      INDArray features=fl[0];
      INDArray labels=fl[1];
      net.setInput(features);
      net.setLabels(labels);
      net.computeGradientAndScore();
      double score1=net.score();
      INDArray grad1=net.gradient().gradient();
      INDArray maskZeroLocations=Nd4j.getExecutioner().execAndReturn(new Not(labelMask.dup()));
      INDArray rand=Nd4j.rand(maskZeroLocations.shape()).muli(0.5);
      INDArray newLabels=labels.add(rand.muli(maskZeroLocations));
      net.setLabels(newLabels);
      net.computeGradientAndScore();
      assertNotEquals(labels,newLabels);
      double score2=net.score();
      INDArray grad2=net.gradient().gradient();
      assertEquals(score1,score2,1e-6);
      assertEquals(grad1,grad2);
      ComputationGraphConfiguration conf2=new NeuralNetConfiguration.Builder().updater(new NoOp()).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).seed(12345).graphBuilder().addInputs("in").addLayer("0",new DenseLayer.Builder().nIn(nIn).nOut(layerSize).activation(Activation.TANH).build(),"in").addLayer("1",new OutputLayer.Builder().nIn(layerSize).nOut(nOut).lossFunction(lf).activation(a).build(),"0").setOutputs("1").validateOutputLayerConfig(false).build();
      ComputationGraph graph=new ComputationGraph(conf2);
      graph.init();
      graph.setLayerMaskArrays(null,new INDArray[]{labelMask});
      graph.setInputs(features);
      graph.setLabels(labels);
      graph.computeGradientAndScore();
      double gScore1=graph.score();
      INDArray gGrad1=graph.gradient().gradient();
      graph.setLayerMaskArrays(null,new INDArray[]{labelMask});
      graph.setInputs(features);
      graph.setLabels(newLabels);
      graph.computeGradientAndScore();
      double gScore2=graph.score();
      INDArray gGrad2=graph.gradient().gradient();
      assertEquals(gScore1,gScore2,1e-6);
      assertEquals(gGrad1,gGrad2);
    }
  }
}
