@Test public void testBasic() throws Exception {
  JavaSparkContext sc=this.sc;
  RecordReader rr=new CSVRecordReader(0,',');
  rr.initialize(new FileSplit(new ClassPathResource("iris.txt").getTempFileFromArchive()));
  MultiDataSetIterator iter=new RecordReaderMultiDataSetIterator.Builder(1).addReader("iris",rr).addInput("iris",0,3).addOutputOneHot("iris",4,3).build();
  List<MultiDataSet> list=new ArrayList<>(150);
  while (iter.hasNext())   list.add(iter.next());
  ComputationGraphConfiguration config=new NeuralNetConfiguration.Builder().updater(new Sgd(0.1)).graphBuilder().addInputs("in").addLayer("dense",new DenseLayer.Builder().nIn(4).nOut(2).build(),"in").addLayer("out",new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).nIn(2).nOut(3).build(),"dense").setOutputs("out").build();
  ComputationGraph cg=new ComputationGraph(config);
  cg.init();
  TrainingMaster tm=new ParameterAveragingTrainingMaster(true,numExecutors(),1,10,1,0);
  SparkComputationGraph scg=new SparkComputationGraph(sc,cg,tm);
  scg.setListeners(Collections.singleton((TrainingListener)new ScoreIterationListener(1)));
  JavaRDD<MultiDataSet> rdd=sc.parallelize(list);
  scg.fitMultiDataSet(rdd);
  DataSetIterator iris=new IrisDataSetIterator(1,150);
  List<DataSet> list2=new ArrayList<>();
  while (iris.hasNext())   list2.add(iris.next());
  JavaRDD<DataSet> rddDS=sc.parallelize(list2);
  scg.fit(rddDS);
}
