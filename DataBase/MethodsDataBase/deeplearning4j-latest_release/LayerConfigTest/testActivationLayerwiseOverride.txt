@Test public void testActivationLayerwiseOverride(){
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().activation(Activation.RELU).list().layer(0,new DenseLayer.Builder().nIn(2).nOut(2).build()).layer(1,new DenseLayer.Builder().nIn(2).nOut(2).build()).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  assertEquals("relu",((BaseLayer)conf.getConf(0).getLayer()).getActivationFn().toString());
  assertEquals("relu",((BaseLayer)conf.getConf(1).getLayer()).getActivationFn().toString());
  conf=new NeuralNetConfiguration.Builder().activation(Activation.RELU).list().layer(0,new DenseLayer.Builder().nIn(2).nOut(2).build()).layer(1,new DenseLayer.Builder().nIn(2).nOut(2).activation(Activation.TANH).build()).build();
  net=new MultiLayerNetwork(conf);
  net.init();
  assertEquals("relu",((BaseLayer)conf.getConf(0).getLayer()).getActivationFn().toString());
  assertEquals("tanh",((BaseLayer)conf.getConf(1).getLayer()).getActivationFn().toString());
}
