@Test public void testLoss2d(){
  Nd4j.getRandom().setSeed(12345);
  List<String> failed=new ArrayList<>();
  for (  String fn : new String[]{"absdiff","cosine","hinge","huber","log","mse","sigmoidxent","sigmoidxent_smooth","softmaxxent","softmaxxent_smooth","mpwse"}) {
    for (    String weights : new String[]{"none","scalar","perExample","perOutput"}) {
      if ((fn.startsWith("softmax") || fn.equals("cosine")) && weights.equals("perOutput"))       continue;
      if (fn.equals("mpwse") && weights.equals("perOutput"))       continue;
      for (      LossReduce reduction : LossReduce.values()) {
        if ((fn.equals("cosine") && (reduction == LossReduce.MEAN_BY_WEIGHT || reduction == LossReduce.MEAN_BY_NONZERO_WEIGHT_COUNT) || fn.equals("mpwse")) && OpValidationSuite.IGNORE_FAILING) {
          continue;
        }
        if (fn.equals("mpwse") && (reduction != LossReduce.MEAN_BY_WEIGHT || weights.equals("perOutput")))         continue;
        SameDiff sd=SameDiff.create();
        int nOut=4;
        int minibatch=10;
        SDVariable predictions=sd.var("in",new int[]{-1,nOut});
        SDVariable labels=sd.var("labels",new int[]{-1,nOut});
        SDVariable w;
        INDArray wArrBroadcast;
switch (weights) {
case "none":
          w=null;
        wArrBroadcast=Nd4j.ones(minibatch,nOut);
      break;
case "scalar":
    w=sd.var("weights",Nd4j.trueScalar(1.0));
  wArrBroadcast=Nd4j.valueArrayOf(minibatch,nOut,1.0);
break;
case "perExample":
w=sd.var("weights",Nd4j.trueVector(new double[]{0,0,1,1,2,2,3,3,4,4}).reshape(minibatch,1));
wArrBroadcast=Nd4j.create(minibatch,nOut).addiColumnVector(w.getArr());
break;
case "perOutput":
w=sd.var("weights",Nd4j.create(new double[][]{{0,0,0,0},{0,0,1,1},{1,1,0,0},{1,1,1,1},{1,1,1,1},{2,2,2,2},{2,2,2,2},{2,2,2,2},{2,2,2,2},{2,2,2,2}}));
wArrBroadcast=w.getArr();
break;
default :
throw new RuntimeException();
}
INDArray wArr=w == null ? Nd4j.trueScalar(1.0) : w.getArr();
INDArray predictionsArr=Nd4j.randn(minibatch,nOut);
INDArray labelsArr=Nd4j.randn(minibatch,nOut);
INDArray expOut=null;
SDVariable loss=null;
switch (fn) {
case "absdiff":
expOut=Transforms.abs(predictionsArr.sub(labelsArr));
loss=sd.lossAbsoluteDifference("loss",labels,predictions,w,reduction);
break;
case "cosine":
predictionsArr.diviColumnVector(predictionsArr.norm2(1));
labelsArr.diviColumnVector(labelsArr.norm2(1));
expOut=predictionsArr.mul(labelsArr).sum(1).rsub(1.0);
loss=sd.lossCosineDistance("loss",labels,predictions,w,reduction,1);
break;
case "hinge":
Nd4j.getExecutioner().exec(new BernoulliDistribution(labelsArr,0.5));
INDArray labelMinusOneToOne=labelsArr.mul(2).subi(1);
expOut=Transforms.max(predictionsArr.mul(labelMinusOneToOne).rsubi(1),0);
loss=sd.lossHinge("loss",labels,predictions,w,reduction);
break;
case "huber":
double delta=1.0;
INDArray absDiff=Transforms.abs(labelsArr.sub(predictionsArr));
INDArray diff=labelsArr.sub(predictionsArr);
INDArray lte=absDiff.lte(delta);
INDArray gt=absDiff.gt(delta);
expOut=diff.mul(diff).mul(0.5).muli(lte);
expOut.addi(absDiff.mul(delta).subi(0.5 * delta * delta).mul(gt));
loss=sd.lossHuber("loss",labels,predictions,w,reduction,delta);
break;
case "log":
double eps=1e-7;
Nd4j.getExecutioner().exec(new BernoulliDistribution(labelsArr,0.5));
predictionsArr=Nd4j.rand(predictionsArr.shape());
INDArray logP=Transforms.log(predictionsArr.add(eps),true);
INDArray log1p=Transforms.log(predictionsArr.rsub(1.0).add(eps),true);
expOut=labelsArr.mul(logP).addi(labelsArr.rsub(1).mul(log1p)).negi();
loss=sd.lossLog("loss",labels,predictions,w,reduction,eps);
break;
case "mse":
INDArray sqDiff=labelsArr.sub(predictionsArr);
sqDiff.muli(sqDiff);
expOut=sqDiff;
loss=sd.lossMeanSquaredError("loss",labels,predictions,w,reduction);
break;
case "sigmoidxent_smooth":
case "sigmoidxent":
Nd4j.getExecutioner().exec(new BernoulliDistribution(labelsArr,0.5));
double lblSmoothing=fn.equals("sigmoidxent_smooth") ? 0.3 : 0.0;
INDArray labelArrCopy=labelsArr.dup();
if (fn.equals("sigmoidxent_smooth")) {
labelArrCopy.muli(1.0 - lblSmoothing).addi(0.5 * lblSmoothing);
}
INDArray onePlusExpNegX=Transforms.log(Transforms.exp(predictionsArr.neg()).add(1.0));
expOut=predictionsArr.mul(labelArrCopy.rsub(1.0)).add(onePlusExpNegX);
loss=sd.lossSigmoidCrossEntropy("loss",labels,predictions,w,reduction,lblSmoothing);
break;
case "softmaxxent":
case "softmaxxent_smooth":
INDArray softmaxPredictions=Transforms.softmax(predictionsArr,true);
labelsArr.assign(0);
for (int i=0; i < labelsArr.size(0); i++) {
labelsArr.putScalar(i,i % labelsArr.size(1),1.0);
}
double lblSmooth2=fn.equals("softmaxxent_smooth") ? 0.1 : 0.0;
INDArray labelsArrCopy=labelsArr.dup();
if (fn.equals("softmaxxent_smooth")) {
labelsArrCopy.muli(1.0 - lblSmooth2).addi(lblSmooth2 / labelsArrCopy.size(1));
}
INDArray logP2=Transforms.log(softmaxPredictions,true);
expOut=labelsArrCopy.mul(logP2).negi().sum(1);
loss=sd.lossSoftmaxCrossEntropy("loss",labels,predictions,w,reduction,lblSmooth2);
break;
case "mpwse":
expOut=Nd4j.create(labelsArr.size(0));
int pairCount=0;
for (int i=0; i < labelsArr.size(0); i++) {
for (int j=0; j < labelsArr.size(1); j++) {
for (int k=j + 1; k < labelsArr.size(1); k++) {
double d1=predictionsArr.getDouble(i,j);
double d2=predictionsArr.getDouble(i,k);
double d3=labelsArr.getDouble(i,j);
double d4=labelsArr.getDouble(i,k);
double add=((d1 - d2) - (d3 - d4));
add*=add;
expOut.putScalar(i,expOut.getDouble(i) + add);
if (i == 0) pairCount++;
}
}
}
loss=sd.lossMeanPairwiseSquaredError("loss",labels,predictions,w);
break;
default :
throw new RuntimeException();
}
switch (weights) {
case "none":
break;
case "scalar":
expOut.muli(wArr.getDouble(0));
break;
case "perExample":
expOut.muliColumnVector(wArr);
break;
case "perOutput":
expOut.muli(wArr);
break;
default :
throw new RuntimeException();
}
INDArray expOutBefore=expOut;
switch (reduction) {
case SUM:
expOut=expOut.sum().reshape();
break;
case MEAN_BY_WEIGHT:
if ((fn.startsWith("softmax") || fn.equals("cosine"))) {
expOut=expOut.sum().divi(wArrBroadcast.getColumn(0).sumNumber().doubleValue());
}
 else {
expOut=expOut.sum().divi(wArrBroadcast.sumNumber().doubleValue());
}
break;
case MEAN_BY_NONZERO_WEIGHT_COUNT:
if ((fn.startsWith("softmax") || fn.equals("cosine"))) {
int countNonZero=wArrBroadcast.getColumn(0).neq(0.0).sumNumber().intValue();
expOut=expOut.sum().divi(countNonZero);
}
 else {
int countNonZero=wArrBroadcast.neq(0.0).sumNumber().intValue();
expOut=expOut.sum().divi(countNonZero);
}
break;
}
String msg="test: " + fn + ", reduction="+ reduction+ ", weights="+ weights;
log.info("*** Starting test: " + msg);
sd.associateArrayWithVariable(predictionsArr,predictions);
sd.associateArrayWithVariable(labelsArr,labels);
TestCase tc=new TestCase(sd).expectedOutput("loss",expOut).gradientCheck(false).testFlatBufferSerialization(TestCase.TestSerialization.NONE);
String error=OpValidation.validate(tc);
if (error != null) {
failed.add(msg + error);
}
}
}
}
assertEquals(failed.toString(),0,failed.size());
}
