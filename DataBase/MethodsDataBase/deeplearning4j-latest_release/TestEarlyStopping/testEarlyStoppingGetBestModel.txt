@Test public void testEarlyStoppingGetBestModel(){
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).updater(new Sgd(0.001)).weightInit(WeightInit.XAVIER).list().layer(0,new OutputLayer.Builder().nIn(4).nOut(3).activation(Activation.SOFTMAX).lossFunction(LossFunctions.LossFunction.MCXENT).build()).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.setListeners(new ScoreIterationListener(1));
  DataSetIterator irisIter=new IrisDataSetIterator(150,150);
  MultipleEpochsIterator mIter=new MultipleEpochsIterator(10,irisIter);
  EarlyStoppingModelSaver<MultiLayerNetwork> saver=new InMemoryModelSaver<>();
  EarlyStoppingConfiguration<MultiLayerNetwork> esConf=new EarlyStoppingConfiguration.Builder<MultiLayerNetwork>().epochTerminationConditions(new MaxEpochsTerminationCondition(5)).iterationTerminationConditions(new MaxTimeIterationTerminationCondition(1,TimeUnit.MINUTES)).scoreCalculator(new DataSetLossCalculator(irisIter,true)).modelSaver(saver).build();
  IEarlyStoppingTrainer<MultiLayerNetwork> trainer=new EarlyStoppingTrainer(esConf,net,mIter);
  EarlyStoppingResult<MultiLayerNetwork> result=trainer.fit();
  System.out.println(result);
  MultiLayerNetwork mln=result.getBestModel();
  assertEquals(net.getnLayers(),mln.getnLayers());
  assertEquals(net.conf().getOptimizationAlgo(),mln.conf().getOptimizationAlgo());
  BaseLayer bl=(BaseLayer)net.conf().getLayer();
  assertEquals(bl.getActivationFn().toString(),((BaseLayer)mln.conf().getLayer()).getActivationFn().toString());
  assertEquals(bl.getIUpdater(),((BaseLayer)mln.conf().getLayer()).getIUpdater());
}
