@Test public void testGradientApplyComputationGraph(){
  int minibatch=7;
  int nIn=10;
  int nOut=10;
  for (  boolean regularization : new boolean[]{false,true}) {
    for (    IUpdater u : new IUpdater[]{new Sgd(0.1),new Adam(0.1)}) {
      ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).activation(Activation.TANH).weightInit(WeightInit.XAVIER).updater(u).l1(regularization ? 0.2 : 0.0).l2(regularization ? 0.3 : 0.0).graphBuilder().addInputs("in").addLayer("0",new DenseLayer.Builder().nIn(nIn).nOut(10).build(),"in").addLayer("1",new DenseLayer.Builder().nIn(10).nOut(10).build(),"0").addLayer("2",new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(10).nOut(nOut).build(),"1").setOutputs("2").build();
      Nd4j.getRandom().setSeed(12345);
      ComputationGraph net1GradCalc=new ComputationGraph(conf);
      net1GradCalc.init();
      Nd4j.getRandom().setSeed(12345);
      ComputationGraph net2GradUpd=new ComputationGraph(conf.clone());
      net2GradUpd.init();
      assertEquals(net1GradCalc.params(),net2GradUpd.params());
      INDArray f=Nd4j.rand(minibatch,nIn);
      INDArray l=Nd4j.create(minibatch,nOut);
      for (int i=0; i < minibatch; i++) {
        l.putScalar(i,i % nOut,1.0);
      }
      net1GradCalc.setInputs(f);
      net1GradCalc.setLabels(l);
      net2GradUpd.setInputs(f);
      net2GradUpd.setLabels(l);
      net1GradCalc.computeGradientAndScore();
      net2GradUpd.computeGradientAndScore();
      Gradient g=net1GradCalc.gradient();
      INDArray gBefore=g.gradient().dup();
      INDArray net2GradBefore=net2GradUpd.gradient().gradient().dup();
      net2GradUpd.getUpdater().update(g,0,0,minibatch,LayerWorkspaceMgr.noWorkspaces());
      INDArray gAfter=g.gradient().dup();
      INDArray net2GradAfter=net2GradUpd.gradient().gradient().dup();
      assertNotEquals(gBefore,gAfter);
      assertEquals(net2GradBefore,net2GradAfter);
      net2GradUpd.params().subi(g.gradient());
      net1GradCalc.fit(new INDArray[]{f},new INDArray[]{l});
      assertEquals(net1GradCalc.params(),net2GradUpd.params());
      if (!(u instanceof Sgd)) {
        net2GradUpd.getUpdater().getStateViewArray().assign(net1GradCalc.getUpdater().getStateViewArray());
      }
      assertEquals(net1GradCalc.params(),net2GradUpd.params());
      assertEquals(net1GradCalc.getUpdater().getStateViewArray(),net2GradUpd.getUpdater().getStateViewArray());
      net1GradCalc.getConfiguration().setIterationCount(0);
      net2GradUpd.getConfiguration().setIterationCount(0);
      for (int i=0; i < 100; i++) {
        net1GradCalc.fit(new INDArray[]{f},new INDArray[]{l});
        net2GradUpd.fit(new INDArray[]{f},new INDArray[]{l});
        assertEquals(net1GradCalc.params(),net2GradUpd.params());
      }
    }
  }
}
