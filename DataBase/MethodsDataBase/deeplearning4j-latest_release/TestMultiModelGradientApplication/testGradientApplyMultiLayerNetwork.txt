@Test public void testGradientApplyMultiLayerNetwork(){
  int minibatch=7;
  int nIn=10;
  int nOut=10;
  for (  boolean regularization : new boolean[]{false,true}) {
    for (    IUpdater u : new IUpdater[]{new Sgd(0.1),new Nesterovs(0.1),new Adam(0.1)}) {
      MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).activation(Activation.TANH).weightInit(WeightInit.XAVIER).updater(u).l1(regularization ? 0.2 : 0.0).l2(regularization ? 0.3 : 0.0).list().layer(0,new DenseLayer.Builder().nIn(nIn).nOut(10).build()).layer(1,new DenseLayer.Builder().nIn(10).nOut(10).build()).layer(2,new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(10).nOut(nOut).build()).build();
      Nd4j.getRandom().setSeed(12345);
      MultiLayerNetwork net1GradCalc=new MultiLayerNetwork(conf);
      net1GradCalc.init();
      Nd4j.getRandom().setSeed(12345);
      MultiLayerNetwork net2GradUpd=new MultiLayerNetwork(conf.clone());
      net2GradUpd.init();
      assertEquals(net1GradCalc.params(),net2GradUpd.params());
      INDArray f=Nd4j.rand(minibatch,nIn);
      INDArray l=Nd4j.create(minibatch,nOut);
      for (int i=0; i < minibatch; i++) {
        l.putScalar(i,i % nOut,1.0);
      }
      net1GradCalc.setInput(f);
      net1GradCalc.setLabels(l);
      net2GradUpd.setInput(f);
      net2GradUpd.setLabels(l);
      net1GradCalc.computeGradientAndScore();
      net2GradUpd.computeGradientAndScore();
      Gradient g=net1GradCalc.gradient();
      INDArray gBefore=g.gradient().dup();
      INDArray net2GradBefore=net2GradUpd.gradient().gradient().dup();
      net2GradUpd.getUpdater().update(net2GradUpd,g,0,0,minibatch,LayerWorkspaceMgr.noWorkspaces());
      INDArray gAfter=g.gradient().dup();
      INDArray net2GradAfter=net2GradUpd.gradient().gradient().dup();
      assertNotEquals(gBefore,gAfter);
      assertEquals(net2GradBefore,net2GradAfter);
      net2GradUpd.params().subi(g.gradient());
      net1GradCalc.fit(f,l);
      assertEquals(net1GradCalc.params(),net2GradUpd.params());
      if (!(u instanceof Sgd)) {
        net2GradUpd.getUpdater().getStateViewArray().assign(net1GradCalc.getUpdater().getStateViewArray());
      }
      assertEquals(net1GradCalc.params(),net2GradUpd.params());
      assertEquals(net1GradCalc.getUpdater().getStateViewArray(),net2GradUpd.getUpdater().getStateViewArray());
      net1GradCalc.getLayerWiseConfigurations().setIterationCount(0);
      net2GradUpd.getLayerWiseConfigurations().setIterationCount(0);
      for (int i=0; i < 100; i++) {
        net1GradCalc.fit(f,l);
        net2GradUpd.fit(f,l);
        assertEquals(net1GradCalc.params(),net2GradUpd.params());
      }
    }
  }
}
