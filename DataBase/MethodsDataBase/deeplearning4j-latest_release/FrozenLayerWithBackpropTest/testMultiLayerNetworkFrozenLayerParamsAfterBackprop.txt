@Test public void testMultiLayerNetworkFrozenLayerParamsAfterBackprop(){
  DataSet randomData=new DataSet(Nd4j.rand(100,4,12345),Nd4j.rand(100,1,12345));
  MultiLayerConfiguration conf1=new NeuralNetConfiguration.Builder().seed(12345).weightInit(WeightInit.XAVIER).updater(new Sgd(2)).list().layer(0,new DenseLayer.Builder().nIn(4).nOut(3).build()).layer(1,new org.deeplearning4j.nn.conf.layers.misc.FrozenLayerWithBackprop(new DenseLayer.Builder().nIn(3).nOut(4).build())).layer(2,new org.deeplearning4j.nn.conf.layers.misc.FrozenLayerWithBackprop(new DenseLayer.Builder().nIn(4).nOut(2).build())).layer(3,new org.deeplearning4j.nn.conf.layers.misc.FrozenLayerWithBackprop(new OutputLayer.Builder(LossFunctions.LossFunction.MSE).nIn(2).nOut(1).build())).build();
  MultiLayerNetwork network=new MultiLayerNetwork(conf1);
  network.init();
  INDArray unfrozenLayerParams=network.getLayer(0).params().dup();
  INDArray frozenLayerParams1=network.getLayer(1).params().dup();
  INDArray frozenLayerParams2=network.getLayer(2).params().dup();
  INDArray frozenOutputLayerParams=network.getLayer(3).params().dup();
  for (int i=0; i < 100; i++) {
    network.fit(randomData);
  }
  assertNotEquals(unfrozenLayerParams,network.getLayer(0).params());
  assertEquals(frozenLayerParams1,network.getLayer(1).params());
  assertEquals(frozenLayerParams2,network.getLayer(2).params());
  assertEquals(frozenOutputLayerParams,network.getLayer(3).params());
}
