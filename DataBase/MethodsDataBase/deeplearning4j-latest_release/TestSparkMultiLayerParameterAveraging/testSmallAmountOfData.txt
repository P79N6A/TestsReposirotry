@Test public void testSmallAmountOfData(){
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new RmsProp()).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).list().layer(0,new org.deeplearning4j.nn.conf.layers.DenseLayer.Builder().nIn(nIn).nOut(3).activation(Activation.TANH).build()).layer(1,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MSE).nIn(3).nOut(nOut).activation(Activation.SOFTMAX).build()).build();
  SparkDl4jMultiLayer sparkNet=new SparkDl4jMultiLayer(sc,conf,new ParameterAveragingTrainingMaster(true,numExecutors(),1,10,1,0));
  Nd4j.getRandom().setSeed(12345);
  DataSet d1=new DataSet(Nd4j.rand(1,nIn),Nd4j.rand(1,nOut));
  DataSet d2=new DataSet(Nd4j.rand(1,nIn),Nd4j.rand(1,nOut));
  JavaRDD<DataSet> rddData=sc.parallelize(Arrays.asList(d1,d2));
  sparkNet.fit(rddData);
}
