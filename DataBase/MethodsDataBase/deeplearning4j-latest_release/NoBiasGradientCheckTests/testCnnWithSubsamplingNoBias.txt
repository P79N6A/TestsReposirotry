@Test public void testCnnWithSubsamplingNoBias(){
  int nOut=4;
  int[] minibatchSizes={1,3};
  int width=5;
  int height=5;
  int inputDepth=1;
  int[] kernel={2,2};
  int[] stride={1,1};
  int[] padding={0,0};
  int pNorm=3;
  for (  int minibatchSize : minibatchSizes) {
    INDArray input=Nd4j.rand(minibatchSize,width * height * inputDepth);
    INDArray labels=Nd4j.zeros(minibatchSize,nOut);
    for (int i=0; i < minibatchSize; i++) {
      labels.putScalar(new int[]{i,i % nOut},1.0);
    }
    for (    boolean cnnHasBias : new boolean[]{true,false}) {
      MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new NoOp()).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).list().layer(new ConvolutionLayer.Builder(kernel,stride,padding).nIn(inputDepth).hasBias(false).nOut(3).build()).layer(new SubsamplingLayer.Builder(PoolingType.MAX).kernelSize(kernel).stride(stride).padding(padding).pnorm(pNorm).build()).layer(new ConvolutionLayer.Builder(kernel,stride,padding).hasBias(cnnHasBias).nOut(2).build()).layer(new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nOut(4).build()).setInputType(InputType.convolutionalFlat(height,width,inputDepth)).build();
      MultiLayerNetwork net=new MultiLayerNetwork(conf);
      net.init();
      if (cnnHasBias) {
        assertEquals(3 * 2 * kernel[0]* kernel[1] + 2,net.getLayer(2).numParams());
      }
 else {
        assertEquals(3 * 2 * kernel[0]* kernel[1],net.getLayer(2).numParams());
      }
      String msg="testCnnWithSubsamplingNoBias(), minibatch = " + minibatchSize + ", cnnHasBias = "+ cnnHasBias;
      System.out.println(msg);
      boolean gradOK=GradientCheckUtil.checkGradients(net,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
      assertTrue(msg,gradOK);
      TestUtils.testModelSerialization(net);
    }
  }
}
