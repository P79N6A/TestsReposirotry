@Test public void testGradientNoBiasEmbedding(){
  int nIn=5;
  int nOut=3;
  int layerSize=6;
  for (  int minibatch : new int[]{1,4}) {
    INDArray input=Nd4j.zeros(minibatch,1);
    for (int i=0; i < minibatch; i++) {
      input.putScalar(i,0,i % layerSize);
    }
    INDArray labels=Nd4j.zeros(minibatch,nOut);
    for (int i=0; i < minibatch; i++) {
      labels.putScalar(i,i % nOut,1.0);
    }
    for (    boolean embeddingHasBias : new boolean[]{true,false}) {
      MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new NoOp()).seed(12345L).list().layer(0,new EmbeddingLayer.Builder().nIn(nIn).nOut(layerSize).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).activation(Activation.TANH).hasBias(embeddingHasBias).build()).layer(1,new OutputLayer.Builder(LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(layerSize).nOut(nOut).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).build()).build();
      MultiLayerNetwork mln=new MultiLayerNetwork(conf);
      mln.init();
      if (embeddingHasBias) {
        assertEquals(nIn * layerSize + layerSize,mln.getLayer(0).numParams());
      }
 else {
        assertEquals(nIn * layerSize,mln.getLayer(0).numParams());
      }
      String msg="testGradientNoBiasEmbedding(), minibatch = " + minibatch + ", embeddingHasBias = "+ embeddingHasBias+ ")";
      if (PRINT_RESULTS) {
        System.out.println(msg);
      }
      boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
      assertTrue(msg,gradOK);
      TestUtils.testModelSerialization(mln);
    }
  }
}
