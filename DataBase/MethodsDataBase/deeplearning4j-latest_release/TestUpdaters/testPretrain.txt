@Test public void testPretrain(){
  gradients=Nd4j.ones(nIn * nOut + nOut + nIn);
  weightGradient=gradients.get(point(0),interval(0,nIn * nOut));
  biasGradient=gradients.get(point(0),interval(nIn * nOut,nIn * nOut + nOut));
  INDArray vbiasGradient=gradients.get(point(0),interval(nIn * nOut + nOut,nIn * nOut + nOut + nIn));
  gradient.setFlattenedGradient(gradients);
  double lr=0.05;
  gradient.setGradientFor(DefaultParamInitializer.WEIGHT_KEY,weightGradient);
  gradient.setGradientFor(DefaultParamInitializer.BIAS_KEY,biasGradient);
  gradient.setGradientFor(PretrainParamInitializer.VISIBLE_BIAS_KEY,vbiasGradient);
  NeuralNetConfiguration conf=new NeuralNetConfiguration.Builder().updater(new Sgd(lr)).seed(42).layer(new AutoEncoder.Builder().lossFunction(LossFunctions.LossFunction.COSINE_PROXIMITY).activation(Activation.IDENTITY).nIn(nIn).nOut(nOut).build()).build();
  long numParams=conf.getLayer().initializer().numParams(conf);
  conf.setPretrain(true);
  INDArray params=Nd4j.create(1,numParams);
  BaseLayer layer=(BaseLayer)conf.getLayer().instantiate(conf,null,0,params,true);
  layer.setBackpropGradientsViewArray(gradients);
  Updater updater=UpdaterCreator.getUpdater(layer);
  DefaultGradient gradientCopyPreUpdate=new DefaultGradient();
  INDArray g=gradients.dup();
  INDArray wg=g.get(point(0),interval(0,nIn * nOut));
  INDArray bg=g.get(point(0),interval(nIn * nOut,nIn * nOut + nOut));
  INDArray vbg=g.get(point(0),interval(nIn * nOut + nOut,nIn * nOut + nOut + nIn));
  gradientCopyPreUpdate.setGradientFor(DefaultParamInitializer.WEIGHT_KEY,wg);
  gradientCopyPreUpdate.setGradientFor(DefaultParamInitializer.BIAS_KEY,bg);
  gradientCopyPreUpdate.setGradientFor(PretrainParamInitializer.VISIBLE_BIAS_KEY,vbg);
  updater.update(layer,gradient,-1,0,1,LayerWorkspaceMgr.noWorkspaces());
  for (  Map.Entry<String,INDArray> entry : gradientCopyPreUpdate.gradientForVariable().entrySet()) {
    val=entry.getValue();
    gradExpected=val.mul(lr);
    assertEquals(gradExpected,gradient.getGradientFor(entry.getKey()));
  }
  assertEquals(lr,((Sgd)layer.layerConf().getIUpdater()).getLearningRate(),1e-4);
  gradients=Nd4j.ones(nIn * nOut + nOut + nIn);
  weightGradient=gradients.get(point(0),interval(0,nIn * nOut));
  biasGradient=gradients.get(point(0),interval(nIn * nOut,nIn * nOut + nOut));
  vbiasGradient=gradients.get(point(0),interval(nIn * nOut + nOut,nIn * nOut + nOut + nIn));
  gradient.setGradientFor(DefaultParamInitializer.WEIGHT_KEY,weightGradient);
  gradient.setGradientFor(DefaultParamInitializer.BIAS_KEY,biasGradient);
  gradient.setGradientFor(PretrainParamInitializer.VISIBLE_BIAS_KEY,vbiasGradient);
  gradient.setFlattenedGradient(gradients);
  gradientCopyPreUpdate=new DefaultGradient();
  g=gradients.dup();
  wg=g.get(point(0),interval(0,nIn * nOut));
  bg=g.get(point(0),interval(nIn * nOut,nIn * nOut + nOut));
  vbg=g.get(point(0),interval(nIn * nOut + nOut,nIn * nOut + nOut + nIn));
  gradientCopyPreUpdate.setGradientFor(DefaultParamInitializer.WEIGHT_KEY,wg);
  gradientCopyPreUpdate.setGradientFor(DefaultParamInitializer.BIAS_KEY,bg);
  gradientCopyPreUpdate.setGradientFor(PretrainParamInitializer.VISIBLE_BIAS_KEY,vbg);
  gradientCopyPreUpdate.setFlattenedGradient(g);
  conf.setPretrain(false);
  params=Nd4j.create(1,numParams);
  layer=(BaseLayer)conf.getLayer().instantiate(conf,null,0,params,true);
  layer.setBackpropGradientsViewArray(gradients);
  updater=UpdaterCreator.getUpdater(layer);
  updater.update(layer,gradient,-1,0,1,LayerWorkspaceMgr.noWorkspaces());
  for (  Map.Entry<String,INDArray> entry : gradientCopyPreUpdate.gradientForVariable().entrySet()) {
    val=entry.getValue();
    if (!entry.getKey().equals("vb")) {
      gradExpected=val.mul(lr);
    }
 else {
      gradExpected=val;
    }
    assertEquals(gradExpected,gradient.getGradientFor(entry.getKey()));
  }
  assertEquals(lr,((Sgd)layer.layerConf().getIUpdater()).getLearningRate(),1e-4);
}
