@Test public void testRMSPropUpdater(){
  double lr=0.01;
  double rmsDecay=0.25;
  Map<String,INDArray> lastG=new HashMap<>();
  NeuralNetConfiguration conf=new NeuralNetConfiguration.Builder().updater(new RmsProp(lr,rmsDecay,RmsProp.DEFAULT_RMSPROP_EPSILON)).layer(new DenseLayer.Builder().nIn(nIn).nOut(nOut).build()).build();
  long numParams=conf.getLayer().initializer().numParams(conf);
  INDArray params=Nd4j.create(1,numParams);
  BaseLayer layer=(BaseLayer)conf.getLayer().instantiate(conf,null,0,params,true);
  layer.setBackpropGradientsViewArray(gradients);
  Updater updater=UpdaterCreator.getUpdater(layer);
  int updaterStateSize=(int)layer.layerConf().getIUpdater().stateSize(numParams);
  INDArray updaterState=Nd4j.create(1,updaterStateSize);
  updater.setStateViewArray(layer,updaterState,true);
  Gradient gradientCopyPreUpdate=new DefaultGradient();
  INDArray g=gradients.dup();
  INDArray wg=g.get(point(0),interval(0,nIn * nOut));
  INDArray bg=g.get(point(0),interval(nIn * nOut,nIn * nOut + nOut));
  gradientCopyPreUpdate.setGradientFor(DefaultParamInitializer.WEIGHT_KEY,wg);
  gradientCopyPreUpdate.setGradientFor(DefaultParamInitializer.BIAS_KEY,bg);
  updater.update(layer,gradient,-1,0,1,LayerWorkspaceMgr.noWorkspaces());
  double epsilon=1e-8;
  for (  Map.Entry<String,INDArray> entry : gradientCopyPreUpdate.gradientForVariable().entrySet()) {
    key=entry.getKey();
    val=entry.getValue();
    INDArray lastGTmp=lastG.get(key);
    if (lastGTmp == null)     lastGTmp=Nd4j.zeros(val.shape());
    lastGTmp.muli(rmsDecay).addi(val.mul(val).muli(1 - rmsDecay));
    gradExpected=val.mul(lr).div(Transforms.sqrt(lastGTmp.add(epsilon)));
    assertEquals(gradExpected,gradient.getGradientFor(entry.getKey()));
    lastG.put(key,lastGTmp);
  }
  assertEquals(rmsDecay,((RmsProp)layer.layerConf().getIUpdater()).getRmsDecay(),1e-4);
}
