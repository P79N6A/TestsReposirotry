@Test public void testMultiLayerUpdater() throws Exception {
  Nd4j.getRandom().setSeed(12345L);
  double lr=0.03;
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().list().layer(0,new DenseLayer.Builder().nIn(4).nOut(5).updater(new Sgd(lr)).build()).layer(1,new DenseLayer.Builder().nIn(5).nOut(6).updater(new NoOp()).build()).layer(2,new DenseLayer.Builder().nIn(6).nOut(7).updater(new AdaGrad(lr)).build()).layer(3,new OutputLayer.Builder().nIn(7).nOut(8).updater(new Nesterovs(0.6)).activation(Activation.TANH).lossFunction(LossFunctions.LossFunction.MSE).build()).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  net.fit(Nd4j.create(1,4),Nd4j.create(1,8));
  Updater updater=net.getUpdater();
  assertNotNull(updater);
  assertTrue(updater.getClass() == MultiLayerUpdater.class);
  MultiLayerUpdater mlu=(MultiLayerUpdater)updater;
  int count=0;
  for (  UpdaterBlock u : mlu.getUpdaterBlocks()) {
    GradientUpdater gu=u.getGradientUpdater();
switch (count) {
case 0:
      assertTrue(gu instanceof SgdUpdater);
    break;
case 1:
  assertTrue(gu instanceof org.nd4j.linalg.learning.NoOpUpdater);
break;
case 2:
assertTrue(gu instanceof AdaGradUpdater);
break;
case 3:
assertTrue(gu instanceof NesterovsUpdater);
break;
default :
throw new RuntimeException();
}
count++;
}
GradientUpdater[] uArr=new GradientUpdater[4];
uArr[0]=new SgdUpdater(new Sgd(lr));
uArr[1]=new NoOpUpdater(new NoOp());
uArr[2]=new AdaGradUpdater(new AdaGrad(lr,AdaGrad.DEFAULT_ADAGRAD_EPSILON));
INDArray updaterState=Nd4j.create(1,6 * 7 + 7,'f');
uArr[2].setStateViewArray(updaterState,new long[]{1,6 * 7 + 7},'f',true);
uArr[3]=new NesterovsUpdater(new Nesterovs(lr,0.6));
updaterState=Nd4j.create(1,7 * 8 + 8,'f');
uArr[3].setStateViewArray(updaterState,new long[]{1,7 * 8 + 8},'f',true);
int[] nIns={4,5,6,7};
int[] nOuts={5,6,7,8};
for (int i=0; i < 5; i++) {
Gradient gradient=new DefaultGradient();
Map<String,INDArray> expectedGradient=new LinkedHashMap<>();
for (int j=0; j < net.getnLayers(); j++) {
INDArray wGrad=Nd4j.rand(nIns[j],nOuts[j]);
INDArray bGrad=Nd4j.rand(1,nOuts[j]);
String wKey=j + "_" + DefaultParamInitializer.WEIGHT_KEY;
String bKey=j + "_" + DefaultParamInitializer.BIAS_KEY;
gradient.setGradientFor(wKey,wGrad);
gradient.setGradientFor(bKey,bGrad);
Gradient layerGradient=new DefaultGradient();
layerGradient.setGradientFor(DefaultParamInitializer.WEIGHT_KEY,wGrad.dup());
layerGradient.setGradientFor(DefaultParamInitializer.BIAS_KEY,bGrad.dup());
for (String s : layerGradient.gradientForVariable().keySet()) {
expectedGradient.put(j + "_" + s,layerGradient.getGradientFor(s));
}
}
updater.update(net,gradient,i,0,1,LayerWorkspaceMgr.noWorkspaces());
assertEquals(gradient.gradientForVariable(),expectedGradient);
}
}
