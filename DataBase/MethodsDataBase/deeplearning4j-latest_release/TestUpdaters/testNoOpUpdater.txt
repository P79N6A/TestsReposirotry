@Test public void testNoOpUpdater(){
  Random r=new Random(12345L);
  double lr=0.5;
  NeuralNetConfiguration conf=new NeuralNetConfiguration.Builder().updater(new NoOp()).layer(new DenseLayer.Builder().nIn(nIn).nOut(nOut).build()).build();
  long numParams=conf.getLayer().initializer().numParams(conf);
  INDArray params=Nd4j.create(1,numParams);
  Layer layer=conf.getLayer().instantiate(conf,null,0,params,true);
  layer.setBackpropGradientsViewArray(gradients);
  Updater updater=UpdaterCreator.getUpdater(layer);
  for (int i=0; i < weightGradient.length(); i++)   weightGradient.putScalar(i,r.nextDouble());
  for (int i=0; i < biasGradient.length(); i++)   biasGradient.putScalar(i,r.nextDouble());
  INDArray g=gradients.dup();
  INDArray wg=g.get(point(0),interval(0,nIn * nOut));
  INDArray bg=g.get(point(0),interval(nIn * nOut,nIn * nOut + nOut));
  gradient.gradientForVariable().put(DefaultParamInitializer.WEIGHT_KEY,wg);
  gradient.gradientForVariable().put(DefaultParamInitializer.BIAS_KEY,bg);
  updater.update(layer,gradient,-1,0,1,LayerWorkspaceMgr.noWorkspaces());
  INDArray weightGradActual=gradient.getGradientFor(DefaultParamInitializer.WEIGHT_KEY);
  INDArray biasGradActual=gradient.getGradientFor(DefaultParamInitializer.BIAS_KEY);
  assertEquals(wg,weightGradActual);
  assertEquals(bg,biasGradActual);
}
