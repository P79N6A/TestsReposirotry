@Test public void testSGDUpdater(){
  double lr=0.05;
  NeuralNetConfiguration conf=new NeuralNetConfiguration.Builder().updater(new Sgd(lr)).layer(new DenseLayer.Builder().nIn(nIn).nOut(nOut).build()).build();
  long numParams=conf.getLayer().initializer().numParams(conf);
  INDArray params=Nd4j.create(1,numParams);
  BaseLayer layer=(BaseLayer)conf.getLayer().instantiate(conf,null,0,params,true);
  layer.setBackpropGradientsViewArray(gradients);
  Updater updater=UpdaterCreator.getUpdater(layer);
  Gradient gradientCopyPreUpdate=new DefaultGradient();
  INDArray g=gradients.dup();
  INDArray wg=g.get(point(0),interval(0,nIn * nOut));
  INDArray bg=g.get(point(0),interval(nIn * nOut,nIn * nOut + nOut));
  gradientCopyPreUpdate.setGradientFor(DefaultParamInitializer.WEIGHT_KEY,wg);
  gradientCopyPreUpdate.setGradientFor(DefaultParamInitializer.BIAS_KEY,bg);
  updater.update(layer,gradient,-1,0,1,LayerWorkspaceMgr.noWorkspaces());
  for (  Map.Entry<String,INDArray> entry : gradientCopyPreUpdate.gradientForVariable().entrySet()) {
    val=entry.getValue();
    gradExpected=val.mul(lr);
    assertEquals(gradExpected,gradient.getGradientFor(entry.getKey()));
  }
  assertEquals(lr,((Sgd)layer.layerConf().getIUpdater()).getLearningRate(),1e-4);
}
