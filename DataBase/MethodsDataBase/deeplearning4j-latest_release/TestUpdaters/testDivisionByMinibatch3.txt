@Test public void testDivisionByMinibatch3() throws Exception {
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().list().layer(new BatchNormalization.Builder().nOut(6).build()).layer(new ConvolutionLayer.Builder().nIn(6).nOut(5).kernelSize(2,2).build()).layer(new BatchNormalization.Builder().nOut(5).build()).layer(new ConvolutionLayer.Builder().nIn(5).nOut(4).kernelSize(2,2).build()).layer(new BatchNormalization.Builder().nOut(4).build()).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  net.init();
  BaseMultiLayerUpdater u=(BaseMultiLayerUpdater)net.getUpdater();
  Method m=BaseMultiLayerUpdater.class.getDeclaredMethod("divideByMinibatch",boolean.class,Gradient.class,int.class);
  m.setAccessible(true);
  m.invoke(u,false,null,32);
  List<INDArray> l=u.getGradientsForMinibatchDivision();
  assertNotNull(l);
  assertEquals(3,l.size());
  assertEquals(2 * 6,l.get(0).length());
  assertEquals(6 * 5 * 2* 2 + 5 + 2 * 5,l.get(1).length());
  assertEquals(5 * 4 * 2* 2 + 4 + 2 * 4,l.get(2).length());
  INDArray view=((BaseMultiLayerUpdater)net.getUpdater()).getFlattenedGradientsView();
  view.assign(Nd4j.linspace(1,view.length(),view.length()));
  INDArray expView1=view.get(point(0),interval(0,2 * 6));
  assertEquals(expView1,l.get(0));
  long start2=2 * 6 + 2 * 6;
  long length2=6 * 5 * 2* 2 + 5 + 2 * 5;
  INDArray expView2=view.get(point(0),interval(start2,start2 + length2));
  assertEquals(expView2,l.get(1));
  long start3=start2 + length2 + 2 * 5;
  long length3=5 * 4 * 2* 2 + 4 + 2 * 4;
  INDArray expView3=view.get(point(0),interval(start3,start3 + length3));
  assertEquals(expView3,l.get(2));
}
