@Test public void testAdaGradUpdater(){
  double lr=1e-2;
  double epsilon=AdaGrad.DEFAULT_ADAGRAD_EPSILON;
  NeuralNetConfiguration conf=new NeuralNetConfiguration.Builder().updater(new AdaGrad(lr)).layer(new DenseLayer.Builder().nIn(nIn).nOut(nOut).build()).build();
  long numParams=conf.getLayer().initializer().numParams(conf);
  INDArray params=Nd4j.create(1,numParams);
  BaseLayer layer=(BaseLayer)conf.getLayer().instantiate(conf,null,0,params,true);
  layer.setBackpropGradientsViewArray(gradients);
  Updater updater=UpdaterCreator.getUpdater(layer);
  int updaterStateSize=(int)layer.layerConf().getIUpdater().stateSize(numParams);
  INDArray updaterState=Nd4j.create(1,updaterStateSize);
  updater.setStateViewArray(layer,updaterState,true);
  Gradient gradientCopyPreUpdate=new DefaultGradient();
  INDArray g=gradients.dup();
  INDArray wg=g.get(point(0),interval(0,nIn * nOut));
  INDArray bg=g.get(point(0),interval(nIn * nOut,nIn * nOut + nOut));
  gradientCopyPreUpdate.setGradientFor(DefaultParamInitializer.WEIGHT_KEY,wg);
  gradientCopyPreUpdate.setGradientFor(DefaultParamInitializer.BIAS_KEY,bg);
  updater.update(layer,gradient,-1,0,1,LayerWorkspaceMgr.noWorkspaces());
  int count=0;
  for (  Map.Entry<String,INDArray> entry : gradientCopyPreUpdate.gradientForVariable().entrySet()) {
    val=entry.getValue();
    gradExpected=Transforms.sqrt(val.mul(val).add(epsilon)).rdiv(lr).mul(val);
    assertEquals(gradExpected,gradient.getGradientFor(entry.getKey()));
    count++;
  }
  assertEquals(lr,((AdaGrad)layer.layerConf().getIUpdater()).getLearningRate(),1e-4);
  assertEquals(2,count);
}
