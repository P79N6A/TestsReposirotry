@Test public void testLSTMGlobalPoolingBasicMultiLayer(){
  Nd4j.getRandom().setSeed(12345L);
  int timeSeriesLength=10;
  int nIn=5;
  int layerSize=4;
  int nOut=2;
  int[] minibatchSizes=new int[]{1,3};
  PoolingType[] poolingTypes=new PoolingType[]{PoolingType.AVG,PoolingType.SUM,PoolingType.MAX,PoolingType.PNORM};
  for (  int miniBatchSize : minibatchSizes) {
    for (    PoolingType pt : poolingTypes) {
      MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new NoOp()).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1.0)).seed(12345L).list().layer(0,new GravesLSTM.Builder().nIn(nIn).nOut(layerSize).activation(Activation.TANH).build()).layer(1,new GlobalPoolingLayer.Builder().poolingType(pt).build()).layer(2,new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(layerSize).nOut(nOut).build()).build();
      MultiLayerNetwork mln=new MultiLayerNetwork(conf);
      mln.init();
      Random r=new Random(12345L);
      INDArray input=Nd4j.zeros(miniBatchSize,nIn,timeSeriesLength);
      for (int i=0; i < miniBatchSize; i++) {
        for (int j=0; j < nIn; j++) {
          for (int k=0; k < timeSeriesLength; k++) {
            input.putScalar(new int[]{i,j,k},r.nextDouble() - 0.5);
          }
        }
      }
      INDArray labels=Nd4j.zeros(miniBatchSize,nOut);
      for (int i=0; i < miniBatchSize; i++) {
        int idx=r.nextInt(nOut);
        labels.putScalar(i,idx,1.0);
      }
      if (PRINT_RESULTS) {
        System.out.println("testLSTMGlobalPoolingBasicMultiLayer() - " + pt + ", minibatch = "+ miniBatchSize);
        for (int j=0; j < mln.getnLayers(); j++)         System.out.println("Layer " + j + " # params: "+ mln.getLayer(j).numParams());
      }
      boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
      assertTrue(gradOK);
      TestUtils.testModelSerialization(mln);
    }
  }
}
