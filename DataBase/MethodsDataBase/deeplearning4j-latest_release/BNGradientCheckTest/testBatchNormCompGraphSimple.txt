@Test public void testBatchNormCompGraphSimple(){
  int numClasses=2;
  int height=3;
  int width=3;
  int channels=1;
  long seed=123;
  int minibatchSize=3;
  ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(seed).updater(new NoOp()).weightInit(WeightInit.XAVIER).graphBuilder().addInputs("in").setInputTypes(InputType.convolutional(height,width,channels)).addLayer("bn",new BatchNormalization.Builder().build(),"in").addLayer("out",new OutputLayer.Builder().lossFunction(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nOut(numClasses).build(),"bn").setOutputs("out").build();
  ComputationGraph net=new ComputationGraph(conf);
  net.init();
  Random r=new Random(12345);
  INDArray input=Nd4j.rand(new int[]{minibatchSize,channels,height,width});
  INDArray labels=Nd4j.zeros(minibatchSize,numClasses);
  for (int i=0; i < minibatchSize; i++) {
    labels.putScalar(new int[]{i,r.nextInt(numClasses)},1.0);
  }
  Set<String> excludeParams=new HashSet<>(Arrays.asList("bn_mean","bn_var"));
  boolean gradOK=GradientCheckUtil.checkGradients(net,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,new INDArray[]{input},new INDArray[]{labels},null,null,excludeParams);
  assertTrue(gradOK);
  TestUtils.testModelSerialization(net);
}
