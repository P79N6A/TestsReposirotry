@Test public void testGradientBNWithCNNandSubsamplingCompGraph(){
  Activation[] activFns={Activation.SIGMOID,Activation.TANH,Activation.IDENTITY};
  boolean[] characteristic={false,true};
  LossFunctions.LossFunction[] lossFunctions={LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD,LossFunctions.LossFunction.MSE};
  Activation[] outputActivations={Activation.SOFTMAX,Activation.TANH};
  double[] l2vals={0.0,0.1,0.1};
  double[] l1vals={0.0,0.0,0.2};
  Nd4j.getRandom().setSeed(12345);
  int minibatch=10;
  int depth=2;
  int hw=5;
  int nOut=3;
  INDArray input=Nd4j.rand(new int[]{minibatch,depth,hw,hw});
  INDArray labels=Nd4j.zeros(minibatch,nOut);
  Random r=new Random(12345);
  for (int i=0; i < minibatch; i++) {
    labels.putScalar(i,r.nextInt(nOut),1.0);
  }
  DataSet ds=new DataSet(input,labels);
  for (  Activation afn : activFns) {
    for (    boolean doLearningFirst : characteristic) {
      for (int i=0; i < lossFunctions.length; i++) {
        for (int j=0; j < l2vals.length; j++) {
          LossFunctions.LossFunction lf=lossFunctions[i];
          Activation outputActivation=outputActivations[i];
          ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.LINE_GRADIENT_DESCENT).updater(new NoOp()).weightInit(WeightInit.DISTRIBUTION).dist(new UniformDistribution(-2,2)).seed(12345L).graphBuilder().addInputs("in").addLayer("0",new ConvolutionLayer.Builder(2,2).stride(1,1).nOut(3).activation(afn).build(),"in").addLayer("1",new BatchNormalization.Builder().build(),"0").addLayer("2",new SubsamplingLayer.Builder(SubsamplingLayer.PoolingType.MAX).kernelSize(2,2).stride(1,1).build(),"1").addLayer("3",new BatchNormalization(),"2").addLayer("4",new ActivationLayer.Builder().activation(afn).build(),"3").addLayer("5",new OutputLayer.Builder(lf).activation(outputActivation).nOut(nOut).build(),"4").setOutputs("5").setInputTypes(InputType.convolutional(hw,hw,depth)).build();
          ComputationGraph net=new ComputationGraph(conf);
          net.init();
          String name=new Object(){
          }
.getClass().getEnclosingMethod().getName();
          if (doLearningFirst) {
            net.setInput(0,ds.getFeatures());
            net.setLabels(ds.getLabels());
            net.computeGradientAndScore();
            double scoreBefore=net.score();
            for (int k=0; k < 20; k++)             net.fit(ds);
            net.computeGradientAndScore();
            double scoreAfter=net.score();
            String msg=name + " - score did not (sufficiently) decrease during learning - activationFn=" + afn+ ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst= "+ doLearningFirst+ " (before="+ scoreBefore+ ", scoreAfter="+ scoreAfter+ ")";
            assertTrue(msg,scoreAfter < 0.9 * scoreBefore);
          }
          if (PRINT_RESULTS) {
            System.out.println(name + " - activationFn=" + afn+ ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst+ ", l1="+ l1vals[j]+ ", l2="+ l2vals[j]);
            for (int k=0; k < net.getNumLayers(); k++)             System.out.println("Layer " + k + " # params: "+ net.getLayer(k).numParams());
          }
          Set<String> excludeParams=new HashSet<>(Arrays.asList("1_mean","1_var","3_mean","3_var"));
          boolean gradOK=GradientCheckUtil.checkGradients(net,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,new INDArray[]{input},new INDArray[]{labels},null,null,excludeParams);
          assertTrue(gradOK);
          TestUtils.testModelSerialization(net);
        }
      }
    }
  }
}
