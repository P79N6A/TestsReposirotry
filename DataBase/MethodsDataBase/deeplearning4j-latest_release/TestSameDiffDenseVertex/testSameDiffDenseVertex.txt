@Test public void testSameDiffDenseVertex(){
  int nIn=3;
  int nOut=4;
  for (  boolean workspaces : new boolean[]{false,true}) {
    for (    int minibatch : new int[]{5,1}) {
      Activation[] afns=new Activation[]{Activation.TANH,Activation.SIGMOID};
      for (      Activation a : afns) {
        log.info("Starting test - " + a + " - minibatch "+ minibatch+ ", workspaces: "+ workspaces);
        ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().trainingWorkspaceMode(workspaces ? WorkspaceMode.ENABLED : WorkspaceMode.NONE).inferenceWorkspaceMode(workspaces ? WorkspaceMode.ENABLED : WorkspaceMode.NONE).updater(new Sgd(0.0)).graphBuilder().addInputs("in").addVertex("0",new SameDiffDenseVertex(nIn,nOut,a,WeightInit.XAVIER),"in").addVertex("1",new SameDiffDenseVertex(nOut,nOut,a,WeightInit.XAVIER),"0").layer("2",new OutputLayer.Builder().nIn(nOut).nOut(nOut).activation(Activation.SOFTMAX).lossFunction(LossFunctions.LossFunction.MCXENT).build(),"1").setOutputs("2").build();
        ComputationGraph netSD=new ComputationGraph(conf);
        netSD.init();
        ComputationGraphConfiguration conf2=new NeuralNetConfiguration.Builder().trainingWorkspaceMode(workspaces ? WorkspaceMode.ENABLED : WorkspaceMode.NONE).inferenceWorkspaceMode(workspaces ? WorkspaceMode.ENABLED : WorkspaceMode.NONE).updater(new Sgd(0.0)).graphBuilder().addInputs("in").addLayer("0",new DenseLayer.Builder().nIn(nIn).nOut(nOut).activation(a).build(),"in").addLayer("1",new DenseLayer.Builder().nIn(nOut).nOut(nOut).activation(a).build(),"0").layer("2",new OutputLayer.Builder().nIn(nOut).nOut(nOut).activation(Activation.SOFTMAX).lossFunction(LossFunctions.LossFunction.MCXENT).build(),"1").setOutputs("2").build();
        ComputationGraph netStandard=new ComputationGraph(conf2);
        netStandard.init();
        netSD.params().assign(netStandard.params());
        assertEquals(netStandard.params(),netSD.params());
        assertEquals(netStandard.paramTable(),netSD.paramTable());
        INDArray in=Nd4j.rand(minibatch,nIn);
        INDArray l=TestUtils.randomOneHot(minibatch,nOut,12345);
        INDArray outSD=netSD.outputSingle(in);
        INDArray outStd=netStandard.outputSingle(in);
        assertEquals(outStd,outSD);
        netSD.setInput(0,in);
        netStandard.setInput(0,in);
        netSD.setLabels(l);
        netStandard.setLabels(l);
        netSD.computeGradientAndScore();
        netStandard.computeGradientAndScore();
        Gradient gSD=netSD.gradient();
        Gradient gStd=netStandard.gradient();
        Map<String,INDArray> m1=gSD.gradientForVariable();
        Map<String,INDArray> m2=gStd.gradientForVariable();
        assertEquals(m2.keySet(),m1.keySet());
        for (        String s : m1.keySet()) {
          INDArray i1=m1.get(s);
          INDArray i2=m2.get(s);
          assertEquals(s,i2,i1);
        }
        assertEquals(gStd.gradient(),gSD.gradient());
        in=Nd4j.rand(2 * minibatch,nIn);
        l=TestUtils.randomOneHot(2 * minibatch,nOut,12345);
        netSD.setInputs(in);
        netStandard.setInputs(in);
        netSD.setLabels(l);
        netStandard.setLabels(l);
        netSD.computeGradientAndScore();
        netStandard.computeGradientAndScore();
        assertEquals(netStandard.gradient().gradient(),netSD.gradient().gradient());
        DataSet ds=new DataSet(in,l);
        for (int i=0; i < 3; i++) {
          netSD.fit(ds);
          netStandard.fit(ds);
          assertEquals(netStandard.paramTable(),netSD.paramTable());
          assertEquals(netStandard.params(),netSD.params());
          assertEquals(netStandard.getFlattenedGradients(),netSD.getFlattenedGradients());
        }
        ComputationGraph loaded=TestUtils.testModelSerialization(netSD);
        outSD=loaded.outputSingle(in);
        outStd=netStandard.outputSingle(in);
        assertEquals(outStd,outSD);
      }
    }
  }
}
