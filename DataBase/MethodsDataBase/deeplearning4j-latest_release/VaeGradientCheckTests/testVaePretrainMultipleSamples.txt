@Test public void testVaePretrainMultipleSamples(){
  int minibatch=2;
  Nd4j.getRandom().setSeed(12345);
  for (  int numSamples : new int[]{1,3}) {
    INDArray features=Nd4j.rand(minibatch,4);
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().l2(0.2).l1(0.3).updater(new NoOp()).seed(12345L).weightInit(WeightInit.XAVIER).list().layer(0,new VariationalAutoencoder.Builder().nIn(4).nOut(3).encoderLayerSizes(5,6).decoderLayerSizes(7,8).pzxActivationFunction(Activation.TANH).reconstructionDistribution(new GaussianReconstructionDistribution(Activation.TANH)).numSamples(numSamples).activation(Activation.TANH).build()).pretrain(true).backprop(false).build();
    MultiLayerNetwork mln=new MultiLayerNetwork(conf);
    mln.init();
    mln.initGradientsView();
    org.deeplearning4j.nn.api.Layer layer=mln.getLayer(0);
    String msg="testVaePretrainMultipleSamples() - numSamples = " + numSamples;
    if (PRINT_RESULTS) {
      System.out.println(msg);
      for (int j=0; j < mln.getnLayers(); j++)       System.out.println("Layer " + j + " # params: "+ mln.getLayer(j).numParams());
    }
    boolean gradOK=GradientCheckUtil.checkGradientsPretrainLayer(layer,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,features,12345);
    assertTrue(msg,gradOK);
    TestUtils.testModelSerialization(mln);
  }
}
