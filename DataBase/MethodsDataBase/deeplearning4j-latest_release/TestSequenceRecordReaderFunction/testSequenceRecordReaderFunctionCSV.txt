@Test public void testSequenceRecordReaderFunctionCSV() throws Exception {
  JavaSparkContext sc=getContext();
  File f=testDir.newFolder();
  new ClassPathResource("datavec-spark/csvsequence/").copyDirectory(f);
  String path=f.getAbsolutePath() + "/*";
  JavaPairRDD<String,PortableDataStream> origData=sc.binaryFiles(path);
  assertEquals(3,origData.count());
  SequenceRecordReaderFunction srrf=new SequenceRecordReaderFunction(new CSVSequenceRecordReader(1,","));
  JavaRDD<List<List<Writable>>> rdd=origData.map(srrf);
  List<List<List<Writable>>> listSpark=rdd.collect();
  assertEquals(3,listSpark.size());
  for (int i=0; i < 3; i++) {
    List<List<Writable>> thisSequence=listSpark.get(i);
    assertEquals(4,thisSequence.size());
    for (    List<Writable> c : thisSequence) {
      assertEquals(3,c.size());
    }
  }
  InputSplit is=new FileSplit(f,new String[]{"txt"},true);
  SequenceRecordReader srr=new CSVSequenceRecordReader(1,",");
  srr.initialize(is);
  List<List<List<Writable>>> list=new ArrayList<>(3);
  while (srr.hasNext()) {
    list.add(srr.sequenceRecord());
  }
  assertEquals(3,list.size());
  boolean[] found=new boolean[3];
  for (int i=0; i < 3; i++) {
    int foundIndex=-1;
    List<List<Writable>> collection=listSpark.get(i);
    for (int j=0; j < 3; j++) {
      if (collection.equals(list.get(j))) {
        if (foundIndex != -1)         fail();
        foundIndex=j;
        if (found[foundIndex])         fail();
        found[foundIndex]=true;
      }
    }
  }
  int count=0;
  for (  boolean b : found)   if (b)   count++;
  assertEquals(3,count);
}
