@Test public void testCompareRnnOutputRnnLoss(){
  Nd4j.getRandom().setSeed(12345);
  int timeSeriesLength=4;
  int nIn=5;
  int layerSize=6;
  int nOut=6;
  int miniBatchSize=3;
  MultiLayerConfiguration conf1=new NeuralNetConfiguration.Builder().seed(12345L).updater(new NoOp()).list().layer(new LSTM.Builder().nIn(nIn).nOut(layerSize).activation(Activation.TANH).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1.0)).updater(new NoOp()).build()).layer(new DenseLayer.Builder().nIn(layerSize).nOut(nOut).activation(Activation.IDENTITY).build()).layer(new RnnLossLayer.Builder(LossFunction.MCXENT).activation(Activation.SOFTMAX).build()).build();
  MultiLayerNetwork mln=new MultiLayerNetwork(conf1);
  mln.init();
  MultiLayerConfiguration conf2=new NeuralNetConfiguration.Builder().seed(12345L).updater(new NoOp()).list().layer(new LSTM.Builder().nIn(nIn).nOut(layerSize).activation(Activation.TANH).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1.0)).updater(new NoOp()).build()).layer(new org.deeplearning4j.nn.conf.layers.RnnOutputLayer.Builder(LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(layerSize).nOut(nOut).build()).build();
  MultiLayerNetwork mln2=new MultiLayerNetwork(conf2);
  mln2.init();
  mln2.setParams(mln.params());
  INDArray in=Nd4j.rand(new int[]{miniBatchSize,nIn,timeSeriesLength});
  INDArray out1=mln.output(in);
  INDArray out2=mln.output(in);
  assertEquals(out1,out2);
  Random r=new Random(12345);
  INDArray labels=Nd4j.create(miniBatchSize,nOut,timeSeriesLength);
  for (int i=0; i < miniBatchSize; i++) {
    for (int j=0; j < timeSeriesLength; j++) {
      labels.putScalar(i,r.nextInt(nOut),j,1.0);
    }
  }
  mln.setInput(in);
  mln.setLabels(labels);
  mln2.setInput(in);
  mln2.setLabels(labels);
  mln.computeGradientAndScore();
  mln2.computeGradientAndScore();
  assertEquals(mln.gradient().gradient(),mln2.gradient().gradient());
  assertEquals(mln.score(),mln2.score(),1e-6);
  TestUtils.testModelSerialization(mln);
}
