@Test public void testDataVecSequencePairDataSetFunctionVariableLength() throws Exception {
  File dirFeatures=testDir.newFolder();
  ClassPathResource cpr=new ClassPathResource("dl4j-spark/csvsequence/");
  cpr.copyDirectory(dirFeatures);
  File dirLabels=testDir.newFolder();
  ClassPathResource cpr2=new ClassPathResource("dl4j-spark/csvsequencelabels/");
  cpr2.copyDirectory(dirLabels);
  PathToKeyConverter pathConverter=new PathToKeyConverterNumber();
  JavaPairRDD<Text,BytesPairWritable> toWrite=DataVecSparkUtil.combineFilesForSequenceFile(sc,dirFeatures.getAbsolutePath(),dirLabels.getAbsolutePath(),pathConverter);
  Path p=testDir.newFolder("dl4j_testSeqPairFnVarLength").toPath();
  p.toFile().deleteOnExit();
  String outPath=p.toFile().getAbsolutePath() + "/out";
  new File(outPath).deleteOnExit();
  toWrite.saveAsNewAPIHadoopFile(outPath,Text.class,BytesPairWritable.class,SequenceFileOutputFormat.class);
  JavaPairRDD<Text,BytesPairWritable> fromSeq=sc.sequenceFile(outPath,Text.class,BytesPairWritable.class);
  SequenceRecordReader srr1=new CSVSequenceRecordReader(1,",");
  SequenceRecordReader srr2=new CSVSequenceRecordReader(1,",");
  PairSequenceRecordReaderBytesFunction psrbf=new PairSequenceRecordReaderBytesFunction(srr1,srr2);
  JavaRDD<Tuple2<List<List<Writable>>,List<List<Writable>>>> writables=fromSeq.map(psrbf);
  DataVecSequencePairDataSetFunction pairFn=new DataVecSequencePairDataSetFunction(4,false,DataVecSequencePairDataSetFunction.AlignmentMode.ALIGN_END);
  JavaRDD<DataSet> data=writables.map(pairFn);
  List<DataSet> sparkData=data.collect();
  String featuresPath=FilenameUtils.concat(dirFeatures.getAbsolutePath(),"csvsequence_%d.txt");
  String labelsPath=FilenameUtils.concat(dirLabels.getAbsolutePath(),"csvsequencelabelsShort_%d.txt");
  SequenceRecordReader featureReader=new CSVSequenceRecordReader(1,",");
  SequenceRecordReader labelReader=new CSVSequenceRecordReader(1,",");
  featureReader.initialize(new NumberedFileInputSplit(featuresPath,0,2));
  labelReader.initialize(new NumberedFileInputSplit(labelsPath,0,2));
  SequenceRecordReaderDataSetIterator iter=new SequenceRecordReaderDataSetIterator(featureReader,labelReader,1,4,false,SequenceRecordReaderDataSetIterator.AlignmentMode.ALIGN_END);
  List<DataSet> localData=new ArrayList<>(3);
  while (iter.hasNext())   localData.add(iter.next());
  assertEquals(3,sparkData.size());
  assertEquals(3,localData.size());
  val fShapeExp=new long[]{1,3,4};
  val lShapeExp=new long[]{1,4,4};
  for (int i=0; i < 3; i++) {
    DataSet dsSpark=sparkData.get(i);
    DataSet dsLocal=localData.get(i);
    assertNotNull(dsSpark.getLabelsMaskArray());
    INDArray fSpark=dsSpark.getFeatures();
    INDArray fLocal=dsLocal.getFeatures();
    INDArray lSpark=dsSpark.getLabels();
    INDArray lLocal=dsLocal.getLabels();
    assertArrayEquals(fShapeExp,fSpark.shape());
    assertArrayEquals(fShapeExp,fLocal.shape());
    assertArrayEquals(lShapeExp,lSpark.shape());
    assertArrayEquals(lShapeExp,lLocal.shape());
  }
  boolean[] found=new boolean[3];
  for (int i=0; i < 3; i++) {
    int foundIndex=-1;
    DataSet ds=sparkData.get(i);
    for (int j=0; j < 3; j++) {
      if (dataSetsEqual(ds,localData.get(j))) {
        if (foundIndex != -1)         fail();
        foundIndex=j;
        if (found[foundIndex])         fail();
        found[foundIndex]=true;
      }
    }
  }
  int count=0;
  for (  boolean b : found) {
    if (b) {
      count++;
    }
  }
  assertEquals(3,count);
  DataVecSequencePairDataSetFunction pairFnAlignStart=new DataVecSequencePairDataSetFunction(4,false,DataVecSequencePairDataSetFunction.AlignmentMode.ALIGN_START);
  JavaRDD<DataSet> rddDataAlignStart=writables.map(pairFnAlignStart);
  List<DataSet> sparkDataAlignStart=rddDataAlignStart.collect();
  featureReader.initialize(new NumberedFileInputSplit(featuresPath,0,2));
  labelReader.initialize(new NumberedFileInputSplit(labelsPath,0,2));
  SequenceRecordReaderDataSetIterator iterAlignStart=new SequenceRecordReaderDataSetIterator(featureReader,labelReader,1,4,false,SequenceRecordReaderDataSetIterator.AlignmentMode.ALIGN_START);
  List<DataSet> localDataAlignStart=new ArrayList<>(3);
  while (iterAlignStart.hasNext())   localDataAlignStart.add(iterAlignStart.next());
  assertEquals(3,sparkDataAlignStart.size());
  assertEquals(3,localDataAlignStart.size());
  for (int i=0; i < 3; i++) {
    DataSet dsSpark=sparkDataAlignStart.get(i);
    DataSet dsLocal=localDataAlignStart.get(i);
    assertNotNull(dsSpark.getLabelsMaskArray());
    INDArray fSpark=dsSpark.getFeatures();
    INDArray fLocal=dsLocal.getFeatures();
    INDArray lSpark=dsSpark.getLabels();
    INDArray lLocal=dsLocal.getLabels();
    assertArrayEquals(fShapeExp,fSpark.shape());
    assertArrayEquals(fShapeExp,fLocal.shape());
    assertArrayEquals(lShapeExp,lSpark.shape());
    assertArrayEquals(lShapeExp,lLocal.shape());
  }
  found=new boolean[3];
  for (int i=0; i < 3; i++) {
    int foundIndex=-1;
    DataSet ds=sparkData.get(i);
    for (int j=0; j < 3; j++) {
      if (dataSetsEqual(ds,localData.get(j))) {
        if (foundIndex != -1)         fail();
        foundIndex=j;
        if (found[foundIndex])         fail();
        found[foundIndex]=true;
      }
    }
  }
  count=0;
  for (  boolean b : found)   if (b)   count++;
  assertEquals(3,count);
}
