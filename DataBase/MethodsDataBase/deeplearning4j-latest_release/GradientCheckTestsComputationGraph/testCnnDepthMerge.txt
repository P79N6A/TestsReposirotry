@Test public void testCnnDepthMerge(){
  Nd4j.getRandom().setSeed(12345);
  ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,0.1)).updater(new NoOp()).graphBuilder().addInputs("input").addLayer("l1",new ConvolutionLayer.Builder().kernelSize(2,2).stride(1,1).padding(0,0).nIn(2).nOut(2).activation(Activation.TANH).build(),"input").addLayer("l2",new ConvolutionLayer.Builder().kernelSize(2,2).stride(1,1).padding(0,0).nIn(2).nOut(2).activation(Activation.TANH).build(),"input").addVertex("merge",new MergeVertex(),"l1","l2").addLayer("outputLayer",new OutputLayer.Builder().lossFunction(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(5 * 5 * (2 + 2)).nOut(3).build(),"merge").setOutputs("outputLayer").inputPreProcessor("outputLayer",new CnnToFeedForwardPreProcessor(5,5,4)).build();
  ComputationGraph graph=new ComputationGraph(conf);
  graph.init();
  Random r=new Random(12345);
  INDArray input=Nd4j.rand(new int[]{5,2,6,6});
  INDArray labels=Nd4j.zeros(5,3);
  for (int i=0; i < 5; i++)   labels.putScalar(new int[]{i,r.nextInt(3)},1.0);
  if (PRINT_RESULTS) {
    System.out.println("testCnnDepthMerge()");
    for (int j=0; j < graph.getNumLayers(); j++)     System.out.println("Layer " + j + " # params: "+ graph.getLayer(j).numParams());
  }
  boolean gradOK=GradientCheckUtil.checkGradients(graph,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,new INDArray[]{input},new INDArray[]{labels});
  String msg="testCnnDepthMerge()";
  assertTrue(msg,gradOK);
  TestUtils.testModelSerialization(graph);
}
