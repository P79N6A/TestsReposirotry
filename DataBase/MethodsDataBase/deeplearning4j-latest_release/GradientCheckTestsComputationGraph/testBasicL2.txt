@Test public void testBasicL2(){
  Nd4j.getRandom().setSeed(12345);
  ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).activation(Activation.TANH).updater(new NoOp()).graphBuilder().addInputs("in1","in2").addLayer("d0",new DenseLayer.Builder().nIn(2).nOut(2).build(),"in1").addLayer("d1",new DenseLayer.Builder().nIn(2).nOut(2).build(),"in2").addVertex("l2",new L2Vertex(),"d0","d1").addLayer("out",new OutputLayer.Builder().lossFunction(LossFunctions.LossFunction.L2).nIn(1).nOut(1).activation(Activation.IDENTITY).build(),"l2").setOutputs("out").build();
  ComputationGraph graph=new ComputationGraph(conf);
  graph.init();
  Nd4j.getRandom().setSeed(12345);
  long nParams=graph.numParams();
  INDArray newParams=Nd4j.rand(new long[]{1,nParams});
  graph.setParams(newParams);
  int[] mbSizes=new int[]{1,3,10};
  for (  int minibatch : mbSizes) {
    INDArray in1=Nd4j.rand(minibatch,2);
    INDArray in2=Nd4j.rand(minibatch,2);
    INDArray labels=Nd4j.rand(minibatch,1);
    String testName="testBasicL2() - minibatch = " + minibatch;
    if (PRINT_RESULTS) {
      System.out.println(testName);
      for (int j=0; j < graph.getNumLayers(); j++)       System.out.println("Layer " + j + " # params: "+ graph.getLayer(j).numParams());
    }
    boolean gradOK=GradientCheckUtil.checkGradients(graph,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,new INDArray[]{in1,in2},new INDArray[]{labels});
    assertTrue(testName,gradOK);
    TestUtils.testModelSerialization(graph);
  }
}
