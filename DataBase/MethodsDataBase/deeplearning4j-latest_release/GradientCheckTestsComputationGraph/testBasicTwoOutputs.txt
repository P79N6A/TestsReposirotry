@Test public void testBasicTwoOutputs(){
  Nd4j.getRandom().setSeed(12345);
  ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).activation(Activation.TANH).updater(new NoOp()).graphBuilder().addInputs("in1","in2").addLayer("d0",new DenseLayer.Builder().nIn(2).nOut(2).build(),"in1").addLayer("d1",new DenseLayer.Builder().nIn(2).nOut(2).build(),"in2").addLayer("out1",new OutputLayer.Builder().lossFunction(LossFunctions.LossFunction.L2).nIn(2).nOut(2).activation(Activation.IDENTITY).build(),"d0").addLayer("out2",new OutputLayer.Builder().lossFunction(LossFunctions.LossFunction.L2).nIn(2).nOut(2).activation(Activation.IDENTITY).build(),"d1").setOutputs("out1","out2").build();
  ComputationGraph graph=new ComputationGraph(conf);
  graph.init();
  System.out.println("Num layers: " + graph.getNumLayers());
  System.out.println("Num params: " + graph.numParams());
  Nd4j.getRandom().setSeed(12345);
  long nParams=graph.numParams();
  INDArray newParams=Nd4j.rand(new long[]{1,nParams});
  graph.setParams(newParams);
  int[] mbSizes=new int[]{1,3,10};
  for (  int minibatch : mbSizes) {
    INDArray in1=Nd4j.rand(minibatch,2);
    INDArray in2=Nd4j.rand(minibatch,2);
    INDArray labels1=Nd4j.rand(minibatch,2);
    INDArray labels2=Nd4j.rand(minibatch,2);
    String testName="testBasicStackUnstack() - minibatch = " + minibatch;
    if (PRINT_RESULTS) {
      System.out.println(testName);
      for (int j=0; j < graph.getNumLayers(); j++)       System.out.println("Layer " + j + " # params: "+ graph.getLayer(j).numParams());
    }
    boolean gradOK=GradientCheckUtil.checkGradients(graph,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,new INDArray[]{in1,in2},new INDArray[]{labels1,labels2});
    assertTrue(testName,gradOK);
    TestUtils.testModelSerialization(graph);
  }
}
