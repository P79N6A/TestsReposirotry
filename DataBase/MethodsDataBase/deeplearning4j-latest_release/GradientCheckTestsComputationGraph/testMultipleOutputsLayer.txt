@Test public void testMultipleOutputsLayer(){
  Nd4j.getRandom().setSeed(12345);
  ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().seed(12345).optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1)).updater(new NoOp()).activation(Activation.TANH).graphBuilder().addInputs("i0").addLayer("d0",new DenseLayer.Builder().nIn(2).nOut(2).build(),"i0").addLayer("d1",new DenseLayer.Builder().nIn(2).nOut(2).build(),"d0").addLayer("d2",new DenseLayer.Builder().nIn(2).nOut(2).build(),"d0").addLayer("d3",new DenseLayer.Builder().nIn(2).nOut(2).build(),"d0").addLayer("out",new OutputLayer.Builder().lossFunction(LossFunctions.LossFunction.MSE).nIn(6).nOut(2).build(),"d1","d2","d3").setOutputs("out").build();
  ComputationGraph graph=new ComputationGraph(conf);
  graph.init();
  int[] minibatchSizes={1,3};
  for (  int mb : minibatchSizes) {
    INDArray input=Nd4j.rand(mb,2);
    INDArray out=Nd4j.rand(mb,2);
    String msg="testMultipleOutputsLayer() - minibatchSize = " + mb;
    if (PRINT_RESULTS) {
      System.out.println(msg);
      for (int j=0; j < graph.getNumLayers(); j++)       System.out.println("Layer " + j + " # params: "+ graph.getLayer(j).numParams());
    }
    boolean gradOK=GradientCheckUtil.checkGradients(graph,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,new INDArray[]{input},new INDArray[]{out});
    assertTrue(msg,gradOK);
    TestUtils.testModelSerialization(graph);
  }
}
