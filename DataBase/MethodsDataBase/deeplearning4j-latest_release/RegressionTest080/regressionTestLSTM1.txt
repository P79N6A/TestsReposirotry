@Test public void regressionTestLSTM1() throws Exception {
  File f=new ClassPathResource("regression_testing/080/080_ModelSerializer_Regression_LSTM_1.zip").getTempFileFromArchive();
  MultiLayerNetwork net=ModelSerializer.restoreMultiLayerNetwork(f,true);
  MultiLayerConfiguration conf=net.getLayerWiseConfigurations();
  assertEquals(3,conf.getConfs().size());
  assertTrue(conf.isBackprop());
  assertFalse(conf.isPretrain());
  GravesLSTM l0=(GravesLSTM)conf.getConf(0).getLayer();
  assertTrue(l0.getActivationFn() instanceof ActivationTanH);
  assertEquals(3,l0.getNIn());
  assertEquals(4,l0.getNOut());
  assertEquals(GradientNormalization.ClipElementWiseAbsoluteValue,l0.getGradientNormalization());
  assertEquals(1.5,l0.getGradientNormalizationThreshold(),1e-5);
  GravesBidirectionalLSTM l1=(GravesBidirectionalLSTM)conf.getConf(1).getLayer();
  assertTrue(l1.getActivationFn() instanceof ActivationSoftSign);
  assertEquals(4,l1.getNIn());
  assertEquals(4,l1.getNOut());
  assertEquals(GradientNormalization.ClipElementWiseAbsoluteValue,l1.getGradientNormalization());
  assertEquals(1.5,l1.getGradientNormalizationThreshold(),1e-5);
  RnnOutputLayer l2=(RnnOutputLayer)conf.getConf(2).getLayer();
  assertEquals(4,l2.getNIn());
  assertEquals(5,l2.getNOut());
  assertTrue(l2.getActivationFn() instanceof ActivationSoftmax);
  assertTrue(l2.getLossFn() instanceof LossMCXENT);
  assertTrue(net.getLayerWiseConfigurations().isLegacyBatchScaledL2());
  assertTrue(l2.isLegacyBatchScaledL2());
}
