@Test public void regressionTestMLP2() throws Exception {
  File f=new ClassPathResource("regression_testing/080/080_ModelSerializer_Regression_MLP_2.zip").getTempFileFromArchive();
  MultiLayerNetwork net=ModelSerializer.restoreMultiLayerNetwork(f,true);
  MultiLayerConfiguration conf=net.getLayerWiseConfigurations();
  assertEquals(2,conf.getConfs().size());
  assertTrue(conf.isBackprop());
  assertFalse(conf.isPretrain());
  DenseLayer l0=(DenseLayer)conf.getConf(0).getLayer();
  assertTrue(l0.getActivationFn() instanceof ActivationLReLU);
  assertEquals(3,l0.getNIn());
  assertEquals(4,l0.getNOut());
  assertEquals(WeightInit.DISTRIBUTION,l0.getWeightInit());
  assertEquals(new NormalDistribution(0.1,1.2),l0.getDist());
  assertTrue(l0.getIUpdater() instanceof RmsProp);
  RmsProp r=(RmsProp)l0.getIUpdater();
  assertEquals(0.96,r.getRmsDecay(),1e-6);
  assertEquals(0.15,r.getLearningRate(),1e-6);
  assertEquals(0.15,((RmsProp)l0.getIUpdater()).getLearningRate(),1e-6);
  assertEquals(new Dropout(0.6),l0.getIDropout());
  assertEquals(0.1,l0.getL1(),1e-6);
  assertEquals(0.2,l0.getL2(),1e-6);
  assertEquals(GradientNormalization.ClipElementWiseAbsoluteValue,l0.getGradientNormalization());
  assertEquals(1.5,l0.getGradientNormalizationThreshold(),1e-5);
  OutputLayer l1=(OutputLayer)conf.getConf(1).getLayer();
  assertTrue(l1.getActivationFn() instanceof ActivationIdentity);
  assertTrue(l1.getLossFn() instanceof LossMSE);
  assertEquals(4,l1.getNIn());
  assertEquals(5,l1.getNOut());
  assertEquals(WeightInit.DISTRIBUTION,l1.getWeightInit());
  assertEquals(new NormalDistribution(0.1,1.2),l1.getDist());
  assertTrue(l1.getIUpdater() instanceof RmsProp);
  r=(RmsProp)l1.getIUpdater();
  assertEquals(0.96,r.getRmsDecay(),1e-6);
  assertEquals(0.15,r.getLearningRate(),1e-6);
  assertEquals(0.15,((RmsProp)l0.getIUpdater()).getLearningRate(),1e-6);
  assertEquals(new Dropout(0.6),l1.getIDropout());
  assertEquals(0.1,l1.getL1(),1e-6);
  assertEquals(0.2,l1.getL2(),1e-6);
  assertEquals(GradientNormalization.ClipElementWiseAbsoluteValue,l1.getGradientNormalization());
  assertEquals(1.5,l1.getGradientNormalizationThreshold(),1e-5);
  int numParams=(int)net.numParams();
  assertEquals(Nd4j.linspace(1,numParams,numParams),net.params());
  int updaterSize=(int)new RmsProp().stateSize(numParams);
  assertEquals(Nd4j.linspace(1,updaterSize,updaterSize),net.getUpdater().getStateViewArray());
  assertTrue(net.getLayerWiseConfigurations().isLegacyBatchScaledL2());
  assertTrue(l1.isLegacyBatchScaledL2());
}
