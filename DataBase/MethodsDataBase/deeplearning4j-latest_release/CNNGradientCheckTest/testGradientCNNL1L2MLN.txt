@Test public void testGradientCNNL1L2MLN(){
  Activation[] activFns={Activation.SIGMOID,Activation.TANH};
  boolean[] characteristic={false,true};
  LossFunctions.LossFunction[] lossFunctions={LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD,LossFunctions.LossFunction.MSE};
  Activation[] outputActivations={Activation.SOFTMAX,Activation.TANH};
  DataSet ds=new IrisDataSetIterator(150,150).next();
  ds.normalizeZeroMeanZeroUnitVariance();
  INDArray input=ds.getFeatures();
  INDArray labels=ds.getLabels();
  double[] l2vals={0.4,0.0,0.4,0.4};
  double[] l1vals={0.0,0.0,0.5,0.0};
  double[] biasL2={0.0,0.0,0.0,0.2};
  double[] biasL1={0.0,0.0,0.6,0.0};
  for (  Activation afn : activFns) {
    for (    boolean doLearningFirst : characteristic) {
      for (int i=0; i < lossFunctions.length; i++) {
        for (int k=0; k < l2vals.length; k++) {
          LossFunctions.LossFunction lf=lossFunctions[i];
          Activation outputActivation=outputActivations[i];
          double l2=l2vals[k];
          double l1=l1vals[k];
          MultiLayerConfiguration.Builder builder=new NeuralNetConfiguration.Builder().l2(l2).l1(l1).l2Bias(biasL2[k]).l1Bias(biasL1[k]).optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT).seed(12345L).list().layer(0,new ConvolutionLayer.Builder(new int[]{1,1}).nIn(1).nOut(6).cudnnAllowFallback(false).weightInit(WeightInit.XAVIER).activation(afn).updater(new NoOp()).build()).layer(1,new OutputLayer.Builder(lf).activation(outputActivation).nOut(3).weightInit(WeightInit.XAVIER).updater(new NoOp()).build()).setInputType(InputType.convolutionalFlat(1,4,1));
          MultiLayerConfiguration conf=builder.build();
          MultiLayerNetwork mln=new MultiLayerNetwork(conf);
          mln.init();
          String testName=new Object(){
          }
.getClass().getEnclosingMethod().getName();
          if (doLearningFirst) {
            mln.setInput(ds.getFeatures());
            mln.setLabels(ds.getLabels());
            mln.computeGradientAndScore();
            double scoreBefore=mln.score();
            for (int j=0; j < 10; j++)             mln.fit(ds);
            mln.computeGradientAndScore();
            double scoreAfter=mln.score();
            String msg=testName + "- score did not (sufficiently) decrease during learning - activationFn=" + afn+ ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst+ " (before="+ scoreBefore+ ", scoreAfter="+ scoreAfter+ ")";
            assertTrue(msg,scoreAfter < 0.8 * scoreBefore);
          }
          if (PRINT_RESULTS) {
            System.out.println(testName + "- activationFn=" + afn+ ", lossFn="+ lf+ ", outputActivation="+ outputActivation+ ", doLearningFirst="+ doLearningFirst);
            for (int j=0; j < mln.getnLayers(); j++)             System.out.println("Layer " + j + " # params: "+ mln.getLayer(j).numParams());
          }
          boolean gradOK=GradientCheckUtil.checkGradients(mln,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,input,labels);
          assertTrue(gradOK);
          TestUtils.testModelSerialization(mln);
        }
      }
    }
  }
}
