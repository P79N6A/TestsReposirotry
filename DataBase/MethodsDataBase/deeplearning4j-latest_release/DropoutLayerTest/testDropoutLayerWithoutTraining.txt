@Test public void testDropoutLayerWithoutTraining() throws Exception {
  MultiLayerConfiguration confIntegrated=new NeuralNetConfiguration.Builder().seed(3648).list().layer(0,new ConvolutionLayer.Builder(1,1).stride(1,1).nIn(1).nOut(1).dropOut(0.25).activation(Activation.IDENTITY).weightInit(WeightInit.XAVIER).build()).layer(1,new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).weightInit(WeightInit.XAVIER).dropOut(0.25).nOut(4).build()).setInputType(InputType.convolutionalFlat(2,2,1)).build();
  MultiLayerNetwork netIntegrated=new MultiLayerNetwork(confIntegrated);
  netIntegrated.init();
  netIntegrated.getLayer(0).setParam("W",Nd4j.eye(1));
  netIntegrated.getLayer(0).setParam("b",Nd4j.zeros(1,1));
  netIntegrated.getLayer(1).setParam("W",Nd4j.eye(4));
  netIntegrated.getLayer(1).setParam("b",Nd4j.zeros(4,1));
  MultiLayerConfiguration confSeparate=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).seed(3648).list().layer(0,new DropoutLayer.Builder(0.25).build()).layer(1,new ConvolutionLayer.Builder(1,1).stride(1,1).nIn(1).nOut(1).activation(Activation.IDENTITY).weightInit(WeightInit.XAVIER).build()).layer(2,new DropoutLayer.Builder(0.25).build()).layer(3,new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).weightInit(WeightInit.XAVIER).activation(Activation.SOFTMAX).nOut(4).build()).setInputType(InputType.convolutionalFlat(2,2,1)).build();
  MultiLayerNetwork netSeparate=new MultiLayerNetwork(confSeparate);
  netSeparate.init();
  netSeparate.getLayer(1).setParam("W",Nd4j.eye(1));
  netSeparate.getLayer(1).setParam("b",Nd4j.zeros(1,1));
  netSeparate.getLayer(3).setParam("W",Nd4j.eye(4));
  netSeparate.getLayer(3).setParam("b",Nd4j.zeros(4,1));
  for (  Layer l : netIntegrated.getLayers()) {
    l.allowInputModification(false);
  }
  for (  Layer l : netSeparate.getLayers()) {
    l.allowInputModification(false);
  }
  INDArray in=Nd4j.arange(1,5);
  Nd4j.getRandom().setSeed(12345);
  List<INDArray> actTrainIntegrated=netIntegrated.feedForward(in.dup(),true);
  Nd4j.getRandom().setSeed(12345);
  List<INDArray> actTrainSeparate=netSeparate.feedForward(in.dup(),true);
  Nd4j.getRandom().setSeed(12345);
  List<INDArray> actTestIntegrated=netIntegrated.feedForward(in.dup(),false);
  Nd4j.getRandom().setSeed(12345);
  List<INDArray> actTestSeparate=netSeparate.feedForward(in.dup(),false);
  INDArray maskIntegrated=((Dropout)netIntegrated.getLayer(0).conf().getLayer().getIDropout()).getMask();
  INDArray maskSeparate=((Dropout)netSeparate.getLayer(0).conf().getLayer().getIDropout()).getMask();
  assertEquals(maskIntegrated,maskSeparate);
  assertEquals(actTrainIntegrated.get(1),actTrainSeparate.get(2));
  assertEquals(actTrainIntegrated.get(2),actTrainSeparate.get(4));
  assertEquals(actTestIntegrated.get(1),actTestSeparate.get(2));
  assertEquals(actTestIntegrated.get(2),actTestSeparate.get(4));
}
