@Test public void testMSEOutputLayer(){
  Nd4j.getRandom().setSeed(12345);
  for (  Activation a : new Activation[]{Activation.IDENTITY,Activation.TANH,Activation.SOFTMAX}) {
    log.info("Starting test: " + a);
    MultiLayerConfiguration confSD=new NeuralNetConfiguration.Builder().seed(12345).updater(new Adam(0.01)).list().layer(new DenseLayer.Builder().nIn(5).nOut(5).activation(Activation.TANH).build()).layer(new SameDiffMSEOutputLayer(5,5,a,WeightInit.XAVIER)).build();
    MultiLayerConfiguration confStd=new NeuralNetConfiguration.Builder().seed(12345).updater(new Adam(0.01)).list().layer(new DenseLayer.Builder().nIn(5).nOut(5).activation(Activation.TANH).build()).layer(new OutputLayer.Builder().nIn(5).nOut(5).activation(a).lossFunction(LossFunctions.LossFunction.MSE).build()).build();
    MultiLayerNetwork netSD=new MultiLayerNetwork(confSD);
    netSD.init();
    MultiLayerNetwork netStd=new MultiLayerNetwork(confStd);
    netStd.init();
    netSD.params().assign(netStd.params());
    assertEquals(netStd.paramTable(),netSD.paramTable());
    int minibatch=2;
    INDArray in=Nd4j.rand(minibatch,5);
    INDArray label=Nd4j.rand(minibatch,5);
    INDArray outSD=netSD.output(in);
    INDArray outStd=netStd.output(in);
    assertEquals(outStd,outSD);
    DataSet ds=new DataSet(in,label);
    double scoreSD=netSD.score(ds);
    double scoreStd=netStd.score(ds);
    assertEquals(scoreStd,scoreSD,1e-6);
    netSD.setInput(in);
    netSD.setLabels(label);
    netStd.setInput(in);
    netStd.setLabels(label);
    netSD.computeGradientAndScore();
    netStd.computeGradientAndScore();
    assertEquals(netStd.getFlattenedGradients(),netSD.getFlattenedGradients());
    for (int i=0; i < 3; i++) {
      netSD.fit(ds);
      netStd.fit(ds);
      String s=String.valueOf(i);
      assertEquals(s,netStd.params(),netSD.params());
      assertEquals(s,netStd.getFlattenedGradients(),netSD.getFlattenedGradients());
    }
  }
}
