@Test public void cloneMLNFrozen(){
  DataSet randomData=new DataSet(Nd4j.rand(10,4),Nd4j.rand(10,3));
  NeuralNetConfiguration.Builder overallConf=new NeuralNetConfiguration.Builder().updater(new Sgd(0.1)).activation(Activation.IDENTITY);
  MultiLayerNetwork modelToFineTune=new MultiLayerNetwork(overallConf.list().layer(0,new DenseLayer.Builder().nIn(4).nOut(3).build()).layer(1,new DenseLayer.Builder().nIn(3).nOut(2).build()).layer(2,new DenseLayer.Builder().nIn(2).nOut(3).build()).layer(3,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(3).nOut(3).build()).build());
  modelToFineTune.init();
  INDArray asFrozenFeatures=modelToFineTune.feedForwardToLayer(2,randomData.getFeatures(),false).get(2);
  MultiLayerNetwork modelNow=new TransferLearning.Builder(modelToFineTune).setFeatureExtractor(1).build();
  MultiLayerNetwork clonedModel=modelNow.clone();
  assertEquals(modelNow.getLayerWiseConfigurations().toJson(),clonedModel.getLayerWiseConfigurations().toJson());
  assertEquals(modelNow.params(),clonedModel.params());
  MultiLayerNetwork notFrozen=new MultiLayerNetwork(overallConf.list().layer(0,new DenseLayer.Builder().nIn(2).nOut(3).build()).layer(1,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(3).nOut(3).build()).build(),Nd4j.hstack(modelToFineTune.getLayer(2).params(),modelToFineTune.getLayer(3).params()));
  int i=0;
  while (i < 5) {
    notFrozen.fit(new DataSet(asFrozenFeatures,randomData.getLabels()));
    modelNow.fit(randomData);
    clonedModel.fit(randomData);
    i++;
  }
  INDArray expectedParams=Nd4j.hstack(modelToFineTune.getLayer(0).params(),modelToFineTune.getLayer(1).params(),notFrozen.params());
  assertEquals(expectedParams,modelNow.params());
  assertEquals(expectedParams,clonedModel.params());
}
