@Test public void testFrozen(){
  DataSet randomData=new DataSet(Nd4j.rand(10,4),Nd4j.rand(10,3));
  NeuralNetConfiguration.Builder overallConf=new NeuralNetConfiguration.Builder().updater(new Sgd(0.1)).activation(Activation.IDENTITY);
  FineTuneConfiguration finetune=new FineTuneConfiguration.Builder().updater(new Sgd(0.1)).build();
  MultiLayerNetwork modelToFineTune=new MultiLayerNetwork(overallConf.clone().list().layer(0,new DenseLayer.Builder().nIn(4).nOut(3).build()).layer(1,new DenseLayer.Builder().nIn(3).nOut(2).build()).layer(2,new DenseLayer.Builder().nIn(2).nOut(3).build()).layer(3,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(3).nOut(3).build()).build());
  modelToFineTune.init();
  List<INDArray> ff=modelToFineTune.feedForwardToLayer(2,randomData.getFeatures(),false);
  INDArray asFrozenFeatures=ff.get(2);
  MultiLayerNetwork modelNow=new TransferLearning.Builder(modelToFineTune).fineTuneConfiguration(finetune).setFeatureExtractor(1).build();
  INDArray paramsLastTwoLayers=Nd4j.hstack(modelToFineTune.getLayer(2).params(),modelToFineTune.getLayer(3).params());
  MultiLayerNetwork notFrozen=new MultiLayerNetwork(overallConf.clone().list().layer(0,new DenseLayer.Builder().nIn(2).nOut(3).build()).layer(1,new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(3).nOut(3).build()).build(),paramsLastTwoLayers);
  INDArray outNow=modelNow.output(randomData.getFeatures());
  INDArray outNotFrozen=notFrozen.output(asFrozenFeatures);
  assertEquals(outNow,outNotFrozen);
  for (int i=0; i < 5; i++) {
    notFrozen.fit(new DataSet(asFrozenFeatures,randomData.getLabels()));
    modelNow.fit(randomData);
  }
  INDArray expected=Nd4j.hstack(modelToFineTune.getLayer(0).params(),modelToFineTune.getLayer(1).params(),notFrozen.params());
  INDArray act=modelNow.params();
  assertEquals(expected,act);
}
