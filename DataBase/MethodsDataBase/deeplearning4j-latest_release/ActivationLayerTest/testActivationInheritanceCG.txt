@Test public void testActivationInheritanceCG(){
  ComputationGraphConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).seed(123).weightInit(WeightInit.XAVIER).activation(Activation.RATIONALTANH).graphBuilder().addInputs("in").addLayer("0",new DenseLayer.Builder().nIn(10).nOut(10).build(),"in").addLayer("1",new ActivationLayer(),"0").addLayer("2",new ActivationLayer.Builder().build(),"1").addLayer("3",new ActivationLayer.Builder().activation(Activation.ELU).build(),"2").addLayer("4",new org.deeplearning4j.nn.conf.layers.OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(10).nOut(10).build(),"3").setOutputs("4").build();
  ComputationGraph network=new ComputationGraph(conf);
  network.init();
  assertNotNull(((ActivationLayer)network.getLayer("1").conf().getLayer()).getActivationFn());
  assertTrue(((DenseLayer)network.getLayer("0").conf().getLayer()).getActivationFn() instanceof ActivationRationalTanh);
  assertTrue(((ActivationLayer)network.getLayer("1").conf().getLayer()).getActivationFn() instanceof ActivationRationalTanh);
  assertTrue(((ActivationLayer)network.getLayer("2").conf().getLayer()).getActivationFn() instanceof ActivationRationalTanh);
  assertTrue(((ActivationLayer)network.getLayer("3").conf().getLayer()).getActivationFn() instanceof ActivationELU);
  assertTrue(((OutputLayer)network.getLayer("4").conf().getLayer()).getActivationFn() instanceof ActivationSoftmax);
}
