@Test public void gradientCheckReconstructionDistributions(){
  double eps=1e-6;
  double maxRelError=1e-6;
  double minAbsoluteError=1e-9;
  Nd4j.getRandom().setSeed(12345);
  int inputSize=4;
  int[] mbs=new int[]{1,3};
  Random r=new Random(12345);
  ReconstructionDistribution[] distributions=new ReconstructionDistribution[]{new GaussianReconstructionDistribution(Activation.IDENTITY),new GaussianReconstructionDistribution(Activation.TANH),new BernoulliReconstructionDistribution(Activation.SIGMOID),new ExponentialReconstructionDistribution(Activation.IDENTITY),new ExponentialReconstructionDistribution(Activation.TANH)};
  List<String> passes=new ArrayList<>();
  List<String> failures=new ArrayList<>();
  for (  ReconstructionDistribution rd : distributions) {
    for (    int minibatch : mbs) {
      INDArray x;
      INDArray distributionParams;
      if (rd instanceof GaussianReconstructionDistribution) {
        distributionParams=Nd4j.rand(minibatch,inputSize * 2).muli(2).subi(1);
        x=Nd4j.rand(minibatch,inputSize);
      }
 else       if (rd instanceof BernoulliReconstructionDistribution) {
        distributionParams=Nd4j.rand(minibatch,inputSize).muli(2).subi(1);
        x=Nd4j.zeros(minibatch,inputSize);
        for (int i=0; i < minibatch; i++) {
          for (int j=0; j < inputSize; j++) {
            x.putScalar(i,j,r.nextInt(2));
          }
        }
      }
 else       if (rd instanceof ExponentialReconstructionDistribution) {
        distributionParams=Nd4j.rand(minibatch,inputSize).muli(2).subi(1);
        x=Nd4j.rand(minibatch,inputSize);
      }
 else {
        throw new RuntimeException();
      }
      INDArray gradient=rd.gradient(x,distributionParams);
      String testName="minibatch = " + minibatch + ", size = "+ inputSize+ ", Distribution = "+ rd;
      System.out.println("\n\n***** Starting test: " + testName + "*****");
      int totalFailureCount=0;
      for (int i=0; i < distributionParams.size(1); i++) {
        for (int j=0; j < distributionParams.size(0); j++) {
          double initial=distributionParams.getDouble(j,i);
          distributionParams.putScalar(j,i,initial + eps);
          double scorePlus=rd.negLogProbability(x,distributionParams,false);
          distributionParams.putScalar(j,i,initial - eps);
          double scoreMinus=rd.negLogProbability(x,distributionParams,false);
          distributionParams.putScalar(j,i,initial);
          double numericalGrad=(scorePlus - scoreMinus) / (2.0 * eps);
          double backpropGrad=gradient.getDouble(j,i);
          double relError=Math.abs(numericalGrad - backpropGrad) / (Math.abs(numericalGrad) + Math.abs(backpropGrad));
          double absError=Math.abs(backpropGrad - numericalGrad);
          if (relError > maxRelError || Double.isNaN(relError)) {
            if (absError < minAbsoluteError) {
              log.info("Input (" + j + ","+ i+ ") passed: grad= "+ backpropGrad+ ", numericalGrad= "+ numericalGrad+ ", relError= "+ relError+ "; absolute error = "+ absError+ " < minAbsoluteError = "+ minAbsoluteError);
            }
 else {
              log.info("Input (" + j + ","+ i+ ") FAILED: grad= "+ backpropGrad+ ", numericalGrad= "+ numericalGrad+ ", relError= "+ relError+ ", scorePlus="+ scorePlus+ ", scoreMinus= "+ scoreMinus);
              totalFailureCount++;
            }
          }
 else {
            log.info("Input (" + j + ","+ i+ ") passed: grad= "+ backpropGrad+ ", numericalGrad= "+ numericalGrad+ ", relError= "+ relError);
          }
        }
      }
      if (totalFailureCount > 0) {
        failures.add(testName);
      }
 else {
        passes.add(testName);
      }
    }
  }
  System.out.println("\n\n\n +++++ Test Passes +++++");
  for (  String s : passes) {
    System.out.println(s);
  }
  System.out.println("\n\n\n +++++ Test Faliures +++++");
  for (  String s : failures) {
    System.out.println(s);
  }
  assertEquals(0,failures.size());
}
