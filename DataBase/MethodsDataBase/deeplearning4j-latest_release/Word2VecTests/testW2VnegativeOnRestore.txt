@Test public void testW2VnegativeOnRestore() throws Exception {
  SentenceIterator iter=new BasicLineIterator(inputFile.getAbsolutePath());
  TokenizerFactory t=new DefaultTokenizerFactory();
  t.setTokenPreProcessor(new CommonPreprocessor());
  Word2Vec vec=new Word2Vec.Builder().minWordFrequency(1).iterations(3).batchSize(64).layerSize(100).stopWords(new ArrayList<String>()).seed(42).learningRate(0.025).minLearningRate(0.001).sampling(0).elementsLearningAlgorithm(new SkipGram<VocabWord>()).negativeSample(10).epochs(1).windowSize(5).useHierarchicSoftmax(false).allowParallelTokenization(true).modelUtils(new FlatModelUtils<VocabWord>()).iterate(iter).tokenizerFactory(t).build();
  assertEquals(false,vec.getConfiguration().isUseHierarchicSoftmax());
  log.info("Fit 1");
  vec.fit();
  File tmpFile=File.createTempFile("temp","file");
  tmpFile.deleteOnExit();
  WordVectorSerializer.writeWord2VecModel(vec,tmpFile);
  iter.reset();
  Word2Vec restoredVec=WordVectorSerializer.readWord2VecModel(tmpFile,true);
  restoredVec.setTokenizerFactory(t);
  restoredVec.setSentenceIterator(iter);
  assertEquals(false,restoredVec.getConfiguration().isUseHierarchicSoftmax());
  assertTrue(restoredVec.getModelUtils() instanceof FlatModelUtils);
  assertTrue(restoredVec.getConfiguration().isAllowParallelTokenization());
  log.info("Fit 2");
  restoredVec.fit();
  iter.reset();
  restoredVec=WordVectorSerializer.readWord2VecModel(tmpFile,false);
  restoredVec.setTokenizerFactory(t);
  restoredVec.setSentenceIterator(iter);
  assertEquals(false,restoredVec.getConfiguration().isUseHierarchicSoftmax());
  assertTrue(restoredVec.getModelUtils() instanceof BasicModelUtils);
  log.info("Fit 3");
  restoredVec.fit();
}
