@Test public void testLogisticRegression(){
  Map<String,INDArray> vars=this.variablesForInput();
  SameDiff outside=SameDiff.create();
  outside.defineFunction("activate",new SameDiffFunctionDefinition(){
    @Override public SDVariable[] define(    SameDiff sameDiff,    Map<String,INDArray> inputs,    SDVariable[] variableInputs){
      sameDiff.enableDebugMode();
      SDVariable x=sameDiff.var("x",inputs.get("x"));
      SDVariable w=sameDiff.var("w",inputs.get("w"));
      SDVariable y=sameDiff.var("y",inputs.get("y"));
      SDVariable activation=sameDiff.sigmoid("activation",sameDiff.mmul("mmul",x,w));
      SDVariable oneMinusY=y.rsub("oneminusy",1.0);
      SDVariable oneMinusPredictions=activation.rsub("oneminusactivations",1.0);
      SDVariable outputTimesY=y.mul("output * y",activation);
      SDVariable yHat=oneMinusPredictions.mul("yhat",oneMinusY);
      SDVariable probs=outputTimesY.add("probs",yHat);
      SDVariable logProbs=sameDiff.log("logprob",probs);
      SDVariable ret=sameDiff.sum("totalsum",logProbs,Integer.MAX_VALUE);
      SDVariable ret2=sameDiff.neg("negtotalsum",ret);
      return new SDVariable[]{ret2};
    }
  }
,vars);
  Pair<Map<SDVariable,DifferentialFunction>,List<DifferentialFunction>> opsBackward=outside.getFunction("activate").execBackwards();
  SameDiff gradSameDiff=outside.getFunction("activate").getFunction("grad");
  SDVariable gradWrtX=outside.getFunction("activate").grad("x");
  SDVariable gradWrtW=outside.getFunction("activate").grad("w");
  assertNotNull(gradWrtX);
  assertNotNull(gradWrtW);
  INDArray wGradAssertion=Nd4j.create(new double[]{-0.81,1.255,-1.80499983}).reshape(3,1);
  INDArray inputAssertion=Nd4j.valueArrayOf(vars.get("x").shape(),1e-1);
  INDArray yGradAssertion=Nd4j.zeros(vars.get("y").shape());
  INDArray mmulGrad=Nd4j.create(new double[]{-0.5,-0.5,0.5,-0.5}).reshape(4,1);
  INDArray predsGradAssertion=Nd4j.create(new double[]{-2,-2,2,-2}).reshape(4,1);
  INDArray oneMinusPredsGradAssertion=Nd4j.create(new double[]{0,0,-2,0}).reshape(4,1);
  INDArray oneMinusLabelsAssertion=Nd4j.valueArrayOf(4,-1).reshape(4,1);
  INDArray outputTimesYGradAssertion=Nd4j.valueArrayOf(4,-2).reshape(4,1);
  INDArray yHatAssertion=outputTimesYGradAssertion.dup();
  INDArray labelProbsGradAssertion=yHatAssertion.dup();
  INDArray logProbsGradAssertion=Nd4j.valueArrayOf(4,-1).reshape(4,1);
  assertEquals(logProbsGradAssertion,outside.getFunction("activate").grad("logprob").getArr());
  assertEquals(labelProbsGradAssertion,outside.getFunction("activate").grad("probs").getArr());
  assertEquals(yHatAssertion,outside.getFunction("activate").grad("yhat").getArr());
  assertEquals(outputTimesYGradAssertion,outside.getFunction("activate").grad("output * y").getArr());
  assertEquals(oneMinusLabelsAssertion,outside.getFunction("activate").grad("oneminusy").getArr());
  assertEquals(oneMinusPredsGradAssertion,outside.getFunction("activate").grad("oneminusactivations").getArr());
  assertEquals(predsGradAssertion,outside.getFunction("activate").grad("activation").getArr());
  assertEquals(mmulGrad,outside.getFunction("activate").grad("mmul").getArr());
  assertEquals(yGradAssertion,outside.getFunction("activate").grad("y").getArr());
  assertEquals(wGradAssertion,outside.getFunction("activate").grad("w").getArr());
  System.out.println(gradWrtX);
  System.out.println(gradWrtW);
}
