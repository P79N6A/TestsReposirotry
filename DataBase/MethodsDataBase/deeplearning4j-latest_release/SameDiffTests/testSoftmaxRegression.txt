@Test public void testSoftmaxRegression(){
  Map<String,INDArray> vars=this.variablesForInput();
  SameDiff outside=SameDiff.create();
  outside.defineFunction("activate",new SameDiffFunctionDefinition(){
    @Override public SDVariable[] define(    SameDiff sameDiff,    Map<String,INDArray> inputs,    SDVariable[] variableInputs){
      sameDiff.enableDebugMode();
      SDVariable x=sameDiff.var("x",inputs.get("x"));
      SDVariable w=sameDiff.var("w",inputs.get("w"));
      SDVariable y=sameDiff.var("y",inputs.get("y"));
      SDVariable activation=sameDiff.softmax("activation",sameDiff.mmul("mmul",x,w));
      SDVariable ret=sameDiff.sum("totalsum",activation,Integer.MAX_VALUE);
      SDVariable ret2=sameDiff.neg("negtotalsum",ret);
      return new SDVariable[]{y.sub(ret2)};
    }
  }
,vars);
  Pair<Map<SDVariable,DifferentialFunction>,List<DifferentialFunction>> opsBackward=outside.getFunction("activate").execBackwards();
  SameDiff gradSameDiff=outside.getFunction("activate").getFunction("grad");
  SDVariable gradWrtX=outside.getFunction("activate").grad("x");
  SDVariable gradWrtW=outside.getFunction("activate").grad("w");
  assertNotNull(gradWrtX);
  assertNotNull(gradWrtW);
  INDArray wGradAssertion=Nd4j.create(new double[]{0,0,0}).reshape(3,1);
  assertEquals(wGradAssertion,outside.getFunction("activate").grad("w").getArr());
  System.out.println(gradWrtX);
  System.out.println(gradWrtW);
}
