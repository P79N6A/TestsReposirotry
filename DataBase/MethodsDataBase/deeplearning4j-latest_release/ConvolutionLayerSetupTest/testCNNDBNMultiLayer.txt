@Test public void testCNNDBNMultiLayer() throws Exception {
  DataSetIterator iter=new MnistDataSetIterator(2,2);
  DataSet next=iter.next();
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).seed(123).weightInit(WeightInit.XAVIER).list().layer(0,new ConvolutionLayer.Builder(new int[]{1,1},new int[]{1,1}).nIn(1).nOut(6).activation(Activation.IDENTITY).build()).layer(1,new BatchNormalization.Builder().build()).layer(2,new ActivationLayer.Builder().activation(Activation.RELU).build()).layer(3,new DenseLayer.Builder().nIn(28 * 28 * 6).nOut(10).activation(Activation.IDENTITY).build()).layer(4,new BatchNormalization.Builder().nOut(10).build()).layer(5,new ActivationLayer.Builder().activation(Activation.RELU).build()).layer(6,new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nOut(10).build()).setInputType(InputType.convolutionalFlat(28,28,1)).build();
  MultiLayerNetwork network=new MultiLayerNetwork(conf);
  network.init();
  network.setInput(next.getFeatures());
  INDArray activationsActual=network.activate(next.getFeatures());
  assertEquals(10,activationsActual.shape()[1],1e-2);
  network.fit(next);
  INDArray actualGammaParam=network.getLayer(1).getParam(BatchNormalizationParamInitializer.GAMMA);
  INDArray actualBetaParam=network.getLayer(1).getParam(BatchNormalizationParamInitializer.BETA);
  assertTrue(actualGammaParam != null);
  assertTrue(actualBetaParam != null);
}
