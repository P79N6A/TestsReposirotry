@Test public void testEmbeddingSequenceBackwardPass(){
  int nClassesIn=10;
  int embeddingDim=5;
  int nOut=4;
  int inputLength=1;
  MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().activation(Activation.TANH).list().layer(new EmbeddingSequenceLayer.Builder().inputLength(inputLength).hasBias(true).nIn(nClassesIn).nOut(embeddingDim).build()).layer(new RnnOutputLayer.Builder().nIn(embeddingDim).nOut(nOut).activation(Activation.SOFTMAX).build()).build();
  MultiLayerConfiguration conf2=new NeuralNetConfiguration.Builder().activation(Activation.TANH).list().layer(new DenseLayer.Builder().nIn(nClassesIn).nOut(embeddingDim).build()).layer(new OutputLayer.Builder().nIn(embeddingDim).nOut(nOut).activation(Activation.SOFTMAX).build()).inputPreProcessor(0,new RnnToFeedForwardPreProcessor()).build();
  MultiLayerNetwork net=new MultiLayerNetwork(conf);
  MultiLayerNetwork net2=new MultiLayerNetwork(conf2);
  net.init();
  net2.init();
  net2.setParams(net.params().dup());
  int batchSize=3;
  INDArray inEmbedding=Nd4j.create(batchSize,1);
  INDArray inOneHot=Nd4j.create(batchSize,nClassesIn,1);
  INDArray outLabels=Nd4j.create(batchSize,4);
  Random r=new Random(1337);
  for (int i=0; i < batchSize; i++) {
    int classIdx=r.nextInt(nClassesIn);
    inEmbedding.putScalar(i,classIdx);
    inOneHot.putScalar(new int[]{i,classIdx,0},1.0);
    int labelIdx=r.nextInt(4);
    outLabels.putScalar(new int[]{i,labelIdx},1.0);
  }
  net.setInput(inEmbedding);
  net2.setInput(inOneHot);
  net.setLabels(outLabels);
  net2.setLabels(outLabels);
  net.computeGradientAndScore();
  net2.computeGradientAndScore();
  System.out.println(net.score() + "\t" + net2.score());
  assertEquals(net2.score(),net.score(),1e-6);
  Map<String,INDArray> gradient=net.gradient().gradientForVariable();
  Map<String,INDArray> gradient2=net2.gradient().gradientForVariable();
  assertEquals(gradient.size(),gradient2.size());
  for (  String s : gradient.keySet()) {
    assertEquals(gradient2.get(s),gradient.get(s));
  }
}
