@Test public void testFrozenWithBackprop(){
  for (  int minibatch : new int[]{1,5}) {
    MultiLayerConfiguration conf2=new NeuralNetConfiguration.Builder().seed(12345).updater(Updater.NONE).list().layer(new DenseLayer.Builder().nIn(10).nOut(10).activation(Activation.TANH).weightInit(WeightInit.XAVIER).build()).layer(new FrozenLayerWithBackprop(new DenseLayer.Builder().nIn(10).nOut(10).activation(Activation.TANH).weightInit(WeightInit.XAVIER).build())).layer(new FrozenLayerWithBackprop(new DenseLayer.Builder().nIn(10).nOut(10).activation(Activation.TANH).weightInit(WeightInit.XAVIER).build())).layer(new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(10).nOut(10).build()).build();
    MultiLayerNetwork net=new MultiLayerNetwork(conf2);
    net.init();
    INDArray in=Nd4j.rand(minibatch,10);
    INDArray labels=TestUtils.randomOneHot(minibatch,10);
    Set<String> excludeParams=new HashSet<>();
    excludeParams.addAll(Arrays.asList("1_W","1_b","2_W","2_b"));
    boolean gradOK=GradientCheckUtil.checkGradients(net,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,in,labels,null,null,false,-1,excludeParams);
    assertTrue(gradOK);
    TestUtils.testModelSerialization(net);
    ComputationGraph g=net.toComputationGraph();
    boolean gradOKCG=GradientCheckUtil.checkGradients(g,DEFAULT_EPS,DEFAULT_MAX_REL_ERROR,DEFAULT_MIN_ABS_ERROR,PRINT_RESULTS,RETURN_ON_FIRST_FAILURE,new INDArray[]{in},new INDArray[]{labels},null,null,excludeParams);
    assertTrue(gradOKCG);
    TestUtils.testModelSerialization(g);
  }
}
