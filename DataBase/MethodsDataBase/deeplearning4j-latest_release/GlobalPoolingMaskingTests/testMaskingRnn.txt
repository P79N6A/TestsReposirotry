@Test public void testMaskingRnn(){
  int timeSeriesLength=5;
  int nIn=5;
  int layerSize=4;
  int nOut=2;
  int[] minibatchSizes=new int[]{1,3};
  for (  int miniBatchSize : minibatchSizes) {
    MultiLayerConfiguration conf=new NeuralNetConfiguration.Builder().updater(new NoOp()).weightInit(WeightInit.DISTRIBUTION).dist(new NormalDistribution(0,1.0)).seed(12345L).list().layer(0,new GravesLSTM.Builder().nIn(nIn).nOut(layerSize).activation(Activation.TANH).build()).layer(1,new org.deeplearning4j.nn.conf.layers.GlobalPoolingLayer.Builder().poolingType(PoolingType.AVG).build()).layer(2,new OutputLayer.Builder(LossFunctions.LossFunction.MCXENT).activation(Activation.SOFTMAX).nIn(layerSize).nOut(nOut).build()).build();
    MultiLayerNetwork net=new MultiLayerNetwork(conf);
    net.init();
    Random r=new Random(12345L);
    INDArray input=Nd4j.rand(new int[]{miniBatchSize,nIn,timeSeriesLength}).subi(0.5);
    INDArray mask;
    if (miniBatchSize == 1) {
      mask=Nd4j.create(new double[]{1,1,1,1,0});
    }
 else {
      mask=Nd4j.create(new double[][]{{1,1,1,1,1},{1,1,1,1,0},{1,1,1,0,0}});
    }
    INDArray labels=Nd4j.zeros(miniBatchSize,nOut);
    for (int i=0; i < miniBatchSize; i++) {
      int idx=r.nextInt(nOut);
      labels.putScalar(i,idx,1.0);
    }
    net.setLayerMaskArrays(mask,null);
    INDArray outputMasked=net.output(input);
    net.clearLayerMaskArrays();
    for (int i=0; i < miniBatchSize; i++) {
      INDArray maskRow=mask.getRow(i);
      int tsLength=maskRow.sumNumber().intValue();
      INDArray inputSubset=input.get(NDArrayIndex.interval(i,i,true),NDArrayIndex.all(),NDArrayIndex.interval(0,tsLength));
      INDArray outSubset=net.output(inputSubset);
      INDArray outputMaskedSubset=outputMasked.getRow(i);
      assertEquals(outSubset,outputMaskedSubset);
    }
  }
}
