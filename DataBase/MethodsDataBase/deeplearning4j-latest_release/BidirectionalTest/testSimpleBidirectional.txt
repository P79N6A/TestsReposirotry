@Test public void testSimpleBidirectional(){
  for (  WorkspaceMode wsm : WorkspaceMode.values()) {
    log.info("*** Starting workspace mode: " + wsm);
    Nd4j.getRandom().setSeed(12345);
    Bidirectional.Mode[] modes=new Bidirectional.Mode[]{Bidirectional.Mode.CONCAT,Bidirectional.Mode.ADD,Bidirectional.Mode.AVERAGE,Bidirectional.Mode.MUL};
    INDArray in=Nd4j.rand(new int[]{3,10,6});
    for (    Bidirectional.Mode m : modes) {
      MultiLayerConfiguration conf1=new NeuralNetConfiguration.Builder().activation(Activation.TANH).weightInit(WeightInit.XAVIER).trainingWorkspaceMode(wsm).inferenceWorkspaceMode(wsm).updater(new Adam()).list().layer(new Bidirectional(m,new SimpleRnn.Builder().nIn(10).nOut(10).build())).build();
      MultiLayerNetwork net1=new MultiLayerNetwork(conf1);
      net1.init();
      MultiLayerConfiguration conf2=new NeuralNetConfiguration.Builder().activation(Activation.TANH).weightInit(WeightInit.XAVIER).updater(new Adam()).list().layer(new SimpleRnn.Builder().nIn(10).nOut(10).build()).build();
      MultiLayerNetwork net2=new MultiLayerNetwork(conf2.clone());
      net2.init();
      MultiLayerNetwork net3=new MultiLayerNetwork(conf2.clone());
      net3.init();
      net2.setParam("0_W",net1.getParam("0_fW"));
      net2.setParam("0_RW",net1.getParam("0_fRW"));
      net2.setParam("0_b",net1.getParam("0_fb"));
      net3.setParam("0_W",net1.getParam("0_bW"));
      net3.setParam("0_RW",net1.getParam("0_bRW"));
      net3.setParam("0_b",net1.getParam("0_bb"));
      INDArray inReverse=TimeSeriesUtils.reverseTimeSeries(in,LayerWorkspaceMgr.noWorkspaces(),ArrayType.INPUT);
      INDArray out1=net1.output(in);
      INDArray out2=net2.output(in);
      INDArray out3=TimeSeriesUtils.reverseTimeSeries(net3.output(inReverse),LayerWorkspaceMgr.noWorkspaces(),ArrayType.INPUT);
      INDArray outExp;
switch (m) {
case ADD:
        outExp=out2.add(out3);
      break;
case MUL:
    outExp=out2.mul(out3);
  break;
case AVERAGE:
outExp=out2.add(out3).muli(0.5);
break;
case CONCAT:
outExp=Nd4j.concat(1,out2,out3);
break;
default :
throw new RuntimeException();
}
assertEquals(m.toString(),outExp,out1);
if (m == Bidirectional.Mode.ADD || m == Bidirectional.Mode.CONCAT) {
INDArray eps=Nd4j.rand(new int[]{3,10,6});
INDArray eps1;
if (m == Bidirectional.Mode.CONCAT) {
eps1=Nd4j.concat(1,eps,eps);
}
 else {
eps1=eps;
}
net1.setInput(in);
net2.setInput(in);
net3.setInput(TimeSeriesUtils.reverseTimeSeries(in,LayerWorkspaceMgr.noWorkspaces(),ArrayType.INPUT));
net1.feedForward(true,false);
net2.feedForward(true,false);
net3.feedForward(true,false);
Pair<Gradient,INDArray> p1=net1.backpropGradient(eps1,LayerWorkspaceMgr.noWorkspaces());
Pair<Gradient,INDArray> p2=net2.backpropGradient(eps,LayerWorkspaceMgr.noWorkspaces());
Pair<Gradient,INDArray> p3=net3.backpropGradient(TimeSeriesUtils.reverseTimeSeries(eps,LayerWorkspaceMgr.noWorkspaces(),ArrayType.INPUT),LayerWorkspaceMgr.noWorkspaces());
Gradient g1=p1.getFirst();
Gradient g2=p2.getFirst();
Gradient g3=p3.getFirst();
for (boolean updates : new boolean[]{false,true}) {
if (updates) {
net1.getUpdater().update(net1,g1,0,0,3,LayerWorkspaceMgr.noWorkspaces());
net2.getUpdater().update(net2,g2,0,0,3,LayerWorkspaceMgr.noWorkspaces());
net3.getUpdater().update(net3,g3,0,0,3,LayerWorkspaceMgr.noWorkspaces());
}
assertEquals(g2.gradientForVariable().get("0_W"),g1.gradientForVariable().get("0_fW"));
assertEquals(g2.gradientForVariable().get("0_RW"),g1.gradientForVariable().get("0_fRW"));
assertEquals(g2.gradientForVariable().get("0_b"),g1.gradientForVariable().get("0_fb"));
assertEquals(g3.gradientForVariable().get("0_W"),g1.gradientForVariable().get("0_bW"));
assertEquals(g3.gradientForVariable().get("0_RW"),g1.gradientForVariable().get("0_bRW"));
assertEquals(g3.gradientForVariable().get("0_b"),g1.gradientForVariable().get("0_bb"));
}
}
}
}
}
