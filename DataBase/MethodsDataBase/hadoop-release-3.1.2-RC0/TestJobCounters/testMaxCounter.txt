/** 
 * End to end test of maximum counters.
 * @throws IOException test failed
 * @throws ClassNotFoundException test failed
 * @throws InterruptedException test failed
 */
@Test public void testMaxCounter() throws IOException, ClassNotFoundException, InterruptedException {
  MiniMRClientCluster mrCluster=MiniMRClientClusterFactory.create(this.getClass(),2,new Configuration());
  try {
    Path rootDir=new Path(System.getProperty("test.build.data","/tmp"));
    Path testRootDir=new Path(rootDir,"testMaxCounter");
    Path testInputDir=new Path(testRootDir,"input");
    Path testOutputDir=new Path(testRootDir,"output");
    FileSystem fs=FileSystem.getLocal(new Configuration());
    fs.mkdirs(testInputDir);
    Path testInputFile=new Path(testInputDir,"file01");
    FSDataOutputStream stream=fs.create(testInputFile);
    stream.writeChars("foo");
    stream.writeChars("bar");
    stream.close();
    fs.delete(testOutputDir,true);
    Configuration conf=new Configuration();
    conf.setClass(MRConfig.RESOURCE_CALCULATOR_PROCESS_TREE,MockResourceCalculatorProcessTree.class,ResourceCalculatorProcessTree.class);
    Job job=Job.getInstance(conf,"word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    job.setNumReduceTasks(2);
    org.apache.hadoop.mapreduce.lib.input.FileInputFormat.addInputPath(job,testInputDir);
    org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.setOutputPath(job,testOutputDir);
    assertTrue(job.waitForCompletion(true));
    org.apache.hadoop.mapreduce.Counter maxMap=job.getCounters().findCounter(TaskCounter.MAP_PHYSICAL_MEMORY_BYTES_MAX);
    org.apache.hadoop.mapreduce.Counter maxReduce=job.getCounters().findCounter(TaskCounter.REDUCE_PHYSICAL_MEMORY_BYTES_MAX);
    org.apache.hadoop.mapreduce.Counter allP=job.getCounters().findCounter(TaskCounter.PHYSICAL_MEMORY_BYTES);
    assertEquals(1024,maxMap.getValue());
    assertEquals(1024,maxReduce.getValue());
    assertEquals(3072,allP.getValue());
    org.apache.hadoop.mapreduce.Counter maxMapV=job.getCounters().findCounter(TaskCounter.MAP_VIRTUAL_MEMORY_BYTES_MAX);
    org.apache.hadoop.mapreduce.Counter maxReduceV=job.getCounters().findCounter(TaskCounter.REDUCE_VIRTUAL_MEMORY_BYTES_MAX);
    org.apache.hadoop.mapreduce.Counter allV=job.getCounters().findCounter(TaskCounter.VIRTUAL_MEMORY_BYTES);
    assertEquals(2000,maxMapV.getValue());
    assertEquals(2000,maxReduceV.getValue());
    assertEquals(6000,allV.getValue());
    org.apache.hadoop.mapreduce.Counter customerCounter=job.getCounters().findCounter(IntSumReducer.Counters.MY_COUNTER_MAX);
    assertEquals(200,customerCounter.getValue());
    fs.delete(testInputDir,true);
    fs.delete(testOutputDir,true);
  }
  finally {
    mrCluster.stop();
  }
}
