@Test(timeout=60000) public void testMaxBlocksPerFileLimit() throws Exception {
  Configuration conf=new HdfsConfiguration();
  final long blockSize=4096;
  final long numBlocks=2;
  conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,blockSize);
  conf.setLong(DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY,numBlocks);
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).build();
  FileSystem fs=cluster.getFileSystem();
  HdfsDataOutputStream fout=(HdfsDataOutputStream)fs.create(new Path("/testmaxfilelimit"));
  try {
    fout.write(new byte[(int)blockSize * (int)numBlocks]);
    fout.hflush();
    try {
      fout.write(new byte[1]);
      fout.hflush();
      assert false : "Expected IOException after writing too many blocks";
    }
 catch (    IOException e) {
      GenericTestUtils.assertExceptionContains("File has reached the limit" + " on maximum number of",e);
    }
  }
  finally {
    cluster.shutdown();
  }
}
