@Test(timeout=10000) public void testConcurrentAppendRead() throws IOException, TimeoutException, InterruptedException {
  Configuration conf=new HdfsConfiguration();
  conf.setInt(DFSConfigKeys.DFS_BLOCK_SIZE_KEY,1024);
  conf.setInt(DFSConfigKeys.DFS_REPLICATION_KEY,1);
  conf.setInt("dfs.min.replication",1);
  File builderBaseDir=new File(GenericTestUtils.getRandomizedTempPath());
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf,builderBaseDir).build();
  try {
    cluster.waitActive();
    DataNode dn=cluster.getDataNodes().get(0);
    FsDatasetSpi<?> dataSet=DataNodeTestUtils.getFSDataset(dn);
    long initialFileLength=1;
    DistributedFileSystem fs=cluster.getFileSystem();
    Path fileName=new Path("/appendCorruptBlock");
    DFSTestUtil.createFile(fs,fileName,initialFileLength,(short)1,0);
    DFSTestUtil.waitReplication(fs,fileName,(short)1);
    Assert.assertTrue("File not created",fs.exists(fileName));
    ExtendedBlock block=DFSTestUtil.getFirstBlock(fs,fileName);
    long newGS=block.getGenerationStamp() + 1;
    ReplicaHandler replicaHandler=dataSet.append(block,newGS,initialFileLength);
    ReplicaBeingWritten rbw=(ReplicaBeingWritten)replicaHandler.getReplica();
    ReplicaOutputStreams outputStreams=rbw.createStreams(false,DEFAULT_CHECKSUM);
    OutputStream dataOutput=outputStreams.getDataOut();
    byte[] appendBytes=new byte[1];
    dataOutput.write(appendBytes,0,1);
    dataOutput.flush();
    dataOutput.close();
    final int smallBufferSize=DFSUtilClient.getSmallBufferSize(conf);
    FsDatasetUtil.computeChecksum(rbw.getMetaFile(),rbw.getMetaFile(),rbw.getBlockFile(),smallBufferSize,conf);
    final byte[] readBlock=DFSTestUtil.readFileBuffer(fs,fileName);
    assertEquals("should have read only one byte!",1,readBlock.length);
  }
  finally {
    cluster.shutdown();
  }
}
