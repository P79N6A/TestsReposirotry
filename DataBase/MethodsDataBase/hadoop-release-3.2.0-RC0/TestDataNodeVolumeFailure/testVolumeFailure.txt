@Test(timeout=120000) public void testVolumeFailure() throws Exception {
  System.out.println("Data dir: is " + dataDir.getPath());
  String filename="/test.txt";
  Path filePath=new Path(filename);
  int filesize=block_size * blocks_num;
  DFSTestUtil.createFile(fs,filePath,filesize,repl,1L);
  DFSTestUtil.waitReplication(fs,filePath,repl);
  System.out.println("file " + filename + "(size "+ filesize+ ") is created and replicated");
  data_fail=cluster.getInstanceStorageDir(1,0);
  failedDir=MiniDFSCluster.getFinalizedDir(data_fail,cluster.getNamesystem().getBlockPoolId());
  if (failedDir.exists() && !deteteBlocks(failedDir)) {
    throw new IOException("Could not delete hdfs directory '" + failedDir + "'");
  }
  data_fail.setReadOnly();
  failedDir.setReadOnly();
  System.out.println("Deleteing " + failedDir.getPath() + "; exist="+ failedDir.exists());
  triggerFailure(filename,filesize);
  final DataNode dn=cluster.getDataNodes().get(1);
  GenericTestUtils.waitFor(new Supplier<Boolean>(){
    @Override public Boolean get(){
      final VolumeFailureSummary summary=dn.getFSDataset().getVolumeFailureSummary();
      return summary != null && summary.getFailedStorageLocations() != null && summary.getFailedStorageLocations().length == 1;
    }
  }
,10,30 * 1000);
  DataNodeTestUtils.triggerHeartbeat(dn);
  final BlockManager bm=cluster.getNamesystem().getBlockManager();
  BlockManagerTestUtil.checkHeartbeat(bm);
  assertEquals(1,cluster.getNamesystem().getVolumeFailuresTotal());
  verify(filename,filesize);
  System.out.println("creating file test1.txt");
  Path fileName1=new Path("/test1.txt");
  DFSTestUtil.createFile(fs,fileName1,filesize,repl,1L);
  DFSTestUtil.waitReplication(fs,fileName1,repl);
  System.out.println("file " + fileName1.getName() + " is created and replicated");
}
