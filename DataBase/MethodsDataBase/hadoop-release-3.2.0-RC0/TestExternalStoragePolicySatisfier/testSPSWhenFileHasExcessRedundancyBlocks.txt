/** 
 * Test SPS for extra redundant file blocks. 1. Create cluster with 5 datanode. 2. Create one file with 5 replica. 3. Set file replication to 3. 4. Set policy and call satisfyStoragePolicy for file. 5. Block should be moved successfully.
 */
@Test(timeout=300000) public void testSPSWhenFileHasExcessRedundancyBlocks() throws Exception {
  try {
    config.set(DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_RECHECK_TIMEOUT_MILLIS_KEY,"3000");
    config.set(DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_SELF_RETRY_TIMEOUT_MILLIS_KEY,"5000");
    StorageType[][] newtypes=new StorageType[][]{{StorageType.ARCHIVE,StorageType.DISK},{StorageType.ARCHIVE,StorageType.DISK},{StorageType.ARCHIVE,StorageType.DISK},{StorageType.ARCHIVE,StorageType.DISK},{StorageType.ARCHIVE,StorageType.DISK}};
    hdfsCluster=startCluster(config,newtypes,5,2,CAPACITY);
    hdfsCluster.waitActive();
    DistributedFileSystem fs=hdfsCluster.getFileSystem();
    Path filePath=new Path("/zeroSizeFile");
    DFSTestUtil.createFile(fs,filePath,1024,(short)5,0);
    fs.setReplication(filePath,(short)3);
    LogCapturer logs=GenericTestUtils.LogCapturer.captureLogs(LoggerFactory.getLogger(BlockStorageMovementAttemptedItems.class));
    fs.setStoragePolicy(filePath,"COLD");
    fs.satisfyStoragePolicy(filePath);
    DFSTestUtil.waitExpectedStorageType(filePath.toString(),StorageType.ARCHIVE,3,60000,hdfsCluster.getFileSystem());
    assertFalse("Log output does not contain expected log message: ",logs.getOutput().contains("some of the blocks are low redundant"));
  }
  finally {
    shutdownCluster();
  }
}
