/** 
 * Test SPS for low redundant file blocks. 1. Create cluster with 3 datanode. 1. Create one file with 3 replica. 2. Set policy and call satisfyStoragePolicy for file. 3. Stop NameNode and Datanodes. 4. Start NameNode with 2 datanode and wait for block movement. 5. Start third datanode. 6. Third Datanode replica also should be moved in proper sorage based on policy.
 */
@Test(timeout=300000) public void testSPSWhenFileHasLowRedundancyBlocks() throws Exception {
  try {
    config.set(DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_RECHECK_TIMEOUT_MILLIS_KEY,"3000");
    config.set(DFSConfigKeys.DFS_STORAGE_POLICY_SATISFIER_SELF_RETRY_TIMEOUT_MILLIS_KEY,"5000");
    StorageType[][] newtypes=new StorageType[][]{{StorageType.ARCHIVE,StorageType.DISK},{StorageType.ARCHIVE,StorageType.DISK},{StorageType.ARCHIVE,StorageType.DISK}};
    hdfsCluster=startCluster(config,newtypes,3,2,CAPACITY);
    hdfsCluster.waitActive();
    DistributedFileSystem fs=hdfsCluster.getFileSystem();
    Path filePath=new Path("/zeroSizeFile");
    DFSTestUtil.createFile(fs,filePath,1024,(short)3,0);
    fs.setStoragePolicy(filePath,"COLD");
    List<DataNodeProperties> list=new ArrayList<>();
    list.add(hdfsCluster.stopDataNode(0));
    list.add(hdfsCluster.stopDataNode(0));
    list.add(hdfsCluster.stopDataNode(0));
    restartNamenode();
    hdfsCluster.restartDataNode(list.get(0),false);
    hdfsCluster.restartDataNode(list.get(1),false);
    hdfsCluster.waitActive();
    fs.satisfyStoragePolicy(filePath);
    DFSTestUtil.waitExpectedStorageType(filePath.toString(),StorageType.ARCHIVE,2,30000,hdfsCluster.getFileSystem());
    hdfsCluster.restartDataNode(list.get(2),false);
    DFSTestUtil.waitExpectedStorageType(filePath.toString(),StorageType.ARCHIVE,3,30000,hdfsCluster.getFileSystem());
  }
  finally {
    shutdownCluster();
  }
}
