@Test(timeout=30000) public void testHedgedReadFromAllDNFailed() throws IOException {
  Configuration conf=new Configuration();
  int numHedgedReadPoolThreads=5;
  final int hedgedReadTimeoutMillis=50;
  conf.setInt(HdfsClientConfigKeys.HedgedRead.THREADPOOL_SIZE_KEY,numHedgedReadPoolThreads);
  conf.setLong(HdfsClientConfigKeys.HedgedRead.THRESHOLD_MILLIS_KEY,hedgedReadTimeoutMillis);
  conf.setInt(HdfsClientConfigKeys.Retry.WINDOW_BASE_KEY,0);
  DFSClientFaultInjector.set(Mockito.mock(DFSClientFaultInjector.class));
  DFSClientFaultInjector injector=DFSClientFaultInjector.get();
  Mockito.doAnswer(new Answer<Void>(){
    @Override public Void answer(    InvocationOnMock invocation) throws Throwable {
      if (true) {
        LOG.info("-------------- throw Checksum Exception");
        throw new ChecksumException("ChecksumException test",100);
      }
      return null;
    }
  }
).when(injector).fetchFromDatanodeException();
  MiniDFSCluster cluster=new MiniDFSCluster.Builder(conf).numDataNodes(3).format(true).build();
  DistributedFileSystem fileSys=cluster.getFileSystem();
  DFSClient dfsClient=fileSys.getClient();
  FSDataOutputStream output=null;
  DFSInputStream input=null;
  String filename="/hedgedReadMaxOut.dat";
  DFSHedgedReadMetrics metrics=dfsClient.getHedgedReadMetrics();
  metrics.hedgedReadOps.set(0);
  try {
    Path file=new Path(filename);
    output=fileSys.create(file,(short)2);
    byte[] data=new byte[64 * 1024];
    output.write(data);
    output.flush();
    output.close();
    byte[] buffer=new byte[64 * 1024];
    input=dfsClient.open(filename);
    input.read(0,buffer,0,1024);
    Assert.fail("Reading the block should have thrown BlockMissingException");
  }
 catch (  BlockMissingException e) {
    assertEquals(3,input.getHedgedReadOpsLoopNumForTesting());
    assertTrue(metrics.getHedgedReadOps() == 0);
  }
 finally {
    Mockito.reset(injector);
    IOUtils.cleanupWithLogger(LOG,input);
    IOUtils.cleanupWithLogger(LOG,output);
    fileSys.close();
    cluster.shutdown();
  }
}
