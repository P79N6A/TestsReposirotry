@Test public void testDeleteRecords() throws Exception {
  HashMap<Integer,Node> nodes=new HashMap<>();
  nodes.put(0,new Node(0,"localhost",8121));
  List<PartitionInfo> partitionInfos=new ArrayList<>();
  partitionInfos.add(new PartitionInfo("my_topic",0,nodes.get(0),new Node[]{nodes.get(0)},new Node[]{nodes.get(0)}));
  partitionInfos.add(new PartitionInfo("my_topic",1,nodes.get(0),new Node[]{nodes.get(0)},new Node[]{nodes.get(0)}));
  partitionInfos.add(new PartitionInfo("my_topic",2,null,new Node[]{nodes.get(0)},new Node[]{nodes.get(0)}));
  partitionInfos.add(new PartitionInfo("my_topic",3,nodes.get(0),new Node[]{nodes.get(0)},new Node[]{nodes.get(0)}));
  partitionInfos.add(new PartitionInfo("my_topic",4,nodes.get(0),new Node[]{nodes.get(0)},new Node[]{nodes.get(0)}));
  Cluster cluster=new Cluster("mockClusterId",nodes.values(),partitionInfos,Collections.<String>emptySet(),Collections.<String>emptySet(),nodes.get(0));
  TopicPartition myTopicPartition0=new TopicPartition("my_topic",0);
  TopicPartition myTopicPartition1=new TopicPartition("my_topic",1);
  TopicPartition myTopicPartition2=new TopicPartition("my_topic",2);
  TopicPartition myTopicPartition3=new TopicPartition("my_topic",3);
  TopicPartition myTopicPartition4=new TopicPartition("my_topic",4);
  try (AdminClientUnitTestEnv env=new AdminClientUnitTestEnv(cluster)){
    env.kafkaClient().setNodeApiVersions(NodeApiVersions.create());
    env.kafkaClient().setNode(env.cluster().nodes().get(0));
    Map<TopicPartition,DeleteRecordsResponse.PartitionResponse> m=new HashMap<>();
    m.put(myTopicPartition0,new DeleteRecordsResponse.PartitionResponse(3,Errors.NONE));
    m.put(myTopicPartition1,new DeleteRecordsResponse.PartitionResponse(DeleteRecordsResponse.INVALID_LOW_WATERMARK,Errors.OFFSET_OUT_OF_RANGE));
    m.put(myTopicPartition3,new DeleteRecordsResponse.PartitionResponse(DeleteRecordsResponse.INVALID_LOW_WATERMARK,Errors.NOT_LEADER_FOR_PARTITION));
    m.put(myTopicPartition4,new DeleteRecordsResponse.PartitionResponse(DeleteRecordsResponse.INVALID_LOW_WATERMARK,Errors.UNKNOWN_TOPIC_OR_PARTITION));
    List<MetadataResponse.TopicMetadata> t=new ArrayList<>();
    List<MetadataResponse.PartitionMetadata> p=new ArrayList<>();
    p.add(new MetadataResponse.PartitionMetadata(Errors.NONE,0,nodes.get(0),Optional.of(5),singletonList(nodes.get(0)),singletonList(nodes.get(0)),Collections.emptyList()));
    p.add(new MetadataResponse.PartitionMetadata(Errors.NONE,1,nodes.get(0),Optional.of(5),singletonList(nodes.get(0)),singletonList(nodes.get(0)),Collections.emptyList()));
    p.add(new MetadataResponse.PartitionMetadata(Errors.LEADER_NOT_AVAILABLE,2,null,Optional.empty(),singletonList(nodes.get(0)),singletonList(nodes.get(0)),Collections.emptyList()));
    p.add(new MetadataResponse.PartitionMetadata(Errors.NONE,3,nodes.get(0),Optional.of(5),singletonList(nodes.get(0)),singletonList(nodes.get(0)),Collections.emptyList()));
    p.add(new MetadataResponse.PartitionMetadata(Errors.NONE,4,nodes.get(0),Optional.of(5),singletonList(nodes.get(0)),singletonList(nodes.get(0)),Collections.emptyList()));
    t.add(new MetadataResponse.TopicMetadata(Errors.NONE,"my_topic",false,p));
    env.kafkaClient().prepareResponse(new MetadataResponse(cluster.nodes(),cluster.clusterResource().clusterId(),cluster.controller().id(),t));
    env.kafkaClient().prepareResponse(new DeleteRecordsResponse(0,m));
    Map<TopicPartition,RecordsToDelete> recordsToDelete=new HashMap<>();
    recordsToDelete.put(myTopicPartition0,RecordsToDelete.beforeOffset(3L));
    recordsToDelete.put(myTopicPartition1,RecordsToDelete.beforeOffset(10L));
    recordsToDelete.put(myTopicPartition2,RecordsToDelete.beforeOffset(10L));
    recordsToDelete.put(myTopicPartition3,RecordsToDelete.beforeOffset(10L));
    recordsToDelete.put(myTopicPartition4,RecordsToDelete.beforeOffset(10L));
    DeleteRecordsResult results=env.adminClient().deleteRecords(recordsToDelete);
    Map<TopicPartition,KafkaFuture<DeletedRecords>> values=results.lowWatermarks();
    KafkaFuture<DeletedRecords> myTopicPartition0Result=values.get(myTopicPartition0);
    long lowWatermark=myTopicPartition0Result.get().lowWatermark();
    assertEquals(lowWatermark,3);
    KafkaFuture<DeletedRecords> myTopicPartition1Result=values.get(myTopicPartition1);
    try {
      myTopicPartition1Result.get();
      fail("get() should throw ExecutionException");
    }
 catch (    ExecutionException e0) {
      assertTrue(e0.getCause() instanceof OffsetOutOfRangeException);
    }
    KafkaFuture<DeletedRecords> myTopicPartition2Result=values.get(myTopicPartition2);
    try {
      myTopicPartition2Result.get();
      fail("get() should throw ExecutionException");
    }
 catch (    ExecutionException e1) {
      assertTrue(e1.getCause() instanceof LeaderNotAvailableException);
    }
    KafkaFuture<DeletedRecords> myTopicPartition3Result=values.get(myTopicPartition3);
    try {
      myTopicPartition3Result.get();
      fail("get() should throw ExecutionException");
    }
 catch (    ExecutionException e1) {
      assertTrue(e1.getCause() instanceof NotLeaderForPartitionException);
    }
    KafkaFuture<DeletedRecords> myTopicPartition4Result=values.get(myTopicPartition4);
    try {
      myTopicPartition4Result.get();
      fail("get() should throw ExecutionException");
    }
 catch (    ExecutionException e1) {
      assertTrue(e1.getCause() instanceof UnknownTopicOrPartitionException);
    }
  }
 }
