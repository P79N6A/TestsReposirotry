@Test public void shouldProcessDataFromStoresWithLoggingDisabled() throws InterruptedException, ExecutionException {
  IntegrationTestUtils.produceKeyValuesSynchronously(INPUT_STREAM_2,Arrays.asList(KeyValue.pair(1,1),KeyValue.pair(2,2),KeyValue.pair(3,3)),TestUtils.producerConfig(CLUSTER.bootstrapServers(),IntegerSerializer.class,IntegerSerializer.class),CLUSTER.time);
  final KeyValueBytesStoreSupplier lruMapSupplier=Stores.lruMap(INPUT_STREAM_2,10);
  final StoreBuilder<KeyValueStore<Integer,Integer>> storeBuilder=new KeyValueStoreBuilder<>(lruMapSupplier,Serdes.Integer(),Serdes.Integer(),CLUSTER.time).withLoggingDisabled();
  final StreamsBuilder streamsBuilder=new StreamsBuilder();
  streamsBuilder.addStateStore(storeBuilder);
  final KStream<Integer,Integer> stream=streamsBuilder.stream(INPUT_STREAM_2);
  final CountDownLatch processorLatch=new CountDownLatch(3);
  stream.process(new ProcessorSupplier<Integer,Integer>(){
    @Override public Processor<Integer,Integer> get(){
      return new KeyValueStoreProcessor(INPUT_STREAM_2,processorLatch);
    }
  }
,INPUT_STREAM_2);
  final Topology topology=streamsBuilder.build();
  kafkaStreams=new KafkaStreams(topology,props(APPID + "-logging-disabled"));
  final CountDownLatch latch=new CountDownLatch(1);
  kafkaStreams.setStateListener(new KafkaStreams.StateListener(){
    @Override public void onChange(    final KafkaStreams.State newState,    final KafkaStreams.State oldState){
      if (newState == KafkaStreams.State.RUNNING && oldState == KafkaStreams.State.REBALANCING) {
        latch.countDown();
      }
    }
  }
);
  kafkaStreams.start();
  latch.await(30,TimeUnit.SECONDS);
  assertTrue(processorLatch.await(30,TimeUnit.SECONDS));
}
