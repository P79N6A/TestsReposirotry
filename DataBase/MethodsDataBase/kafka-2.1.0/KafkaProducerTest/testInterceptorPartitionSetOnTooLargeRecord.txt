@Test public void testInterceptorPartitionSetOnTooLargeRecord(){
  Properties props=new Properties();
  props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,"localhost:9999");
  props.setProperty(ProducerConfig.MAX_REQUEST_SIZE_CONFIG,"1");
  ProducerConfig config=new ProducerConfig(ProducerConfig.addSerializerToConfig(props,new StringSerializer(),new StringSerializer()));
  String topic="topic";
  ProducerRecord<String,String> record=new ProducerRecord<>(topic,"value");
  Metadata metadata=new Metadata(0,90000,true);
  final Cluster cluster=new Cluster("dummy",Collections.singletonList(new Node(0,"host1",1000)),Collections.singletonList(new PartitionInfo(topic,0,null,new Node[0],new Node[0])),Collections.emptySet(),Collections.emptySet());
  metadata.update(cluster,Collections.emptySet(),Time.SYSTEM.milliseconds());
  @SuppressWarnings("unchecked") ProducerInterceptors<String,String> interceptors=mock(ProducerInterceptors.class);
  KafkaProducer<String,String> producer=new KafkaProducer<>(config,null,null,metadata,null,interceptors,Time.SYSTEM);
  when(interceptors.onSend(any())).then(invocation -> invocation.getArgument(0));
  producer.send(record);
  verify(interceptors).onSend(record);
  verify(interceptors).onSendError(eq(record),notNull(),notNull());
  producer.close(0,TimeUnit.MILLISECONDS);
}
